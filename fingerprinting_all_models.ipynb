{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading modules\n",
    "\n",
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import Features\n",
    "from rdkit.Chem import Fragments as fr\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:08:10] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DiskDataset X.shape: (6264, 1024), y.shape: (6264, 12), w.shape: (6264, 12), task_names: ['NR-AR' 'NR-AR-LBD' 'NR-AhR' ... 'SR-HSE' 'SR-MMP' 'SR-p53']>\n"
     ]
    }
   ],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='ECFP', splitter='random', reload=False)\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataset.to_dataframe()\n",
    "test_df = test_dataset.to_dataframe()\n",
    "valid_df = valid_dataset.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.filter(like='X', axis=1)\n",
    "X_test = test_df.filter(like='X', axis=1)\n",
    "X_valid = valid_df.filter(like='X', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df = train_df[['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7']]\n",
    "y_test_df = test_df[['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7']]\n",
    "y_valid_df = valid_df[['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_df = np.where(y_train_df.sum(axis=1) > 0, 1, 0)\n",
    "y_test_df = np.where(y_test_df.sum(axis=1) > 0, 1, 0)\n",
    "y_valid_df = np.where(y_valid_df.sum(axis=1) > 0, 1, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train\n",
    "train['tox'] = y_train_df\n",
    "\n",
    "test = X_test\n",
    "test['tox'] = y_test_df\n",
    "\n",
    "valid = X_valid\n",
    "valid['tox'] = y_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = y_test_df\n",
    "y_valid = y_valid_df\n",
    "\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "percent_toxic = 100*sum(train['tox'])/len(train)\n",
    "percent_non_toxic = 100 - percent_toxic\n",
    "\n",
    "df_majority = train[train['tox']==0]\n",
    "df_minority = train[train['tox']==1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4757 1507\n"
     ]
    }
   ],
   "source": [
    "print(len(df_majority), len(df_minority))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X1016</th>\n",
       "      <th>X1017</th>\n",
       "      <th>X1018</th>\n",
       "      <th>X1019</th>\n",
       "      <th>X1020</th>\n",
       "      <th>X1021</th>\n",
       "      <th>X1022</th>\n",
       "      <th>X1023</th>\n",
       "      <th>X1024</th>\n",
       "      <th>tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5975</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9514 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1   X2   X3   X4   X5   X6   X7   X8   X9  X10  ...  X1016  X1017  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...   \n",
       "3742  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "294   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "5975  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "3668  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "5841  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   \n",
       "\n",
       "      X1018  X1019  X1020  X1021  X1022  X1023  X1024  tox  \n",
       "0       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0  \n",
       "1       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0  \n",
       "2       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0  \n",
       "3       0.0    0.0    1.0    0.0    0.0    0.0    0.0    0  \n",
       "4       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0  \n",
       "...     ...    ...    ...    ...    ...    ...    ...  ...  \n",
       "3742    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1  \n",
       "294     0.0    0.0    0.0    0.0    0.0    0.0    0.0    1  \n",
       "5975    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1  \n",
       "3668    0.0    0.0    1.0    0.0    0.0    0.0    0.0    1  \n",
       "5841    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1  \n",
       "\n",
       "[9514 rows x 1025 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_minority_oversampled = resample(df_minority, \n",
    "                                 replace=True, \n",
    "                                 n_samples= len(df_majority))\n",
    "\n",
    "df_oversampled = pd.concat([df_majority, df_minority_oversampled])\n",
    "\n",
    "df_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x27e101b59c8>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAG9CAYAAADz17cDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAv5ElEQVR4nO3de1iUdeL//9dwkIMCmgVIopJh5SlNDVNLWw9lueXHWjXtYJhhZEW2mYQlloJaKSVp6ZayuiT5Kz9Zu6nYtmhiv/CA59UyUkyIbBVQFIS5v3/4cdYJLQ4zznD7fFzXXFfznnve88K9XF6+7/c9t8UwDEMAAAAm5eHqAAAAAM5E2QEAAKZG2QEAAKZG2QEAAKZG2QEAAKZG2QEAAKZG2QEAAKbm5eoA7sBqterIkSMKCAiQxWJxdRwAAFADhmGotLRUYWFh8vC4+PoNZUfSkSNHFB4e7uoYAACgDvLz89WyZcuLvk7ZkRQQECDp7B9WYGCgi9MAAICaKCkpUXh4uO33+MVQdiTbqavAwEDKDgAADczvbUFhgzIAADA1yg4AADA1yg4AADA19uwAAEzLarWqoqLC1TFQR97e3vL09Kz3PJQdAIApVVRUKC8vT1ar1dVRUA9NmzZVaGhovb4Hj7IDADAdwzBUUFAgT09PhYeH/+YXzsE9GYahsrIyFRUVSZJatGhR57koOwAA06msrFRZWZnCwsLk7+/v6jioIz8/P0lSUVGRgoOD63xKi6oLADCdqqoqSVKjRo1cnAT1da6snjlzps5zUHYAAKbF/Q4bPkf8b0jZAQAApkbZAQAApubSDcrr16/Xa6+9pi1btqigoEArV67U0KFDba8bhqFp06Zp4cKFOnbsmKKiovT222+rQ4cOtmPKy8v15z//WR988IFOnTql/v37a/78+b9591MAwOWpzeS/X9LP+2Hm3Zf08y7mhx9+UEREhLZt26YuXbq4Os4l59KVnZMnT+rGG29UamrqBV+fPXu25syZo9TUVOXk5Cg0NFQDBw5UaWmp7Zi4uDitXLlSy5cv11dffaUTJ05oyJAhts1pAAA0BBaL5TcfY8aMqfPc4eHhKigoUMeOHR0XuAFx6crO4MGDNXjw4Au+ZhiGUlJSlJCQoGHDhkmS0tLSFBISovT0dMXExKi4uFjvvfeeli5dqgEDBkiSli1bpvDwcK1bt0533HHHJftZAACoj4KCAtt/Z2Rk6OWXX9a+fftsY+cuw64LT09PhYaG1itfQ+a2e3by8vJUWFioQYMG2cZ8fHzUt29fZWdnS5K2bNmiM2fO2B0TFhamjh072o65kPLycpWUlNg9AABwpdDQUNsjKChIFovFbiw9PV1t27ZVo0aNdN1112np0qW290ZHR6tz584qLy+XdPYy7W7dumn06NGSzp7Gslgsys3Ntb1n9+7duvvuuxUYGKiAgADdeuutOnDgwCX9mS8Vt/1SwcLCQklSSEiI3XhISIgOHjxoO6ZRo0Zq1qxZtWPOvf9CkpOTNW3aNAcnvojEoEvzOQCA/2oSLvV+Qyo6JXm58PLzI9vq9r5jByWjyvb+lZ//U888M1kpiX/WgFuj9Nm6DXr00UfV0veUbu/dQ2/Fj9WNAzM1eUK05k77s15KektHfzqi+X+be3aOn46cnbfo39IRQz8WFOm2ASPUr1c3/TNjgQKbNNbGzbmq/HGH5OeEBYCwro6fsxbctuyc8+vr6w3D+N1r7n/vmPj4eE2cONH2vKSkROHh4fULCgCAk7z+zlKNGf5HxY4ZLkma2La1vt66U6+/s1S39+6hJo39teytV9X3/nEKaNJYb7y7TF9kLFBQYMAF53t7SYaCApto+fxkeXt7S5LatW19yX6eS81tT2OdO7f46xWaoqIi22pPaGioKioqdOzYsYsecyE+Pj4KDAy0ewAA4K72fpen3t272I317nGj9n6XZ3t+S/cb9eeYh/RqyiI9F/OgbuvZ7aLz5e7Zr1tv7morOmbntmUnIiJCoaGhyszMtI1VVFQoKytLvXr1kiR169ZN3t7edscUFBRo165dtmMAADCD3zvTYbVatXHzdnl6eurbvEO/OZefr49TMrorl5adEydOKDc317ZhKi8vT7m5uTp06JAsFovi4uKUlJSklStXateuXRozZoz8/f01atQoSVJQUJDGjh2r5557Tl988YW2bdumBx98UJ06dbJdnQUAQEN3w7UR+irHfv9P9uYduuHaNrbnry34q/Z+m6esjxZpzb82aXHGJxedr/MNkdrwzbZ63W+qIXHpnp3Nmzfr9ttvtz0/t4/mkUce0ZIlSzRp0iSdOnVKsbGxti8VXLt2rQIC/nsOcu7cufLy8tLw4cNtXyq4ZMmSOt8ZFQAAd/P8Ew9r+PgXdFPHG9S/z836NHO9Pv78n1q3fIEkKXfXPr38+gL9f+/OVu8eXfTmK8/rmZdfV9+e3XRN6+pfsjthzAjNez9DI2PjFT8hWkEBTfT11h26uUtHXXdegTILi2EYhqtDuFpJSYmCgoJUXFzs+P07XI0FAJfc6Sbhyuv9hiKuvkq+rrwaq46WZKxSXOLrOr53vW1sQdoKvf7uUuUfKVRE+NWa8sxYPXT/EJ0+Xa5ug0erT48uenf2FNvxwx57Tj/9/B+t//gvyj/ykyJ6DtG2NR+oS8frJEk79uzX89NT9NU3ufL09FSXDu20ZO60C5ajeqvH1VinT59WXl6eIiIi5Ovra/daTX9/U3ZE2QEAs2noZcd0XFx23HaDMgAAgCNQdgAAgKlRdgAAgKlRdgAAgKlRdgAAgKlRdgAAgKlRdgAAgKlRdgAAgKlRdgAAgNO0ibpbKSkpLs3g0ntjAQBwSS3sd2k/7/F/1erwMXFTlbbiUyXHP6XJEx61jf/v6i/1P2Ofk/HjVgcHlPrdP05Zm7Zc9PXWLVvoh///73WeP+cfy9S4bc86v98RKDsAALgRX18fzZq/RDEP3qdmTR18C6ML+HjR66r4v7uf5x/5STff/ZDWLV+gDte1laR631j7qubNJH//euesD05jAQDgRgb0uVmhVzVXcur7Fz3mo79/oQ633y+fiCi1ibpbb7yz1O71NlF3K+mt9xQ9MVEB7fqoVY+7tHDZRxec64pmQQoNvlKhwVeeLSaSmjdrahvbs/973Xz3Q/KJiFKLroM0OektVVZWSpL+uuIzNYnsrW+/P2Sb76kps9Suz1CdLDtly3L+aazjx4/r8ccfV0hIiHx9fdWxY0d99tlndfmjqjHKDgAAbsTT01NJkydo3uIMHT7yU7XXt+zYo+HjX9DIe+7QznUfKnFijF56bYGWZKyyO+6Nd5epe+f22rYmXbGP/ElPxCfr39/l1SrLjwVFuuuhp9TjxvbanrlcC5Lj9d4H/6vpb/5FkvTwn4borj/00einElRZWanVX27Uu8s+0t9SZ6ixv1+1+axWqwYPHqzs7GwtW7ZMe/bs0cyZM+u9evR7OI0FAICb+Z/Bf1CX9u009Y139N4bU+1em7Pwb+rf52a99Ow4SVK7tq2159vv9do7f9WYEffYjrvrD70VO2a4JOmFJ8do7qK/6V/ZW3T9tRE1zjE/7UOFh4UqdcZkWSwWXX9thI4U/qwXkt7Sy88+Lg8PD707K0GdB4zQ0y+9po8//6emPvu4enTpcMH51q1bp2+++UZ79+5Vu3btJEnXXHNNrf5s6oKVHQAA3NCshKeVtuIz7dn/vd343m/z1LvHjXZjvXt00bd5h1RVVWUb69w+0vbfFotFoVc1V9Ev/5EkDX5wgppE9laTyN7qcPv9F82w97s83dKtkywWi91nnThZpsMFZ1edmjUN1HtvvKwFf12htq1b2m2s/rXc3Fy1bNnSVnQuFVZ2AABwQ7f17KY7+t6iF2emaszwP9rGDcOwKx/nxn7N28v+V7zFYpHVapUk/eW1l3Xq9Omzx3lfvAoYhi76WRb9d3z911vl6empIz/9rJNlpxQY0OSC8/n5VT+1dSmwsgMAgJua+eJT+jRzvbI3b7eNtW93jb76JtfuuOzN29XumtY13vtydYtgXRvRStdGtFLrlmEXPa59ZISyN++wK1PZm7croEljXd0i+OzznO2avSBNny5JUWCTxnpqyuyLzte5c2cdPnxY+/fvr1FOR6HsAADgpjrdEKnR/zNY8xZn2Maei3lQX3z1jV6du0j7DxxU2oefKnXxh/pzzEMO//zYR4Yr/0ihnpoyS//+Lk+frPmXpr7xjiY+PloeHh4qPXFSDz3zkp56dKQG/6G30t9O0oefZWrFp5kXnK9v37667bbbdN999ykzM1N5eXn6/PPPtXr1aodnPx9lBwAAN/bqpCfsVlZu6nSDPnxnlpavWqOO/f+kl19foFeeH2+3OdlRrm4RrH8snadvcnfrxoEjNX5yksY+MFRTnnlMkvTMy6+psb+vkiZPkCR1uK6tZr34tMZPTtKPBUUXnPOjjz5Sjx499MADD6h9+/aaNGmS3V4jZ7AYFzrRd5kpKSlRUFCQiouLFRjo4C9wSgxy7HwAgN91ukm48nq/oYirr5Kvl+X33wDnCuta57eePn1aeXl5ioiIkK+vr91rNf39zcoOAAAwNcoOAAAwNcoOAAAwNcoOAAAwNcoOAMB8/u/aGy7BafgccR0VZQcAYDqeZ05I1kpVWF2dBPVVVlYmSfL29q7zHNwuAgBgOl4VxfL/OVc/N24m72a+8uDqc9f6v1tT1IZhGCorK1NRUZGaNm1arzujU3YAAKZjkaEW/35feYEROnjqCkm0HZc6mVfntzZt2lShoaH1+njKDgDAlBqdPqrIDU+pwi9Y8qj7qgAcYMLmOr3N29u7Xis651B2AACm5WFUyrfsiKtj4FfffHypsUEZAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYGmUHAACYmluXncrKSk2ZMkURERHy8/PTNddco1deeUVWq9V2jGEYSkxMVFhYmPz8/NSvXz/t3r3bhakBAIA7ceuyM2vWLL3zzjtKTU3V3r17NXv2bL322muaN2+e7ZjZs2drzpw5Sk1NVU5OjkJDQzVw4ECVlpa6MDkAAHAXbl12Nm3apHvvvVd333232rRpo/vvv1+DBg3S5s2bJZ1d1UlJSVFCQoKGDRumjh07Ki0tTWVlZUpPT3dxegAA4A7cuuz06dNHX3zxhfbv3y9J2r59u7766ivdddddkqS8vDwVFhZq0KBBtvf4+Piob9++ys7Ovui85eXlKikpsXsAAABz8nJ1gN/ywgsvqLi4WNdff708PT1VVVWlGTNm6IEHHpAkFRYWSpJCQkLs3hcSEqKDBw9edN7k5GRNmzbNecEBAIDbcOuVnYyMDC1btkzp6enaunWr0tLS9PrrrystLc3uOIvFYvfcMIxqY+eLj49XcXGx7ZGfn++U/AAAwPXcemXn+eef1+TJkzVy5EhJUqdOnXTw4EElJyfrkUceUWhoqKSzKzwtWrSwva+oqKjaas/5fHx85OPj49zwAADALbj1yk5ZWZk8POwjenp62i49j4iIUGhoqDIzM22vV1RUKCsrS7169bqkWQEAgHty65WdP/7xj5oxY4ZatWqlDh06aNu2bZozZ46io6MlnT19FRcXp6SkJEVGRioyMlJJSUny9/fXqFGjXJweAAC4A7cuO/PmzdNLL72k2NhYFRUVKSwsTDExMXr55Zdtx0yaNEmnTp1SbGysjh07pqioKK1du1YBAQEuTA4AANyFxTAMw9UhXK2kpERBQUEqLi5WYGCgYydPDHLsfAAANDSJxU6Ztqa/v916zw4AAEB9UXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpUXYAAICpedXnzbt27VJWVpaqqqrUq1cvde/e3VG5AAAAHKLOKztvv/22+vfvr6ysLH355Zfq37+/ZsyY4chsAAAA9VbjlZ3Dhw+rZcuWtuepqanavXu3rrzySknSpk2bdM899yghIcHxKQEAAOqoxis7/fv315tvvinDMCRJzZs315o1a1ReXq7S0lKtW7dOV111ldOCAgAA1EWNy05OTo7+/e9/KyoqStu2bdPChQs1Z84c+fn5qWnTpsrIyFBaWpozswIAANRajU9jBQYGasGCBdq4caPGjBmjAQMGaMOGDaqqqlJVVZWaNm3qxJgAAAB1U+sNyr1799bmzZsVFBSkrl27av369RQdAADgtmq8slNZWalFixZpz549uvHGG5WQkKCRI0cqJiZGS5Ys0bx58xQaGurMrAAAALVW45WdcePGad68eWrcuLEWL16sZ599Vu3atdOXX36pO+64Q7fccosWLFjgzKwAAAC1ZjHOXV71O5o1a6bs7GzdcMMNOnXqlDp27KgDBw7YXi8qKlJcXJzS09OdFtZZSkpKFBQUpOLiYgUGBjp28sQgx84HAEBDk1jslGlr+vu7xis7wcHBWrt2rSoqKvTFF1+oefPm1V5viEUHAACYW43LTmpqqpKSkuTn56fx48crJSXFibH+68cff9SDDz6o5s2by9/fX126dNGWLVtsrxuGocTERIWFhcnPz0/9+vXT7t27L0k2AADg/mpcdgYOHKjCwkIVFhbq8OHD6tWrlzNzSZKOHTum3r17y9vbW59//rn27NmjN954w+7qr9mzZ2vOnDlKTU1VTk6OQkNDNXDgQJWWljo9HwAAcH+1uhGoxWK5pN+SPGvWLIWHh2vx4sW2sTZt2tj+2zAMpaSkKCEhQcOGDZMkpaWlKSQkROnp6YqJiblkWQEAgHuq841AL4VVq1ape/fu+tOf/qTg4GB17dpVixYtsr2el5enwsJCDRo0yDbm4+Ojvn37Kjs7+6LzlpeXq6SkxO4BAADMya3Lzvfff68FCxYoMjJSa9as0fjx4/X000/rr3/9qySpsLBQkhQSEmL3vpCQENtrF5KcnKygoCDbIzw83Hk/BAAAcCm3LjtWq1U33XSTkpKS1LVrV8XExGjcuHHVvs/HYrHYPTcMo9rY+eLj41VcXGx75OfnOyU/AABwPYeUnePHjztimmpatGih9u3b243dcMMNOnTokCTZvrH516s4RUVF1VZ7zufj46PAwEC7BwAAMKdal51Zs2YpIyPD9nz48OFq3ry5rr76am3fvt2h4Xr37q19+/bZje3fv1+tW7eWJEVERCg0NFSZmZm21ysqKpSVlXVJrhYDAADur9Zl591337XtccnMzFRmZqY+//xzDR48WM8//7xDwz377LP6+uuvlZSUpO+++07p6elauHChnnzySUlnT1/FxcUpKSlJK1eu1K5duzRmzBj5+/tr1KhRDs0CAAAaplpdei5JBQUFtrLz2Wefafjw4Ro0aJDatGmjqKgoh4br0aOHVq5cqfj4eL3yyiuKiIhQSkqKRo8ebTtm0qRJOnXqlGJjY3Xs2DFFRUVp7dq1CggIcGgWAADQMNW67DRr1kz5+fkKDw/X6tWrNX36dElnNwVXVVU5POCQIUM0ZMiQi75usViUmJioxMREh382AABo+GpddoYNG6ZRo0YpMjJSv/zyiwYPHixJys3N1bXXXuvwgAAAAPVR67Izd+5ctWnTRvn5+Zo9e7aaNGki6ezprdjYWIcHBAAAqA+LYRiGq0O4Wk1vEV8niUGOnQ8AgIYmsdgp09b093edvmdn6dKl6tOnj8LCwnTw4EFJUkpKij755JO6pQUAAHCSWpedBQsWaOLEiRo8eLCOHz9u25TctGlTpaSkODofAABAvdS67MybN0+LFi1SQkKCPD09bePdu3fXzp07HRoOAACgvmpddvLy8tS1a9dq4z4+Pjp58qRDQgEAADhKrctORESEcnNzq41//vnn1e5jBQAA4Gq1vvT8+eef15NPPqnTp0/LMAx98803+uCDD5ScnKy//OUvzsgIAABQZ7UuO48++qgqKys1adIklZWVadSoUbr66qv15ptvauTIkc7ICAAAUGe1LjuSNG7cOI0bN05Hjx6V1WpVcHCwo3MBAAA4RJ3KzjlXXnmlo3IAAAA4RY3KTteuXWWxWGo04datW+sVCAAAwJFqVHaGDh3q5BgAAADOUaOyM3XqVGfnAAAAcIo63RsLAACgoaj1BmUPD4/f3L9z7l5ZAAAA7qDWZWflypV2z8+cOaNt27YpLS1N06ZNc1gwAAAAR6h12bn33nurjd1///3q0KGDMjIyNHbsWIcEAwAAcASH7dmJiorSunXrHDUdAACAQzik7Jw6dUrz5s1Ty5YtHTEdAACAw9T6NFazZs3sNigbhqHS0lL5+/tr2bJlDg0HAABQX7UuO3PnzrUrOx4eHrrqqqsUFRWlZs2aOTQcAABAfdW67IwZM8YJMQAAAJyj1nt2Fi9erBUrVlQbX7FihdLS0hwSCgAAwFFqXXZmzpx5wbudBwcHKykpySGhAAAAHKXWZefgwYOKiIioNt66dWsdOnTIIaEAAAAcpdZlJzg4WDt27Kg2vn37djVv3twhoQAAAByl1huUR44cqaeffloBAQG67bbbJElZWVl65plnNHLkSIcHbOjanE53dQQAAFzqBxd/fq3LzvTp03Xw4EH1799fXl5n3261WvXwww+zZwcAALidWpedRo0aKSMjQ6+++qq2b98uPz8/derUSa1bt3ZGPgAAgHqpddk5p02bNjIMQ23btrWt8AAAALibWm9QLisr09ixY+Xv768OHTrYrsB6+umnNXPmTIcHBAAAqI9al534+Hht375d//rXv+Tr62sbHzBggDIyMhwaDgAAoL5qff7pf//3f5WRkaGePXva3SOrffv2OnDggEPDAQAA1FetV3Z+/vlnBQcHVxs/efKkXfkBAABwB7UuOz169NDf//532/NzBWfRokW65ZZbHJcMAADAAWp9Gis5OVl33nmn9uzZo8rKSr355pvavXu3Nm3apKysLGdkBAAAqLNar+z06tVLGzduVFlZmdq2bau1a9cqJCREmzZtUrdu3ZyREQAAoM7q9AU5nTp1UlpamqOzAAAAOFyNyk5JSUmNJwwMDKxzGAAAAEerUdlp2rTp715pZRiGLBaLqqqqHBIMAADAEWpUdr788ktn5wAAAHCKGpWdvn37OjsHAACAU9Rpg/Lx48f13nvvae/evbJYLGrfvr2io6MVFBTk6HwAAAD1UutLzzdv3qy2bdtq7ty5+s9//qOjR49qzpw5atu2rbZu3eqMjAAAAHVW65WdZ599Vvfcc48WLVokL6+zb6+srNRjjz2muLg4rV+/3uEhAQAA6qrWZWfz5s12RUeSvLy8NGnSJHXv3t2h4QAAAOqr1qexAgMDdejQoWrj+fn5CggIcEgoAAAAR6l12RkxYoTGjh2rjIwM5efn6/Dhw1q+fLkee+wxPfDAA87ICAAAUGe1Po31+uuvy2Kx6OGHH1ZlZaUkydvbW0888YRmzpzp8IAAAAD1Ueuy06hRI7355ptKTk7WgQMHZBiGrr32Wvn7+zsjHwAAQL3U6Xt2JMnf31+dOnVyZBYAAACHq3HZiY6OrtFx77//fp3DAAAAOFqNy86SJUvUunVrde3aVYZhODMTAACAw9S47IwfP17Lly/X999/r+joaD344IO64oornJkNAACg3mp86fn8+fNVUFCgF154QZ9++qnCw8M1fPhwrVmzhpUeAADgtmr1PTs+Pj564IEHlJmZqT179qhDhw6KjY1V69atdeLECWdlBAAAqLNaf6ngORaLRRaLRYZhyGq1OjITAACAw9Sq7JSXl+uDDz7QwIEDdd1112nnzp1KTU3VoUOH1KRJE2dltElOTpbFYlFcXJxtzDAMJSYmKiwsTH5+furXr592797t9CwAAKBhqHHZiY2NVYsWLTRr1iwNGTJEhw8f1ooVK3TXXXfJw6POC0Q1lpOTo4ULF6pz585247Nnz9acOXOUmpqqnJwchYaGauDAgSotLXV6JgAA4P5qfDXWO++8o1atWikiIkJZWVnKysq64HEff/yxw8Kdc+LECY0ePVqLFi3S9OnTbeOGYSglJUUJCQkaNmyYJCktLU0hISFKT09XTEyMw7MAAICGpcZLMg8//LBuv/12NW3aVEFBQRd9OMOTTz6pu+++WwMGDLAbz8vLU2FhoQYNGmQb8/HxUd++fZWdnX3R+crLy1VSUmL3AAAA5lSrLxV0heXLl2vr1q3Kycmp9lphYaEkKSQkxG48JCREBw8evOicycnJmjZtmmODAgAAt+T8zTb1kJ+fr2eeeUbLli2Tr6/vRY+zWCx2zw3DqDZ2vvj4eBUXF9se+fn5DssMAADcS51vBHopbNmyRUVFRerWrZttrKqqSuvXr1dqaqr27dsn6ewKT4sWLWzHFBUVVVvtOZ+Pj498fHycFxwAALgNt17Z6d+/v3bu3Knc3Fzbo3v37ho9erRyc3N1zTXXKDQ0VJmZmbb3VFRUKCsrS7169XJhcgAA4C7cemUnICBAHTt2tBtr3LixmjdvbhuPi4tTUlKSIiMjFRkZqaSkJPn7+2vUqFGuiAwAANyMW5edmpg0aZJOnTql2NhYHTt2TFFRUVq7dq0CAgJcHQ0AALgBi8FdPFVSUqKgoCAVFxcrMDDQoXO3mfx3h84HAEBD88PMu50yb01/f7v1nh0AAID6ouwAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTo+wAAABTc+uyk5ycrB49eiggIEDBwcEaOnSo9u3bZ3eMYRhKTExUWFiY/Pz81K9fP+3evdtFiQEAgLtx67KTlZWlJ598Ul9//bUyMzNVWVmpQYMG6eTJk7ZjZs+erTlz5ig1NVU5OTkKDQ3VwIEDVVpa6sLkAADAXXi5OsBvWb16td3zxYsXKzg4WFu2bNFtt90mwzCUkpKihIQEDRs2TJKUlpamkJAQpaenKyYm5oLzlpeXq7y83Pa8pKTEeT8EAABwKbde2fm14uJiSdIVV1whScrLy1NhYaEGDRpkO8bHx0d9+/ZVdnb2RedJTk5WUFCQ7REeHu7c4AAAwGUaTNkxDEMTJ05Unz591LFjR0lSYWGhJCkkJMTu2JCQENtrFxIfH6/i4mLbIz8/33nBAQCAS7n1aazzTZgwQTt27NBXX31V7TWLxWL33DCMamPn8/HxkY+Pj8MzAgAA99MgVnaeeuoprVq1Sl9++aVatmxpGw8NDZWkaqs4RUVF1VZ7AADA5cmty45hGJowYYI+/vhj/fOf/1RERITd6xEREQoNDVVmZqZtrKKiQllZWerVq9eljgsAANyQW5/GevLJJ5Wenq5PPvlEAQEBthWcoKAg+fn5yWKxKC4uTklJSYqMjFRkZKSSkpLk7++vUaNGuTg9AABwB25ddhYsWCBJ6tevn9344sWLNWbMGEnSpEmTdOrUKcXGxurYsWOKiorS2rVrFRAQcInTAgAAd+TWZccwjN89xmKxKDExUYmJic4PBAAAGhy33rMDAABQX5QdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgapQdAABgaqYpO/Pnz1dERIR8fX3VrVs3bdiwwdWRAACAGzBF2cnIyFBcXJwSEhK0bds23XrrrRo8eLAOHTrk6mgAAMDFTFF25syZo7Fjx+qxxx7TDTfcoJSUFIWHh2vBggWujgYAAFzMy9UB6quiokJbtmzR5MmT7cYHDRqk7OzsC76nvLxc5eXltufFxcWSpJKSEofns5aXOXxOAAAaEmf8fj1/XsMwfvO4Bl92jh49qqqqKoWEhNiNh4SEqLCw8ILvSU5O1rRp06qNh4eHOyUjAACXs6AU585fWlqqoKCgi77e4MvOORaLxe65YRjVxs6Jj4/XxIkTbc+tVqv+85//qHnz5hd9D4CGqaSkROHh4crPz1dgYKCr4wBwIMMwVFpaqrCwsN88rsGXnSuvvFKenp7VVnGKioqqrfac4+PjIx8fH7uxpk2bOisiADcQGBhI2QFM6LdWdM5p8BuUGzVqpG7duikzM9NuPDMzU7169XJRKgAA4C4a/MqOJE2cOFEPPfSQunfvrltuuUULFy7UoUOHNH78eFdHAwAALmaKsjNixAj98ssveuWVV1RQUKCOHTvqH//4h1q3bu3qaABczMfHR1OnTq126hrA5cNi/N71WgAAAA1Yg9+zAwAA8FsoOwAAwNQoOwAAwNQoOwAAwNQoOwAgKSsrSykpKa6OAcAJKDsAIKmgoEATJ07UokWLXB0FgIOZ4nt2AKA+DMPQyJEjFRQUpCFDhsjb21tjxoxxdSwADsLKDoDL3rkbAA8ePFjR0dGKjo7W0qVLXZwKgKOwsgMAkoqLi7V8+XL94x//0IABAxQdHa2ysjLFxMS4OhqAemJlB8Blr6SkRBkZGZoyZYruu+8+rV27Vp988omeeOIJLV682NXxANQTZQfAZa2kpETLly9XQkKCRowYobfeekuSdNddd2nVqlVav369jh075uKUAOqDe2MBuGydX3SGDx+ut99+W5L0008/qaSkREFBQQoODpYkVVVVydPT05VxAdQRKzsALkvHjx+3nbo6v+hMmjRJw4cPV9euXXXHHXcoISFBkuTp6Sn+bQg0TGxQBnBZysrKUkxMjJ5++mnblwlGR0dr9erVGj9+vJ599ll5eHhozJgxatKkieLj421XbQFoWDiNBeCyVFVVpdTUVD3zzDOSpNTUVM2cOVOvvfaa/vjHP6pJkyaSpPnz5+uzzz7T8uXLFRgY6MrIAOqI01gALjuVlZXy9PS0FR3DMLRjxw717NnTruhI0g8//KBvv/1WXl72C+FWq/WSZgZQd5QdAJedXxeXqqoqHTp0SK1atbIrOnv37tW+fft05513ytfXV/v27bN92aCHB//3CTQU/G0FcNnz8vJSZGSktm3bphMnTkiScnNzNX/+fOXm5uree++Vh4eHPD09FRsbq127drGyAzQg7NkBcFkzDEMWi0VnzpzRTTfdpMaNG8tisai8vFxlZWWaOXOmhg4dKkn6+OOP9eKLL2rv3r2yWCy291qtVlZ6ADfG1VgALmsWi0WVlZXy9vbWtm3blJqaqry8PPXo0UPXX3+9unfvbjt279698vHxsV2Vde67eDw8PGzFB4D7YWUHAHTxLw2srKzUqVOntHv3bqWmpqqkpERXXnmlfvnlF+Xk5OjFF1/UhAkTJElnzpzRlClTNGvWrEsdH8BvYGUHAKRqRWfdunVKTU3VgQMHZLVadfToUf3888/q1q2bDMPQzTffrBEjRsjf39/ufTk5Odq9e7c6dOhwKeMD+A2s7ADABVRWVurFF1+Un5+fbr31VoWFhWn48OF66aWXNGLECLtjrVarPvzwQ40cOdJFaQH8FsoOAPzKhU5pffPNNxoyZIi2b9+uFi1a2I45ffq07rvvPq1Zs0aHDh1SWFiYi1IDuBguHwCAX7nQ3p0DBw6oUaNGatSokQzDkKenpyoqKjRixAjl5uZqw4YNFB3ATbFnBwBq4MCBA2rdurWaN28uSSovL9fw4cO1adMmrVq1Sj179pSkapehc1k64HqUHQCogcmTJyskJESSdPr0aY0YMUJff/21Pv30U0VFRWn79u3auHGjPvroI3Xp0kUdOnRQdHS0PDw8KDyAi7FnBwB+x6/38Nxyyy3Kz8/XRx99pKioKG3YsEFTp07Vjz/+qOuvv17NmjXT6tWr9fDDD2v27NmSWOEBXIm/eQDwO84vOlarVVdddZWWL1+uqKgoFRQU6K233lJgYKDmzp2rTz75REuWLNHq1au1dOlSvf/++5LO3kurtLRU8+fPd9WPAVy2OI0FADV0bnVm1apVtrGff/5Z69ev1/z583XXXXdJOnvZeufOnfXCCy/oiiuusB3r7++v7OxseXt7a9y4cZc8P3C54jQWANTDwoULNXXqVBUUFEiyP11VVFSk4OBgHTt2TFu3blX//v115swZeXt7uzIycNnhNBYA1EOrVq3UrFkzff/995Jk25BstVoVHBys4uJizZgxQ48//ri++uorW9HJz893ZWzgskLZAYB6GDhwoBo3bqy4uDjl5eXpzJkz8vDwkIeHh44fP64ZM2Zo06ZNGjlypPr06aNdu3Zpw4YNuueee5Sdne3q+MBlgbIDAHV07iqt7OxsHT16VBMnTtSOHTskScePH1dSUpLWr1+vfv36afr06ZKkBx98UA888ICuu+46sYsAuDTYoAwAdeTp6anKykp5e3trw4YNysnJUbdu3VRSUqLk5GStX79e/fv31/Tp02WxWLRu3TpZrVb99NNP8vT0VO/evSVJhmHIYrG4+KcBzIuVHQCoBy8vL9sKT8+ePXXixAlNmjRJGzZs0IABA2xF57PPPtOUKVPUuXNnvffee1qxYoUmTJggSbJYLLJarS7+SQDzYmUHAOrp/O/hady4saqqqtSjRw+9+uqrtqIzffp0tWrVSjExMbr11lt144036ocfftD+/fvVrl07vnAQcCIuPQcABzl3Our801IbN25UfHy8WrZsqdjYWPXp08d2/Pfff68+ffpoyZIlGjRokKtiA6bHPyUAwEF+XXQkaefOnSouLlZMTIyt6FRWVkqSfvzxRwUGBsrf398leYHLBWUHABzo1xuNi4qKZBiG+vbtK+nsFVxeXl6qrKzUxIkTddNNN9mt9pzDHh7AcSg7AOBEL774ok6cOKE777xT0n/L0IYNG+Tt7a24uDi740+cOCFJ7OEBHIi/TQDgJOdWcfbt26dRo0ZJ+m+JSUtLk7e3tzp37ixJysnJ0dSpUxUeHq4lS5a4KjJgSmxQBgAnqqyslJeX/YWvGzdu1KOPPqq//e1vKi8v1wcffKD3339fd9xxh+0Goo0bN3ZRYsB8uPQcAJzIy8tLp0+flq+vr21sz549Kisr0yOPPKLKykpde+21ysjI0B/+8Ac1adLEhWkBc6LsAIATWa1WLV26VGVlZbr55pv13Xff6bnnnpOPj4+6du2q+Ph4tWjRQs2bN+f2EYCTcBoLAJxs586d6t69u9q0aSMvLy8NHTpUo0ePVvv27V0dDbgsUHYA4BI4cuSIysvL5enpqVatWtnGrVYrV14BTkbZAQAX4OafwKXDPycAwAUoOsClQ9kBAACmRtkBAACmRtkBAACmRtkBAACmRtkBAACmRtkBAACmRtkBAACmRtkBAACmRtkBAACmRtkBAACm9v8Av4nNgL/Oak8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "percent_toxic = 100*sum(df_oversampled['tox'])/len(df_oversampled)\n",
    "percent_non_toxic = 100-percent_toxic\n",
    "\n",
    "ax=plt.bar('Toxicity',percent_toxic, label='Toxic')\n",
    "plt.bar('Toxicity',percent_non_toxic, bottom= percent_toxic, label='Non-Toxic')\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.ylabel(\"Molecules %\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_oversampled.drop(['tox'], axis = 1)\n",
    "\n",
    "y_train = df_oversampled[['tox']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(criterion='gini',   # gini criteron measures the probability of misclassification for each split - values close to zero are ideal.\n",
    "                             max_depth = 10,     # maximum depth of each decision tree\n",
    "                             min_samples_split = 20,  # minimum number of samples required to split an internal node\n",
    "                             random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=10, min_samples_split=20, random_state=5)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25404\\1343793370.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[1;34m'knn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[1;34m'svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 ('rf', clf6)], voting='soft', probability=True)\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'probability'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "clf1 = LogisticRegression()\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = nb_clf\n",
    "clf4 = knn\n",
    "clf5 = svm_clf\n",
    "clf6 = rf_clf\n",
    "\n",
    "ensemble = VotingClassifier(\n",
    "    estimators=[('lr', clf1), \n",
    "                ('dt', clf2), \n",
    "                ('nb', clf3), \n",
    "                ('knn', clf4), \n",
    "                ('svc', clf5), \n",
    "                ('rf', clf6)], voting='soft')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25404\\1321080434.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'probability'"
     ]
    }
   ],
   "source": [
    "ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,300, 50), max_iter=400, activation='relu', solver='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(1000, 300, 50), max_iter=400)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(750, activation='relu'), # Create a fully connected dense layer with dense1 number of neurons\n",
    "    tf.keras.layers.BatchNormalization(), # Batch normalisation reduces variability of our model\n",
    "    tf.keras.layers.Dropout(rate=0.8), # Dropout will reduce overtraining\n",
    "    tf.keras.layers.Dense(150, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(rate=0.8),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(rate=0.3),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # output layer\n",
    "])\n",
    "\n",
    "tf_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "298/298 [==============================] - 8s 17ms/step - loss: 0.8385 - accuracy: 0.5120 - val_loss: 0.6936 - val_accuracy: 0.5036\n",
      "Epoch 2/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.7167 - accuracy: 0.5377 - val_loss: 0.6801 - val_accuracy: 0.5316\n",
      "Epoch 3/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.6930 - accuracy: 0.5494 - val_loss: 0.6762 - val_accuracy: 0.5429\n",
      "Epoch 4/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.6843 - accuracy: 0.5575 - val_loss: 0.6827 - val_accuracy: 0.5289\n",
      "Epoch 5/500\n",
      "298/298 [==============================] - 5s 16ms/step - loss: 0.6783 - accuracy: 0.5661 - val_loss: 0.6774 - val_accuracy: 0.5379\n",
      "Epoch 6/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.6760 - accuracy: 0.5708 - val_loss: 0.6742 - val_accuracy: 0.5399\n",
      "Epoch 7/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.6731 - accuracy: 0.5783 - val_loss: 0.6640 - val_accuracy: 0.5690\n",
      "Epoch 8/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.6713 - accuracy: 0.5860 - val_loss: 0.6576 - val_accuracy: 0.5856\n",
      "Epoch 9/500\n",
      "298/298 [==============================] - 4s 13ms/step - loss: 0.6603 - accuracy: 0.6035 - val_loss: 0.6498 - val_accuracy: 0.5981\n",
      "Epoch 10/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.6532 - accuracy: 0.6111 - val_loss: 0.6395 - val_accuracy: 0.6216\n",
      "Epoch 11/500\n",
      "298/298 [==============================] - 5s 17ms/step - loss: 0.6484 - accuracy: 0.6202 - val_loss: 0.6412 - val_accuracy: 0.6120\n",
      "Epoch 12/500\n",
      "298/298 [==============================] - 5s 16ms/step - loss: 0.6500 - accuracy: 0.6202 - val_loss: 0.6361 - val_accuracy: 0.6237\n",
      "Epoch 13/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.6432 - accuracy: 0.6325 - val_loss: 0.6264 - val_accuracy: 0.6385\n",
      "Epoch 14/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.6390 - accuracy: 0.6358 - val_loss: 0.6207 - val_accuracy: 0.6453\n",
      "Epoch 15/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.6309 - accuracy: 0.6445 - val_loss: 0.6179 - val_accuracy: 0.6490\n",
      "Epoch 16/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.6248 - accuracy: 0.6495 - val_loss: 0.6059 - val_accuracy: 0.6649\n",
      "Epoch 17/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.6238 - accuracy: 0.6504 - val_loss: 0.6070 - val_accuracy: 0.6593\n",
      "Epoch 18/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.6167 - accuracy: 0.6662 - val_loss: 0.5887 - val_accuracy: 0.6881\n",
      "Epoch 19/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.6080 - accuracy: 0.6668 - val_loss: 0.5891 - val_accuracy: 0.6812\n",
      "Epoch 20/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.6110 - accuracy: 0.6697 - val_loss: 0.5834 - val_accuracy: 0.6863\n",
      "Epoch 21/500\n",
      "298/298 [==============================] - 5s 16ms/step - loss: 0.6018 - accuracy: 0.6767 - val_loss: 0.5814 - val_accuracy: 0.6845\n",
      "Epoch 22/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.5957 - accuracy: 0.6757 - val_loss: 0.5725 - val_accuracy: 0.6920\n",
      "Epoch 23/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5916 - accuracy: 0.6819 - val_loss: 0.5669 - val_accuracy: 0.7010\n",
      "Epoch 24/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5888 - accuracy: 0.6912 - val_loss: 0.5587 - val_accuracy: 0.7081\n",
      "Epoch 25/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5939 - accuracy: 0.6852 - val_loss: 0.5584 - val_accuracy: 0.7076\n",
      "Epoch 26/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5826 - accuracy: 0.6956 - val_loss: 0.5552 - val_accuracy: 0.7090\n",
      "Epoch 27/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5775 - accuracy: 0.6998 - val_loss: 0.5468 - val_accuracy: 0.7140\n",
      "Epoch 28/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5744 - accuracy: 0.7055 - val_loss: 0.5408 - val_accuracy: 0.7182\n",
      "Epoch 29/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5719 - accuracy: 0.7080 - val_loss: 0.5369 - val_accuracy: 0.7218\n",
      "Epoch 30/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.5630 - accuracy: 0.7124 - val_loss: 0.5297 - val_accuracy: 0.7284\n",
      "Epoch 31/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5607 - accuracy: 0.7138 - val_loss: 0.5209 - val_accuracy: 0.7395\n",
      "Epoch 32/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.5546 - accuracy: 0.7220 - val_loss: 0.5180 - val_accuracy: 0.7362\n",
      "Epoch 33/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5466 - accuracy: 0.7224 - val_loss: 0.5045 - val_accuracy: 0.7502\n",
      "Epoch 34/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5377 - accuracy: 0.7361 - val_loss: 0.4972 - val_accuracy: 0.7523\n",
      "Epoch 35/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5371 - accuracy: 0.7352 - val_loss: 0.4876 - val_accuracy: 0.7661\n",
      "Epoch 36/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5327 - accuracy: 0.7381 - val_loss: 0.4846 - val_accuracy: 0.7703\n",
      "Epoch 37/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.5236 - accuracy: 0.7410 - val_loss: 0.4765 - val_accuracy: 0.7733\n",
      "Epoch 38/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.5208 - accuracy: 0.7493 - val_loss: 0.4699 - val_accuracy: 0.7774\n",
      "Epoch 39/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.5136 - accuracy: 0.7487 - val_loss: 0.4712 - val_accuracy: 0.7720\n",
      "Epoch 40/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.5137 - accuracy: 0.7523 - val_loss: 0.4590 - val_accuracy: 0.7839\n",
      "Epoch 41/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.5052 - accuracy: 0.7583 - val_loss: 0.4421 - val_accuracy: 0.7976\n",
      "Epoch 42/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.4987 - accuracy: 0.7624 - val_loss: 0.4288 - val_accuracy: 0.8062\n",
      "Epoch 43/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.4997 - accuracy: 0.7653 - val_loss: 0.4327 - val_accuracy: 0.8021\n",
      "Epoch 44/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.4842 - accuracy: 0.7716 - val_loss: 0.4152 - val_accuracy: 0.8164\n",
      "Epoch 45/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.4816 - accuracy: 0.7700 - val_loss: 0.4130 - val_accuracy: 0.8177\n",
      "Epoch 46/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.4814 - accuracy: 0.7731 - val_loss: 0.4011 - val_accuracy: 0.8281\n",
      "Epoch 47/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.4721 - accuracy: 0.7806 - val_loss: 0.3875 - val_accuracy: 0.8371\n",
      "Epoch 48/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.4673 - accuracy: 0.7844 - val_loss: 0.3928 - val_accuracy: 0.8297\n",
      "Epoch 49/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.4533 - accuracy: 0.7887 - val_loss: 0.3659 - val_accuracy: 0.8487\n",
      "Epoch 50/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.4548 - accuracy: 0.7957 - val_loss: 0.3681 - val_accuracy: 0.8466\n",
      "Epoch 51/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.4399 - accuracy: 0.7998 - val_loss: 0.3574 - val_accuracy: 0.8470\n",
      "Epoch 52/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.4382 - accuracy: 0.7989 - val_loss: 0.3511 - val_accuracy: 0.8510\n",
      "Epoch 53/500\n",
      "298/298 [==============================] - 5s 17ms/step - loss: 0.4345 - accuracy: 0.8024 - val_loss: 0.3378 - val_accuracy: 0.8620\n",
      "Epoch 54/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.4257 - accuracy: 0.8123 - val_loss: 0.3198 - val_accuracy: 0.8736\n",
      "Epoch 55/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.4158 - accuracy: 0.8191 - val_loss: 0.3106 - val_accuracy: 0.8732\n",
      "Epoch 56/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.4056 - accuracy: 0.8233 - val_loss: 0.3020 - val_accuracy: 0.8811\n",
      "Epoch 57/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.4093 - accuracy: 0.8161 - val_loss: 0.2907 - val_accuracy: 0.8891\n",
      "Epoch 58/500\n",
      "298/298 [==============================] - 5s 16ms/step - loss: 0.3926 - accuracy: 0.8304 - val_loss: 0.2877 - val_accuracy: 0.8882\n",
      "Epoch 59/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.3905 - accuracy: 0.8262 - val_loss: 0.2763 - val_accuracy: 0.8941\n",
      "Epoch 60/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.3782 - accuracy: 0.8365 - val_loss: 0.2681 - val_accuracy: 0.8954\n",
      "Epoch 61/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.3752 - accuracy: 0.8355 - val_loss: 0.2544 - val_accuracy: 0.9037\n",
      "Epoch 62/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.3664 - accuracy: 0.8422 - val_loss: 0.2491 - val_accuracy: 0.9053\n",
      "Epoch 63/500\n",
      "298/298 [==============================] - 5s 16ms/step - loss: 0.3654 - accuracy: 0.8451 - val_loss: 0.2405 - val_accuracy: 0.9119\n",
      "Epoch 64/500\n",
      "298/298 [==============================] - 5s 16ms/step - loss: 0.3582 - accuracy: 0.8446 - val_loss: 0.2352 - val_accuracy: 0.9159\n",
      "Epoch 65/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.3563 - accuracy: 0.8468 - val_loss: 0.2248 - val_accuracy: 0.9174\n",
      "Epoch 66/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.3501 - accuracy: 0.8505 - val_loss: 0.2169 - val_accuracy: 0.9244\n",
      "Epoch 67/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.3410 - accuracy: 0.8522 - val_loss: 0.2098 - val_accuracy: 0.9287\n",
      "Epoch 68/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.3350 - accuracy: 0.8617 - val_loss: 0.1986 - val_accuracy: 0.9344\n",
      "Epoch 69/500\n",
      "298/298 [==============================] - 5s 16ms/step - loss: 0.3328 - accuracy: 0.8574 - val_loss: 0.1955 - val_accuracy: 0.9373\n",
      "Epoch 70/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.3206 - accuracy: 0.8667 - val_loss: 0.1854 - val_accuracy: 0.9384\n",
      "Epoch 71/500\n",
      "298/298 [==============================] - 5s 16ms/step - loss: 0.3143 - accuracy: 0.8685 - val_loss: 0.1850 - val_accuracy: 0.9391\n",
      "Epoch 72/500\n",
      "298/298 [==============================] - 5s 16ms/step - loss: 0.3024 - accuracy: 0.8733 - val_loss: 0.1667 - val_accuracy: 0.9470\n",
      "Epoch 73/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.3105 - accuracy: 0.8713 - val_loss: 0.1756 - val_accuracy: 0.9435\n",
      "Epoch 74/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.2974 - accuracy: 0.8750 - val_loss: 0.1676 - val_accuracy: 0.9484\n",
      "Epoch 75/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.2958 - accuracy: 0.8758 - val_loss: 0.1579 - val_accuracy: 0.9512\n",
      "Epoch 76/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.2904 - accuracy: 0.8829 - val_loss: 0.1492 - val_accuracy: 0.9563\n",
      "Epoch 77/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.2870 - accuracy: 0.8791 - val_loss: 0.1516 - val_accuracy: 0.9544\n",
      "Epoch 78/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.2799 - accuracy: 0.8849 - val_loss: 0.1458 - val_accuracy: 0.9549\n",
      "Epoch 79/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.2768 - accuracy: 0.8850 - val_loss: 0.1380 - val_accuracy: 0.9567\n",
      "Epoch 80/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.2732 - accuracy: 0.8878 - val_loss: 0.1336 - val_accuracy: 0.9580\n",
      "Epoch 81/500\n",
      "298/298 [==============================] - 4s 14ms/step - loss: 0.2749 - accuracy: 0.8880 - val_loss: 0.1284 - val_accuracy: 0.9629\n",
      "Epoch 82/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.2700 - accuracy: 0.8916 - val_loss: 0.1292 - val_accuracy: 0.9614\n",
      "Epoch 83/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.2626 - accuracy: 0.8935 - val_loss: 0.1237 - val_accuracy: 0.9648\n",
      "Epoch 84/500\n",
      "298/298 [==============================] - 4s 15ms/step - loss: 0.2601 - accuracy: 0.8956 - val_loss: 0.1210 - val_accuracy: 0.9634\n",
      "Epoch 85/500\n",
      "298/298 [==============================] - 5s 16ms/step - loss: 0.2544 - accuracy: 0.8995 - val_loss: 0.1157 - val_accuracy: 0.9659\n",
      "Epoch 86/500\n",
      "298/298 [==============================] - 5s 15ms/step - loss: 0.2557 - accuracy: 0.8986 - val_loss: 0.1142 - val_accuracy: 0.9668\n",
      "Epoch 87/500\n",
      "298/298 [==============================] - 5s 16ms/step - loss: 0.2569 - accuracy: 0.8966 - val_loss: 0.1085 - val_accuracy: 0.9688\n",
      "Epoch 88/500\n",
      "111/298 [==========>...................] - ETA: 2s - loss: 0.2313 - accuracy: 0.9023"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25404\\1792989370.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m--> 135\u001b[1;33m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    381\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 53\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf_model.fit(X_train, y_train, validation_data=(X_train, y_train), epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Random Forest': rf_clf,\n",
    "    'SGD': sgd_clf,\n",
    "    'SVM': svm_clf,\n",
    "    'KNN': knn,\n",
    "    'Naive Bayes': nb_clf,\n",
    "    #'Ensemble': ensemble,\n",
    "    'MLP': mlp,\n",
    "    'Tensorflow': tf_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest _____________________________________________ \n",
      " [[442 133]\n",
      " [ 84 125]] \n",
      " 0.5353319057815845 \n",
      " 0.5712979890310786 \n",
      " 0.7232142857142857\n",
      "SGD _____________________________________________ \n",
      " [[440 135]\n",
      " [ 98 111]] \n",
      " 0.48791208791208796 \n",
      " 0.5129390018484289 \n",
      " 0.7028061224489796\n",
      "SVM _____________________________________________ \n",
      " [[509  66]\n",
      " [114  95]] \n",
      " 0.5135135135135135 \n",
      " 0.4764292878635908 \n",
      " 0.7704081632653061\n",
      "KNN _____________________________________________ \n",
      " [[443 132]\n",
      " [ 88 121]] \n",
      " 0.5238095238095238 \n",
      " 0.5555555555555556 \n",
      " 0.7193877551020408\n",
      "Naive Bayes _____________________________________________ \n",
      " [[439 136]\n",
      " [107 102]] \n",
      " 0.4563758389261745 \n",
      " 0.4748603351955308 \n",
      " 0.6900510204081632\n",
      "MLP _____________________________________________ \n",
      " [[500  75]\n",
      " [125  84]] \n",
      " 0.45652173913043476 \n",
      " 0.42211055276381915 \n",
      " 0.7448979591836735\n",
      "25/25 [==============================] - 1s 7ms/step\n",
      "Tensorflow _____________________________________________ \n",
      " [[519  56]\n",
      " [123  86]] \n",
      " 0.49002849002848997 \n",
      " 0.43967280163599176 \n",
      " 0.7716836734693877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "for key, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "\n",
    "    f1_scores = f1_score(y_test, y_pred)\n",
    "    f2_score = fbeta_score(y_test, y_pred, beta=2)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f'{key} _____________________________________________ \\n {cm} \\n {f1_scores} \\n {f2_score} \\n {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-for-chemists-tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
