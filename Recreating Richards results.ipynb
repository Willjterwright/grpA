{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import tensorflow as tf\n",
    "from rdkit.Chem import Descriptors as ds\n",
    "from rdkit.Chem import Fragments as fr\n",
    "\n",
    "import model_stats as ms\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is not important, it just gets the number of threads on your CPU and returns that number.\n",
    "import threading\n",
    "\n",
    "total_threads =threading.active_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TensorFlow to use multiple CPU cores - This will speed up the training (hopefully)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(total_threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(total_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import the dataset - the featurizer and splitter dont matter as we will be combining the datasets and\n",
    "#designing our own featurizer\n",
    "tasks, datasets, transformers = dc.molnet.load_tox21(\n",
    "    featurizer='GraphConv', \n",
    "    splitter='random')\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11',\n",
       "       'y12', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6', 'w7', 'w8', 'w9', 'w10',\n",
       "       'w11', 'w12', 'ids'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets convert them to dataframes because deepchem is a piece of shit and it cant handle numpy arrays <-codeium generated this sentence\n",
    "\n",
    "train_df = train_dataset.to_dataframe()\n",
    "test_dataset = test_dataset.to_dataframe()\n",
    "valid_dataset = valid_dataset.to_dataframe()\n",
    "\n",
    "#This line concatenates all the dataframes meaning all the data is in one dataframe, we will resplit them later with SKLEARN\n",
    "dataset = pd.concat([train_df, test_dataset, valid_dataset])\n",
    "\n",
    "\n",
    "#print the columns\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>...</th>\n",
       "      <th>w4</th>\n",
       "      <th>w5</th>\n",
       "      <th>w6</th>\n",
       "      <th>w7</th>\n",
       "      <th>w8</th>\n",
       "      <th>w9</th>\n",
       "      <th>w10</th>\n",
       "      <th>w11</th>\n",
       "      <th>w12</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.056284</td>\n",
       "      <td>1.147522</td>\n",
       "      <td>1.055366</td>\n",
       "      <td>1.027673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.037877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.191992</td>\n",
       "      <td>1.067100</td>\n",
       "      <td>FCOC(C(F)(F)F)C(F)(F)F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.056284</td>\n",
       "      <td>1.147522</td>\n",
       "      <td>1.055366</td>\n",
       "      <td>1.027673</td>\n",
       "      <td>1.197585</td>\n",
       "      <td>1.037877</td>\n",
       "      <td>1.060569</td>\n",
       "      <td>1.191992</td>\n",
       "      <td>1.067100</td>\n",
       "      <td>ClCCCCl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.147522</td>\n",
       "      <td>1.055366</td>\n",
       "      <td>1.027673</td>\n",
       "      <td>1.197585</td>\n",
       "      <td>1.037877</td>\n",
       "      <td>1.060569</td>\n",
       "      <td>1.191992</td>\n",
       "      <td>1.067100</td>\n",
       "      <td>Nc1ccc2ccccc2c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.056284</td>\n",
       "      <td>1.147522</td>\n",
       "      <td>1.055366</td>\n",
       "      <td>1.027673</td>\n",
       "      <td>1.197585</td>\n",
       "      <td>1.037877</td>\n",
       "      <td>1.060569</td>\n",
       "      <td>1.191992</td>\n",
       "      <td>1.067100</td>\n",
       "      <td>CCCOC(=O)c1ccccc1C(=O)OCCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.056284</td>\n",
       "      <td>1.147522</td>\n",
       "      <td>1.055366</td>\n",
       "      <td>1.027673</td>\n",
       "      <td>1.197585</td>\n",
       "      <td>1.037877</td>\n",
       "      <td>1.060569</td>\n",
       "      <td>1.191992</td>\n",
       "      <td>1.067100</td>\n",
       "      <td>O=c1nc([O-])n(Cl)c(=O)n1Cl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.056284</td>\n",
       "      <td>1.147522</td>\n",
       "      <td>1.055366</td>\n",
       "      <td>1.027673</td>\n",
       "      <td>1.197585</td>\n",
       "      <td>1.037877</td>\n",
       "      <td>1.060569</td>\n",
       "      <td>1.191992</td>\n",
       "      <td>1.067100</td>\n",
       "      <td>C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.056284</td>\n",
       "      <td>1.147522</td>\n",
       "      <td>1.055366</td>\n",
       "      <td>1.027673</td>\n",
       "      <td>1.197585</td>\n",
       "      <td>1.037877</td>\n",
       "      <td>1.060569</td>\n",
       "      <td>1.191992</td>\n",
       "      <td>1.067100</td>\n",
       "      <td>O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.056284</td>\n",
       "      <td>1.147522</td>\n",
       "      <td>1.055366</td>\n",
       "      <td>1.027673</td>\n",
       "      <td>1.197585</td>\n",
       "      <td>1.037877</td>\n",
       "      <td>1.060569</td>\n",
       "      <td>1.191992</td>\n",
       "      <td>1.067100</td>\n",
       "      <td>O=C1OC(=O)C2CC=CCC12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.055366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.061118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.208556</td>\n",
       "      <td>15.903226</td>\n",
       "      <td>CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.056284</td>\n",
       "      <td>1.147522</td>\n",
       "      <td>1.055366</td>\n",
       "      <td>1.027673</td>\n",
       "      <td>1.197585</td>\n",
       "      <td>1.037877</td>\n",
       "      <td>1.060569</td>\n",
       "      <td>1.191992</td>\n",
       "      <td>1.067100</td>\n",
       "      <td>CCOC(=O)N(C)N=O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     X   y1   y2   y3   y4  \\\n",
       "0    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "1    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "2    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  1.0  0.0   \n",
       "3    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "4    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "..                                                 ...  ...  ...  ...  ...   \n",
       "778  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "779  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "780  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "781  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "782  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      y5   y6   y7   y8   y9  ...        w4        w5        w6        w7  \\\n",
       "0    0.0  0.0  0.0  0.0  0.0  ...  1.056284  1.147522  1.055366  1.027673   \n",
       "1    0.0  0.0  0.0  0.0  0.0  ...  1.056284  1.147522  1.055366  1.027673   \n",
       "2    0.0  0.0  0.0  0.0  0.0  ...  0.000000  1.147522  1.055366  1.027673   \n",
       "3    0.0  0.0  0.0  0.0  0.0  ...  1.056284  1.147522  1.055366  1.027673   \n",
       "4    0.0  0.0  0.0  0.0  0.0  ...  1.056284  1.147522  1.055366  1.027673   \n",
       "..   ...  ...  ...  ...  ...  ...       ...       ...       ...       ...   \n",
       "778  0.0  0.0  0.0  0.0  0.0  ...  1.056284  1.147522  1.055366  1.027673   \n",
       "779  0.0  0.0  0.0  0.0  0.0  ...  1.056284  1.147522  1.055366  1.027673   \n",
       "780  0.0  0.0  0.0  0.0  0.0  ...  1.056284  1.147522  1.055366  1.027673   \n",
       "781  0.0  0.0  0.0  1.0  0.0  ...  0.000000  0.000000  1.055366  0.000000   \n",
       "782  0.0  0.0  0.0  0.0  0.0  ...  1.056284  1.147522  1.055366  1.027673   \n",
       "\n",
       "           w8        w9       w10       w11        w12  \\\n",
       "0    0.000000  1.037877  0.000000  1.191992   1.067100   \n",
       "1    1.197585  1.037877  1.060569  1.191992   1.067100   \n",
       "2    1.197585  1.037877  1.060569  1.191992   1.067100   \n",
       "3    1.197585  1.037877  1.060569  1.191992   1.067100   \n",
       "4    1.197585  1.037877  1.060569  1.191992   1.067100   \n",
       "..        ...       ...       ...       ...        ...   \n",
       "778  1.197585  1.037877  1.060569  1.191992   1.067100   \n",
       "779  1.197585  1.037877  1.060569  1.191992   1.067100   \n",
       "780  1.197585  1.037877  1.060569  1.191992   1.067100   \n",
       "781  6.061118  0.000000  0.000000  6.208556  15.903226   \n",
       "782  1.197585  1.037877  1.060569  1.191992   1.067100   \n",
       "\n",
       "                                                   ids  \n",
       "0                               FCOC(C(F)(F)F)C(F)(F)F  \n",
       "1                                              ClCCCCl  \n",
       "2                                      Nc1ccc2ccccc2c1  \n",
       "3                           CCCOC(=O)c1ccccc1C(=O)OCCC  \n",
       "4                           O=c1nc([O-])n(Cl)c(=O)n1Cl  \n",
       "..                                                 ...  \n",
       "778  C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...  \n",
       "779  O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...  \n",
       "780                               O=C1OC(=O)C2CC=CCC12  \n",
       "781                     CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O  \n",
       "782                                    CCOC(=O)N(C)N=O  \n",
       "\n",
       "[7831 rows x 26 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>y10</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FCOC(C(F)(F)F)C(F)(F)F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ClCCCCl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nc1ccc2ccccc2c1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCOC(=O)c1ccccc1C(=O)OCCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=c1nc([O-])n(Cl)c(=O)n1Cl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>O=C1OC(=O)C2CC=CCC12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CCOC(=O)N(C)N=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids   y1   y2   y3   y4  \\\n",
       "0                               FCOC(C(F)(F)F)C(F)(F)F  0.0  0.0  0.0  0.0   \n",
       "1                                              ClCCCCl  0.0  0.0  0.0  0.0   \n",
       "2                                      Nc1ccc2ccccc2c1  0.0  0.0  1.0  0.0   \n",
       "3                           CCCOC(=O)c1ccccc1C(=O)OCCC  0.0  0.0  0.0  0.0   \n",
       "4                           O=c1nc([O-])n(Cl)c(=O)n1Cl  0.0  0.0  0.0  0.0   \n",
       "..                                                 ...  ...  ...  ...  ...   \n",
       "778  C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...  0.0  0.0  0.0  0.0   \n",
       "779  O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...  0.0  0.0  0.0  0.0   \n",
       "780                               O=C1OC(=O)C2CC=CCC12  0.0  0.0  0.0  0.0   \n",
       "781                     CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O  0.0  0.0  0.0  0.0   \n",
       "782                                    CCOC(=O)N(C)N=O  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      y5   y6   y7   y8   y9  y10  y11  y12  tox  \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "778  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "779  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "780  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "781  0.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  3.0  \n",
       "782  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[7831 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets first select only the useful information, ie the y values (toxicity)\n",
    "#As well as the ids - ie the smiles strings\n",
    "data = dataset[['ids', 'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12']]\n",
    "\n",
    "# Lets add up all the y values into one 'toxicity measure\n",
    "col_to_sum = ['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12']\n",
    "\n",
    "#This adds a new column called 'tox' which is the sum of each row \n",
    "data['tox'] = data[col_to_sum].sum(axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>y10</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FCOC(C(F)(F)F)C(F)(F)F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ClCCCCl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nc1ccc2ccccc2c1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCOC(=O)c1ccccc1C(=O)OCCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=c1nc([O-])n(Cl)c(=O)n1Cl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>O=C1OC(=O)C2CC=CCC12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CCOC(=O)N(C)N=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids   y1   y2   y3   y4  \\\n",
       "0                               FCOC(C(F)(F)F)C(F)(F)F  0.0  0.0  0.0  0.0   \n",
       "1                                              ClCCCCl  0.0  0.0  0.0  0.0   \n",
       "2                                      Nc1ccc2ccccc2c1  0.0  0.0  1.0  0.0   \n",
       "3                           CCCOC(=O)c1ccccc1C(=O)OCCC  0.0  0.0  0.0  0.0   \n",
       "4                           O=c1nc([O-])n(Cl)c(=O)n1Cl  0.0  0.0  0.0  0.0   \n",
       "..                                                 ...  ...  ...  ...  ...   \n",
       "778  C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...  0.0  0.0  0.0  0.0   \n",
       "779  O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...  0.0  0.0  0.0  0.0   \n",
       "780                               O=C1OC(=O)C2CC=CCC12  0.0  0.0  0.0  0.0   \n",
       "781                     CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O  0.0  0.0  0.0  0.0   \n",
       "782                                    CCOC(=O)N(C)N=O  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      y5   y6   y7   y8   y9  y10  y11  y12  tox  tox_bin  \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0        1  \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...      ...  \n",
       "778  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "779  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "780  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "781  0.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0  3.0        1  \n",
       "782  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "\n",
       "[7831 rows x 15 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets add another row with a binary (1 or 0) that tells us if the compound is toxic against one or more of the assays or not\n",
    "data['tox_bin'] = data['tox'].apply(lambda x: 1 if x>0 else 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of the y columns - we dont need them anymore\n",
    "data.drop(columns=['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FCOC(C(F)(F)F)C(F)(F)F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ClCCCCl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nc1ccc2ccccc2c1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCOC(=O)c1ccccc1C(=O)OCCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=c1nc([O-])n(Cl)c(=O)n1Cl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>O=C1OC(=O)C2CC=CCC12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CCOC(=O)N(C)N=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids  tox  tox_bin\n",
       "0                               FCOC(C(F)(F)F)C(F)(F)F  0.0        0\n",
       "1                                              ClCCCCl  0.0        0\n",
       "2                                      Nc1ccc2ccccc2c1  1.0        1\n",
       "3                           CCCOC(=O)c1ccccc1C(=O)OCCC  0.0        0\n",
       "4                           O=c1nc([O-])n(Cl)c(=O)n1Cl  0.0        0\n",
       "..                                                 ...  ...      ...\n",
       "778  C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...  0.0        0\n",
       "779  O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...  0.0        0\n",
       "780                               O=C1OC(=O)C2CC=CCC12  0.0        0\n",
       "781                     CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O  3.0        1\n",
       "782                                    CCOC(=O)N(C)N=O  0.0        0\n",
       "\n",
       "[7831 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurisation of the dataset\n",
    "\n",
    "Lets get all the features we think we need from the dataset, I have copied richard here but feel free to have a play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fr_Al_COO',\n",
       " 'fr_Al_OH',\n",
       " 'fr_Al_OH_noTert',\n",
       " 'fr_ArN',\n",
       " 'fr_Ar_COO',\n",
       " 'fr_Ar_N',\n",
       " 'fr_Ar_NH',\n",
       " 'fr_Ar_OH',\n",
       " 'fr_COO',\n",
       " 'fr_COO2',\n",
       " 'fr_C_O',\n",
       " 'fr_C_O_noCOO',\n",
       " 'fr_C_S',\n",
       " 'fr_HOCCN',\n",
       " 'fr_Imine',\n",
       " 'fr_NH0',\n",
       " 'fr_NH1',\n",
       " 'fr_NH2',\n",
       " 'fr_N_O',\n",
       " 'fr_Ndealkylation1',\n",
       " 'fr_Ndealkylation2',\n",
       " 'fr_Nhpyrrole',\n",
       " 'fr_SH',\n",
       " 'fr_aldehyde',\n",
       " 'fr_alkyl_carbamate',\n",
       " 'fr_alkyl_halide',\n",
       " 'fr_allylic_oxid',\n",
       " 'fr_amide',\n",
       " 'fr_amidine',\n",
       " 'fr_aniline',\n",
       " 'fr_aryl_methyl',\n",
       " 'fr_azide',\n",
       " 'fr_azo',\n",
       " 'fr_barbitur',\n",
       " 'fr_benzene',\n",
       " 'fr_benzodiazepine',\n",
       " 'fr_bicyclic',\n",
       " 'fr_diazo',\n",
       " 'fr_dihydropyridine',\n",
       " 'fr_epoxide',\n",
       " 'fr_ester',\n",
       " 'fr_ether',\n",
       " 'fr_furan',\n",
       " 'fr_guanido',\n",
       " 'fr_halogen',\n",
       " 'fr_hdrzine',\n",
       " 'fr_hdrzone',\n",
       " 'fr_imidazole',\n",
       " 'fr_imide',\n",
       " 'fr_isocyan',\n",
       " 'fr_isothiocyan',\n",
       " 'fr_ketone',\n",
       " 'fr_ketone_Topliss',\n",
       " 'fr_lactam',\n",
       " 'fr_lactone',\n",
       " 'fr_methoxy',\n",
       " 'fr_morpholine',\n",
       " 'fr_nitrile',\n",
       " 'fr_nitro',\n",
       " 'fr_nitro_arom',\n",
       " 'fr_nitro_arom_nonortho',\n",
       " 'fr_nitroso',\n",
       " 'fr_oxazole',\n",
       " 'fr_oxime',\n",
       " 'fr_para_hydroxylation',\n",
       " 'fr_phenol',\n",
       " 'fr_phenol_noOrthoHbond',\n",
       " 'fr_phos_acid',\n",
       " 'fr_phos_ester',\n",
       " 'fr_piperdine',\n",
       " 'fr_piperzine',\n",
       " 'fr_priamide',\n",
       " 'fr_prisulfonamd',\n",
       " 'fr_pyridine',\n",
       " 'fr_quatN',\n",
       " 'fr_sulfide',\n",
       " 'fr_sulfonamd',\n",
       " 'fr_sulfone',\n",
       " 'fr_term_acetylene',\n",
       " 'fr_tetrazole',\n",
       " 'fr_thiazole',\n",
       " 'fr_thiocyan',\n",
       " 'fr_thiophene',\n",
       " 'fr_unbrch_alkane',\n",
       " 'fr_urea']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#Get all the possible functions within the fr(fragments) module and put them into a list\n",
    "#This is called a list comprehension it says 'for each element in this list, do this'\n",
    "#E.g for each name in the directory of the fr module, if the function is callable (i.e. it has a value) then append it to the list\n",
    "function_names = [name for name in dir(fr) if callable(getattr(fr, name))]\n",
    "\n",
    "#Remove the first 2 functions (they are not important for us)\n",
    "function_names.pop(0)\n",
    "function_names.pop(0)\n",
    "\n",
    "function_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:08:13] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_sulfide</th>\n",
       "      <th>fr_sulfonamd</th>\n",
       "      <th>fr_sulfone</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FCOC(C(F)(F)F)C(F)(F)F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ClCCCCl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nc1ccc2ccccc2c1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCOC(=O)c1ccccc1C(=O)OCCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=c1nc([O-])n(Cl)c(=O)n1Cl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>O=C1OC(=O)C2CC=CCC12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CCOC(=O)N(C)N=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids  tox  tox_bin  \\\n",
       "0                               FCOC(C(F)(F)F)C(F)(F)F  0.0        0   \n",
       "1                                              ClCCCCl  0.0        0   \n",
       "2                                      Nc1ccc2ccccc2c1  1.0        1   \n",
       "3                           CCCOC(=O)c1ccccc1C(=O)OCCC  0.0        0   \n",
       "4                           O=c1nc([O-])n(Cl)c(=O)n1Cl  0.0        0   \n",
       "..                                                 ...  ...      ...   \n",
       "778  C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...  0.0        0   \n",
       "779  O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...  0.0        0   \n",
       "780                               O=C1OC(=O)C2CC=CCC12  0.0        0   \n",
       "781                     CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O  3.0        1   \n",
       "782                                    CCOC(=O)N(C)N=O  0.0        0   \n",
       "\n",
       "     fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  fr_ArN  fr_Ar_COO  fr_Ar_N  \\\n",
       "0          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "1          0.0       0.0              0.0     1.0        0.0      0.0   \n",
       "2          2.0       0.0              0.0     0.0        0.0      0.0   \n",
       "3          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "4          0.0       0.0              0.0     0.0        0.0      1.0   \n",
       "..         ...       ...              ...     ...        ...      ...   \n",
       "778        1.0       3.0              3.0     0.0        0.0      0.0   \n",
       "779        0.0       2.0              2.0     0.0        0.0      0.0   \n",
       "780        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "781        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "782        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "\n",
       "     fr_Ar_NH  ...  fr_sulfide  fr_sulfonamd  fr_sulfone  fr_term_acetylene  \\\n",
       "0         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "1         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "2         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "3         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "4         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "..        ...  ...         ...           ...         ...                ...   \n",
       "778       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "779       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "780       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "781       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "782       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "\n",
       "     fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  fr_unbrch_alkane  \\\n",
       "0             0.0          0.0          0.0           0.0              13.0   \n",
       "1             0.0          0.0          0.0           0.0               0.0   \n",
       "2             0.0          0.0          0.0           0.0               0.0   \n",
       "3             0.0          0.0          0.0           0.0               0.0   \n",
       "4             0.0          0.0          0.0           0.0               0.0   \n",
       "..            ...          ...          ...           ...               ...   \n",
       "778           0.0          0.0          0.0           0.0               0.0   \n",
       "779           0.0          0.0          0.0           0.0               0.0   \n",
       "780           0.0          0.0          0.0           0.0               0.0   \n",
       "781           0.0          0.0          0.0           0.0               0.0   \n",
       "782           0.0          0.0          0.0           0.0               0.0   \n",
       "\n",
       "     fr_urea  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "..       ...  \n",
       "778      0.0  \n",
       "779      0.0  \n",
       "780      0.0  \n",
       "781      0.0  \n",
       "782      0.0  \n",
       "\n",
       "[7831 rows x 88 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Iterate through the rows in the dataframe\n",
    "# For each row, create a molecule object from the smiles strings\n",
    "# For each function in the list of functions, get the value for that function (in this case the total number of each functional group)\n",
    "#Then add that value to the dataframe for that row with the column heading as the name of the function\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    mol = Chem.MolFromSmiles(row['ids']) # Create molecule object\n",
    "    for function in function_names: # For each function in functions\n",
    "        data.at[index, function] = getattr(fr, function)(mol) # Set the value in the dataframe at the molecule index and column name to the value of the function\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:09:21] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "#We have a list of functional groups, lets also get a binary value for if the molecule matches any of the tests below:\n",
    "#PAINS, BRENK, NIH\n",
    "\n",
    "from rdkit.Chem.FilterCatalog import FilterCatalog, FilterCatalogParams\n",
    "from rdkit.Chem import Descriptors as ds\n",
    "\n",
    "params_pains = FilterCatalogParams()\n",
    "params_pains.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS)\n",
    "pains = FilterCatalog(params_pains)\n",
    "\n",
    "params_brenk = FilterCatalogParams()\n",
    "params_brenk.AddCatalog(FilterCatalogParams.FilterCatalogs.BRENK)\n",
    "brenk = FilterCatalog(params_brenk)\n",
    "\n",
    "params_nih = FilterCatalogParams()\n",
    "params_nih.AddCatalog(FilterCatalogParams.FilterCatalogs.NIH)\n",
    "nih = FilterCatalog(params_nih)\n",
    "\n",
    "\n",
    "SMILES_strings = data['ids'] # Get smiles strings\n",
    "mol = [Chem.MolFromSmiles(formula) for formula in SMILES_strings] # Get a list of molecule objects\n",
    "\n",
    "for row in data.index: # For each row\n",
    "    molecule = mol[row] # Get the molecule\n",
    "    data.loc[row, 'pain'] = 1 if pains.HasMatch(molecule) else 0 # Does it match the PAINS filter?\n",
    "    data.loc[row, 'brenk'] = 1 if pains.HasMatch(molecule) else 0 # Does it match the BRENK filter?\n",
    "    data.loc[row, 'nih'] = 1 if pains.HasMatch(molecule) else 0 # Does it match the NIH filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "      <th>pain</th>\n",
       "      <th>brenk</th>\n",
       "      <th>nih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FCOC(C(F)(F)F)C(F)(F)F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ClCCCCl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nc1ccc2ccccc2c1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCOC(=O)c1ccccc1C(=O)OCCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O=c1nc([O-])n(Cl)c(=O)n1Cl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>O=C1OC(=O)C2CC=CCC12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CCOC(=O)N(C)N=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids  tox  tox_bin  \\\n",
       "0                               FCOC(C(F)(F)F)C(F)(F)F  0.0        0   \n",
       "1                                              ClCCCCl  0.0        0   \n",
       "2                                      Nc1ccc2ccccc2c1  1.0        1   \n",
       "3                           CCCOC(=O)c1ccccc1C(=O)OCCC  0.0        0   \n",
       "4                           O=c1nc([O-])n(Cl)c(=O)n1Cl  0.0        0   \n",
       "..                                                 ...  ...      ...   \n",
       "778  C[C@H](CCC(=O)[O-])[C@H]1CC[C@H]2[C@H]3[C@H](C...  0.0        0   \n",
       "779  O=C(N[C@H](CO)[C@H](O)c1ccc([N+](=O)[O-])cc1)C...  0.0        0   \n",
       "780                               O=C1OC(=O)C2CC=CCC12  0.0        0   \n",
       "781                     CCCC[Sn](CCCC)(OC(C)=O)OC(C)=O  3.0        1   \n",
       "782                                    CCOC(=O)N(C)N=O  0.0        0   \n",
       "\n",
       "     fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  fr_ArN  fr_Ar_COO  fr_Ar_N  \\\n",
       "0          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "1          0.0       0.0              0.0     1.0        0.0      0.0   \n",
       "2          2.0       0.0              0.0     0.0        0.0      0.0   \n",
       "3          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "4          0.0       0.0              0.0     0.0        0.0      1.0   \n",
       "..         ...       ...              ...     ...        ...      ...   \n",
       "778        1.0       3.0              3.0     0.0        0.0      0.0   \n",
       "779        0.0       2.0              2.0     0.0        0.0      0.0   \n",
       "780        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "781        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "782        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "\n",
       "     fr_Ar_NH  ...  fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  \\\n",
       "0         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "1         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "2         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "3         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "4         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "..        ...  ...                ...           ...          ...          ...   \n",
       "778       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "779       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "780       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "781       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "782       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "\n",
       "     fr_thiophene  fr_unbrch_alkane  fr_urea  pain  brenk  nih  \n",
       "0             0.0              13.0      0.0   0.0    0.0  0.0  \n",
       "1             0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "2             0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "3             0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "4             0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "..            ...               ...      ...   ...    ...  ...  \n",
       "778           0.0               0.0      0.0   1.0    1.0  1.0  \n",
       "779           0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "780           0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "781           0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "782           0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "\n",
       "[7831 rows x 91 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2393703b2c8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets have a look at the distribution of the data:\n",
    "\n",
    "percent_toxic = len(data[data['tox_bin'] == 1]) / len(data) # Divides the length of the dataframe where tox_bin == 1 (ie toxic) by the length of the dataframe (all mols)\n",
    "percent_non_toxic = 1 - percent_toxic\n",
    "\n",
    "\n",
    "#Lets plot the train_df as a stacked bar graph:\n",
    "ax=plt.bar('Toxicity',percent_toxic, label='Toxic')\n",
    "plt.bar('Toxicity',percent_non_toxic, bottom= percent_toxic, label='Non-Toxic')\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.ylabel(\"Molecules %\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this dataset is unbalanced so we will need to deal with this before training- but first we need to split the data so we can balance ONLY the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>fr_Ar_OH</th>\n",
       "      <th>fr_COO</th>\n",
       "      <th>fr_COO2</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "      <th>pain</th>\n",
       "      <th>brenk</th>\n",
       "      <th>nih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5903</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3756</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2285</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5033</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6264 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  fr_ArN  fr_Ar_COO  fr_Ar_N  \\\n",
       "232         0.0       2.0              2.0     0.0        0.0      0.0   \n",
       "6041        1.0       1.0              1.0     0.0        0.0      3.0   \n",
       "34          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "5903        0.0       1.0              1.0     0.0        0.0      0.0   \n",
       "1810        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "...         ...       ...              ...     ...        ...      ...   \n",
       "3756        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "2285        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "2073        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "5033        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "307         0.0       8.0              8.0     0.0        0.0      0.0   \n",
       "\n",
       "      fr_Ar_NH  fr_Ar_OH  fr_COO  fr_COO2  ...  fr_term_acetylene  \\\n",
       "232        0.0       0.0     0.0      0.0  ...                0.0   \n",
       "6041       0.0       0.0     1.0      1.0  ...                0.0   \n",
       "34         0.0       0.0     0.0      0.0  ...                0.0   \n",
       "5903       0.0       1.0     0.0      0.0  ...                0.0   \n",
       "1810       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "...        ...       ...     ...      ...  ...                ...   \n",
       "3756       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "2285       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "2073       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "5033       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "307        0.0       0.0     0.0      0.0  ...                0.0   \n",
       "\n",
       "      fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  fr_unbrch_alkane  \\\n",
       "232            0.0          0.0          0.0           0.0               0.0   \n",
       "6041           0.0          0.0          0.0           0.0               0.0   \n",
       "34             0.0          0.0          0.0           0.0               1.0   \n",
       "5903           0.0          0.0          0.0           0.0               6.0   \n",
       "1810           0.0          0.0          0.0           0.0               0.0   \n",
       "...            ...          ...          ...           ...               ...   \n",
       "3756           0.0          0.0          0.0           0.0               0.0   \n",
       "2285           0.0          0.0          0.0           0.0               0.0   \n",
       "2073           0.0          0.0          0.0           0.0               0.0   \n",
       "5033           0.0          0.0          0.0           0.0               0.0   \n",
       "307            0.0          0.0          0.0           0.0               0.0   \n",
       "\n",
       "      fr_urea  pain  brenk  nih  \n",
       "232       0.0   0.0    0.0  0.0  \n",
       "6041      0.0   0.0    0.0  0.0  \n",
       "34        0.0   0.0    0.0  0.0  \n",
       "5903      0.0   0.0    0.0  0.0  \n",
       "1810      0.0   0.0    0.0  0.0  \n",
       "...       ...   ...    ...  ...  \n",
       "3756      0.0   0.0    0.0  0.0  \n",
       "2285      0.0   0.0    0.0  0.0  \n",
       "2073      0.0   0.0    0.0  0.0  \n",
       "5033      0.0   0.0    0.0  0.0  \n",
       "307       0.0   0.0    0.0  0.0  \n",
       "\n",
       "[6264 rows x 88 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data['tox_bin'], test_size=0.2) # Split the entire dataset into train and test sets\n",
    "\n",
    "X_train = X_train.drop(['ids', 'tox', 'tox_bin'], axis=1) # Drop the targets (toxicity indicator) from the X values of the training set\n",
    "X_test = X_test.drop(['ids', 'tox', 'tox_bin'], axis=1) # Drop the targets (toxicity indicator) from the X values of the test set\n",
    "\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set:  (6264, 88)\n",
      "Shape of test set:  (1567, 88)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of train set: ', X_train.shape)\n",
    "print('Shape of test set: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23937560b48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets resample our data - there are a number of ways of doing this but the simplest is to jsut upsample the minority class\n",
    "#Ie duplicate the samples\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#First concatenate the training X and y into one dataframe\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "#Select the data in the majority and minority class\n",
    "df_majority = train_df[train_df['tox_bin']==0]\n",
    "df_minority = train_df[train_df['tox_bin']==1]\n",
    "\n",
    "#Then resample the minority class - this will duplicate randomly the training data in the minority class to match the number of samples in the majority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "#Then combine the majority and upsampled minority class\n",
    "train_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#Split them back into X and y values\n",
    "y_train = train_df['tox_bin']\n",
    "X_train = train_df.drop(['tox_bin'], axis=1)\n",
    "\n",
    "#Then lets do what we did above to check it has worked!\n",
    "percent_toxic = len(y_train[y_train == 1]) / len(y_train)\n",
    "percent_non_toxic = 1 - percent_toxic\n",
    "\n",
    "\n",
    "#Lets plot the train_df as a stacked bar graph:\n",
    "ax=plt.bar('Toxicity',percent_toxic, label='Toxic')\n",
    "plt.bar('Toxicity',percent_non_toxic, bottom= percent_toxic, label='Non-Toxic')\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.ylabel(\"Molecules %\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, this looks perfectly balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test set:  (783, 88)\n",
      "Shape of validation set:  (784, 88)\n"
     ]
    }
   ],
   "source": [
    "#Lets now split our test set into vaL and test sets\n",
    "#Test will be used to test our model at the END of training\n",
    "#Whereas validation will be used to test the model DURING training\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "print('Shape of test set: ', X_test.shape)\n",
    "print('Shape of validation set: ', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order for the model to work, we first have to convert our dataframes into tensors:\n",
    "\n",
    "X_train_array = X_train.values\n",
    "X_test_array = X_test.values\n",
    "X_val_array = X_val.values\n",
    "\n",
    "y_train_array = y_train.values\n",
    "y_test_array = y_test.values\n",
    "y_val_array = y_val.values\n",
    "\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_array)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_array)\n",
    "X_val_tensor = tf.convert_to_tensor(X_val_array)\n",
    "\n",
    "y_train_tensor = tf.convert_to_tensor(y_train_array)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test_array)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is simply creating a tensorflow keras model for our machine learning\n",
    "#I have initialised it inside a function as it will allow me to search through all different combinations of parameters\n",
    "# e.g number of dense neurones, dropout rate, etc.\n",
    "\n",
    "def init_keras_model(dense1=700, dense2=100, dropout=0.7, optimizer='adam', callbacks=[]):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dense1, activation='relu'), # Create a fully connected dense layer with dense1 number of neurons\n",
    "        tf.keras.layers.BatchNormalization(), # Batch normalisation reduces variability of our model\n",
    "        tf.keras.layers.Dropout(rate=dropout), # Dropout will reduce overtraining\n",
    "        tf.keras.layers.Dense(dense2, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(rate=dropout),\n",
    "        tf.keras.layers.Dense(50, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(rate=0.3),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid') # output layer\n",
    "    ])\n",
    "\n",
    "    # Call backs are a way of monitoring our model during training, for example to reduce the learning rate or save the best model as we go\n",
    "    #If callbacks is empty, compile the model without them\n",
    "    if callbacks == []: \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'],\n",
    "        )\n",
    "    else: # If we specify callbacks the model will be compiled with them\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7803 - accuracy: 0.5458 - val_loss: 0.6621 - val_accuracy: 0.6314\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7147 - accuracy: 0.5570 - val_loss: 0.6428 - val_accuracy: 0.6454\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.5867 - val_loss: 0.6389 - val_accuracy: 0.6429\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.6005 - val_loss: 0.6401 - val_accuracy: 0.6327\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6091 - val_loss: 0.6356 - val_accuracy: 0.6531\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6202 - val_loss: 0.6343 - val_accuracy: 0.6518\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6320 - val_loss: 0.6242 - val_accuracy: 0.6696\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6451 - val_loss: 0.6164 - val_accuracy: 0.6798\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6493 - val_loss: 0.6218 - val_accuracy: 0.6773\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6467 - val_loss: 0.6127 - val_accuracy: 0.6824\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6467 - val_loss: 0.6163 - val_accuracy: 0.6824\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6521 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6493 - val_loss: 0.6210 - val_accuracy: 0.6722\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6211 - accuracy: 0.6672 - val_loss: 0.6151 - val_accuracy: 0.6811\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6152 - accuracy: 0.6653 - val_loss: 0.6134 - val_accuracy: 0.6837\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6593 - val_loss: 0.6176 - val_accuracy: 0.6735\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6159 - accuracy: 0.6675 - val_loss: 0.6164 - val_accuracy: 0.6760\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6085 - accuracy: 0.6740 - val_loss: 0.6229 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6106 - accuracy: 0.6694 - val_loss: 0.6233 - val_accuracy: 0.6760\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6038 - accuracy: 0.6756 - val_loss: 0.6258 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6742 - val_loss: 0.6237 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6848 - val_loss: 0.6304 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5989 - accuracy: 0.6834 - val_loss: 0.6251 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5961 - accuracy: 0.6889 - val_loss: 0.6246 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5984 - accuracy: 0.6816 - val_loss: 0.6276 - val_accuracy: 0.6671\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5934 - accuracy: 0.6863 - val_loss: 0.6344 - val_accuracy: 0.6671\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5954 - accuracy: 0.6853 - val_loss: 0.6311 - val_accuracy: 0.6607\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5867 - accuracy: 0.6917 - val_loss: 0.6307 - val_accuracy: 0.6735\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5861 - accuracy: 0.6909 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5796 - accuracy: 0.6945 - val_loss: 0.6334 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5844 - accuracy: 0.6937 - val_loss: 0.6338 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5781 - accuracy: 0.7001 - val_loss: 0.6367 - val_accuracy: 0.6696\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5781 - accuracy: 0.7038 - val_loss: 0.6317 - val_accuracy: 0.6735\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5741 - accuracy: 0.6989 - val_loss: 0.6296 - val_accuracy: 0.6798\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5688 - accuracy: 0.7069 - val_loss: 0.6358 - val_accuracy: 0.6798\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5727 - accuracy: 0.7085 - val_loss: 0.6384 - val_accuracy: 0.6645\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5719 - accuracy: 0.7105 - val_loss: 0.6442 - val_accuracy: 0.6531\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5672 - accuracy: 0.7100 - val_loss: 0.6372 - val_accuracy: 0.6709\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5662 - accuracy: 0.7163 - val_loss: 0.6321 - val_accuracy: 0.6773\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5652 - accuracy: 0.7114 - val_loss: 0.6380 - val_accuracy: 0.6556\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5611 - accuracy: 0.7154 - val_loss: 0.6366 - val_accuracy: 0.6607\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.5580 - accuracy: 0.7209 - val_loss: 0.6410 - val_accuracy: 0.6773\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.5597 - accuracy: 0.7100 - val_loss: 0.6384 - val_accuracy: 0.6811\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5597 - accuracy: 0.7170 - val_loss: 0.6413 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5508 - accuracy: 0.7254 - val_loss: 0.6396 - val_accuracy: 0.6888\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5571 - accuracy: 0.7160 - val_loss: 0.6348 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5527 - accuracy: 0.7199 - val_loss: 0.6444 - val_accuracy: 0.6684\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5431 - accuracy: 0.7302 - val_loss: 0.6600 - val_accuracy: 0.6594\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5496 - accuracy: 0.7189 - val_loss: 0.6437 - val_accuracy: 0.6786\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5477 - accuracy: 0.7209 - val_loss: 0.6526 - val_accuracy: 0.6671\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5387 - accuracy: 0.7287 - val_loss: 0.6493 - val_accuracy: 0.6696\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5449 - accuracy: 0.7222 - val_loss: 0.6490 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5387 - accuracy: 0.7286 - val_loss: 0.6645 - val_accuracy: 0.6696\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5411 - accuracy: 0.7268 - val_loss: 0.6523 - val_accuracy: 0.6696\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5379 - accuracy: 0.7265 - val_loss: 0.6479 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5355 - accuracy: 0.7352 - val_loss: 0.6568 - val_accuracy: 0.6684\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5292 - accuracy: 0.7347 - val_loss: 0.6636 - val_accuracy: 0.6633\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5392 - accuracy: 0.7277 - val_loss: 0.6638 - val_accuracy: 0.6773\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5344 - accuracy: 0.7311 - val_loss: 0.6546 - val_accuracy: 0.6773\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5300 - accuracy: 0.7352 - val_loss: 0.6631 - val_accuracy: 0.6722\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5317 - accuracy: 0.7350 - val_loss: 0.6756 - val_accuracy: 0.6709\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5316 - accuracy: 0.7379 - val_loss: 0.6531 - val_accuracy: 0.6773\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5296 - accuracy: 0.7391 - val_loss: 0.6617 - val_accuracy: 0.6684\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5252 - accuracy: 0.7381 - val_loss: 0.6653 - val_accuracy: 0.6811\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5276 - accuracy: 0.7404 - val_loss: 0.6536 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7427 - val_loss: 0.6588 - val_accuracy: 0.6862\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5260 - accuracy: 0.7386 - val_loss: 0.6617 - val_accuracy: 0.6811\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5235 - accuracy: 0.7464 - val_loss: 0.6492 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5219 - accuracy: 0.7415 - val_loss: 0.6626 - val_accuracy: 0.6786\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5225 - accuracy: 0.7443 - val_loss: 0.6551 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5237 - accuracy: 0.7375 - val_loss: 0.6554 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7386 - val_loss: 0.6648 - val_accuracy: 0.6811\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5147 - accuracy: 0.7482 - val_loss: 0.6570 - val_accuracy: 0.6849\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7469 - val_loss: 0.6504 - val_accuracy: 0.6786\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7481 - val_loss: 0.6633 - val_accuracy: 0.6862\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7477 - val_loss: 0.6601 - val_accuracy: 0.6824\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5102 - accuracy: 0.7513 - val_loss: 0.6641 - val_accuracy: 0.6786\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7498 - val_loss: 0.6624 - val_accuracy: 0.6798\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5081 - accuracy: 0.7504 - val_loss: 0.6797 - val_accuracy: 0.6901\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7496 - val_loss: 0.6747 - val_accuracy: 0.6773\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7496 - val_loss: 0.6727 - val_accuracy: 0.6786\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5079 - accuracy: 0.7521 - val_loss: 0.6673 - val_accuracy: 0.6824\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7491 - val_loss: 0.6774 - val_accuracy: 0.6709\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7509 - val_loss: 0.6662 - val_accuracy: 0.6849\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5026 - accuracy: 0.7566 - val_loss: 0.6740 - val_accuracy: 0.6798\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5077 - accuracy: 0.7492 - val_loss: 0.6824 - val_accuracy: 0.6658\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5050 - accuracy: 0.7516 - val_loss: 0.6870 - val_accuracy: 0.6811\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5002 - accuracy: 0.7575 - val_loss: 0.6815 - val_accuracy: 0.6645\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4974 - accuracy: 0.7538 - val_loss: 0.6879 - val_accuracy: 0.6760\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4993 - accuracy: 0.7573 - val_loss: 0.6770 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5039 - accuracy: 0.7557 - val_loss: 0.6658 - val_accuracy: 0.6786\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7455 - val_loss: 0.6724 - val_accuracy: 0.6747\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5022 - accuracy: 0.7504 - val_loss: 0.6885 - val_accuracy: 0.6824\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5008 - accuracy: 0.7567 - val_loss: 0.6848 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4892 - accuracy: 0.7589 - val_loss: 0.6902 - val_accuracy: 0.6747\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4907 - accuracy: 0.7615 - val_loss: 0.6891 - val_accuracy: 0.6798\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.4939 - accuracy: 0.7601 - val_loss: 0.6898 - val_accuracy: 0.6811\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4999 - accuracy: 0.7514 - val_loss: 0.6801 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4976 - accuracy: 0.7576 - val_loss: 0.6888 - val_accuracy: 0.6747\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4988 - accuracy: 0.7538 - val_loss: 0.6827 - val_accuracy: 0.6722\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4930 - accuracy: 0.7591 - val_loss: 0.6919 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5003 - accuracy: 0.7561 - val_loss: 0.6926 - val_accuracy: 0.6798\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4944 - accuracy: 0.7581 - val_loss: 0.6865 - val_accuracy: 0.6786\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4962 - accuracy: 0.7625 - val_loss: 0.6811 - val_accuracy: 0.6786\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4908 - accuracy: 0.7631 - val_loss: 0.6873 - val_accuracy: 0.6735\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4877 - accuracy: 0.7638 - val_loss: 0.6944 - val_accuracy: 0.6747\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4933 - accuracy: 0.7601 - val_loss: 0.6824 - val_accuracy: 0.6952\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4854 - accuracy: 0.7681 - val_loss: 0.6907 - val_accuracy: 0.6849\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4933 - accuracy: 0.7606 - val_loss: 0.7027 - val_accuracy: 0.6773\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4818 - accuracy: 0.7689 - val_loss: 0.6933 - val_accuracy: 0.6849\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4871 - accuracy: 0.7679 - val_loss: 0.6870 - val_accuracy: 0.6773\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4826 - accuracy: 0.7633 - val_loss: 0.6995 - val_accuracy: 0.6747\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4874 - accuracy: 0.7619 - val_loss: 0.6759 - val_accuracy: 0.6913\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4903 - accuracy: 0.7621 - val_loss: 0.6899 - val_accuracy: 0.6849\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4818 - accuracy: 0.7654 - val_loss: 0.6986 - val_accuracy: 0.6658\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4870 - accuracy: 0.7646 - val_loss: 0.6984 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4802 - accuracy: 0.7679 - val_loss: 0.6919 - val_accuracy: 0.6607\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4861 - accuracy: 0.7639 - val_loss: 0.6975 - val_accuracy: 0.6798\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4849 - accuracy: 0.7625 - val_loss: 0.7122 - val_accuracy: 0.6837\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4871 - accuracy: 0.7705 - val_loss: 0.6863 - val_accuracy: 0.6798\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4861 - accuracy: 0.7665 - val_loss: 0.6945 - val_accuracy: 0.6735\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4765 - accuracy: 0.7695 - val_loss: 0.7005 - val_accuracy: 0.6824\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4761 - accuracy: 0.7670 - val_loss: 0.6981 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4773 - accuracy: 0.7719 - val_loss: 0.7054 - val_accuracy: 0.6747\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4748 - accuracy: 0.7764 - val_loss: 0.7081 - val_accuracy: 0.6747\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.4710 - accuracy: 0.7747 - val_loss: 0.7034 - val_accuracy: 0.6633\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4795 - accuracy: 0.7664 - val_loss: 0.7019 - val_accuracy: 0.6696\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4784 - accuracy: 0.7615 - val_loss: 0.7027 - val_accuracy: 0.6607\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4799 - accuracy: 0.7615 - val_loss: 0.6953 - val_accuracy: 0.6875\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4801 - accuracy: 0.7665 - val_loss: 0.6966 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4757 - accuracy: 0.7720 - val_loss: 0.6982 - val_accuracy: 0.6786\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4776 - accuracy: 0.7695 - val_loss: 0.7109 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4799 - accuracy: 0.7699 - val_loss: 0.7027 - val_accuracy: 0.6747\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4780 - accuracy: 0.7714 - val_loss: 0.6958 - val_accuracy: 0.6811\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4729 - accuracy: 0.7743 - val_loss: 0.7001 - val_accuracy: 0.6709\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4685 - accuracy: 0.7807 - val_loss: 0.7035 - val_accuracy: 0.6760\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4684 - accuracy: 0.7738 - val_loss: 0.7007 - val_accuracy: 0.6735\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4742 - accuracy: 0.7742 - val_loss: 0.6828 - val_accuracy: 0.6811\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4728 - accuracy: 0.7729 - val_loss: 0.6899 - val_accuracy: 0.6722\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4701 - accuracy: 0.7702 - val_loss: 0.6956 - val_accuracy: 0.6735\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4729 - accuracy: 0.7710 - val_loss: 0.6898 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4738 - accuracy: 0.7733 - val_loss: 0.7072 - val_accuracy: 0.6735\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4625 - accuracy: 0.7779 - val_loss: 0.7234 - val_accuracy: 0.6582\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4722 - accuracy: 0.7754 - val_loss: 0.7059 - val_accuracy: 0.6773\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4621 - accuracy: 0.7784 - val_loss: 0.7184 - val_accuracy: 0.6798\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4698 - accuracy: 0.7753 - val_loss: 0.7035 - val_accuracy: 0.6696\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4700 - accuracy: 0.7719 - val_loss: 0.7051 - val_accuracy: 0.6735\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.4732 - accuracy: 0.7754 - val_loss: 0.6977 - val_accuracy: 0.6722\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4550 - accuracy: 0.7817 - val_loss: 0.7193 - val_accuracy: 0.6786\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4677 - accuracy: 0.7781 - val_loss: 0.7012 - val_accuracy: 0.6722\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4619 - accuracy: 0.7795 - val_loss: 0.7149 - val_accuracy: 0.6760\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4626 - accuracy: 0.7787 - val_loss: 0.7196 - val_accuracy: 0.6735\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4732 - accuracy: 0.7783 - val_loss: 0.6933 - val_accuracy: 0.6862\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4586 - accuracy: 0.7779 - val_loss: 0.7088 - val_accuracy: 0.6824\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4568 - accuracy: 0.7837 - val_loss: 0.7249 - val_accuracy: 0.6786\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4707 - accuracy: 0.7670 - val_loss: 0.7104 - val_accuracy: 0.6747\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4628 - accuracy: 0.7772 - val_loss: 0.6942 - val_accuracy: 0.6837\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4632 - accuracy: 0.7798 - val_loss: 0.7152 - val_accuracy: 0.6722\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4542 - accuracy: 0.7846 - val_loss: 0.7328 - val_accuracy: 0.6786\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.4543 - accuracy: 0.7782 - val_loss: 0.7311 - val_accuracy: 0.6798\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4674 - accuracy: 0.7744 - val_loss: 0.7126 - val_accuracy: 0.6811\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4627 - accuracy: 0.7796 - val_loss: 0.7154 - val_accuracy: 0.6760\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4644 - accuracy: 0.7724 - val_loss: 0.7265 - val_accuracy: 0.6837\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4617 - accuracy: 0.7796 - val_loss: 0.7146 - val_accuracy: 0.6798\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4562 - accuracy: 0.7865 - val_loss: 0.7211 - val_accuracy: 0.6747\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4578 - accuracy: 0.7817 - val_loss: 0.7149 - val_accuracy: 0.6722\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4651 - accuracy: 0.7806 - val_loss: 0.7036 - val_accuracy: 0.6798\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4579 - accuracy: 0.7821 - val_loss: 0.7130 - val_accuracy: 0.6875\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.4563 - accuracy: 0.7805 - val_loss: 0.7136 - val_accuracy: 0.6760\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4545 - accuracy: 0.7802 - val_loss: 0.7282 - val_accuracy: 0.6760\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4651 - accuracy: 0.7803 - val_loss: 0.7095 - val_accuracy: 0.6824\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4636 - accuracy: 0.7748 - val_loss: 0.7023 - val_accuracy: 0.6798\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4643 - accuracy: 0.7778 - val_loss: 0.7175 - val_accuracy: 0.6837\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4489 - accuracy: 0.7870 - val_loss: 0.7332 - val_accuracy: 0.6735\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4598 - accuracy: 0.7807 - val_loss: 0.6960 - val_accuracy: 0.6786\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4521 - accuracy: 0.7826 - val_loss: 0.7245 - val_accuracy: 0.6620\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4591 - accuracy: 0.7812 - val_loss: 0.7073 - val_accuracy: 0.6709\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4585 - accuracy: 0.7815 - val_loss: 0.7242 - val_accuracy: 0.6735\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4507 - accuracy: 0.7842 - val_loss: 0.7112 - val_accuracy: 0.6786\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4576 - accuracy: 0.7802 - val_loss: 0.7235 - val_accuracy: 0.6709\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4541 - accuracy: 0.7859 - val_loss: 0.7069 - val_accuracy: 0.6569\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4490 - accuracy: 0.7827 - val_loss: 0.7435 - val_accuracy: 0.6633\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4569 - accuracy: 0.7840 - val_loss: 0.7163 - val_accuracy: 0.6773\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4527 - accuracy: 0.7881 - val_loss: 0.7241 - val_accuracy: 0.6722\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4603 - accuracy: 0.7812 - val_loss: 0.7226 - val_accuracy: 0.6645\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4547 - accuracy: 0.7838 - val_loss: 0.7368 - val_accuracy: 0.6747\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4469 - accuracy: 0.7919 - val_loss: 0.7261 - val_accuracy: 0.6696\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4607 - accuracy: 0.7820 - val_loss: 0.7299 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4493 - accuracy: 0.7860 - val_loss: 0.6996 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4573 - accuracy: 0.7793 - val_loss: 0.7262 - val_accuracy: 0.6811\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4500 - accuracy: 0.7865 - val_loss: 0.7145 - val_accuracy: 0.6645\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4548 - accuracy: 0.7838 - val_loss: 0.7198 - val_accuracy: 0.6786\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4553 - accuracy: 0.7820 - val_loss: 0.7138 - val_accuracy: 0.6709\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4495 - accuracy: 0.7841 - val_loss: 0.7345 - val_accuracy: 0.6709\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4511 - accuracy: 0.7828 - val_loss: 0.7189 - val_accuracy: 0.6709\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4518 - accuracy: 0.7859 - val_loss: 0.7160 - val_accuracy: 0.6684\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4541 - accuracy: 0.7894 - val_loss: 0.7244 - val_accuracy: 0.6786\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4503 - accuracy: 0.7895 - val_loss: 0.7086 - val_accuracy: 0.6735\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4410 - accuracy: 0.7940 - val_loss: 0.7284 - val_accuracy: 0.6709\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4463 - accuracy: 0.7869 - val_loss: 0.7193 - val_accuracy: 0.6811\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001 # This is how fast the model should learn\n",
    "n_epochs = 200 # This is how many times to train the model \n",
    "# (more epochs may lead to better accuracy but will also lead to overfitting)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE) # The optimizer is Adam and is the algorithm used to train the model\n",
    "lrr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=15, verbose=1, factor=0.5, min_lr=0.00001) # Reduce learning rate on plateau tells the model to reduce the speed at which it learns when it gets close to converging.\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath='weights_chk.h5',\n",
    "                                        monitor='val_accuracy',\n",
    "                                        verbose=0,\n",
    "                                        save_best_only=True,\n",
    "                                        save_weights_only=True,\n",
    "                                        mode='max') # Model Checkpoint is used to save the best model during training\n",
    "\n",
    "\n",
    "\n",
    "#precision = tf.keras.metrics.Precision()\n",
    "#recall = tf.keras.metrics.Recall()\n",
    "\n",
    "keras_model = init_keras_model() # Initialise the model\n",
    "\n",
    "\n",
    "history = keras_model.fit(X_train_tensor, \n",
    "                          y_train_tensor, \n",
    "                          epochs=n_epochs, \n",
    "                          validation_data=(X_val_tensor, y_val_tensor), \n",
    "                          ) # Fit the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss & accuracy\n",
    "\n",
    "This will plot the loss (how far each prediction is from the truth) - lower is better\n",
    "And the accuracy measures the % of correct predictions - higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "# Plot the training loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss', c='red')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "\n",
    "ax2=ax1.twinx()\n",
    "\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', c='blue')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "plt.title('Training Loss/Accuracy')\n",
    "\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='center right')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training accuracy and validation accuracy\n",
    "\n",
    "This will tell us if the model is overfitting (training accuracy increases and val accuracy decreases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e55952ba58>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy', c='orange')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', c='purple')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS BLOCK WILL TAKE A LONG TIME, DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for: 700 100 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_132 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8133 - accuracy: 0.5534 - val_loss: 0.6403 - val_accuracy: 0.6543\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7064 - accuracy: 0.5697 - val_loss: 0.6339 - val_accuracy: 0.6684\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6776 - accuracy: 0.5836 - val_loss: 0.6391 - val_accuracy: 0.6633\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5942 - val_loss: 0.6358 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6112 - val_loss: 0.6298 - val_accuracy: 0.6786\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6150 - val_loss: 0.6304 - val_accuracy: 0.6760\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6011 - val_loss: 0.6292 - val_accuracy: 0.6824\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6169 - val_loss: 0.6274 - val_accuracy: 0.6747\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6215 - val_loss: 0.6301 - val_accuracy: 0.6786\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6226 - val_loss: 0.6292 - val_accuracy: 0.6824\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6189 - val_loss: 0.6249 - val_accuracy: 0.6786\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6255 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6315 - val_loss: 0.6278 - val_accuracy: 0.6849\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6304 - val_loss: 0.6239 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6299 - val_loss: 0.6263 - val_accuracy: 0.6798\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6339 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6301 - val_loss: 0.6270 - val_accuracy: 0.6824\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6394 - val_loss: 0.6206 - val_accuracy: 0.6875\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6392 - val_loss: 0.6259 - val_accuracy: 0.6875\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6353 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6400 - val_loss: 0.6205 - val_accuracy: 0.6952\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6385 - val_loss: 0.6239 - val_accuracy: 0.6837\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6403 - val_loss: 0.6216 - val_accuracy: 0.6875\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6497 - val_loss: 0.6212 - val_accuracy: 0.6862\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6451 - val_loss: 0.6190 - val_accuracy: 0.6964\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6559 - val_loss: 0.6184 - val_accuracy: 0.6901\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6500 - val_loss: 0.6198 - val_accuracy: 0.6952\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6482 - val_loss: 0.6178 - val_accuracy: 0.6952\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6491 - val_loss: 0.6213 - val_accuracy: 0.6939\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6583 - val_loss: 0.6170 - val_accuracy: 0.6952\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6574 - val_loss: 0.6163 - val_accuracy: 0.6990\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6565 - val_loss: 0.6172 - val_accuracy: 0.7028\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6505 - val_loss: 0.6158 - val_accuracy: 0.6952\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6507 - val_loss: 0.6168 - val_accuracy: 0.7003\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6555 - val_loss: 0.6171 - val_accuracy: 0.6977\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6629 - val_loss: 0.6175 - val_accuracy: 0.6913\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6583 - val_loss: 0.6211 - val_accuracy: 0.6926\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6198 - accuracy: 0.6576 - val_loss: 0.6176 - val_accuracy: 0.6939\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6567 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6613 - val_loss: 0.6138 - val_accuracy: 0.6952\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6662 - val_loss: 0.6171 - val_accuracy: 0.6926\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6650 - val_loss: 0.6169 - val_accuracy: 0.7003\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6738 - val_loss: 0.6160 - val_accuracy: 0.6901\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6073 - accuracy: 0.6750 - val_loss: 0.6144 - val_accuracy: 0.6901\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6137 - accuracy: 0.6620 - val_loss: 0.6212 - val_accuracy: 0.6926\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.6670 - val_loss: 0.6150 - val_accuracy: 0.6939\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6067 - accuracy: 0.6713 - val_loss: 0.6174 - val_accuracy: 0.6939\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6652 - val_loss: 0.6139 - val_accuracy: 0.6964\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6098 - accuracy: 0.6713 - val_loss: 0.6128 - val_accuracy: 0.6977\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6060 - accuracy: 0.6728 - val_loss: 0.6158 - val_accuracy: 0.6977\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6073 - accuracy: 0.6722 - val_loss: 0.6221 - val_accuracy: 0.6888\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6086 - accuracy: 0.6677 - val_loss: 0.6197 - val_accuracy: 0.6952\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6006 - accuracy: 0.6751 - val_loss: 0.6191 - val_accuracy: 0.6901\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6011 - accuracy: 0.6801 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6048 - accuracy: 0.6747 - val_loss: 0.6217 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6033 - accuracy: 0.6786 - val_loss: 0.6172 - val_accuracy: 0.6939\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5990 - accuracy: 0.6814 - val_loss: 0.6210 - val_accuracy: 0.6888\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6020 - accuracy: 0.6778 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6024 - accuracy: 0.6786 - val_loss: 0.6204 - val_accuracy: 0.6964\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5988 - accuracy: 0.6776 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5999 - accuracy: 0.6806 - val_loss: 0.6274 - val_accuracy: 0.6798\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5971 - accuracy: 0.6856 - val_loss: 0.6221 - val_accuracy: 0.6862\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5901 - accuracy: 0.6876 - val_loss: 0.6235 - val_accuracy: 0.6824\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5951 - accuracy: 0.6876 - val_loss: 0.6233 - val_accuracy: 0.6901\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5964 - accuracy: 0.6800 - val_loss: 0.6239 - val_accuracy: 0.6952\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5966 - accuracy: 0.6859 - val_loss: 0.6279 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5920 - accuracy: 0.6925 - val_loss: 0.6262 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5942 - accuracy: 0.6822 - val_loss: 0.6276 - val_accuracy: 0.6888\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5882 - accuracy: 0.6938 - val_loss: 0.6268 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5860 - accuracy: 0.6907 - val_loss: 0.6240 - val_accuracy: 0.6875\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5859 - accuracy: 0.6966 - val_loss: 0.6261 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5838 - accuracy: 0.6959 - val_loss: 0.6260 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5850 - accuracy: 0.6938 - val_loss: 0.6276 - val_accuracy: 0.6837\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5842 - accuracy: 0.6953 - val_loss: 0.6315 - val_accuracy: 0.6837\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5803 - accuracy: 0.6971 - val_loss: 0.6269 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5854 - accuracy: 0.6886 - val_loss: 0.6335 - val_accuracy: 0.6798\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5825 - accuracy: 0.6952 - val_loss: 0.6270 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5776 - accuracy: 0.6991 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5797 - accuracy: 0.6924 - val_loss: 0.6302 - val_accuracy: 0.6849\n",
      "Calculating for: 700 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_136 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8537 - accuracy: 0.5237 - val_loss: 0.6479 - val_accuracy: 0.6390\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.7240 - accuracy: 0.5501 - val_loss: 0.6423 - val_accuracy: 0.6467\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6984 - accuracy: 0.5558 - val_loss: 0.6462 - val_accuracy: 0.6505\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6860 - accuracy: 0.5569 - val_loss: 0.6497 - val_accuracy: 0.6531\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6782 - accuracy: 0.5698 - val_loss: 0.6446 - val_accuracy: 0.6518\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5727 - val_loss: 0.6492 - val_accuracy: 0.6645\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6758 - accuracy: 0.5786 - val_loss: 0.6472 - val_accuracy: 0.6645\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6721 - accuracy: 0.5874 - val_loss: 0.6437 - val_accuracy: 0.6531\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5909 - val_loss: 0.6458 - val_accuracy: 0.6594\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6720 - accuracy: 0.5878 - val_loss: 0.6458 - val_accuracy: 0.6582\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6680 - accuracy: 0.5993 - val_loss: 0.6367 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6684 - accuracy: 0.5961 - val_loss: 0.6463 - val_accuracy: 0.6518\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6040 - val_loss: 0.6413 - val_accuracy: 0.6582\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6031 - val_loss: 0.6377 - val_accuracy: 0.6620\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6004 - val_loss: 0.6376 - val_accuracy: 0.6594\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5975 - val_loss: 0.6435 - val_accuracy: 0.6671\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6045 - val_loss: 0.6346 - val_accuracy: 0.6696\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.6024 - val_loss: 0.6342 - val_accuracy: 0.6709\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6017 - val_loss: 0.6401 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6609 - accuracy: 0.6058 - val_loss: 0.6328 - val_accuracy: 0.6658\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6592 - accuracy: 0.6073 - val_loss: 0.6371 - val_accuracy: 0.6684\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6076 - val_loss: 0.6399 - val_accuracy: 0.6735\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6104 - val_loss: 0.6386 - val_accuracy: 0.6696\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6103 - val_loss: 0.6356 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6569 - accuracy: 0.6154 - val_loss: 0.6402 - val_accuracy: 0.6696\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6560 - accuracy: 0.6216 - val_loss: 0.6389 - val_accuracy: 0.6620\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6566 - accuracy: 0.6199 - val_loss: 0.6298 - val_accuracy: 0.6811\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6587 - accuracy: 0.6122 - val_loss: 0.6402 - val_accuracy: 0.6620\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6559 - accuracy: 0.6154 - val_loss: 0.6330 - val_accuracy: 0.6798\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6171 - val_loss: 0.6278 - val_accuracy: 0.6786\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6548 - accuracy: 0.6129 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6216 - val_loss: 0.6267 - val_accuracy: 0.6849\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6203 - val_loss: 0.6276 - val_accuracy: 0.6747\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6496 - accuracy: 0.6262 - val_loss: 0.6326 - val_accuracy: 0.6735\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6236 - val_loss: 0.6293 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6231 - val_loss: 0.6299 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6489 - accuracy: 0.6289 - val_loss: 0.6312 - val_accuracy: 0.6709\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6498 - accuracy: 0.6241 - val_loss: 0.6317 - val_accuracy: 0.6620\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6281 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6503 - accuracy: 0.6323 - val_loss: 0.6294 - val_accuracy: 0.6747\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6222 - val_loss: 0.6298 - val_accuracy: 0.6684\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6502 - accuracy: 0.6211 - val_loss: 0.6283 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6321 - val_loss: 0.6320 - val_accuracy: 0.6607\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6258 - val_loss: 0.6302 - val_accuracy: 0.6633\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6487 - accuracy: 0.6323 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6445 - accuracy: 0.6329 - val_loss: 0.6214 - val_accuracy: 0.6837\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6345 - val_loss: 0.6292 - val_accuracy: 0.6633\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6472 - accuracy: 0.6311 - val_loss: 0.6257 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6323 - val_loss: 0.6249 - val_accuracy: 0.6811\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6343 - val_loss: 0.6257 - val_accuracy: 0.6760\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6414 - accuracy: 0.6390 - val_loss: 0.6247 - val_accuracy: 0.6786\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6465 - accuracy: 0.6331 - val_loss: 0.6238 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6425 - accuracy: 0.6361 - val_loss: 0.6188 - val_accuracy: 0.6773\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6384 - val_loss: 0.6223 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6341 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6419 - accuracy: 0.6360 - val_loss: 0.6167 - val_accuracy: 0.6811\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6412 - val_loss: 0.6210 - val_accuracy: 0.6811\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6456 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6374 - val_loss: 0.6253 - val_accuracy: 0.6773\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6393 - val_loss: 0.6206 - val_accuracy: 0.6837\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6373 - val_loss: 0.6175 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6410 - val_loss: 0.6201 - val_accuracy: 0.6862\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6400 - accuracy: 0.6442 - val_loss: 0.6168 - val_accuracy: 0.6786\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6385 - accuracy: 0.6427 - val_loss: 0.6227 - val_accuracy: 0.6811\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6436 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6446 - val_loss: 0.6236 - val_accuracy: 0.6786\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6473 - val_loss: 0.6211 - val_accuracy: 0.6811\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6480 - val_loss: 0.6185 - val_accuracy: 0.6849\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6420 - val_loss: 0.6125 - val_accuracy: 0.6888\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6451 - val_loss: 0.6170 - val_accuracy: 0.6875\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6368 - accuracy: 0.6409 - val_loss: 0.6204 - val_accuracy: 0.6849\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6428 - val_loss: 0.6172 - val_accuracy: 0.6862\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6490 - val_loss: 0.6236 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6510 - val_loss: 0.6198 - val_accuracy: 0.6862\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6433 - val_loss: 0.6231 - val_accuracy: 0.6901\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6507 - val_loss: 0.6255 - val_accuracy: 0.6773\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6485 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6512 - val_loss: 0.6153 - val_accuracy: 0.6926\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6500 - val_loss: 0.6202 - val_accuracy: 0.6862\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6532 - val_loss: 0.6185 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6541 - val_loss: 0.6187 - val_accuracy: 0.6875\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6572 - val_loss: 0.6158 - val_accuracy: 0.6926\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6574 - val_loss: 0.6113 - val_accuracy: 0.6952\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6284 - accuracy: 0.6540 - val_loss: 0.6115 - val_accuracy: 0.6964\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6500 - val_loss: 0.6161 - val_accuracy: 0.6926\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6576 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6262 - accuracy: 0.6560 - val_loss: 0.6153 - val_accuracy: 0.6901\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6320 - accuracy: 0.6559 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6259 - accuracy: 0.6570 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6611 - val_loss: 0.6138 - val_accuracy: 0.6939\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6606 - val_loss: 0.6138 - val_accuracy: 0.6862\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6556 - val_loss: 0.6156 - val_accuracy: 0.6837\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6579 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6550 - val_loss: 0.6103 - val_accuracy: 0.6926\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6282 - accuracy: 0.6610 - val_loss: 0.6173 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6614 - val_loss: 0.6130 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6657 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6253 - accuracy: 0.6623 - val_loss: 0.6173 - val_accuracy: 0.6888\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6561 - val_loss: 0.6156 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6628 - val_loss: 0.6122 - val_accuracy: 0.6888\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6692 - val_loss: 0.6126 - val_accuracy: 0.6926\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6611 - val_loss: 0.6141 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6598 - val_loss: 0.6155 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6643 - val_loss: 0.6135 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6616 - val_loss: 0.6190 - val_accuracy: 0.6773\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6691 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6644 - val_loss: 0.6208 - val_accuracy: 0.6786\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6577 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6600 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6625 - val_loss: 0.6173 - val_accuracy: 0.6837\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6670 - val_loss: 0.6188 - val_accuracy: 0.6862\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6171 - accuracy: 0.6670 - val_loss: 0.6207 - val_accuracy: 0.6811\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.6692 - val_loss: 0.6115 - val_accuracy: 0.6913\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6629 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6727 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6663 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6664 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6615 - val_loss: 0.6146 - val_accuracy: 0.6913\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6712 - val_loss: 0.6197 - val_accuracy: 0.6722\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.6712 - val_loss: 0.6206 - val_accuracy: 0.6786\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6728 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.6728 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6134 - accuracy: 0.6707 - val_loss: 0.6168 - val_accuracy: 0.6862\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6142 - accuracy: 0.6682 - val_loss: 0.6184 - val_accuracy: 0.6888\n",
      "Calculating for: 700 100 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_140 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8488 - accuracy: 0.5085 - val_loss: 0.6684 - val_accuracy: 0.6212\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7409 - accuracy: 0.5149 - val_loss: 0.6685 - val_accuracy: 0.6173\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7102 - accuracy: 0.5151 - val_loss: 0.6778 - val_accuracy: 0.6250\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6996 - accuracy: 0.5225 - val_loss: 0.6755 - val_accuracy: 0.6212\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5180 - val_loss: 0.6765 - val_accuracy: 0.6224\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5232 - val_loss: 0.6674 - val_accuracy: 0.6199\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6905 - accuracy: 0.5310 - val_loss: 0.6712 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5178 - val_loss: 0.6727 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5300 - val_loss: 0.6668 - val_accuracy: 0.6199\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5291 - val_loss: 0.6713 - val_accuracy: 0.6212\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5207 - val_loss: 0.6700 - val_accuracy: 0.6212\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5319 - val_loss: 0.6685 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6879 - accuracy: 0.5416 - val_loss: 0.6634 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5322 - val_loss: 0.6685 - val_accuracy: 0.6237\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5391 - val_loss: 0.6682 - val_accuracy: 0.6250\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6900 - accuracy: 0.5353 - val_loss: 0.6689 - val_accuracy: 0.6237\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6889 - accuracy: 0.5433 - val_loss: 0.6648 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5403 - val_loss: 0.6664 - val_accuracy: 0.6224\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.5339 - val_loss: 0.6617 - val_accuracy: 0.6237\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5328 - val_loss: 0.6650 - val_accuracy: 0.6263\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6863 - accuracy: 0.5528 - val_loss: 0.6612 - val_accuracy: 0.6237\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.5515 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6879 - accuracy: 0.5394 - val_loss: 0.6632 - val_accuracy: 0.6263\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6873 - accuracy: 0.5445 - val_loss: 0.6632 - val_accuracy: 0.6263\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5526 - val_loss: 0.6592 - val_accuracy: 0.6263\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6882 - accuracy: 0.5418 - val_loss: 0.6646 - val_accuracy: 0.6263\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6875 - accuracy: 0.5406 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6875 - accuracy: 0.5471 - val_loss: 0.6618 - val_accuracy: 0.6301\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6844 - accuracy: 0.5504 - val_loss: 0.6618 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6846 - accuracy: 0.5539 - val_loss: 0.6608 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5463 - val_loss: 0.6588 - val_accuracy: 0.6390\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5538 - val_loss: 0.6595 - val_accuracy: 0.6403\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6873 - accuracy: 0.5441 - val_loss: 0.6606 - val_accuracy: 0.6327\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6835 - accuracy: 0.5531 - val_loss: 0.6569 - val_accuracy: 0.6378\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5509 - val_loss: 0.6612 - val_accuracy: 0.6416\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5452 - val_loss: 0.6596 - val_accuracy: 0.6416\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6864 - accuracy: 0.5453 - val_loss: 0.6591 - val_accuracy: 0.6416\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6835 - accuracy: 0.5580 - val_loss: 0.6587 - val_accuracy: 0.6429\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5569 - val_loss: 0.6565 - val_accuracy: 0.6429\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6818 - accuracy: 0.5620 - val_loss: 0.6525 - val_accuracy: 0.6403\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6820 - accuracy: 0.5602 - val_loss: 0.6562 - val_accuracy: 0.6403\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6851 - accuracy: 0.5520 - val_loss: 0.6556 - val_accuracy: 0.6429\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6806 - accuracy: 0.5649 - val_loss: 0.6530 - val_accuracy: 0.6441\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.5569 - val_loss: 0.6548 - val_accuracy: 0.6429\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5592 - val_loss: 0.6533 - val_accuracy: 0.6416\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6832 - accuracy: 0.5605 - val_loss: 0.6550 - val_accuracy: 0.6441\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.5622 - val_loss: 0.6520 - val_accuracy: 0.6429\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6819 - accuracy: 0.5659 - val_loss: 0.6511 - val_accuracy: 0.6441\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6805 - accuracy: 0.5662 - val_loss: 0.6521 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.5653 - val_loss: 0.6528 - val_accuracy: 0.6429\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5556 - val_loss: 0.6572 - val_accuracy: 0.6429\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6811 - accuracy: 0.5653 - val_loss: 0.6536 - val_accuracy: 0.6416\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6793 - accuracy: 0.5602 - val_loss: 0.6523 - val_accuracy: 0.6403\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6773 - accuracy: 0.5721 - val_loss: 0.6549 - val_accuracy: 0.6403\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6803 - accuracy: 0.5678 - val_loss: 0.6520 - val_accuracy: 0.6429\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6812 - accuracy: 0.5602 - val_loss: 0.6557 - val_accuracy: 0.6416\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6791 - accuracy: 0.5654 - val_loss: 0.6535 - val_accuracy: 0.6403\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6784 - accuracy: 0.5691 - val_loss: 0.6517 - val_accuracy: 0.6429\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5735 - val_loss: 0.6528 - val_accuracy: 0.6416\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.5679 - val_loss: 0.6493 - val_accuracy: 0.6429\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6780 - accuracy: 0.5706 - val_loss: 0.6509 - val_accuracy: 0.6416\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6762 - accuracy: 0.5776 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6776 - accuracy: 0.5770 - val_loss: 0.6524 - val_accuracy: 0.6429\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6785 - accuracy: 0.5653 - val_loss: 0.6505 - val_accuracy: 0.6390\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.5759 - val_loss: 0.6470 - val_accuracy: 0.6403\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6764 - accuracy: 0.5749 - val_loss: 0.6488 - val_accuracy: 0.6390\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5733 - val_loss: 0.6485 - val_accuracy: 0.6441\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6756 - accuracy: 0.5756 - val_loss: 0.6452 - val_accuracy: 0.6429\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6763 - accuracy: 0.5809 - val_loss: 0.6472 - val_accuracy: 0.6416\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6765 - accuracy: 0.5759 - val_loss: 0.6446 - val_accuracy: 0.6429\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5813 - val_loss: 0.6495 - val_accuracy: 0.6467\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.5808 - val_loss: 0.6465 - val_accuracy: 0.6454\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5808 - val_loss: 0.6451 - val_accuracy: 0.6441\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5823 - val_loss: 0.6476 - val_accuracy: 0.6492\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6751 - accuracy: 0.5774 - val_loss: 0.6477 - val_accuracy: 0.6441\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5801 - val_loss: 0.6454 - val_accuracy: 0.6403\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5740 - val_loss: 0.6445 - val_accuracy: 0.6454\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5847 - val_loss: 0.6430 - val_accuracy: 0.6429\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5826 - val_loss: 0.6421 - val_accuracy: 0.6467\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6742 - accuracy: 0.5785 - val_loss: 0.6473 - val_accuracy: 0.6492\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5819 - val_loss: 0.6432 - val_accuracy: 0.6441\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.5781 - val_loss: 0.6439 - val_accuracy: 0.6429\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6726 - accuracy: 0.5836 - val_loss: 0.6444 - val_accuracy: 0.6518\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6727 - accuracy: 0.5873 - val_loss: 0.6427 - val_accuracy: 0.6441\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.5854 - val_loss: 0.6460 - val_accuracy: 0.6531\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6738 - accuracy: 0.5831 - val_loss: 0.6460 - val_accuracy: 0.6518\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6690 - accuracy: 0.5922 - val_loss: 0.6439 - val_accuracy: 0.6518\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6728 - accuracy: 0.5889 - val_loss: 0.6422 - val_accuracy: 0.6492\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5899 - val_loss: 0.6405 - val_accuracy: 0.6480\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.5761 - val_loss: 0.6440 - val_accuracy: 0.6492\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5884 - val_loss: 0.6411 - val_accuracy: 0.6505\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6738 - accuracy: 0.5843 - val_loss: 0.6433 - val_accuracy: 0.6467\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5835 - val_loss: 0.6442 - val_accuracy: 0.6467\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5838 - val_loss: 0.6434 - val_accuracy: 0.6543\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5934 - val_loss: 0.6409 - val_accuracy: 0.6582\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6718 - accuracy: 0.5854 - val_loss: 0.6431 - val_accuracy: 0.6531\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6727 - accuracy: 0.5872 - val_loss: 0.6422 - val_accuracy: 0.6492\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5978 - val_loss: 0.6414 - val_accuracy: 0.6684\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6021 - val_loss: 0.6382 - val_accuracy: 0.6518\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5885 - val_loss: 0.6398 - val_accuracy: 0.6684\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5926 - val_loss: 0.6392 - val_accuracy: 0.6531\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5859 - val_loss: 0.6426 - val_accuracy: 0.6556\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6671 - accuracy: 0.5908 - val_loss: 0.6386 - val_accuracy: 0.6645\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5968 - val_loss: 0.6381 - val_accuracy: 0.6531\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.5997 - val_loss: 0.6387 - val_accuracy: 0.6505\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6675 - accuracy: 0.5918 - val_loss: 0.6371 - val_accuracy: 0.6633\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5878 - val_loss: 0.6416 - val_accuracy: 0.6684\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5932 - val_loss: 0.6370 - val_accuracy: 0.6594\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6034 - val_loss: 0.6342 - val_accuracy: 0.6696\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5892 - val_loss: 0.6365 - val_accuracy: 0.6722\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5947 - val_loss: 0.6358 - val_accuracy: 0.6696\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.6002 - val_loss: 0.6362 - val_accuracy: 0.6671\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6639 - accuracy: 0.6021 - val_loss: 0.6347 - val_accuracy: 0.6645\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6698 - accuracy: 0.5934 - val_loss: 0.6393 - val_accuracy: 0.6582\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5987 - val_loss: 0.6339 - val_accuracy: 0.6633\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6689 - accuracy: 0.5962 - val_loss: 0.6360 - val_accuracy: 0.6696\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6039 - val_loss: 0.6353 - val_accuracy: 0.6722\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.5968 - val_loss: 0.6331 - val_accuracy: 0.6633\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6671 - accuracy: 0.6060 - val_loss: 0.6360 - val_accuracy: 0.6735\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6654 - accuracy: 0.6035 - val_loss: 0.6361 - val_accuracy: 0.6735\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6019 - val_loss: 0.6319 - val_accuracy: 0.6645\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.6007 - val_loss: 0.6353 - val_accuracy: 0.6684\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6661 - accuracy: 0.6015 - val_loss: 0.6369 - val_accuracy: 0.6722\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6127 - val_loss: 0.6330 - val_accuracy: 0.6658\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5950 - val_loss: 0.6325 - val_accuracy: 0.6620\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6083 - val_loss: 0.6300 - val_accuracy: 0.6684\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5978 - val_loss: 0.6352 - val_accuracy: 0.6709\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6040 - val_loss: 0.6309 - val_accuracy: 0.6722\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6019 - val_loss: 0.6360 - val_accuracy: 0.6773\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6030 - val_loss: 0.6335 - val_accuracy: 0.6773\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6084 - val_loss: 0.6287 - val_accuracy: 0.6684\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6079 - val_loss: 0.6347 - val_accuracy: 0.6760\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6660 - accuracy: 0.5985 - val_loss: 0.6330 - val_accuracy: 0.6696\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6091 - val_loss: 0.6317 - val_accuracy: 0.6633\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.6010 - val_loss: 0.6357 - val_accuracy: 0.6786\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6088 - val_loss: 0.6286 - val_accuracy: 0.6620\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6017 - val_loss: 0.6318 - val_accuracy: 0.6671\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6014 - val_loss: 0.6359 - val_accuracy: 0.6773\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6118 - val_loss: 0.6293 - val_accuracy: 0.6735\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6041 - val_loss: 0.6296 - val_accuracy: 0.6735\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6643 - accuracy: 0.6041 - val_loss: 0.6382 - val_accuracy: 0.6824\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6081 - val_loss: 0.6295 - val_accuracy: 0.6684\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6132 - val_loss: 0.6278 - val_accuracy: 0.6696\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6061 - val_loss: 0.6308 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6099 - val_loss: 0.6299 - val_accuracy: 0.6709\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6080 - val_loss: 0.6302 - val_accuracy: 0.6696\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6107 - val_loss: 0.6332 - val_accuracy: 0.6735\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6604 - accuracy: 0.6075 - val_loss: 0.6307 - val_accuracy: 0.6786\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6100 - val_loss: 0.6292 - val_accuracy: 0.6811\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6079 - val_loss: 0.6342 - val_accuracy: 0.6786\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6604 - accuracy: 0.6075 - val_loss: 0.6312 - val_accuracy: 0.6773\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6078 - val_loss: 0.6296 - val_accuracy: 0.6722\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6079 - val_loss: 0.6369 - val_accuracy: 0.6811\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6139 - val_loss: 0.6263 - val_accuracy: 0.6722\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6103 - val_loss: 0.6273 - val_accuracy: 0.6773\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6144 - val_loss: 0.6295 - val_accuracy: 0.6773\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6134 - val_loss: 0.6243 - val_accuracy: 0.6798\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6093 - val_loss: 0.6260 - val_accuracy: 0.6760\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6149 - val_loss: 0.6283 - val_accuracy: 0.6760\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6181 - val_loss: 0.6320 - val_accuracy: 0.6798\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6155 - val_loss: 0.6285 - val_accuracy: 0.6798\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6122 - val_loss: 0.6322 - val_accuracy: 0.6811\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6099 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6181 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6177 - val_loss: 0.6314 - val_accuracy: 0.6773\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6143 - val_loss: 0.6296 - val_accuracy: 0.6849\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6148 - val_loss: 0.6269 - val_accuracy: 0.6760\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6132 - val_loss: 0.6279 - val_accuracy: 0.6811\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6177 - val_loss: 0.6304 - val_accuracy: 0.6811\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6118 - val_loss: 0.6279 - val_accuracy: 0.6786\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6186 - val_loss: 0.6291 - val_accuracy: 0.6798\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6114 - val_loss: 0.6305 - val_accuracy: 0.6798\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6599 - accuracy: 0.6167 - val_loss: 0.6294 - val_accuracy: 0.6837\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6125 - val_loss: 0.6306 - val_accuracy: 0.6862\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6149 - val_loss: 0.6277 - val_accuracy: 0.6824\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6599 - accuracy: 0.6115 - val_loss: 0.6275 - val_accuracy: 0.6786\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6240 - val_loss: 0.6308 - val_accuracy: 0.6811\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6183 - val_loss: 0.6263 - val_accuracy: 0.6798\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6561 - accuracy: 0.6142 - val_loss: 0.6246 - val_accuracy: 0.6798\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6119 - val_loss: 0.6245 - val_accuracy: 0.6837\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6584 - accuracy: 0.6169 - val_loss: 0.6285 - val_accuracy: 0.6837\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6184 - val_loss: 0.6320 - val_accuracy: 0.6798\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6215 - val_loss: 0.6267 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6215 - val_loss: 0.6312 - val_accuracy: 0.6798\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6174 - val_loss: 0.6267 - val_accuracy: 0.6849\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6158 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6184 - val_loss: 0.6282 - val_accuracy: 0.6798\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6177 - val_loss: 0.6260 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6198 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6280 - val_loss: 0.6251 - val_accuracy: 0.6824\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6264 - val_loss: 0.6283 - val_accuracy: 0.6824\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6218 - val_loss: 0.6263 - val_accuracy: 0.6786\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6193 - val_loss: 0.6253 - val_accuracy: 0.6837\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6194 - val_loss: 0.6277 - val_accuracy: 0.6849\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6153 - val_loss: 0.6273 - val_accuracy: 0.6811\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6206 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6243 - val_loss: 0.6226 - val_accuracy: 0.6849\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6250 - val_loss: 0.6230 - val_accuracy: 0.6798\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6177 - val_loss: 0.6280 - val_accuracy: 0.6862\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6159 - val_loss: 0.6286 - val_accuracy: 0.6811\n",
      "Calculating for: 700 250 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_144 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8208 - accuracy: 0.5554 - val_loss: 0.6750 - val_accuracy: 0.6008\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7154 - accuracy: 0.5799 - val_loss: 0.6592 - val_accuracy: 0.6314\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6837 - accuracy: 0.5878 - val_loss: 0.6449 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6069 - val_loss: 0.6492 - val_accuracy: 0.6467\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6044 - val_loss: 0.6485 - val_accuracy: 0.6352\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6198 - val_loss: 0.6378 - val_accuracy: 0.6505\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6187 - val_loss: 0.6344 - val_accuracy: 0.6518\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6241 - val_loss: 0.6346 - val_accuracy: 0.6505\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6269 - val_loss: 0.6381 - val_accuracy: 0.6543\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6270 - val_loss: 0.6320 - val_accuracy: 0.6620\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6343 - val_loss: 0.6328 - val_accuracy: 0.6556\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6343 - val_loss: 0.6305 - val_accuracy: 0.6620\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6310 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6456 - val_loss: 0.6263 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6387 - val_loss: 0.6289 - val_accuracy: 0.6658\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6384 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6379 - accuracy: 0.6433 - val_loss: 0.6280 - val_accuracy: 0.6594\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6469 - val_loss: 0.6244 - val_accuracy: 0.6709\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6395 - val_loss: 0.6279 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6444 - val_loss: 0.6264 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6529 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6508 - val_loss: 0.6220 - val_accuracy: 0.6773\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6459 - val_loss: 0.6244 - val_accuracy: 0.6760\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6506 - val_loss: 0.6236 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6454 - val_loss: 0.6246 - val_accuracy: 0.6645\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6570 - val_loss: 0.6264 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6567 - val_loss: 0.6234 - val_accuracy: 0.6735\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6591 - val_loss: 0.6258 - val_accuracy: 0.6671\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6539 - val_loss: 0.6299 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6206 - accuracy: 0.6601 - val_loss: 0.6296 - val_accuracy: 0.6696\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6588 - val_loss: 0.6311 - val_accuracy: 0.6645\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6549 - val_loss: 0.6263 - val_accuracy: 0.6696\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6625 - val_loss: 0.6298 - val_accuracy: 0.6709\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6620 - val_loss: 0.6298 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6649 - val_loss: 0.6262 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6688 - val_loss: 0.6264 - val_accuracy: 0.6709\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6642 - val_loss: 0.6286 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6086 - accuracy: 0.6704 - val_loss: 0.6276 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6706 - val_loss: 0.6300 - val_accuracy: 0.6709\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6670 - val_loss: 0.6295 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6056 - accuracy: 0.6770 - val_loss: 0.6291 - val_accuracy: 0.6671\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6688 - val_loss: 0.6269 - val_accuracy: 0.6709\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6036 - accuracy: 0.6703 - val_loss: 0.6313 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6796 - val_loss: 0.6339 - val_accuracy: 0.6671\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6080 - accuracy: 0.6743 - val_loss: 0.6303 - val_accuracy: 0.6671\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6009 - accuracy: 0.6786 - val_loss: 0.6366 - val_accuracy: 0.6645\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6008 - accuracy: 0.6795 - val_loss: 0.6351 - val_accuracy: 0.6645\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6057 - accuracy: 0.6761 - val_loss: 0.6374 - val_accuracy: 0.6696\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6021 - accuracy: 0.6824 - val_loss: 0.6330 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6018 - accuracy: 0.6711 - val_loss: 0.6349 - val_accuracy: 0.6696\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5986 - accuracy: 0.6843 - val_loss: 0.6337 - val_accuracy: 0.6671\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5929 - accuracy: 0.6871 - val_loss: 0.6407 - val_accuracy: 0.6671\n",
      "Calculating for: 700 250 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_148 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8294 - accuracy: 0.5313 - val_loss: 0.6579 - val_accuracy: 0.6658\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7179 - accuracy: 0.5553 - val_loss: 0.6482 - val_accuracy: 0.6582\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5715 - val_loss: 0.6403 - val_accuracy: 0.6671\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6805 - accuracy: 0.5741 - val_loss: 0.6427 - val_accuracy: 0.6671\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6710 - accuracy: 0.5887 - val_loss: 0.6410 - val_accuracy: 0.6696\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6736 - accuracy: 0.5816 - val_loss: 0.6376 - val_accuracy: 0.6709\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6698 - accuracy: 0.5873 - val_loss: 0.6386 - val_accuracy: 0.6684\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5890 - val_loss: 0.6418 - val_accuracy: 0.6658\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6024 - val_loss: 0.6374 - val_accuracy: 0.6722\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.5997 - val_loss: 0.6356 - val_accuracy: 0.6671\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6090 - val_loss: 0.6320 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6007 - val_loss: 0.6374 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6074 - val_loss: 0.6333 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.5978 - val_loss: 0.6334 - val_accuracy: 0.6760\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6098 - val_loss: 0.6321 - val_accuracy: 0.6735\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6059 - val_loss: 0.6307 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6113 - val_loss: 0.6347 - val_accuracy: 0.6811\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6148 - val_loss: 0.6275 - val_accuracy: 0.6773\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6169 - val_loss: 0.6287 - val_accuracy: 0.6760\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6137 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6192 - val_loss: 0.6284 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6122 - val_loss: 0.6283 - val_accuracy: 0.6824\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6203 - val_loss: 0.6272 - val_accuracy: 0.6798\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6232 - val_loss: 0.6299 - val_accuracy: 0.6849\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6154 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6181 - val_loss: 0.6237 - val_accuracy: 0.6913\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6232 - val_loss: 0.6226 - val_accuracy: 0.6939\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6253 - val_loss: 0.6278 - val_accuracy: 0.6939\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6240 - val_loss: 0.6274 - val_accuracy: 0.6913\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6247 - val_loss: 0.6230 - val_accuracy: 0.6913\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6479 - accuracy: 0.6325 - val_loss: 0.6214 - val_accuracy: 0.6862\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6243 - val_loss: 0.6229 - val_accuracy: 0.6888\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6225 - val_loss: 0.6232 - val_accuracy: 0.6901\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6220 - val_loss: 0.6230 - val_accuracy: 0.6913\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6323 - val_loss: 0.6230 - val_accuracy: 0.6888\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6506 - accuracy: 0.6295 - val_loss: 0.6308 - val_accuracy: 0.6862\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6284 - val_loss: 0.6199 - val_accuracy: 0.6913\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6316 - val_loss: 0.6240 - val_accuracy: 0.6849\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6230 - val_loss: 0.6222 - val_accuracy: 0.6888\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6331 - val_loss: 0.6201 - val_accuracy: 0.6849\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6345 - val_loss: 0.6243 - val_accuracy: 0.6837\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6319 - val_loss: 0.6251 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6329 - val_loss: 0.6245 - val_accuracy: 0.6837\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6341 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6348 - val_loss: 0.6210 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6416 - accuracy: 0.6397 - val_loss: 0.6284 - val_accuracy: 0.6671\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6361 - val_loss: 0.6205 - val_accuracy: 0.6888\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6387 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6361 - val_loss: 0.6228 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6372 - accuracy: 0.6378 - val_loss: 0.6179 - val_accuracy: 0.6888\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6373 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6384 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6410 - val_loss: 0.6186 - val_accuracy: 0.6773\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6458 - val_loss: 0.6165 - val_accuracy: 0.6773\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6415 - val_loss: 0.6149 - val_accuracy: 0.6786\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6436 - val_loss: 0.6149 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6431 - val_loss: 0.6170 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6431 - val_loss: 0.6228 - val_accuracy: 0.6722\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6443 - val_loss: 0.6163 - val_accuracy: 0.6837\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6454 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6480 - val_loss: 0.6144 - val_accuracy: 0.6862\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6517 - val_loss: 0.6131 - val_accuracy: 0.6849\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6526 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6495 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6508 - val_loss: 0.6168 - val_accuracy: 0.6875\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6496 - val_loss: 0.6144 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6482 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6473 - val_loss: 0.6126 - val_accuracy: 0.6862\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6526 - val_loss: 0.6132 - val_accuracy: 0.6952\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6541 - val_loss: 0.6158 - val_accuracy: 0.6849\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6493 - val_loss: 0.6152 - val_accuracy: 0.6888\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6557 - val_loss: 0.6159 - val_accuracy: 0.6888\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6572 - val_loss: 0.6107 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6550 - val_loss: 0.6112 - val_accuracy: 0.6901\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6492 - val_loss: 0.6123 - val_accuracy: 0.6939\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6562 - val_loss: 0.6177 - val_accuracy: 0.6888\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6567 - val_loss: 0.6185 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6577 - val_loss: 0.6107 - val_accuracy: 0.6952\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6511 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6583 - val_loss: 0.6095 - val_accuracy: 0.6913\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6647 - val_loss: 0.6138 - val_accuracy: 0.6913\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6536 - val_loss: 0.6169 - val_accuracy: 0.6952\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6559 - val_loss: 0.6131 - val_accuracy: 0.6939\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6581 - val_loss: 0.6110 - val_accuracy: 0.6926\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6600 - val_loss: 0.6112 - val_accuracy: 0.6939\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6199 - accuracy: 0.6625 - val_loss: 0.6093 - val_accuracy: 0.6990\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6184 - accuracy: 0.6652 - val_loss: 0.6123 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6654 - val_loss: 0.6100 - val_accuracy: 0.7003\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6590 - val_loss: 0.6109 - val_accuracy: 0.6977\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6605 - val_loss: 0.6142 - val_accuracy: 0.6952\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6610 - val_loss: 0.6134 - val_accuracy: 0.6901\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6634 - val_loss: 0.6150 - val_accuracy: 0.6939\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6599 - val_loss: 0.6134 - val_accuracy: 0.6926\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6618 - val_loss: 0.6123 - val_accuracy: 0.6964\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6639 - val_loss: 0.6143 - val_accuracy: 0.7003\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6162 - accuracy: 0.6643 - val_loss: 0.6158 - val_accuracy: 0.6939\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6616 - val_loss: 0.6136 - val_accuracy: 0.6990\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6154 - accuracy: 0.6630 - val_loss: 0.6088 - val_accuracy: 0.7015\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6647 - val_loss: 0.6096 - val_accuracy: 0.7015\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6691 - val_loss: 0.6090 - val_accuracy: 0.6952\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6691 - val_loss: 0.6088 - val_accuracy: 0.6990\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.6675 - val_loss: 0.6115 - val_accuracy: 0.6926\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6644 - val_loss: 0.6142 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6634 - val_loss: 0.6131 - val_accuracy: 0.6926\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6128 - accuracy: 0.6729 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6713 - val_loss: 0.6158 - val_accuracy: 0.6862\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6075 - accuracy: 0.6751 - val_loss: 0.6205 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6071 - accuracy: 0.6712 - val_loss: 0.6139 - val_accuracy: 0.6926\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6694 - val_loss: 0.6148 - val_accuracy: 0.6888\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6093 - accuracy: 0.6693 - val_loss: 0.6176 - val_accuracy: 0.6875\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6760 - val_loss: 0.6145 - val_accuracy: 0.6837\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6668 - val_loss: 0.6181 - val_accuracy: 0.6888\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6098 - accuracy: 0.6704 - val_loss: 0.6128 - val_accuracy: 0.6926\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6714 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6082 - accuracy: 0.6785 - val_loss: 0.6187 - val_accuracy: 0.6811\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6073 - accuracy: 0.6729 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6145 - accuracy: 0.6644 - val_loss: 0.6154 - val_accuracy: 0.6811\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6088 - accuracy: 0.6760 - val_loss: 0.6168 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6101 - accuracy: 0.6740 - val_loss: 0.6148 - val_accuracy: 0.6913\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6069 - accuracy: 0.6727 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6084 - accuracy: 0.6689 - val_loss: 0.6154 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6088 - accuracy: 0.6751 - val_loss: 0.6183 - val_accuracy: 0.6786\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6036 - accuracy: 0.6812 - val_loss: 0.6166 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6042 - accuracy: 0.6788 - val_loss: 0.6184 - val_accuracy: 0.6824\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6029 - accuracy: 0.6794 - val_loss: 0.6183 - val_accuracy: 0.6849\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6037 - accuracy: 0.6829 - val_loss: 0.6138 - val_accuracy: 0.6888\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6045 - accuracy: 0.6822 - val_loss: 0.6154 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6822 - val_loss: 0.6182 - val_accuracy: 0.6786\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5996 - accuracy: 0.6871 - val_loss: 0.6192 - val_accuracy: 0.6798\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6009 - accuracy: 0.6805 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6027 - accuracy: 0.6761 - val_loss: 0.6164 - val_accuracy: 0.6901\n",
      "Calculating for: 700 250 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_152 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8582 - accuracy: 0.4956 - val_loss: 0.6875 - val_accuracy: 0.5651\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7378 - accuracy: 0.5108 - val_loss: 0.6657 - val_accuracy: 0.6352\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7056 - accuracy: 0.5235 - val_loss: 0.6614 - val_accuracy: 0.6186\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6971 - accuracy: 0.5220 - val_loss: 0.6694 - val_accuracy: 0.6212\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5274 - val_loss: 0.6691 - val_accuracy: 0.6224\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.5383 - val_loss: 0.6640 - val_accuracy: 0.6199\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6908 - accuracy: 0.5357 - val_loss: 0.6665 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5345 - val_loss: 0.6660 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6895 - accuracy: 0.5365 - val_loss: 0.6656 - val_accuracy: 0.6186\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5301 - val_loss: 0.6701 - val_accuracy: 0.6199\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6882 - accuracy: 0.5435 - val_loss: 0.6647 - val_accuracy: 0.6212\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6893 - accuracy: 0.5345 - val_loss: 0.6636 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5289 - val_loss: 0.6661 - val_accuracy: 0.6199\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6905 - accuracy: 0.5239 - val_loss: 0.6672 - val_accuracy: 0.6199\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5383 - val_loss: 0.6657 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6895 - accuracy: 0.5358 - val_loss: 0.6650 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5379 - val_loss: 0.6646 - val_accuracy: 0.6224\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6887 - accuracy: 0.5505 - val_loss: 0.6598 - val_accuracy: 0.6224\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6858 - accuracy: 0.5491 - val_loss: 0.6602 - val_accuracy: 0.6212\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.5484 - val_loss: 0.6618 - val_accuracy: 0.6276\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6847 - accuracy: 0.5502 - val_loss: 0.6555 - val_accuracy: 0.6263\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5463 - val_loss: 0.6561 - val_accuracy: 0.6301\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5416 - val_loss: 0.6624 - val_accuracy: 0.6301\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6870 - accuracy: 0.5504 - val_loss: 0.6582 - val_accuracy: 0.6288\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5473 - val_loss: 0.6578 - val_accuracy: 0.6288\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5558 - val_loss: 0.6590 - val_accuracy: 0.6314\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5521 - val_loss: 0.6567 - val_accuracy: 0.6250\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6830 - accuracy: 0.5574 - val_loss: 0.6542 - val_accuracy: 0.6314\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5543 - val_loss: 0.6542 - val_accuracy: 0.6339\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6871 - accuracy: 0.5534 - val_loss: 0.6591 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5526 - val_loss: 0.6555 - val_accuracy: 0.6339\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6847 - accuracy: 0.5490 - val_loss: 0.6527 - val_accuracy: 0.6352\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5569 - val_loss: 0.6531 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5533 - val_loss: 0.6514 - val_accuracy: 0.6339\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6798 - accuracy: 0.5663 - val_loss: 0.6486 - val_accuracy: 0.6390\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6830 - accuracy: 0.5599 - val_loss: 0.6509 - val_accuracy: 0.6365\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6843 - accuracy: 0.5568 - val_loss: 0.6542 - val_accuracy: 0.6390\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.5730 - val_loss: 0.6473 - val_accuracy: 0.6352\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5744 - val_loss: 0.6454 - val_accuracy: 0.6416\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5664 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6815 - accuracy: 0.5627 - val_loss: 0.6494 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6805 - accuracy: 0.5697 - val_loss: 0.6460 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.5639 - val_loss: 0.6470 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6791 - accuracy: 0.5683 - val_loss: 0.6448 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6834 - accuracy: 0.5590 - val_loss: 0.6475 - val_accuracy: 0.6403\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6814 - accuracy: 0.5698 - val_loss: 0.6465 - val_accuracy: 0.6416\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6797 - accuracy: 0.5691 - val_loss: 0.6456 - val_accuracy: 0.6416\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5760 - val_loss: 0.6431 - val_accuracy: 0.6403\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5732 - val_loss: 0.6430 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6777 - accuracy: 0.5663 - val_loss: 0.6426 - val_accuracy: 0.6441\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6809 - accuracy: 0.5664 - val_loss: 0.6473 - val_accuracy: 0.6416\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5687 - val_loss: 0.6474 - val_accuracy: 0.6429\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6784 - accuracy: 0.5746 - val_loss: 0.6448 - val_accuracy: 0.6429\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5703 - val_loss: 0.6435 - val_accuracy: 0.6441\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5717 - val_loss: 0.6425 - val_accuracy: 0.6429\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.5710 - val_loss: 0.6413 - val_accuracy: 0.6429\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5731 - val_loss: 0.6419 - val_accuracy: 0.6429\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6736 - accuracy: 0.5830 - val_loss: 0.6402 - val_accuracy: 0.6441\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6744 - accuracy: 0.5828 - val_loss: 0.6428 - val_accuracy: 0.6531\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5820 - val_loss: 0.6423 - val_accuracy: 0.6480\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6766 - accuracy: 0.5735 - val_loss: 0.6391 - val_accuracy: 0.6429\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6771 - accuracy: 0.5717 - val_loss: 0.6417 - val_accuracy: 0.6467\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6719 - accuracy: 0.5828 - val_loss: 0.6377 - val_accuracy: 0.6441\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5795 - val_loss: 0.6393 - val_accuracy: 0.6454\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6742 - accuracy: 0.5823 - val_loss: 0.6391 - val_accuracy: 0.6429\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6719 - accuracy: 0.5882 - val_loss: 0.6364 - val_accuracy: 0.6518\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6717 - accuracy: 0.5840 - val_loss: 0.6396 - val_accuracy: 0.6582\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5785 - val_loss: 0.6406 - val_accuracy: 0.6492\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6753 - accuracy: 0.5849 - val_loss: 0.6392 - val_accuracy: 0.6429\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6702 - accuracy: 0.5918 - val_loss: 0.6363 - val_accuracy: 0.6467\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6710 - accuracy: 0.5887 - val_loss: 0.6359 - val_accuracy: 0.6467\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5936 - val_loss: 0.6383 - val_accuracy: 0.6480\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5865 - val_loss: 0.6367 - val_accuracy: 0.6556\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5921 - val_loss: 0.6344 - val_accuracy: 0.6531\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5888 - val_loss: 0.6330 - val_accuracy: 0.6556\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5922 - val_loss: 0.6340 - val_accuracy: 0.6492\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5848 - val_loss: 0.6341 - val_accuracy: 0.6492\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5872 - val_loss: 0.6347 - val_accuracy: 0.6492\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5878 - val_loss: 0.6371 - val_accuracy: 0.6582\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5927 - val_loss: 0.6365 - val_accuracy: 0.6658\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6722 - accuracy: 0.5813 - val_loss: 0.6350 - val_accuracy: 0.6607\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5950 - val_loss: 0.6310 - val_accuracy: 0.6531\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5909 - val_loss: 0.6336 - val_accuracy: 0.6556\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5970 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5934 - val_loss: 0.6308 - val_accuracy: 0.6543\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6000 - val_loss: 0.6322 - val_accuracy: 0.6569\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6653 - accuracy: 0.6017 - val_loss: 0.6322 - val_accuracy: 0.6556\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6680 - accuracy: 0.5896 - val_loss: 0.6341 - val_accuracy: 0.6582\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5986 - val_loss: 0.6313 - val_accuracy: 0.6607\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.6006 - val_loss: 0.6307 - val_accuracy: 0.6531\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5962 - val_loss: 0.6329 - val_accuracy: 0.6633\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5978 - val_loss: 0.6313 - val_accuracy: 0.6786\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5981 - val_loss: 0.6299 - val_accuracy: 0.6747\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5955 - val_loss: 0.6308 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6037 - val_loss: 0.6319 - val_accuracy: 0.6709\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6674 - accuracy: 0.5960 - val_loss: 0.6306 - val_accuracy: 0.6735\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.5988 - val_loss: 0.6296 - val_accuracy: 0.6747\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5971 - val_loss: 0.6313 - val_accuracy: 0.6671\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6016 - val_loss: 0.6306 - val_accuracy: 0.6671\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6044 - val_loss: 0.6295 - val_accuracy: 0.6620\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6172 - val_loss: 0.6275 - val_accuracy: 0.6594\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6088 - val_loss: 0.6312 - val_accuracy: 0.6747\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.6014 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6006 - val_loss: 0.6281 - val_accuracy: 0.6735\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6046 - val_loss: 0.6293 - val_accuracy: 0.6709\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6004 - val_loss: 0.6287 - val_accuracy: 0.6735\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6042 - val_loss: 0.6303 - val_accuracy: 0.6722\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.6024 - val_loss: 0.6266 - val_accuracy: 0.6735\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5955 - val_loss: 0.6279 - val_accuracy: 0.6747\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.6039 - val_loss: 0.6280 - val_accuracy: 0.6684\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6117 - val_loss: 0.6244 - val_accuracy: 0.6747\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6101 - val_loss: 0.6248 - val_accuracy: 0.6696\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6052 - val_loss: 0.6284 - val_accuracy: 0.6696\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6019 - val_loss: 0.6270 - val_accuracy: 0.6747\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6114 - val_loss: 0.6249 - val_accuracy: 0.6722\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6095 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6166 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5993 - val_loss: 0.6285 - val_accuracy: 0.6684\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6027 - val_loss: 0.6260 - val_accuracy: 0.6645\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6117 - val_loss: 0.6251 - val_accuracy: 0.6645\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6076 - val_loss: 0.6252 - val_accuracy: 0.6709\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6095 - val_loss: 0.6278 - val_accuracy: 0.6709\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6085 - val_loss: 0.6258 - val_accuracy: 0.6658\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5992 - val_loss: 0.6288 - val_accuracy: 0.6671\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6044 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6088 - val_loss: 0.6261 - val_accuracy: 0.6747\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6127 - val_loss: 0.6293 - val_accuracy: 0.6722\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6101 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6145 - val_loss: 0.6219 - val_accuracy: 0.6696\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6061 - val_loss: 0.6283 - val_accuracy: 0.6760\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6089 - val_loss: 0.6244 - val_accuracy: 0.6735\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6173 - val_loss: 0.6217 - val_accuracy: 0.6645\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6154 - val_loss: 0.6214 - val_accuracy: 0.6671\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6052 - val_loss: 0.6220 - val_accuracy: 0.6684\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6071 - val_loss: 0.6254 - val_accuracy: 0.6760\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6133 - val_loss: 0.6217 - val_accuracy: 0.6696\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6152 - val_loss: 0.6241 - val_accuracy: 0.6722\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6215 - val_loss: 0.6222 - val_accuracy: 0.6773\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6154 - val_loss: 0.6230 - val_accuracy: 0.6811\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6114 - val_loss: 0.6245 - val_accuracy: 0.6786\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6154 - val_loss: 0.6198 - val_accuracy: 0.6747\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6127 - val_loss: 0.6250 - val_accuracy: 0.6747\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6192 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6089 - val_loss: 0.6252 - val_accuracy: 0.6798\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6144 - val_loss: 0.6209 - val_accuracy: 0.6747\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6101 - val_loss: 0.6264 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6182 - val_loss: 0.6238 - val_accuracy: 0.6735\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6198 - val_loss: 0.6219 - val_accuracy: 0.6671\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6117 - val_loss: 0.6222 - val_accuracy: 0.6696\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6197 - val_loss: 0.6233 - val_accuracy: 0.6786\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6220 - val_loss: 0.6218 - val_accuracy: 0.6773\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6109 - val_loss: 0.6254 - val_accuracy: 0.6837\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6587 - accuracy: 0.6157 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6227 - val_loss: 0.6179 - val_accuracy: 0.6760\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6172 - val_loss: 0.6215 - val_accuracy: 0.6824\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6167 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6119 - val_loss: 0.6224 - val_accuracy: 0.6709\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6149 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6163 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6161 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6181 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6209 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6188 - val_loss: 0.6176 - val_accuracy: 0.6811\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6148 - val_loss: 0.6180 - val_accuracy: 0.6722\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6208 - val_loss: 0.6178 - val_accuracy: 0.6786\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6198 - val_loss: 0.6236 - val_accuracy: 0.6709\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6285 - val_loss: 0.6213 - val_accuracy: 0.6811\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6192 - val_loss: 0.6185 - val_accuracy: 0.6798\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6172 - val_loss: 0.6233 - val_accuracy: 0.6849\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6240 - val_loss: 0.6185 - val_accuracy: 0.6837\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6230 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6193 - val_loss: 0.6179 - val_accuracy: 0.6773\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6223 - val_loss: 0.6179 - val_accuracy: 0.6786\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6248 - val_loss: 0.6184 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6208 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6223 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6177 - val_loss: 0.6182 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6206 - val_loss: 0.6181 - val_accuracy: 0.6837\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6197 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6191 - val_loss: 0.6179 - val_accuracy: 0.6760\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6225 - val_loss: 0.6155 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6243 - val_loss: 0.6223 - val_accuracy: 0.6798\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6261 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6264 - val_loss: 0.6180 - val_accuracy: 0.6773\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6251 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6326 - val_loss: 0.6146 - val_accuracy: 0.6811\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6262 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6256 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6233 - val_loss: 0.6196 - val_accuracy: 0.6773\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6286 - val_loss: 0.6156 - val_accuracy: 0.6798\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6314 - val_loss: 0.6166 - val_accuracy: 0.6824\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6218 - val_loss: 0.6196 - val_accuracy: 0.6824\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6217 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6274 - val_loss: 0.6169 - val_accuracy: 0.6798\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6321 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6269 - val_loss: 0.6165 - val_accuracy: 0.6773\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6310 - val_loss: 0.6166 - val_accuracy: 0.6798\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6290 - val_loss: 0.6170 - val_accuracy: 0.6824\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6228 - val_loss: 0.6191 - val_accuracy: 0.6837\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6496 - accuracy: 0.6299 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Calculating for: 700 400 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_156 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7884 - accuracy: 0.5536 - val_loss: 0.6536 - val_accuracy: 0.6798\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6941 - accuracy: 0.5882 - val_loss: 0.6481 - val_accuracy: 0.6620\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6720 - accuracy: 0.6004 - val_loss: 0.6423 - val_accuracy: 0.6569\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6061 - val_loss: 0.6378 - val_accuracy: 0.6645\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6603 - accuracy: 0.6086 - val_loss: 0.6355 - val_accuracy: 0.6786\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6590 - accuracy: 0.6113 - val_loss: 0.6313 - val_accuracy: 0.6709\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6525 - accuracy: 0.6256 - val_loss: 0.6330 - val_accuracy: 0.6760\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6529 - accuracy: 0.6155 - val_loss: 0.6335 - val_accuracy: 0.6735\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6501 - accuracy: 0.6228 - val_loss: 0.6281 - val_accuracy: 0.6798\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6261 - val_loss: 0.6275 - val_accuracy: 0.6747\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6436 - accuracy: 0.6361 - val_loss: 0.6242 - val_accuracy: 0.6773\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6424 - accuracy: 0.6329 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6446 - accuracy: 0.6286 - val_loss: 0.6257 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6434 - accuracy: 0.6345 - val_loss: 0.6228 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6403 - accuracy: 0.6380 - val_loss: 0.6242 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6412 - val_loss: 0.6229 - val_accuracy: 0.6837\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6336 - val_loss: 0.6256 - val_accuracy: 0.6671\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6352 - accuracy: 0.6410 - val_loss: 0.6197 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6439 - val_loss: 0.6224 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6318 - accuracy: 0.6466 - val_loss: 0.6218 - val_accuracy: 0.6658\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6315 - accuracy: 0.6462 - val_loss: 0.6201 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6329 - accuracy: 0.6476 - val_loss: 0.6200 - val_accuracy: 0.6747\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6302 - accuracy: 0.6518 - val_loss: 0.6208 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6284 - accuracy: 0.6510 - val_loss: 0.6196 - val_accuracy: 0.6773\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6278 - accuracy: 0.6535 - val_loss: 0.6213 - val_accuracy: 0.6811\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6240 - accuracy: 0.6555 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6203 - accuracy: 0.6588 - val_loss: 0.6172 - val_accuracy: 0.6926\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6259 - accuracy: 0.6495 - val_loss: 0.6211 - val_accuracy: 0.6811\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6216 - accuracy: 0.6618 - val_loss: 0.6148 - val_accuracy: 0.6990\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6166 - accuracy: 0.6629 - val_loss: 0.6149 - val_accuracy: 0.6888\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6206 - accuracy: 0.6629 - val_loss: 0.6167 - val_accuracy: 0.6901\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6183 - accuracy: 0.6663 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6167 - accuracy: 0.6616 - val_loss: 0.6166 - val_accuracy: 0.6913\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6159 - accuracy: 0.6608 - val_loss: 0.6194 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6113 - accuracy: 0.6701 - val_loss: 0.6247 - val_accuracy: 0.6798\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6146 - accuracy: 0.6631 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6065 - accuracy: 0.6721 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6104 - accuracy: 0.6752 - val_loss: 0.6188 - val_accuracy: 0.6875\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6090 - accuracy: 0.6662 - val_loss: 0.6211 - val_accuracy: 0.6862\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6085 - accuracy: 0.6727 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6040 - accuracy: 0.6765 - val_loss: 0.6230 - val_accuracy: 0.6888\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6057 - accuracy: 0.6723 - val_loss: 0.6187 - val_accuracy: 0.6837\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6088 - accuracy: 0.6750 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6028 - accuracy: 0.6765 - val_loss: 0.6236 - val_accuracy: 0.6939\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6032 - accuracy: 0.6760 - val_loss: 0.6222 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5970 - accuracy: 0.6830 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5954 - accuracy: 0.6830 - val_loss: 0.6243 - val_accuracy: 0.6786\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5966 - accuracy: 0.6802 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5999 - accuracy: 0.6807 - val_loss: 0.6278 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6017 - accuracy: 0.6768 - val_loss: 0.6228 - val_accuracy: 0.6913\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5956 - accuracy: 0.6797 - val_loss: 0.6236 - val_accuracy: 0.6913\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5958 - accuracy: 0.6829 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5944 - accuracy: 0.6858 - val_loss: 0.6289 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5927 - accuracy: 0.6894 - val_loss: 0.6343 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5962 - accuracy: 0.6832 - val_loss: 0.6283 - val_accuracy: 0.6862\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5949 - accuracy: 0.6831 - val_loss: 0.6237 - val_accuracy: 0.6824\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5908 - accuracy: 0.6858 - val_loss: 0.6330 - val_accuracy: 0.6786\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5903 - accuracy: 0.6874 - val_loss: 0.6305 - val_accuracy: 0.6837\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5887 - accuracy: 0.6937 - val_loss: 0.6299 - val_accuracy: 0.6824\n",
      "Calculating for: 700 400 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_160 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8120 - accuracy: 0.5345 - val_loss: 0.6454 - val_accuracy: 0.6352\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7088 - accuracy: 0.5634 - val_loss: 0.6418 - val_accuracy: 0.6633\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6824 - accuracy: 0.5836 - val_loss: 0.6400 - val_accuracy: 0.6658\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6791 - accuracy: 0.5781 - val_loss: 0.6379 - val_accuracy: 0.6582\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6755 - accuracy: 0.5808 - val_loss: 0.6389 - val_accuracy: 0.6620\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6708 - accuracy: 0.5882 - val_loss: 0.6385 - val_accuracy: 0.6671\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.5929 - val_loss: 0.6380 - val_accuracy: 0.6671\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6680 - accuracy: 0.5975 - val_loss: 0.6369 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6660 - accuracy: 0.6007 - val_loss: 0.6316 - val_accuracy: 0.6684\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6685 - accuracy: 0.5941 - val_loss: 0.6346 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6637 - accuracy: 0.5968 - val_loss: 0.6384 - val_accuracy: 0.6747\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6634 - accuracy: 0.6010 - val_loss: 0.6331 - val_accuracy: 0.6709\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6615 - accuracy: 0.6148 - val_loss: 0.6314 - val_accuracy: 0.6709\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6592 - accuracy: 0.6060 - val_loss: 0.6315 - val_accuracy: 0.6709\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6032 - val_loss: 0.6370 - val_accuracy: 0.6773\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6587 - accuracy: 0.6112 - val_loss: 0.6335 - val_accuracy: 0.6824\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6573 - accuracy: 0.6115 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6563 - accuracy: 0.6130 - val_loss: 0.6257 - val_accuracy: 0.6875\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6560 - accuracy: 0.6105 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6528 - accuracy: 0.6152 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6562 - accuracy: 0.6186 - val_loss: 0.6238 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6525 - accuracy: 0.6246 - val_loss: 0.6236 - val_accuracy: 0.6773\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6542 - accuracy: 0.6215 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6554 - accuracy: 0.6148 - val_loss: 0.6255 - val_accuracy: 0.6811\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6522 - accuracy: 0.6265 - val_loss: 0.6216 - val_accuracy: 0.6875\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6497 - accuracy: 0.6272 - val_loss: 0.6226 - val_accuracy: 0.6824\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6159 - val_loss: 0.6191 - val_accuracy: 0.6862\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6517 - accuracy: 0.6238 - val_loss: 0.6238 - val_accuracy: 0.6760\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6499 - accuracy: 0.6247 - val_loss: 0.6219 - val_accuracy: 0.6786\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6498 - accuracy: 0.6222 - val_loss: 0.6250 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6491 - accuracy: 0.6255 - val_loss: 0.6242 - val_accuracy: 0.6798\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6485 - accuracy: 0.6240 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6493 - accuracy: 0.6206 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6476 - accuracy: 0.6217 - val_loss: 0.6198 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6296 - val_loss: 0.6180 - val_accuracy: 0.6811\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6466 - accuracy: 0.6252 - val_loss: 0.6210 - val_accuracy: 0.6837\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6429 - accuracy: 0.6320 - val_loss: 0.6204 - val_accuracy: 0.6849\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6260 - val_loss: 0.6198 - val_accuracy: 0.6837\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6453 - accuracy: 0.6270 - val_loss: 0.6192 - val_accuracy: 0.6849\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6440 - accuracy: 0.6360 - val_loss: 0.6195 - val_accuracy: 0.6849\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6463 - accuracy: 0.6269 - val_loss: 0.6169 - val_accuracy: 0.6875\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6227 - val_loss: 0.6212 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6453 - accuracy: 0.6318 - val_loss: 0.6169 - val_accuracy: 0.6913\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6424 - accuracy: 0.6333 - val_loss: 0.6204 - val_accuracy: 0.6773\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6386 - accuracy: 0.6400 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6387 - accuracy: 0.6404 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6391 - accuracy: 0.6409 - val_loss: 0.6170 - val_accuracy: 0.6786\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6406 - accuracy: 0.6370 - val_loss: 0.6176 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6418 - accuracy: 0.6388 - val_loss: 0.6190 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6393 - accuracy: 0.6374 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6381 - accuracy: 0.6336 - val_loss: 0.6151 - val_accuracy: 0.6824\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6417 - val_loss: 0.6137 - val_accuracy: 0.6811\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6395 - accuracy: 0.6405 - val_loss: 0.6139 - val_accuracy: 0.6875\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6405 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6413 - val_loss: 0.6132 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6323 - accuracy: 0.6513 - val_loss: 0.6152 - val_accuracy: 0.6926\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6365 - accuracy: 0.6414 - val_loss: 0.6191 - val_accuracy: 0.6875\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6463 - val_loss: 0.6126 - val_accuracy: 0.6990\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6365 - accuracy: 0.6446 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6476 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6368 - accuracy: 0.6462 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6462 - val_loss: 0.6125 - val_accuracy: 0.6952\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6327 - accuracy: 0.6469 - val_loss: 0.6147 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6338 - accuracy: 0.6463 - val_loss: 0.6164 - val_accuracy: 0.6862\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6299 - accuracy: 0.6551 - val_loss: 0.6138 - val_accuracy: 0.6862\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6354 - accuracy: 0.6491 - val_loss: 0.6146 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6488 - val_loss: 0.6158 - val_accuracy: 0.6849\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6451 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6540 - val_loss: 0.6095 - val_accuracy: 0.6926\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6594 - val_loss: 0.6179 - val_accuracy: 0.6811\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6276 - accuracy: 0.6506 - val_loss: 0.6127 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6258 - accuracy: 0.6591 - val_loss: 0.6123 - val_accuracy: 0.6888\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6283 - accuracy: 0.6513 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6279 - accuracy: 0.6537 - val_loss: 0.6135 - val_accuracy: 0.6901\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6572 - val_loss: 0.6094 - val_accuracy: 0.6977\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6240 - accuracy: 0.6530 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6269 - accuracy: 0.6541 - val_loss: 0.6154 - val_accuracy: 0.6926\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6207 - accuracy: 0.6564 - val_loss: 0.6135 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6223 - accuracy: 0.6541 - val_loss: 0.6140 - val_accuracy: 0.6888\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6245 - accuracy: 0.6556 - val_loss: 0.6139 - val_accuracy: 0.6875\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6198 - accuracy: 0.6611 - val_loss: 0.6114 - val_accuracy: 0.6926\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6249 - accuracy: 0.6579 - val_loss: 0.6106 - val_accuracy: 0.6913\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6222 - accuracy: 0.6647 - val_loss: 0.6113 - val_accuracy: 0.6939\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6202 - accuracy: 0.6662 - val_loss: 0.6112 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 3s 10ms/step - loss: 0.6173 - accuracy: 0.6668 - val_loss: 0.6125 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6195 - accuracy: 0.6614 - val_loss: 0.6113 - val_accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6191 - accuracy: 0.6650 - val_loss: 0.6097 - val_accuracy: 0.6926\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6203 - accuracy: 0.6589 - val_loss: 0.6129 - val_accuracy: 0.6811\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6193 - accuracy: 0.6609 - val_loss: 0.6109 - val_accuracy: 0.6901\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6194 - accuracy: 0.6635 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6196 - accuracy: 0.6707 - val_loss: 0.6127 - val_accuracy: 0.6939\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6183 - accuracy: 0.6615 - val_loss: 0.6132 - val_accuracy: 0.6913\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6163 - accuracy: 0.6634 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6123 - accuracy: 0.6677 - val_loss: 0.6123 - val_accuracy: 0.6862\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6179 - accuracy: 0.6647 - val_loss: 0.6123 - val_accuracy: 0.6888\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6143 - accuracy: 0.6692 - val_loss: 0.6148 - val_accuracy: 0.6824\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6185 - accuracy: 0.6701 - val_loss: 0.6151 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6146 - accuracy: 0.6648 - val_loss: 0.6138 - val_accuracy: 0.6901\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6100 - accuracy: 0.6756 - val_loss: 0.6129 - val_accuracy: 0.6926\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6119 - accuracy: 0.6708 - val_loss: 0.6174 - val_accuracy: 0.6849\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6113 - accuracy: 0.6687 - val_loss: 0.6126 - val_accuracy: 0.6875\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6128 - accuracy: 0.6625 - val_loss: 0.6161 - val_accuracy: 0.6824\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6107 - accuracy: 0.6753 - val_loss: 0.6133 - val_accuracy: 0.6875\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6123 - accuracy: 0.6716 - val_loss: 0.6139 - val_accuracy: 0.6824\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6100 - accuracy: 0.6718 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Calculating for: 700 400 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_164 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.8457 - accuracy: 0.5013 - val_loss: 0.6613 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7340 - accuracy: 0.5152 - val_loss: 0.6604 - val_accuracy: 0.6212\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7008 - accuracy: 0.5335 - val_loss: 0.6623 - val_accuracy: 0.6224\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6970 - accuracy: 0.5226 - val_loss: 0.6661 - val_accuracy: 0.6224\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6924 - accuracy: 0.5276 - val_loss: 0.6621 - val_accuracy: 0.6199\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6913 - accuracy: 0.5315 - val_loss: 0.6630 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6887 - accuracy: 0.5391 - val_loss: 0.6624 - val_accuracy: 0.6199\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6892 - accuracy: 0.5473 - val_loss: 0.6637 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6906 - accuracy: 0.5308 - val_loss: 0.6645 - val_accuracy: 0.6224\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.6901 - accuracy: 0.5359 - val_loss: 0.6640 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6908 - accuracy: 0.5325 - val_loss: 0.6636 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6899 - accuracy: 0.5413 - val_loss: 0.6647 - val_accuracy: 0.6237\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6891 - accuracy: 0.5387 - val_loss: 0.6616 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6890 - accuracy: 0.5453 - val_loss: 0.6614 - val_accuracy: 0.6224\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6859 - accuracy: 0.5511 - val_loss: 0.6589 - val_accuracy: 0.6237\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6870 - accuracy: 0.5378 - val_loss: 0.6581 - val_accuracy: 0.6237\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6882 - accuracy: 0.5470 - val_loss: 0.6571 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6870 - accuracy: 0.5457 - val_loss: 0.6584 - val_accuracy: 0.6250\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6878 - accuracy: 0.5465 - val_loss: 0.6577 - val_accuracy: 0.6250\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 3s 12ms/step - loss: 0.6862 - accuracy: 0.5528 - val_loss: 0.6566 - val_accuracy: 0.6263\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6862 - accuracy: 0.5529 - val_loss: 0.6544 - val_accuracy: 0.6250\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6868 - accuracy: 0.5489 - val_loss: 0.6554 - val_accuracy: 0.6250\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6854 - accuracy: 0.5558 - val_loss: 0.6571 - val_accuracy: 0.6263\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6831 - accuracy: 0.5608 - val_loss: 0.6535 - val_accuracy: 0.6263\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6846 - accuracy: 0.5551 - val_loss: 0.6539 - val_accuracy: 0.6276\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6855 - accuracy: 0.5502 - val_loss: 0.6548 - val_accuracy: 0.6276\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6824 - accuracy: 0.5566 - val_loss: 0.6517 - val_accuracy: 0.6352\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6844 - accuracy: 0.5539 - val_loss: 0.6509 - val_accuracy: 0.6352\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6823 - accuracy: 0.5599 - val_loss: 0.6546 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6814 - accuracy: 0.5674 - val_loss: 0.6525 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6825 - accuracy: 0.5647 - val_loss: 0.6511 - val_accuracy: 0.6390\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6815 - accuracy: 0.5676 - val_loss: 0.6510 - val_accuracy: 0.6365\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6831 - accuracy: 0.5570 - val_loss: 0.6507 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6794 - accuracy: 0.5703 - val_loss: 0.6485 - val_accuracy: 0.6390\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6801 - accuracy: 0.5727 - val_loss: 0.6485 - val_accuracy: 0.6403\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6814 - accuracy: 0.5672 - val_loss: 0.6492 - val_accuracy: 0.6429\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6805 - accuracy: 0.5604 - val_loss: 0.6488 - val_accuracy: 0.6429\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6804 - accuracy: 0.5661 - val_loss: 0.6482 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6782 - accuracy: 0.5757 - val_loss: 0.6492 - val_accuracy: 0.6441\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6797 - accuracy: 0.5648 - val_loss: 0.6476 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6780 - accuracy: 0.5642 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6753 - accuracy: 0.5791 - val_loss: 0.6453 - val_accuracy: 0.6441\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6777 - accuracy: 0.5750 - val_loss: 0.6455 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6776 - accuracy: 0.5716 - val_loss: 0.6438 - val_accuracy: 0.6454\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6790 - accuracy: 0.5751 - val_loss: 0.6473 - val_accuracy: 0.6441\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6770 - accuracy: 0.5716 - val_loss: 0.6458 - val_accuracy: 0.6429\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6756 - accuracy: 0.5764 - val_loss: 0.6461 - val_accuracy: 0.6403\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6747 - accuracy: 0.5803 - val_loss: 0.6435 - val_accuracy: 0.6416\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6741 - accuracy: 0.5835 - val_loss: 0.6430 - val_accuracy: 0.6403\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6742 - accuracy: 0.5841 - val_loss: 0.6440 - val_accuracy: 0.6403\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6753 - accuracy: 0.5764 - val_loss: 0.6449 - val_accuracy: 0.6416\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6748 - accuracy: 0.5863 - val_loss: 0.6432 - val_accuracy: 0.6403\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6737 - accuracy: 0.5821 - val_loss: 0.6421 - val_accuracy: 0.6403\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6753 - accuracy: 0.5805 - val_loss: 0.6434 - val_accuracy: 0.6403\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.5775 - val_loss: 0.6417 - val_accuracy: 0.6403\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6738 - accuracy: 0.5836 - val_loss: 0.6415 - val_accuracy: 0.6403\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6720 - accuracy: 0.5841 - val_loss: 0.6409 - val_accuracy: 0.6416\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6726 - accuracy: 0.5927 - val_loss: 0.6405 - val_accuracy: 0.6403\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6715 - accuracy: 0.5924 - val_loss: 0.6400 - val_accuracy: 0.6403\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6769 - accuracy: 0.5739 - val_loss: 0.6424 - val_accuracy: 0.6480\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6724 - accuracy: 0.5918 - val_loss: 0.6394 - val_accuracy: 0.6467\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6737 - accuracy: 0.5869 - val_loss: 0.6404 - val_accuracy: 0.6467\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5858 - val_loss: 0.6400 - val_accuracy: 0.6480\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6697 - accuracy: 0.5834 - val_loss: 0.6383 - val_accuracy: 0.6403\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6698 - accuracy: 0.5916 - val_loss: 0.6367 - val_accuracy: 0.6467\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6704 - accuracy: 0.5903 - val_loss: 0.6377 - val_accuracy: 0.6492\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6737 - accuracy: 0.5848 - val_loss: 0.6372 - val_accuracy: 0.6505\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6726 - accuracy: 0.5864 - val_loss: 0.6376 - val_accuracy: 0.6480\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6701 - accuracy: 0.5887 - val_loss: 0.6366 - val_accuracy: 0.6492\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6656 - accuracy: 0.6002 - val_loss: 0.6324 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6665 - accuracy: 0.5973 - val_loss: 0.6330 - val_accuracy: 0.6518\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6678 - accuracy: 0.6001 - val_loss: 0.6387 - val_accuracy: 0.6492\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6687 - accuracy: 0.5921 - val_loss: 0.6358 - val_accuracy: 0.6467\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6689 - accuracy: 0.5975 - val_loss: 0.6352 - val_accuracy: 0.6505\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6664 - accuracy: 0.5953 - val_loss: 0.6337 - val_accuracy: 0.6505\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6673 - accuracy: 0.5960 - val_loss: 0.6347 - val_accuracy: 0.6518\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6671 - accuracy: 0.5965 - val_loss: 0.6326 - val_accuracy: 0.6518\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6671 - accuracy: 0.6054 - val_loss: 0.6314 - val_accuracy: 0.6531\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6672 - accuracy: 0.5934 - val_loss: 0.6329 - val_accuracy: 0.6518\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6687 - accuracy: 0.5908 - val_loss: 0.6334 - val_accuracy: 0.6543\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6694 - accuracy: 0.5982 - val_loss: 0.6333 - val_accuracy: 0.6518\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6006 - val_loss: 0.6319 - val_accuracy: 0.6556\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6668 - accuracy: 0.6040 - val_loss: 0.6307 - val_accuracy: 0.6518\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.5995 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6665 - accuracy: 0.5967 - val_loss: 0.6330 - val_accuracy: 0.6492\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6063 - val_loss: 0.6309 - val_accuracy: 0.6543\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6652 - accuracy: 0.6031 - val_loss: 0.6318 - val_accuracy: 0.6518\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6024 - val_loss: 0.6303 - val_accuracy: 0.6569\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6655 - accuracy: 0.5978 - val_loss: 0.6314 - val_accuracy: 0.6658\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6035 - val_loss: 0.6288 - val_accuracy: 0.6531\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6071 - val_loss: 0.6288 - val_accuracy: 0.6582\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6638 - accuracy: 0.6054 - val_loss: 0.6284 - val_accuracy: 0.6569\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6089 - val_loss: 0.6287 - val_accuracy: 0.6569\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6066 - val_loss: 0.6298 - val_accuracy: 0.6569\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.6015 - val_loss: 0.6300 - val_accuracy: 0.6492\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6665 - accuracy: 0.6022 - val_loss: 0.6282 - val_accuracy: 0.6582\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6619 - accuracy: 0.6075 - val_loss: 0.6272 - val_accuracy: 0.6658\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.6004 - val_loss: 0.6278 - val_accuracy: 0.6658\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6632 - accuracy: 0.6052 - val_loss: 0.6256 - val_accuracy: 0.6645\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6036 - val_loss: 0.6283 - val_accuracy: 0.6607\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6618 - accuracy: 0.6104 - val_loss: 0.6285 - val_accuracy: 0.6582\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6025 - val_loss: 0.6277 - val_accuracy: 0.6594\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6629 - accuracy: 0.6069 - val_loss: 0.6285 - val_accuracy: 0.6531\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6607 - accuracy: 0.6144 - val_loss: 0.6257 - val_accuracy: 0.6735\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6085 - val_loss: 0.6238 - val_accuracy: 0.6684\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6641 - accuracy: 0.5983 - val_loss: 0.6292 - val_accuracy: 0.6735\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6597 - accuracy: 0.6108 - val_loss: 0.6248 - val_accuracy: 0.6696\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6633 - accuracy: 0.6060 - val_loss: 0.6252 - val_accuracy: 0.6722\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6636 - accuracy: 0.6095 - val_loss: 0.6249 - val_accuracy: 0.6709\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6627 - accuracy: 0.6100 - val_loss: 0.6237 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6608 - accuracy: 0.6081 - val_loss: 0.6257 - val_accuracy: 0.6696\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6605 - accuracy: 0.6107 - val_loss: 0.6220 - val_accuracy: 0.6709\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.6122 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6075 - val_loss: 0.6271 - val_accuracy: 0.6709\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6617 - accuracy: 0.6118 - val_loss: 0.6272 - val_accuracy: 0.6633\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6108 - val_loss: 0.6248 - val_accuracy: 0.6645\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6138 - val_loss: 0.6247 - val_accuracy: 0.6684\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6657 - accuracy: 0.6031 - val_loss: 0.6267 - val_accuracy: 0.6722\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6605 - accuracy: 0.6060 - val_loss: 0.6208 - val_accuracy: 0.6760\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6576 - accuracy: 0.6088 - val_loss: 0.6235 - val_accuracy: 0.6747\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6604 - accuracy: 0.6065 - val_loss: 0.6225 - val_accuracy: 0.6735\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6589 - accuracy: 0.6164 - val_loss: 0.6215 - val_accuracy: 0.6696\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6588 - accuracy: 0.6104 - val_loss: 0.6285 - val_accuracy: 0.6773\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6585 - accuracy: 0.6129 - val_loss: 0.6223 - val_accuracy: 0.6747\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6572 - accuracy: 0.6157 - val_loss: 0.6233 - val_accuracy: 0.6722\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6132 - val_loss: 0.6240 - val_accuracy: 0.6747\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6105 - val_loss: 0.6224 - val_accuracy: 0.6747\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6166 - val_loss: 0.6212 - val_accuracy: 0.6735\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6169 - val_loss: 0.6187 - val_accuracy: 0.6722\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6600 - accuracy: 0.6088 - val_loss: 0.6204 - val_accuracy: 0.6747\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6591 - accuracy: 0.6117 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6569 - accuracy: 0.6177 - val_loss: 0.6209 - val_accuracy: 0.6747\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6570 - accuracy: 0.6222 - val_loss: 0.6197 - val_accuracy: 0.6735\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6557 - accuracy: 0.6189 - val_loss: 0.6185 - val_accuracy: 0.6747\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6557 - accuracy: 0.6159 - val_loss: 0.6212 - val_accuracy: 0.6760\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6567 - accuracy: 0.6172 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6564 - accuracy: 0.6206 - val_loss: 0.6219 - val_accuracy: 0.6760\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6528 - accuracy: 0.6236 - val_loss: 0.6183 - val_accuracy: 0.6735\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6554 - accuracy: 0.6275 - val_loss: 0.6201 - val_accuracy: 0.6735\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6562 - accuracy: 0.6181 - val_loss: 0.6212 - val_accuracy: 0.6773\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6549 - accuracy: 0.6267 - val_loss: 0.6197 - val_accuracy: 0.6760\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6568 - accuracy: 0.6203 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6549 - accuracy: 0.6194 - val_loss: 0.6176 - val_accuracy: 0.6747\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6541 - accuracy: 0.6202 - val_loss: 0.6159 - val_accuracy: 0.6760\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6569 - accuracy: 0.6144 - val_loss: 0.6181 - val_accuracy: 0.6760\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6572 - accuracy: 0.6173 - val_loss: 0.6210 - val_accuracy: 0.6747\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6575 - accuracy: 0.6117 - val_loss: 0.6211 - val_accuracy: 0.6824\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6555 - accuracy: 0.6207 - val_loss: 0.6176 - val_accuracy: 0.6798\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6520 - accuracy: 0.6182 - val_loss: 0.6171 - val_accuracy: 0.6786\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6566 - accuracy: 0.6173 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6531 - accuracy: 0.6251 - val_loss: 0.6202 - val_accuracy: 0.6747\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6523 - accuracy: 0.6204 - val_loss: 0.6156 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6521 - accuracy: 0.6281 - val_loss: 0.6162 - val_accuracy: 0.6786\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6548 - accuracy: 0.6217 - val_loss: 0.6170 - val_accuracy: 0.6747\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6484 - accuracy: 0.6301 - val_loss: 0.6149 - val_accuracy: 0.6811\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6528 - accuracy: 0.6260 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6496 - accuracy: 0.6267 - val_loss: 0.6172 - val_accuracy: 0.6811\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 8s 34ms/step - loss: 0.6499 - accuracy: 0.6262 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 5s 20ms/step - loss: 0.6554 - accuracy: 0.6203 - val_loss: 0.6157 - val_accuracy: 0.6811\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6511 - accuracy: 0.6267 - val_loss: 0.6154 - val_accuracy: 0.6786\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6534 - accuracy: 0.6274 - val_loss: 0.6182 - val_accuracy: 0.6760\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6535 - accuracy: 0.6270 - val_loss: 0.6178 - val_accuracy: 0.6760\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6498 - accuracy: 0.6245 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6509 - accuracy: 0.6260 - val_loss: 0.6201 - val_accuracy: 0.6786\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6510 - accuracy: 0.6235 - val_loss: 0.6143 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6496 - accuracy: 0.6248 - val_loss: 0.6163 - val_accuracy: 0.6786\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6481 - accuracy: 0.6276 - val_loss: 0.6138 - val_accuracy: 0.6811\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6518 - accuracy: 0.6209 - val_loss: 0.6141 - val_accuracy: 0.6760\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6540 - accuracy: 0.6212 - val_loss: 0.6206 - val_accuracy: 0.6798\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6519 - accuracy: 0.6235 - val_loss: 0.6161 - val_accuracy: 0.6773\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6542 - accuracy: 0.6245 - val_loss: 0.6187 - val_accuracy: 0.6798\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6513 - accuracy: 0.6242 - val_loss: 0.6152 - val_accuracy: 0.6760\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6518 - accuracy: 0.6213 - val_loss: 0.6147 - val_accuracy: 0.6786\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6524 - accuracy: 0.6220 - val_loss: 0.6154 - val_accuracy: 0.6773\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6470 - accuracy: 0.6277 - val_loss: 0.6155 - val_accuracy: 0.6786\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6507 - accuracy: 0.6238 - val_loss: 0.6142 - val_accuracy: 0.6786\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6475 - accuracy: 0.6307 - val_loss: 0.6149 - val_accuracy: 0.6811\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6479 - accuracy: 0.6262 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6252 - val_loss: 0.6166 - val_accuracy: 0.6798\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6498 - accuracy: 0.6241 - val_loss: 0.6125 - val_accuracy: 0.6798\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6488 - accuracy: 0.6267 - val_loss: 0.6166 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.6255 - val_loss: 0.6140 - val_accuracy: 0.6811\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6262 - val_loss: 0.6171 - val_accuracy: 0.6824\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6532 - accuracy: 0.6267 - val_loss: 0.6157 - val_accuracy: 0.6773\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6497 - accuracy: 0.6256 - val_loss: 0.6162 - val_accuracy: 0.6849\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6295 - val_loss: 0.6156 - val_accuracy: 0.6849\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6167 - val_loss: 0.6143 - val_accuracy: 0.6798\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6493 - accuracy: 0.6326 - val_loss: 0.6177 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6466 - accuracy: 0.6365 - val_loss: 0.6124 - val_accuracy: 0.6824\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6483 - accuracy: 0.6364 - val_loss: 0.6141 - val_accuracy: 0.6811\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6455 - accuracy: 0.6392 - val_loss: 0.6122 - val_accuracy: 0.6837\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6295 - val_loss: 0.6151 - val_accuracy: 0.6798\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6289 - val_loss: 0.6127 - val_accuracy: 0.6773\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6478 - accuracy: 0.6325 - val_loss: 0.6133 - val_accuracy: 0.6786\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6299 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6318 - val_loss: 0.6158 - val_accuracy: 0.6798\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6474 - accuracy: 0.6374 - val_loss: 0.6116 - val_accuracy: 0.6798\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6251 - val_loss: 0.6117 - val_accuracy: 0.6824\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6467 - accuracy: 0.6380 - val_loss: 0.6120 - val_accuracy: 0.6798\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6468 - accuracy: 0.6264 - val_loss: 0.6143 - val_accuracy: 0.6824\n",
      "Calculating for: 850 100 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_168 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7770 - accuracy: 0.5499 - val_loss: 0.6650 - val_accuracy: 0.6148\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5795 - val_loss: 0.6375 - val_accuracy: 0.6607\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6776 - accuracy: 0.5927 - val_loss: 0.6403 - val_accuracy: 0.6543\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6096 - val_loss: 0.6330 - val_accuracy: 0.6735\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6603 - accuracy: 0.6100 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6154 - val_loss: 0.6319 - val_accuracy: 0.6658\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.6202 - val_loss: 0.6323 - val_accuracy: 0.6620\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6208 - val_loss: 0.6280 - val_accuracy: 0.6760\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6209 - val_loss: 0.6340 - val_accuracy: 0.6658\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6225 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6450 - accuracy: 0.6301 - val_loss: 0.6246 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6301 - val_loss: 0.6251 - val_accuracy: 0.6684\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6309 - val_loss: 0.6291 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6402 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6368 - val_loss: 0.6260 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6399 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6462 - val_loss: 0.6252 - val_accuracy: 0.6735\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6437 - val_loss: 0.6202 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6436 - val_loss: 0.6241 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6410 - val_loss: 0.6245 - val_accuracy: 0.6722\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6481 - val_loss: 0.6242 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6483 - val_loss: 0.6288 - val_accuracy: 0.6645\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6491 - val_loss: 0.6232 - val_accuracy: 0.6696\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6496 - val_loss: 0.6280 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6564 - val_loss: 0.6213 - val_accuracy: 0.6696\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6277 - accuracy: 0.6498 - val_loss: 0.6225 - val_accuracy: 0.6735\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6510 - val_loss: 0.6273 - val_accuracy: 0.6735\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6601 - val_loss: 0.6262 - val_accuracy: 0.6645\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6209 - accuracy: 0.6655 - val_loss: 0.6272 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6564 - val_loss: 0.6272 - val_accuracy: 0.6620\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6654 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6601 - val_loss: 0.6282 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6600 - val_loss: 0.6309 - val_accuracy: 0.6709\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6624 - val_loss: 0.6272 - val_accuracy: 0.6709\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6112 - accuracy: 0.6658 - val_loss: 0.6296 - val_accuracy: 0.6747\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6689 - val_loss: 0.6304 - val_accuracy: 0.6709\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6687 - val_loss: 0.6294 - val_accuracy: 0.6722\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.6679 - val_loss: 0.6351 - val_accuracy: 0.6709\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6723 - val_loss: 0.6306 - val_accuracy: 0.6709\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6049 - accuracy: 0.6772 - val_loss: 0.6303 - val_accuracy: 0.6747\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6096 - accuracy: 0.6740 - val_loss: 0.6297 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6061 - accuracy: 0.6680 - val_loss: 0.6321 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6026 - accuracy: 0.6777 - val_loss: 0.6265 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6005 - accuracy: 0.6788 - val_loss: 0.6262 - val_accuracy: 0.6773\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6014 - accuracy: 0.6763 - val_loss: 0.6341 - val_accuracy: 0.6645\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6004 - accuracy: 0.6795 - val_loss: 0.6389 - val_accuracy: 0.6607\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6737 - val_loss: 0.6348 - val_accuracy: 0.6773\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6001 - accuracy: 0.6865 - val_loss: 0.6320 - val_accuracy: 0.6709\n",
      "Calculating for: 850 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_172 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7902 - accuracy: 0.5344 - val_loss: 0.6562 - val_accuracy: 0.6467\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7106 - accuracy: 0.5484 - val_loss: 0.6501 - val_accuracy: 0.6416\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5556 - val_loss: 0.6472 - val_accuracy: 0.6556\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5757 - val_loss: 0.6449 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5770 - val_loss: 0.6465 - val_accuracy: 0.6709\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.5769 - val_loss: 0.6406 - val_accuracy: 0.6671\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5838 - val_loss: 0.6395 - val_accuracy: 0.6735\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5965 - val_loss: 0.6402 - val_accuracy: 0.6760\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5912 - val_loss: 0.6364 - val_accuracy: 0.6786\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6021 - val_loss: 0.6368 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6650 - accuracy: 0.6005 - val_loss: 0.6368 - val_accuracy: 0.6735\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.5995 - val_loss: 0.6392 - val_accuracy: 0.6658\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6084 - val_loss: 0.6359 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6075 - val_loss: 0.6340 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6597 - accuracy: 0.6073 - val_loss: 0.6388 - val_accuracy: 0.6620\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6624 - accuracy: 0.6090 - val_loss: 0.6351 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6565 - accuracy: 0.6163 - val_loss: 0.6282 - val_accuracy: 0.6735\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6152 - val_loss: 0.6343 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6169 - val_loss: 0.6288 - val_accuracy: 0.6709\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6124 - val_loss: 0.6359 - val_accuracy: 0.6684\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6566 - accuracy: 0.6142 - val_loss: 0.6337 - val_accuracy: 0.6735\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6189 - val_loss: 0.6326 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6250 - val_loss: 0.6316 - val_accuracy: 0.6671\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6238 - val_loss: 0.6298 - val_accuracy: 0.6786\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6208 - val_loss: 0.6264 - val_accuracy: 0.6824\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6281 - val_loss: 0.6268 - val_accuracy: 0.6798\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6255 - val_loss: 0.6258 - val_accuracy: 0.6837\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6258 - val_loss: 0.6250 - val_accuracy: 0.6849\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6309 - val_loss: 0.6314 - val_accuracy: 0.6747\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6228 - val_loss: 0.6261 - val_accuracy: 0.6773\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6465 - accuracy: 0.6255 - val_loss: 0.6209 - val_accuracy: 0.6862\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6233 - val_loss: 0.6291 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6277 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6280 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6450 - accuracy: 0.6307 - val_loss: 0.6220 - val_accuracy: 0.6824\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6286 - val_loss: 0.6264 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6335 - val_loss: 0.6225 - val_accuracy: 0.6824\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6274 - val_loss: 0.6248 - val_accuracy: 0.6722\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6319 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6435 - accuracy: 0.6349 - val_loss: 0.6206 - val_accuracy: 0.6811\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6454 - accuracy: 0.6306 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6309 - val_loss: 0.6215 - val_accuracy: 0.6811\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6382 - val_loss: 0.6168 - val_accuracy: 0.6849\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.6346 - val_loss: 0.6208 - val_accuracy: 0.6811\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6406 - accuracy: 0.6354 - val_loss: 0.6214 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6449 - val_loss: 0.6219 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6468 - val_loss: 0.6277 - val_accuracy: 0.6722\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6420 - val_loss: 0.6198 - val_accuracy: 0.6786\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6417 - val_loss: 0.6200 - val_accuracy: 0.6735\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6402 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6380 - accuracy: 0.6408 - val_loss: 0.6185 - val_accuracy: 0.6798\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6317 - accuracy: 0.6500 - val_loss: 0.6120 - val_accuracy: 0.6901\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6436 - val_loss: 0.6135 - val_accuracy: 0.6849\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6335 - accuracy: 0.6442 - val_loss: 0.6168 - val_accuracy: 0.6837\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6495 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6341 - accuracy: 0.6497 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6466 - val_loss: 0.6201 - val_accuracy: 0.6798\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6347 - accuracy: 0.6495 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6467 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6554 - val_loss: 0.6216 - val_accuracy: 0.6811\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6459 - val_loss: 0.6204 - val_accuracy: 0.6875\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6304 - accuracy: 0.6459 - val_loss: 0.6192 - val_accuracy: 0.6888\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6473 - val_loss: 0.6159 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6315 - accuracy: 0.6559 - val_loss: 0.6211 - val_accuracy: 0.6849\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6577 - val_loss: 0.6231 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6603 - val_loss: 0.6144 - val_accuracy: 0.6913\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6493 - val_loss: 0.6186 - val_accuracy: 0.6888\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6557 - val_loss: 0.6116 - val_accuracy: 0.6926\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6511 - val_loss: 0.6119 - val_accuracy: 0.6939\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6288 - accuracy: 0.6540 - val_loss: 0.6207 - val_accuracy: 0.6862\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6505 - val_loss: 0.6181 - val_accuracy: 0.6849\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6608 - val_loss: 0.6165 - val_accuracy: 0.6849\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6276 - accuracy: 0.6555 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6234 - accuracy: 0.6556 - val_loss: 0.6191 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6245 - accuracy: 0.6579 - val_loss: 0.6145 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6626 - val_loss: 0.6158 - val_accuracy: 0.6901\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6223 - accuracy: 0.6591 - val_loss: 0.6149 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6255 - accuracy: 0.6576 - val_loss: 0.6149 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6236 - accuracy: 0.6572 - val_loss: 0.6165 - val_accuracy: 0.6849\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6616 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6246 - accuracy: 0.6601 - val_loss: 0.6215 - val_accuracy: 0.6862\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6222 - accuracy: 0.6608 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6169 - accuracy: 0.6686 - val_loss: 0.6168 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6593 - val_loss: 0.6157 - val_accuracy: 0.6811\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6181 - accuracy: 0.6663 - val_loss: 0.6133 - val_accuracy: 0.6798\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6649 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6213 - accuracy: 0.6687 - val_loss: 0.6165 - val_accuracy: 0.6773\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6673 - val_loss: 0.6196 - val_accuracy: 0.6735\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6154 - accuracy: 0.6687 - val_loss: 0.6188 - val_accuracy: 0.6786\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6161 - accuracy: 0.6643 - val_loss: 0.6180 - val_accuracy: 0.6875\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6170 - accuracy: 0.6713 - val_loss: 0.6218 - val_accuracy: 0.6760\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6643 - val_loss: 0.6220 - val_accuracy: 0.6760\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6615 - val_loss: 0.6210 - val_accuracy: 0.6760\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6572 - val_loss: 0.6163 - val_accuracy: 0.6811\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6155 - accuracy: 0.6683 - val_loss: 0.6172 - val_accuracy: 0.6811\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6628 - val_loss: 0.6208 - val_accuracy: 0.6786\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6140 - accuracy: 0.6645 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6133 - accuracy: 0.6639 - val_loss: 0.6249 - val_accuracy: 0.6747\n",
      "Calculating for: 850 100 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_176 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "227/249 [==========================>...] - ETA: 0s - loss: 0.8466 - accuracy: 0.4985WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8391 - accuracy: 0.4976 - val_loss: 0.6624 - val_accuracy: 0.6250\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7290 - accuracy: 0.5133 - val_loss: 0.6630 - val_accuracy: 0.6327\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.7061 - accuracy: 0.5177 - val_loss: 0.6621 - val_accuracy: 0.6250\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6977 - accuracy: 0.5187 - val_loss: 0.6671 - val_accuracy: 0.6263\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5306 - val_loss: 0.6660 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6937 - accuracy: 0.5256 - val_loss: 0.6760 - val_accuracy: 0.6288\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5251 - val_loss: 0.6706 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6897 - accuracy: 0.5360 - val_loss: 0.6705 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5431 - val_loss: 0.6654 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6893 - accuracy: 0.5407 - val_loss: 0.6686 - val_accuracy: 0.6250\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5285 - val_loss: 0.6636 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6877 - accuracy: 0.5430 - val_loss: 0.6658 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5369 - val_loss: 0.6663 - val_accuracy: 0.6212\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5354 - val_loss: 0.6710 - val_accuracy: 0.6301\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6883 - accuracy: 0.5430 - val_loss: 0.6630 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5340 - val_loss: 0.6679 - val_accuracy: 0.6301\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5360 - val_loss: 0.6665 - val_accuracy: 0.6378\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6887 - accuracy: 0.5357 - val_loss: 0.6642 - val_accuracy: 0.6339\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5443 - val_loss: 0.6660 - val_accuracy: 0.6390\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5421 - val_loss: 0.6608 - val_accuracy: 0.6224\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5421 - val_loss: 0.6637 - val_accuracy: 0.6250\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5472 - val_loss: 0.6644 - val_accuracy: 0.6390\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5510 - val_loss: 0.6614 - val_accuracy: 0.6339\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6856 - accuracy: 0.5517 - val_loss: 0.6615 - val_accuracy: 0.6390\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.5477 - val_loss: 0.6591 - val_accuracy: 0.6365\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6857 - accuracy: 0.5433 - val_loss: 0.6565 - val_accuracy: 0.6352\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6878 - accuracy: 0.5367 - val_loss: 0.6610 - val_accuracy: 0.6390\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6850 - accuracy: 0.5544 - val_loss: 0.6606 - val_accuracy: 0.6390\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6844 - accuracy: 0.5504 - val_loss: 0.6599 - val_accuracy: 0.6365\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6814 - accuracy: 0.5656 - val_loss: 0.6509 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6835 - accuracy: 0.5582 - val_loss: 0.6580 - val_accuracy: 0.6390\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6870 - accuracy: 0.5481 - val_loss: 0.6566 - val_accuracy: 0.6416\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6857 - accuracy: 0.5501 - val_loss: 0.6616 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5501 - val_loss: 0.6634 - val_accuracy: 0.6429\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6833 - accuracy: 0.5576 - val_loss: 0.6543 - val_accuracy: 0.6416\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5637 - val_loss: 0.6559 - val_accuracy: 0.6416\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6823 - accuracy: 0.5620 - val_loss: 0.6549 - val_accuracy: 0.6403\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6819 - accuracy: 0.5587 - val_loss: 0.6531 - val_accuracy: 0.6390\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6816 - accuracy: 0.5600 - val_loss: 0.6541 - val_accuracy: 0.6378\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6830 - accuracy: 0.5574 - val_loss: 0.6538 - val_accuracy: 0.6403\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5657 - val_loss: 0.6524 - val_accuracy: 0.6416\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6801 - accuracy: 0.5736 - val_loss: 0.6584 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5672 - val_loss: 0.6500 - val_accuracy: 0.6429\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5641 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6834 - accuracy: 0.5619 - val_loss: 0.6546 - val_accuracy: 0.6378\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5597 - val_loss: 0.6518 - val_accuracy: 0.6390\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5663 - val_loss: 0.6522 - val_accuracy: 0.6378\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6768 - accuracy: 0.5674 - val_loss: 0.6483 - val_accuracy: 0.6454\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5643 - val_loss: 0.6547 - val_accuracy: 0.6378\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6803 - accuracy: 0.5643 - val_loss: 0.6528 - val_accuracy: 0.6416\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.5705 - val_loss: 0.6467 - val_accuracy: 0.6390\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.5746 - val_loss: 0.6519 - val_accuracy: 0.6416\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6782 - accuracy: 0.5705 - val_loss: 0.6485 - val_accuracy: 0.6416\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6782 - accuracy: 0.5676 - val_loss: 0.6484 - val_accuracy: 0.6416\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6778 - accuracy: 0.5703 - val_loss: 0.6500 - val_accuracy: 0.6403\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5715 - val_loss: 0.6504 - val_accuracy: 0.6416\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5754 - val_loss: 0.6441 - val_accuracy: 0.6416\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5751 - val_loss: 0.6513 - val_accuracy: 0.6429\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6731 - accuracy: 0.5847 - val_loss: 0.6459 - val_accuracy: 0.6416\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6757 - accuracy: 0.5855 - val_loss: 0.6470 - val_accuracy: 0.6416\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6768 - accuracy: 0.5781 - val_loss: 0.6456 - val_accuracy: 0.6403\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5839 - val_loss: 0.6454 - val_accuracy: 0.6505\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6716 - accuracy: 0.5898 - val_loss: 0.6407 - val_accuracy: 0.6454\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.5841 - val_loss: 0.6430 - val_accuracy: 0.6480\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6757 - accuracy: 0.5765 - val_loss: 0.6415 - val_accuracy: 0.6429\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6753 - accuracy: 0.5761 - val_loss: 0.6465 - val_accuracy: 0.6441\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5819 - val_loss: 0.6416 - val_accuracy: 0.6416\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5800 - val_loss: 0.6445 - val_accuracy: 0.6429\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5809 - val_loss: 0.6468 - val_accuracy: 0.6556\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6700 - accuracy: 0.5877 - val_loss: 0.6394 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5793 - val_loss: 0.6460 - val_accuracy: 0.6569\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5813 - val_loss: 0.6432 - val_accuracy: 0.6492\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5885 - val_loss: 0.6418 - val_accuracy: 0.6480\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5889 - val_loss: 0.6431 - val_accuracy: 0.6467\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5830 - val_loss: 0.6428 - val_accuracy: 0.6480\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5847 - val_loss: 0.6422 - val_accuracy: 0.6480\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.5841 - val_loss: 0.6462 - val_accuracy: 0.6594\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5970 - val_loss: 0.6407 - val_accuracy: 0.6518\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5877 - val_loss: 0.6377 - val_accuracy: 0.6518\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6699 - accuracy: 0.5904 - val_loss: 0.6430 - val_accuracy: 0.6671\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.5847 - val_loss: 0.6430 - val_accuracy: 0.6607\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5953 - val_loss: 0.6446 - val_accuracy: 0.6671\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6699 - accuracy: 0.5831 - val_loss: 0.6379 - val_accuracy: 0.6480\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6708 - accuracy: 0.5907 - val_loss: 0.6398 - val_accuracy: 0.6543\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5893 - val_loss: 0.6414 - val_accuracy: 0.6556\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6681 - accuracy: 0.5899 - val_loss: 0.6397 - val_accuracy: 0.6671\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5928 - val_loss: 0.6416 - val_accuracy: 0.6582\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6718 - accuracy: 0.5870 - val_loss: 0.6408 - val_accuracy: 0.6582\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5995 - val_loss: 0.6439 - val_accuracy: 0.6747\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5830 - val_loss: 0.6434 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5903 - val_loss: 0.6457 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5943 - val_loss: 0.6374 - val_accuracy: 0.6671\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5903 - val_loss: 0.6369 - val_accuracy: 0.6594\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.5961 - val_loss: 0.6359 - val_accuracy: 0.6594\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6676 - accuracy: 0.5947 - val_loss: 0.6374 - val_accuracy: 0.6696\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5917 - val_loss: 0.6389 - val_accuracy: 0.6747\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5884 - val_loss: 0.6379 - val_accuracy: 0.6696\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5955 - val_loss: 0.6368 - val_accuracy: 0.6696\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6017 - val_loss: 0.6341 - val_accuracy: 0.6735\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.5957 - val_loss: 0.6362 - val_accuracy: 0.6696\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5997 - val_loss: 0.6356 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6630 - accuracy: 0.6015 - val_loss: 0.6339 - val_accuracy: 0.6658\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5972 - val_loss: 0.6410 - val_accuracy: 0.6722\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.6020 - val_loss: 0.6322 - val_accuracy: 0.6773\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.6041 - val_loss: 0.6386 - val_accuracy: 0.6722\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6634 - accuracy: 0.6000 - val_loss: 0.6331 - val_accuracy: 0.6747\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6652 - accuracy: 0.6068 - val_loss: 0.6313 - val_accuracy: 0.6760\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6660 - accuracy: 0.6030 - val_loss: 0.6327 - val_accuracy: 0.6773\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.5981 - val_loss: 0.6374 - val_accuracy: 0.6760\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6662 - accuracy: 0.6014 - val_loss: 0.6321 - val_accuracy: 0.6747\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6025 - val_loss: 0.6334 - val_accuracy: 0.6722\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5921 - val_loss: 0.6347 - val_accuracy: 0.6709\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6641 - accuracy: 0.6042 - val_loss: 0.6343 - val_accuracy: 0.6709\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6623 - accuracy: 0.6042 - val_loss: 0.6329 - val_accuracy: 0.6760\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6642 - accuracy: 0.5991 - val_loss: 0.6332 - val_accuracy: 0.6747\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5977 - val_loss: 0.6352 - val_accuracy: 0.6747\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6632 - accuracy: 0.6050 - val_loss: 0.6330 - val_accuracy: 0.6722\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6059 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6046 - val_loss: 0.6309 - val_accuracy: 0.6786\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6614 - accuracy: 0.6081 - val_loss: 0.6282 - val_accuracy: 0.6773\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6662 - accuracy: 0.5985 - val_loss: 0.6294 - val_accuracy: 0.6735\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6046 - val_loss: 0.6367 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6644 - accuracy: 0.6044 - val_loss: 0.6321 - val_accuracy: 0.6786\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.6005 - val_loss: 0.6302 - val_accuracy: 0.6760\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6000 - val_loss: 0.6273 - val_accuracy: 0.6786\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6618 - accuracy: 0.6036 - val_loss: 0.6281 - val_accuracy: 0.6786\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6022 - val_loss: 0.6313 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6060 - val_loss: 0.6279 - val_accuracy: 0.6773\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6632 - accuracy: 0.6058 - val_loss: 0.6305 - val_accuracy: 0.6760\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6579 - accuracy: 0.6064 - val_loss: 0.6274 - val_accuracy: 0.6735\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6108 - val_loss: 0.6306 - val_accuracy: 0.6786\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6630 - accuracy: 0.6071 - val_loss: 0.6306 - val_accuracy: 0.6824\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6583 - accuracy: 0.6159 - val_loss: 0.6263 - val_accuracy: 0.6786\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6603 - accuracy: 0.6101 - val_loss: 0.6278 - val_accuracy: 0.6811\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6143 - val_loss: 0.6326 - val_accuracy: 0.6824\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.6103 - val_loss: 0.6312 - val_accuracy: 0.6811\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.6109 - val_loss: 0.6293 - val_accuracy: 0.6837\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6593 - accuracy: 0.6112 - val_loss: 0.6283 - val_accuracy: 0.6849\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6142 - val_loss: 0.6250 - val_accuracy: 0.6837\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6611 - accuracy: 0.6125 - val_loss: 0.6284 - val_accuracy: 0.6824\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.6103 - val_loss: 0.6272 - val_accuracy: 0.6773\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6123 - val_loss: 0.6246 - val_accuracy: 0.6811\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6155 - val_loss: 0.6282 - val_accuracy: 0.6811\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6143 - val_loss: 0.6261 - val_accuracy: 0.6849\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6133 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6132 - val_loss: 0.6274 - val_accuracy: 0.6824\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6084 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6575 - accuracy: 0.6118 - val_loss: 0.6271 - val_accuracy: 0.6837\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6252 - val_loss: 0.6254 - val_accuracy: 0.6798\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6587 - accuracy: 0.6133 - val_loss: 0.6259 - val_accuracy: 0.6862\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6158 - val_loss: 0.6268 - val_accuracy: 0.6798\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6081 - val_loss: 0.6242 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6591 - accuracy: 0.6150 - val_loss: 0.6261 - val_accuracy: 0.6837\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6127 - val_loss: 0.6283 - val_accuracy: 0.6849\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6573 - accuracy: 0.6178 - val_loss: 0.6241 - val_accuracy: 0.6811\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6209 - val_loss: 0.6264 - val_accuracy: 0.6862\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6582 - accuracy: 0.6184 - val_loss: 0.6267 - val_accuracy: 0.6798\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6167 - val_loss: 0.6244 - val_accuracy: 0.6824\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6584 - accuracy: 0.6245 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6208 - val_loss: 0.6225 - val_accuracy: 0.6824\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6191 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6216 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6569 - accuracy: 0.6178 - val_loss: 0.6214 - val_accuracy: 0.6786\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6201 - val_loss: 0.6195 - val_accuracy: 0.6862\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6153 - val_loss: 0.6272 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6246 - val_loss: 0.6232 - val_accuracy: 0.6824\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6122 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6238 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6208 - val_loss: 0.6248 - val_accuracy: 0.6837\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6206 - val_loss: 0.6241 - val_accuracy: 0.6837\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6558 - accuracy: 0.6233 - val_loss: 0.6235 - val_accuracy: 0.6837\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6578 - accuracy: 0.6172 - val_loss: 0.6280 - val_accuracy: 0.6849\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6161 - val_loss: 0.6240 - val_accuracy: 0.6811\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6177 - val_loss: 0.6230 - val_accuracy: 0.6849\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6516 - accuracy: 0.6270 - val_loss: 0.6203 - val_accuracy: 0.6875\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6257 - val_loss: 0.6255 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6207 - val_loss: 0.6243 - val_accuracy: 0.6862\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6260 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6262 - val_loss: 0.6245 - val_accuracy: 0.6786\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6556 - accuracy: 0.6230 - val_loss: 0.6255 - val_accuracy: 0.6811\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6242 - val_loss: 0.6234 - val_accuracy: 0.6837\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6265 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6275 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6237 - val_loss: 0.6244 - val_accuracy: 0.6837\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6286 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6336 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6544 - accuracy: 0.6231 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6204 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6526 - accuracy: 0.6236 - val_loss: 0.6227 - val_accuracy: 0.6837\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6289 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6179 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6211 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6329 - val_loss: 0.6205 - val_accuracy: 0.6849\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6211 - val_loss: 0.6226 - val_accuracy: 0.6849\n",
      "Calculating for: 850 250 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_180 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7977 - accuracy: 0.5529 - val_loss: 0.7656 - val_accuracy: 0.3750\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6968 - accuracy: 0.5901 - val_loss: 0.6800 - val_accuracy: 0.5804\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5987 - val_loss: 0.6595 - val_accuracy: 0.6237\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6035 - val_loss: 0.6506 - val_accuracy: 0.6301\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6140 - val_loss: 0.6521 - val_accuracy: 0.6276\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6297 - val_loss: 0.6568 - val_accuracy: 0.6173\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6167 - val_loss: 0.6469 - val_accuracy: 0.6314\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6266 - val_loss: 0.6445 - val_accuracy: 0.6327\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6247 - val_loss: 0.6429 - val_accuracy: 0.6378\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6316 - val_loss: 0.6456 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6331 - val_loss: 0.6407 - val_accuracy: 0.6390\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6410 - val_loss: 0.6446 - val_accuracy: 0.6365\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6368 - val_loss: 0.6420 - val_accuracy: 0.6403\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6419 - val_loss: 0.6438 - val_accuracy: 0.6339\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6371 - accuracy: 0.6399 - val_loss: 0.6383 - val_accuracy: 0.6454\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6405 - val_loss: 0.6372 - val_accuracy: 0.6467\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6437 - val_loss: 0.6358 - val_accuracy: 0.6467\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6505 - val_loss: 0.6322 - val_accuracy: 0.6543\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6487 - val_loss: 0.6343 - val_accuracy: 0.6582\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6463 - val_loss: 0.6392 - val_accuracy: 0.6480\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6458 - val_loss: 0.6349 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6604 - val_loss: 0.6376 - val_accuracy: 0.6594\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6525 - val_loss: 0.6421 - val_accuracy: 0.6569\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6245 - accuracy: 0.6595 - val_loss: 0.6339 - val_accuracy: 0.6620\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6236 - accuracy: 0.6560 - val_loss: 0.6369 - val_accuracy: 0.6633\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6193 - accuracy: 0.6541 - val_loss: 0.6369 - val_accuracy: 0.6658\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6662 - val_loss: 0.6358 - val_accuracy: 0.6582\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6197 - accuracy: 0.6590 - val_loss: 0.6347 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6611 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6595 - val_loss: 0.6370 - val_accuracy: 0.6582\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6682 - val_loss: 0.6405 - val_accuracy: 0.6607\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6780 - val_loss: 0.6411 - val_accuracy: 0.6543\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6062 - accuracy: 0.6724 - val_loss: 0.6469 - val_accuracy: 0.6403\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6097 - accuracy: 0.6704 - val_loss: 0.6421 - val_accuracy: 0.6569\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6082 - accuracy: 0.6736 - val_loss: 0.6355 - val_accuracy: 0.6645\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6085 - accuracy: 0.6628 - val_loss: 0.6384 - val_accuracy: 0.6556\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6087 - accuracy: 0.6716 - val_loss: 0.6396 - val_accuracy: 0.6633\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6067 - accuracy: 0.6687 - val_loss: 0.6442 - val_accuracy: 0.6454\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6051 - accuracy: 0.6745 - val_loss: 0.6400 - val_accuracy: 0.6582\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6011 - accuracy: 0.6753 - val_loss: 0.6427 - val_accuracy: 0.6607\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5981 - accuracy: 0.6829 - val_loss: 0.6342 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5979 - accuracy: 0.6807 - val_loss: 0.6469 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5963 - accuracy: 0.6848 - val_loss: 0.6422 - val_accuracy: 0.6543\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5989 - accuracy: 0.6797 - val_loss: 0.6467 - val_accuracy: 0.6556\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5984 - accuracy: 0.6848 - val_loss: 0.6489 - val_accuracy: 0.6518\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5930 - accuracy: 0.6850 - val_loss: 0.6454 - val_accuracy: 0.6569\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5920 - accuracy: 0.6897 - val_loss: 0.6549 - val_accuracy: 0.6454\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5954 - accuracy: 0.6849 - val_loss: 0.6482 - val_accuracy: 0.6467\n",
      "Calculating for: 850 250 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_184 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8119 - accuracy: 0.5398 - val_loss: 0.6475 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7113 - accuracy: 0.5630 - val_loss: 0.6427 - val_accuracy: 0.6607\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6849 - accuracy: 0.5669 - val_loss: 0.6406 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5764 - val_loss: 0.6374 - val_accuracy: 0.6556\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5843 - val_loss: 0.6392 - val_accuracy: 0.6645\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.5967 - val_loss: 0.6350 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6675 - accuracy: 0.5941 - val_loss: 0.6352 - val_accuracy: 0.6671\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.6010 - val_loss: 0.6360 - val_accuracy: 0.6671\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.5970 - val_loss: 0.6357 - val_accuracy: 0.6620\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6653 - accuracy: 0.5962 - val_loss: 0.6346 - val_accuracy: 0.6620\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6120 - val_loss: 0.6304 - val_accuracy: 0.6671\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6095 - val_loss: 0.6308 - val_accuracy: 0.6722\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6059 - val_loss: 0.6276 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6059 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6118 - val_loss: 0.6255 - val_accuracy: 0.6722\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6164 - val_loss: 0.6284 - val_accuracy: 0.6735\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6140 - val_loss: 0.6317 - val_accuracy: 0.6735\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6128 - val_loss: 0.6265 - val_accuracy: 0.6747\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6582 - accuracy: 0.6150 - val_loss: 0.6240 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6163 - val_loss: 0.6266 - val_accuracy: 0.6798\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6154 - val_loss: 0.6207 - val_accuracy: 0.6786\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6130 - val_loss: 0.6272 - val_accuracy: 0.6875\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6548 - accuracy: 0.6153 - val_loss: 0.6255 - val_accuracy: 0.6773\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.6202 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6176 - val_loss: 0.6281 - val_accuracy: 0.6824\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6182 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6287 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6223 - val_loss: 0.6274 - val_accuracy: 0.6633\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6281 - val_loss: 0.6243 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6494 - accuracy: 0.6246 - val_loss: 0.6219 - val_accuracy: 0.6824\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6328 - val_loss: 0.6197 - val_accuracy: 0.6798\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6290 - val_loss: 0.6253 - val_accuracy: 0.6645\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6350 - val_loss: 0.6229 - val_accuracy: 0.6747\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6356 - val_loss: 0.6195 - val_accuracy: 0.6798\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6481 - accuracy: 0.6264 - val_loss: 0.6231 - val_accuracy: 0.6786\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6297 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6321 - val_loss: 0.6219 - val_accuracy: 0.6696\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6370 - val_loss: 0.6225 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6365 - val_loss: 0.6168 - val_accuracy: 0.6722\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6326 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6407 - val_loss: 0.6224 - val_accuracy: 0.6620\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6403 - accuracy: 0.6361 - val_loss: 0.6181 - val_accuracy: 0.6735\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6390 - val_loss: 0.6227 - val_accuracy: 0.6620\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6390 - accuracy: 0.6402 - val_loss: 0.6187 - val_accuracy: 0.6760\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6355 - accuracy: 0.6447 - val_loss: 0.6172 - val_accuracy: 0.6671\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6379 - accuracy: 0.6405 - val_loss: 0.6192 - val_accuracy: 0.6709\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6413 - val_loss: 0.6149 - val_accuracy: 0.6849\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6408 - val_loss: 0.6150 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6408 - val_loss: 0.6153 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6367 - accuracy: 0.6395 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6409 - val_loss: 0.6184 - val_accuracy: 0.6811\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6415 - val_loss: 0.6156 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6366 - val_loss: 0.6144 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6492 - val_loss: 0.6171 - val_accuracy: 0.6786\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6512 - val_loss: 0.6136 - val_accuracy: 0.6798\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6488 - val_loss: 0.6139 - val_accuracy: 0.6798\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6555 - val_loss: 0.6135 - val_accuracy: 0.6786\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6483 - val_loss: 0.6169 - val_accuracy: 0.6798\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6451 - val_loss: 0.6158 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6511 - val_loss: 0.6152 - val_accuracy: 0.6798\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6427 - val_loss: 0.6169 - val_accuracy: 0.6824\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6541 - val_loss: 0.6146 - val_accuracy: 0.6811\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6520 - val_loss: 0.6170 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6500 - val_loss: 0.6171 - val_accuracy: 0.6747\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6557 - val_loss: 0.6173 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6544 - val_loss: 0.6194 - val_accuracy: 0.6747\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6487 - val_loss: 0.6154 - val_accuracy: 0.6837\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6505 - val_loss: 0.6170 - val_accuracy: 0.6798\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6464 - val_loss: 0.6122 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6554 - val_loss: 0.6144 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6250 - accuracy: 0.6594 - val_loss: 0.6182 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6560 - val_loss: 0.6135 - val_accuracy: 0.6811\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6605 - val_loss: 0.6167 - val_accuracy: 0.6773\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6562 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6521 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6193 - accuracy: 0.6629 - val_loss: 0.6125 - val_accuracy: 0.6901\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6575 - val_loss: 0.6118 - val_accuracy: 0.6875\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6659 - val_loss: 0.6132 - val_accuracy: 0.6824\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6594 - val_loss: 0.6161 - val_accuracy: 0.6773\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6668 - val_loss: 0.6124 - val_accuracy: 0.6824\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6180 - accuracy: 0.6643 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6199 - accuracy: 0.6574 - val_loss: 0.6116 - val_accuracy: 0.6837\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6638 - val_loss: 0.6109 - val_accuracy: 0.6913\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6652 - val_loss: 0.6144 - val_accuracy: 0.6837\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6644 - val_loss: 0.6118 - val_accuracy: 0.6849\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6654 - val_loss: 0.6176 - val_accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6169 - accuracy: 0.6631 - val_loss: 0.6136 - val_accuracy: 0.6849\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6166 - accuracy: 0.6647 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6153 - accuracy: 0.6659 - val_loss: 0.6125 - val_accuracy: 0.6849\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6677 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6663 - val_loss: 0.6143 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6634 - val_loss: 0.6149 - val_accuracy: 0.6849\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6683 - val_loss: 0.6144 - val_accuracy: 0.6901\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6648 - val_loss: 0.6152 - val_accuracy: 0.6849\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6703 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6158 - accuracy: 0.6654 - val_loss: 0.6187 - val_accuracy: 0.6824\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6125 - accuracy: 0.6628 - val_loss: 0.6165 - val_accuracy: 0.6824\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6135 - accuracy: 0.6682 - val_loss: 0.6185 - val_accuracy: 0.6849\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6680 - val_loss: 0.6168 - val_accuracy: 0.6811\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6688 - val_loss: 0.6179 - val_accuracy: 0.6773\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6691 - val_loss: 0.6161 - val_accuracy: 0.6862\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6130 - accuracy: 0.6718 - val_loss: 0.6173 - val_accuracy: 0.6811\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6071 - accuracy: 0.6763 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6085 - accuracy: 0.6708 - val_loss: 0.6191 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6074 - accuracy: 0.6704 - val_loss: 0.6176 - val_accuracy: 0.6773\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6040 - accuracy: 0.6807 - val_loss: 0.6157 - val_accuracy: 0.6773\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6043 - accuracy: 0.6797 - val_loss: 0.6149 - val_accuracy: 0.6760\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6060 - accuracy: 0.6752 - val_loss: 0.6211 - val_accuracy: 0.6747\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6688 - val_loss: 0.6199 - val_accuracy: 0.6709\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6704 - val_loss: 0.6194 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6064 - accuracy: 0.6756 - val_loss: 0.6162 - val_accuracy: 0.6798\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6051 - accuracy: 0.6771 - val_loss: 0.6161 - val_accuracy: 0.6811\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6031 - accuracy: 0.6737 - val_loss: 0.6149 - val_accuracy: 0.6824\n",
      "Calculating for: 850 250 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_188 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8272 - accuracy: 0.5028 - val_loss: 0.6611 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7332 - accuracy: 0.5162 - val_loss: 0.6675 - val_accuracy: 0.6327\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6992 - accuracy: 0.5324 - val_loss: 0.6636 - val_accuracy: 0.6212\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6964 - accuracy: 0.5231 - val_loss: 0.6668 - val_accuracy: 0.6224\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5250 - val_loss: 0.6718 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5262 - val_loss: 0.6755 - val_accuracy: 0.6199\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5274 - val_loss: 0.6666 - val_accuracy: 0.6199\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5269 - val_loss: 0.6711 - val_accuracy: 0.6186\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5319 - val_loss: 0.6685 - val_accuracy: 0.6199\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.5377 - val_loss: 0.6626 - val_accuracy: 0.6186\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5423 - val_loss: 0.6619 - val_accuracy: 0.6186\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6901 - accuracy: 0.5309 - val_loss: 0.6623 - val_accuracy: 0.6199\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5355 - val_loss: 0.6647 - val_accuracy: 0.6212\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.5339 - val_loss: 0.6620 - val_accuracy: 0.6199\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.5402 - val_loss: 0.6654 - val_accuracy: 0.6212\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5406 - val_loss: 0.6635 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5371 - val_loss: 0.6671 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6878 - accuracy: 0.5484 - val_loss: 0.6627 - val_accuracy: 0.6237\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6869 - accuracy: 0.5465 - val_loss: 0.6619 - val_accuracy: 0.6224\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6884 - accuracy: 0.5481 - val_loss: 0.6620 - val_accuracy: 0.6237\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5394 - val_loss: 0.6600 - val_accuracy: 0.6301\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.5430 - val_loss: 0.6578 - val_accuracy: 0.6250\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.5525 - val_loss: 0.6573 - val_accuracy: 0.6352\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6881 - accuracy: 0.5448 - val_loss: 0.6610 - val_accuracy: 0.6352\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5559 - val_loss: 0.6568 - val_accuracy: 0.6339\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6842 - accuracy: 0.5618 - val_loss: 0.6562 - val_accuracy: 0.6301\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5599 - val_loss: 0.6551 - val_accuracy: 0.6352\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5533 - val_loss: 0.6584 - val_accuracy: 0.6378\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.5536 - val_loss: 0.6541 - val_accuracy: 0.6352\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6812 - accuracy: 0.5657 - val_loss: 0.6533 - val_accuracy: 0.6352\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5602 - val_loss: 0.6521 - val_accuracy: 0.6352\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6843 - accuracy: 0.5584 - val_loss: 0.6537 - val_accuracy: 0.6365\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5584 - val_loss: 0.6553 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.5585 - val_loss: 0.6527 - val_accuracy: 0.6365\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6811 - accuracy: 0.5686 - val_loss: 0.6523 - val_accuracy: 0.6390\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6812 - accuracy: 0.5593 - val_loss: 0.6516 - val_accuracy: 0.6390\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6808 - accuracy: 0.5614 - val_loss: 0.6514 - val_accuracy: 0.6467\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6826 - accuracy: 0.5544 - val_loss: 0.6495 - val_accuracy: 0.6467\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6777 - accuracy: 0.5686 - val_loss: 0.6475 - val_accuracy: 0.6403\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6806 - accuracy: 0.5657 - val_loss: 0.6500 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5687 - val_loss: 0.6492 - val_accuracy: 0.6480\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6768 - accuracy: 0.5737 - val_loss: 0.6474 - val_accuracy: 0.6467\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5721 - val_loss: 0.6478 - val_accuracy: 0.6441\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6795 - accuracy: 0.5708 - val_loss: 0.6497 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6781 - accuracy: 0.5749 - val_loss: 0.6481 - val_accuracy: 0.6429\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6769 - accuracy: 0.5785 - val_loss: 0.6469 - val_accuracy: 0.6441\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6769 - accuracy: 0.5799 - val_loss: 0.6459 - val_accuracy: 0.6441\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5770 - val_loss: 0.6461 - val_accuracy: 0.6429\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5749 - val_loss: 0.6470 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6760 - accuracy: 0.5804 - val_loss: 0.6447 - val_accuracy: 0.6416\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6765 - accuracy: 0.5782 - val_loss: 0.6455 - val_accuracy: 0.6403\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5838 - val_loss: 0.6423 - val_accuracy: 0.6416\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5784 - val_loss: 0.6435 - val_accuracy: 0.6429\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5894 - val_loss: 0.6410 - val_accuracy: 0.6441\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5826 - val_loss: 0.6425 - val_accuracy: 0.6429\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6730 - accuracy: 0.5898 - val_loss: 0.6428 - val_accuracy: 0.6467\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5862 - val_loss: 0.6435 - val_accuracy: 0.6467\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6723 - accuracy: 0.5839 - val_loss: 0.6436 - val_accuracy: 0.6467\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5818 - val_loss: 0.6428 - val_accuracy: 0.6480\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5862 - val_loss: 0.6424 - val_accuracy: 0.6416\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5902 - val_loss: 0.6420 - val_accuracy: 0.6403\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.5823 - val_loss: 0.6441 - val_accuracy: 0.6403\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5874 - val_loss: 0.6421 - val_accuracy: 0.6403\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5869 - val_loss: 0.6405 - val_accuracy: 0.6480\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5916 - val_loss: 0.6392 - val_accuracy: 0.6505\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5931 - val_loss: 0.6389 - val_accuracy: 0.6492\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.5919 - val_loss: 0.6394 - val_accuracy: 0.6505\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5898 - val_loss: 0.6398 - val_accuracy: 0.6480\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5936 - val_loss: 0.6400 - val_accuracy: 0.6505\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5906 - val_loss: 0.6403 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5922 - val_loss: 0.6387 - val_accuracy: 0.6492\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6672 - accuracy: 0.5968 - val_loss: 0.6377 - val_accuracy: 0.6531\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5981 - val_loss: 0.6397 - val_accuracy: 0.6492\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5932 - val_loss: 0.6405 - val_accuracy: 0.6518\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5897 - val_loss: 0.6413 - val_accuracy: 0.6518\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5972 - val_loss: 0.6373 - val_accuracy: 0.6505\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5963 - val_loss: 0.6391 - val_accuracy: 0.6543\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.6002 - val_loss: 0.6353 - val_accuracy: 0.6518\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5951 - val_loss: 0.6383 - val_accuracy: 0.6492\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.6027 - val_loss: 0.6378 - val_accuracy: 0.6505\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5934 - val_loss: 0.6381 - val_accuracy: 0.6543\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.5981 - val_loss: 0.6359 - val_accuracy: 0.6505\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.5986 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6020 - val_loss: 0.6335 - val_accuracy: 0.6594\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5933 - val_loss: 0.6345 - val_accuracy: 0.6594\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.6004 - val_loss: 0.6386 - val_accuracy: 0.6633\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5998 - val_loss: 0.6350 - val_accuracy: 0.6607\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.5980 - val_loss: 0.6338 - val_accuracy: 0.6633\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6656 - accuracy: 0.6004 - val_loss: 0.6350 - val_accuracy: 0.6696\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5982 - val_loss: 0.6344 - val_accuracy: 0.6582\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.5858 - val_loss: 0.6338 - val_accuracy: 0.6645\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6012 - val_loss: 0.6338 - val_accuracy: 0.6645\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.5995 - val_loss: 0.6346 - val_accuracy: 0.6645\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.5991 - val_loss: 0.6317 - val_accuracy: 0.6722\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.5992 - val_loss: 0.6335 - val_accuracy: 0.6607\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.6042 - val_loss: 0.6331 - val_accuracy: 0.6722\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6030 - val_loss: 0.6360 - val_accuracy: 0.6709\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5963 - val_loss: 0.6310 - val_accuracy: 0.6696\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6037 - val_loss: 0.6297 - val_accuracy: 0.6709\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6040 - val_loss: 0.6306 - val_accuracy: 0.6735\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6056 - val_loss: 0.6270 - val_accuracy: 0.6633\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6034 - val_loss: 0.6293 - val_accuracy: 0.6735\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6041 - val_loss: 0.6290 - val_accuracy: 0.6722\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6093 - val_loss: 0.6288 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6127 - val_loss: 0.6280 - val_accuracy: 0.6760\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6046 - val_loss: 0.6313 - val_accuracy: 0.6709\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6075 - val_loss: 0.6292 - val_accuracy: 0.6760\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6010 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6107 - val_loss: 0.6294 - val_accuracy: 0.6671\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6094 - val_loss: 0.6311 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6035 - val_loss: 0.6289 - val_accuracy: 0.6735\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6110 - val_loss: 0.6312 - val_accuracy: 0.6760\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6049 - val_loss: 0.6298 - val_accuracy: 0.6760\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6168 - val_loss: 0.6253 - val_accuracy: 0.6760\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6120 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6119 - val_loss: 0.6289 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6081 - val_loss: 0.6248 - val_accuracy: 0.6747\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6014 - val_loss: 0.6259 - val_accuracy: 0.6747\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6133 - val_loss: 0.6260 - val_accuracy: 0.6735\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6127 - val_loss: 0.6281 - val_accuracy: 0.6798\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6155 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6059 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6083 - val_loss: 0.6259 - val_accuracy: 0.6760\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6089 - val_loss: 0.6253 - val_accuracy: 0.6722\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6088 - val_loss: 0.6310 - val_accuracy: 0.6811\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6058 - val_loss: 0.6258 - val_accuracy: 0.6786\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6085 - val_loss: 0.6273 - val_accuracy: 0.6824\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6078 - val_loss: 0.6299 - val_accuracy: 0.6837\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6158 - val_loss: 0.6221 - val_accuracy: 0.6747\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6197 - val_loss: 0.6233 - val_accuracy: 0.6798\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6173 - val_loss: 0.6257 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6101 - val_loss: 0.6241 - val_accuracy: 0.6798\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6125 - val_loss: 0.6261 - val_accuracy: 0.6824\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6128 - val_loss: 0.6283 - val_accuracy: 0.6798\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6196 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6130 - val_loss: 0.6239 - val_accuracy: 0.6760\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6163 - val_loss: 0.6237 - val_accuracy: 0.6837\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6178 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6193 - val_loss: 0.6225 - val_accuracy: 0.6760\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6167 - val_loss: 0.6262 - val_accuracy: 0.6811\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6171 - val_loss: 0.6237 - val_accuracy: 0.6811\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6154 - val_loss: 0.6223 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6217 - val_loss: 0.6246 - val_accuracy: 0.6798\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6144 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6245 - val_loss: 0.6200 - val_accuracy: 0.6773\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6145 - val_loss: 0.6223 - val_accuracy: 0.6786\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6118 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6162 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6178 - val_loss: 0.6169 - val_accuracy: 0.6824\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6186 - val_loss: 0.6190 - val_accuracy: 0.6837\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6222 - val_loss: 0.6251 - val_accuracy: 0.6773\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6563 - accuracy: 0.6155 - val_loss: 0.6213 - val_accuracy: 0.6798\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6531 - accuracy: 0.6186 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6213 - val_loss: 0.6193 - val_accuracy: 0.6811\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6250 - val_loss: 0.6212 - val_accuracy: 0.6786\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6140 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6174 - val_loss: 0.6218 - val_accuracy: 0.6811\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6198 - val_accuracy: 0.6837\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6187 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6196 - val_loss: 0.6205 - val_accuracy: 0.6824\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6267 - val_loss: 0.6203 - val_accuracy: 0.6786\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6192 - val_loss: 0.6187 - val_accuracy: 0.6824\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6150 - val_loss: 0.6214 - val_accuracy: 0.6786\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6264 - val_loss: 0.6178 - val_accuracy: 0.6811\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.6201 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6236 - val_loss: 0.6194 - val_accuracy: 0.6798\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6521 - accuracy: 0.6212 - val_loss: 0.6191 - val_accuracy: 0.6824\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6233 - val_loss: 0.6198 - val_accuracy: 0.6798\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6198 - val_loss: 0.6199 - val_accuracy: 0.6849\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6243 - val_loss: 0.6182 - val_accuracy: 0.6760\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6230 - val_loss: 0.6217 - val_accuracy: 0.6760\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6208 - val_loss: 0.6175 - val_accuracy: 0.6747\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6204 - val_loss: 0.6191 - val_accuracy: 0.6760\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6140 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6275 - val_loss: 0.6186 - val_accuracy: 0.6849\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6222 - val_loss: 0.6191 - val_accuracy: 0.6773\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6230 - val_loss: 0.6185 - val_accuracy: 0.6773\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6202 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6225 - val_loss: 0.6194 - val_accuracy: 0.6824\n",
      "Calculating for: 850 400 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_192 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7962 - accuracy: 0.5462 - val_loss: 0.6430 - val_accuracy: 0.6556\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6953 - accuracy: 0.5883 - val_loss: 0.6276 - val_accuracy: 0.6773\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6698 - accuracy: 0.5990 - val_loss: 0.6278 - val_accuracy: 0.6837\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6566 - accuracy: 0.6140 - val_loss: 0.6249 - val_accuracy: 0.6875\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6559 - accuracy: 0.6177 - val_loss: 0.6239 - val_accuracy: 0.6824\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6477 - accuracy: 0.6306 - val_loss: 0.6205 - val_accuracy: 0.6786\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6211 - val_loss: 0.6250 - val_accuracy: 0.6875\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6458 - accuracy: 0.6253 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6460 - accuracy: 0.6296 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6432 - accuracy: 0.6314 - val_loss: 0.6159 - val_accuracy: 0.6913\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6434 - accuracy: 0.6334 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6428 - accuracy: 0.6374 - val_loss: 0.6208 - val_accuracy: 0.6888\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6356 - val_loss: 0.6154 - val_accuracy: 0.6862\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6358 - accuracy: 0.6451 - val_loss: 0.6184 - val_accuracy: 0.6862\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6339 - accuracy: 0.6468 - val_loss: 0.6205 - val_accuracy: 0.6926\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.6472 - val_loss: 0.6154 - val_accuracy: 0.6875\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6319 - accuracy: 0.6441 - val_loss: 0.6162 - val_accuracy: 0.6875\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6311 - accuracy: 0.6443 - val_loss: 0.6149 - val_accuracy: 0.6811\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6306 - accuracy: 0.6506 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6269 - accuracy: 0.6565 - val_loss: 0.6167 - val_accuracy: 0.6798\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6291 - accuracy: 0.6475 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6286 - accuracy: 0.6539 - val_loss: 0.6160 - val_accuracy: 0.6849\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6272 - accuracy: 0.6525 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6203 - accuracy: 0.6623 - val_loss: 0.6162 - val_accuracy: 0.6773\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6235 - accuracy: 0.6531 - val_loss: 0.6137 - val_accuracy: 0.6862\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6191 - accuracy: 0.6536 - val_loss: 0.6154 - val_accuracy: 0.6849\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6205 - accuracy: 0.6614 - val_loss: 0.6201 - val_accuracy: 0.6786\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6143 - accuracy: 0.6716 - val_loss: 0.6128 - val_accuracy: 0.6849\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6143 - accuracy: 0.6664 - val_loss: 0.6172 - val_accuracy: 0.6875\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6112 - accuracy: 0.6706 - val_loss: 0.6180 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6137 - accuracy: 0.6637 - val_loss: 0.6134 - val_accuracy: 0.6862\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6047 - accuracy: 0.6746 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6134 - accuracy: 0.6697 - val_loss: 0.6143 - val_accuracy: 0.6837\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6113 - accuracy: 0.6721 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6104 - accuracy: 0.6718 - val_loss: 0.6214 - val_accuracy: 0.6811\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6030 - accuracy: 0.6709 - val_loss: 0.6195 - val_accuracy: 0.6849\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6041 - accuracy: 0.6757 - val_loss: 0.6238 - val_accuracy: 0.6773\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6048 - accuracy: 0.6755 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6043 - accuracy: 0.6737 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6023 - accuracy: 0.6777 - val_loss: 0.6244 - val_accuracy: 0.6773\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6002 - accuracy: 0.6817 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6019 - accuracy: 0.6801 - val_loss: 0.6318 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6004 - accuracy: 0.6799 - val_loss: 0.6283 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5966 - accuracy: 0.6820 - val_loss: 0.6235 - val_accuracy: 0.6811\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6004 - accuracy: 0.6801 - val_loss: 0.6246 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5962 - accuracy: 0.6767 - val_loss: 0.6284 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5919 - accuracy: 0.6873 - val_loss: 0.6281 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5927 - accuracy: 0.6881 - val_loss: 0.6267 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5914 - accuracy: 0.6938 - val_loss: 0.6274 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5949 - accuracy: 0.6849 - val_loss: 0.6320 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5921 - accuracy: 0.6791 - val_loss: 0.6273 - val_accuracy: 0.6735\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5821 - accuracy: 0.6922 - val_loss: 0.6289 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5891 - accuracy: 0.6894 - val_loss: 0.6279 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5834 - accuracy: 0.6976 - val_loss: 0.6359 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5866 - accuracy: 0.6919 - val_loss: 0.6287 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5847 - accuracy: 0.6937 - val_loss: 0.6304 - val_accuracy: 0.6760\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5837 - accuracy: 0.6930 - val_loss: 0.6300 - val_accuracy: 0.6709\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5814 - accuracy: 0.6996 - val_loss: 0.6327 - val_accuracy: 0.6760\n",
      "Calculating for: 850 400 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_196 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.8353 - accuracy: 0.5412 - val_loss: 0.6612 - val_accuracy: 0.6684\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.7284 - accuracy: 0.5497 - val_loss: 0.6513 - val_accuracy: 0.6696\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6854 - accuracy: 0.5764 - val_loss: 0.6548 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6804 - accuracy: 0.5777 - val_loss: 0.6493 - val_accuracy: 0.6722\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6723 - accuracy: 0.5873 - val_loss: 0.6527 - val_accuracy: 0.6543\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6679 - accuracy: 0.5987 - val_loss: 0.6496 - val_accuracy: 0.6569\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6672 - accuracy: 0.5936 - val_loss: 0.6474 - val_accuracy: 0.6671\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6678 - accuracy: 0.5952 - val_loss: 0.6414 - val_accuracy: 0.6671\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6663 - accuracy: 0.5971 - val_loss: 0.6386 - val_accuracy: 0.6696\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6624 - accuracy: 0.6054 - val_loss: 0.6420 - val_accuracy: 0.6722\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6635 - accuracy: 0.6058 - val_loss: 0.6439 - val_accuracy: 0.6454\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6583 - accuracy: 0.6112 - val_loss: 0.6366 - val_accuracy: 0.6620\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6572 - accuracy: 0.6119 - val_loss: 0.6358 - val_accuracy: 0.6735\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6578 - accuracy: 0.6171 - val_loss: 0.6392 - val_accuracy: 0.6518\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6529 - accuracy: 0.6242 - val_loss: 0.6354 - val_accuracy: 0.6607\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6558 - accuracy: 0.6115 - val_loss: 0.6302 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6540 - accuracy: 0.6179 - val_loss: 0.6311 - val_accuracy: 0.6658\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6553 - accuracy: 0.6118 - val_loss: 0.6380 - val_accuracy: 0.6543\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6519 - accuracy: 0.6222 - val_loss: 0.6307 - val_accuracy: 0.6633\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6533 - accuracy: 0.6174 - val_loss: 0.6323 - val_accuracy: 0.6607\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6517 - accuracy: 0.6188 - val_loss: 0.6275 - val_accuracy: 0.6582\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6542 - accuracy: 0.6223 - val_loss: 0.6319 - val_accuracy: 0.6671\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6524 - accuracy: 0.6207 - val_loss: 0.6326 - val_accuracy: 0.6633\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6496 - accuracy: 0.6237 - val_loss: 0.6343 - val_accuracy: 0.6582\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6483 - accuracy: 0.6282 - val_loss: 0.6299 - val_accuracy: 0.6607\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6460 - accuracy: 0.6319 - val_loss: 0.6274 - val_accuracy: 0.6633\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6483 - accuracy: 0.6306 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6493 - accuracy: 0.6279 - val_loss: 0.6293 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6481 - accuracy: 0.6305 - val_loss: 0.6249 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6445 - accuracy: 0.6296 - val_loss: 0.6247 - val_accuracy: 0.6645\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6459 - accuracy: 0.6341 - val_loss: 0.6225 - val_accuracy: 0.6773\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6461 - accuracy: 0.6309 - val_loss: 0.6272 - val_accuracy: 0.6607\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6448 - accuracy: 0.6341 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6434 - accuracy: 0.6343 - val_loss: 0.6233 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6406 - accuracy: 0.6374 - val_loss: 0.6212 - val_accuracy: 0.6722\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6375 - accuracy: 0.6414 - val_loss: 0.6182 - val_accuracy: 0.6798\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6446 - accuracy: 0.6304 - val_loss: 0.6218 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6402 - accuracy: 0.6372 - val_loss: 0.6230 - val_accuracy: 0.6722\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6392 - accuracy: 0.6355 - val_loss: 0.6238 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6402 - accuracy: 0.6358 - val_loss: 0.6237 - val_accuracy: 0.6671\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6375 - accuracy: 0.6370 - val_loss: 0.6261 - val_accuracy: 0.6543\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6406 - accuracy: 0.6385 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6380 - accuracy: 0.6402 - val_loss: 0.6206 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6348 - accuracy: 0.6476 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6342 - accuracy: 0.6417 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6355 - accuracy: 0.6458 - val_loss: 0.6201 - val_accuracy: 0.6760\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6375 - accuracy: 0.6380 - val_loss: 0.6226 - val_accuracy: 0.6773\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6330 - accuracy: 0.6444 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6348 - accuracy: 0.6467 - val_loss: 0.6289 - val_accuracy: 0.6569\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6350 - accuracy: 0.6402 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6314 - accuracy: 0.6516 - val_loss: 0.6197 - val_accuracy: 0.6773\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6307 - accuracy: 0.6415 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6312 - accuracy: 0.6503 - val_loss: 0.6283 - val_accuracy: 0.6620\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6300 - accuracy: 0.6478 - val_loss: 0.6174 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6293 - accuracy: 0.6520 - val_loss: 0.6241 - val_accuracy: 0.6735\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6300 - accuracy: 0.6481 - val_loss: 0.6253 - val_accuracy: 0.6722\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6249 - accuracy: 0.6555 - val_loss: 0.6200 - val_accuracy: 0.6760\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6310 - accuracy: 0.6490 - val_loss: 0.6212 - val_accuracy: 0.6747\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6260 - accuracy: 0.6496 - val_loss: 0.6235 - val_accuracy: 0.6798\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6276 - accuracy: 0.6532 - val_loss: 0.6196 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6278 - accuracy: 0.6518 - val_loss: 0.6187 - val_accuracy: 0.6773\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6249 - accuracy: 0.6487 - val_loss: 0.6194 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6266 - accuracy: 0.6506 - val_loss: 0.6184 - val_accuracy: 0.6773\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6252 - accuracy: 0.6575 - val_loss: 0.6181 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6220 - accuracy: 0.6545 - val_loss: 0.6204 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6253 - accuracy: 0.6583 - val_loss: 0.6154 - val_accuracy: 0.6798\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6262 - accuracy: 0.6539 - val_loss: 0.6155 - val_accuracy: 0.6773\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6211 - accuracy: 0.6640 - val_loss: 0.6166 - val_accuracy: 0.6773\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6213 - accuracy: 0.6603 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6224 - accuracy: 0.6581 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6237 - accuracy: 0.6577 - val_loss: 0.6196 - val_accuracy: 0.6786\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6190 - accuracy: 0.6600 - val_loss: 0.6196 - val_accuracy: 0.6773\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6193 - accuracy: 0.6605 - val_loss: 0.6213 - val_accuracy: 0.6747\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6185 - accuracy: 0.6579 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6178 - accuracy: 0.6654 - val_loss: 0.6225 - val_accuracy: 0.6786\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6176 - accuracy: 0.6603 - val_loss: 0.6250 - val_accuracy: 0.6786\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6157 - accuracy: 0.6626 - val_loss: 0.6203 - val_accuracy: 0.6773\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6629 - val_loss: 0.6244 - val_accuracy: 0.6773\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6190 - accuracy: 0.6652 - val_loss: 0.6207 - val_accuracy: 0.6773\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6118 - accuracy: 0.6683 - val_loss: 0.6206 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6174 - accuracy: 0.6640 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6113 - accuracy: 0.6727 - val_loss: 0.6202 - val_accuracy: 0.6747\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6135 - accuracy: 0.6701 - val_loss: 0.6221 - val_accuracy: 0.6773\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6128 - accuracy: 0.6693 - val_loss: 0.6220 - val_accuracy: 0.6798\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6137 - accuracy: 0.6682 - val_loss: 0.6202 - val_accuracy: 0.6786\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6149 - accuracy: 0.6642 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6123 - accuracy: 0.6663 - val_loss: 0.6242 - val_accuracy: 0.6684\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6141 - accuracy: 0.6659 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6099 - accuracy: 0.6719 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6102 - accuracy: 0.6709 - val_loss: 0.6207 - val_accuracy: 0.6747\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6068 - accuracy: 0.6741 - val_loss: 0.6226 - val_accuracy: 0.6760\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6121 - accuracy: 0.6673 - val_loss: 0.6236 - val_accuracy: 0.6786\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6047 - accuracy: 0.6734 - val_loss: 0.6223 - val_accuracy: 0.6798\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6105 - accuracy: 0.6659 - val_loss: 0.6221 - val_accuracy: 0.6709\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6054 - accuracy: 0.6775 - val_loss: 0.6231 - val_accuracy: 0.6684\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6051 - accuracy: 0.6717 - val_loss: 0.6236 - val_accuracy: 0.6735\n",
      "Calculating for: 850 400 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_200 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.8557 - accuracy: 0.5087 - val_loss: 0.6652 - val_accuracy: 0.6186\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.7391 - accuracy: 0.5270 - val_loss: 0.6619 - val_accuracy: 0.6224\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.7060 - accuracy: 0.5310 - val_loss: 0.6615 - val_accuracy: 0.6224\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6948 - accuracy: 0.5299 - val_loss: 0.6619 - val_accuracy: 0.6224\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6905 - accuracy: 0.5388 - val_loss: 0.6666 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6903 - accuracy: 0.5343 - val_loss: 0.6663 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6904 - accuracy: 0.5329 - val_loss: 0.6645 - val_accuracy: 0.6224\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6905 - accuracy: 0.5285 - val_loss: 0.6667 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6889 - accuracy: 0.5458 - val_loss: 0.6634 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6887 - accuracy: 0.5430 - val_loss: 0.6614 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6899 - accuracy: 0.5430 - val_loss: 0.6642 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6878 - accuracy: 0.5416 - val_loss: 0.6633 - val_accuracy: 0.6224\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6863 - accuracy: 0.5447 - val_loss: 0.6605 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6870 - accuracy: 0.5496 - val_loss: 0.6616 - val_accuracy: 0.6224\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6872 - accuracy: 0.5482 - val_loss: 0.6621 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6869 - accuracy: 0.5521 - val_loss: 0.6586 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6848 - accuracy: 0.5553 - val_loss: 0.6589 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6851 - accuracy: 0.5569 - val_loss: 0.6590 - val_accuracy: 0.6250\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6857 - accuracy: 0.5536 - val_loss: 0.6593 - val_accuracy: 0.6301\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6847 - accuracy: 0.5541 - val_loss: 0.6603 - val_accuracy: 0.6339\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6836 - accuracy: 0.5579 - val_loss: 0.6558 - val_accuracy: 0.6339\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6831 - accuracy: 0.5604 - val_loss: 0.6566 - val_accuracy: 0.6365\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6825 - accuracy: 0.5615 - val_loss: 0.6555 - val_accuracy: 0.6365\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6800 - accuracy: 0.5664 - val_loss: 0.6531 - val_accuracy: 0.6352\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6805 - accuracy: 0.5668 - val_loss: 0.6523 - val_accuracy: 0.6352\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6820 - accuracy: 0.5658 - val_loss: 0.6526 - val_accuracy: 0.6352\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6808 - accuracy: 0.5649 - val_loss: 0.6518 - val_accuracy: 0.6352\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6829 - accuracy: 0.5638 - val_loss: 0.6538 - val_accuracy: 0.6429\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6800 - accuracy: 0.5723 - val_loss: 0.6518 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6820 - accuracy: 0.5674 - val_loss: 0.6532 - val_accuracy: 0.6416\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6805 - accuracy: 0.5728 - val_loss: 0.6511 - val_accuracy: 0.6429\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6804 - accuracy: 0.5646 - val_loss: 0.6522 - val_accuracy: 0.6429\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6786 - accuracy: 0.5749 - val_loss: 0.6495 - val_accuracy: 0.6416\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6793 - accuracy: 0.5744 - val_loss: 0.6504 - val_accuracy: 0.6429\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6813 - accuracy: 0.5668 - val_loss: 0.6516 - val_accuracy: 0.6429\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6787 - accuracy: 0.5685 - val_loss: 0.6501 - val_accuracy: 0.6416\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6762 - accuracy: 0.5767 - val_loss: 0.6501 - val_accuracy: 0.6403\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6775 - accuracy: 0.5745 - val_loss: 0.6487 - val_accuracy: 0.6429\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6774 - accuracy: 0.5757 - val_loss: 0.6474 - val_accuracy: 0.6429\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6765 - accuracy: 0.5762 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6756 - accuracy: 0.5794 - val_loss: 0.6466 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6779 - accuracy: 0.5710 - val_loss: 0.6466 - val_accuracy: 0.6429\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6758 - accuracy: 0.5772 - val_loss: 0.6452 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6769 - accuracy: 0.5775 - val_loss: 0.6467 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6732 - accuracy: 0.5852 - val_loss: 0.6463 - val_accuracy: 0.6416\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6734 - accuracy: 0.5785 - val_loss: 0.6456 - val_accuracy: 0.6429\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6761 - accuracy: 0.5804 - val_loss: 0.6446 - val_accuracy: 0.6429\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6723 - accuracy: 0.5806 - val_loss: 0.6431 - val_accuracy: 0.6441\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6722 - accuracy: 0.5844 - val_loss: 0.6441 - val_accuracy: 0.6441\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6712 - accuracy: 0.5903 - val_loss: 0.6442 - val_accuracy: 0.6429\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6756 - accuracy: 0.5823 - val_loss: 0.6450 - val_accuracy: 0.6467\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6726 - accuracy: 0.5834 - val_loss: 0.6439 - val_accuracy: 0.6454\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6717 - accuracy: 0.5875 - val_loss: 0.6443 - val_accuracy: 0.6429\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6724 - accuracy: 0.5860 - val_loss: 0.6445 - val_accuracy: 0.6492\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6691 - accuracy: 0.5877 - val_loss: 0.6425 - val_accuracy: 0.6518\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6681 - accuracy: 0.5904 - val_loss: 0.6390 - val_accuracy: 0.6467\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6706 - accuracy: 0.5875 - val_loss: 0.6414 - val_accuracy: 0.6492\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6704 - accuracy: 0.5870 - val_loss: 0.6417 - val_accuracy: 0.6531\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6708 - accuracy: 0.5908 - val_loss: 0.6405 - val_accuracy: 0.6492\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6737 - accuracy: 0.5855 - val_loss: 0.6424 - val_accuracy: 0.6480\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6695 - accuracy: 0.5879 - val_loss: 0.6411 - val_accuracy: 0.6505\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6726 - accuracy: 0.5884 - val_loss: 0.6422 - val_accuracy: 0.6480\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6699 - accuracy: 0.5904 - val_loss: 0.6395 - val_accuracy: 0.6480\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6668 - accuracy: 0.5941 - val_loss: 0.6405 - val_accuracy: 0.6518\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6700 - accuracy: 0.5942 - val_loss: 0.6405 - val_accuracy: 0.6505\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6691 - accuracy: 0.5950 - val_loss: 0.6408 - val_accuracy: 0.6531\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6663 - accuracy: 0.5971 - val_loss: 0.6386 - val_accuracy: 0.6543\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6687 - accuracy: 0.5975 - val_loss: 0.6416 - val_accuracy: 0.6492\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6660 - accuracy: 0.6002 - val_loss: 0.6390 - val_accuracy: 0.6492\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5975 - val_loss: 0.6395 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6683 - accuracy: 0.5970 - val_loss: 0.6386 - val_accuracy: 0.6518\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6669 - accuracy: 0.5995 - val_loss: 0.6379 - val_accuracy: 0.6518\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6644 - accuracy: 0.6046 - val_loss: 0.6370 - val_accuracy: 0.6531\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6684 - accuracy: 0.5938 - val_loss: 0.6370 - val_accuracy: 0.6531\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6685 - accuracy: 0.6004 - val_loss: 0.6358 - val_accuracy: 0.6531\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6650 - accuracy: 0.6064 - val_loss: 0.6342 - val_accuracy: 0.6556\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6669 - accuracy: 0.5976 - val_loss: 0.6354 - val_accuracy: 0.6556\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6661 - accuracy: 0.5951 - val_loss: 0.6352 - val_accuracy: 0.6543\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6663 - accuracy: 0.5956 - val_loss: 0.6344 - val_accuracy: 0.6505\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6633 - accuracy: 0.6022 - val_loss: 0.6338 - val_accuracy: 0.6607\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6677 - accuracy: 0.6004 - val_loss: 0.6321 - val_accuracy: 0.6633\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6622 - accuracy: 0.6074 - val_loss: 0.6320 - val_accuracy: 0.6620\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6662 - accuracy: 0.5941 - val_loss: 0.6335 - val_accuracy: 0.6582\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6646 - accuracy: 0.6064 - val_loss: 0.6332 - val_accuracy: 0.6543\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6620 - accuracy: 0.6093 - val_loss: 0.6317 - val_accuracy: 0.6543\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6675 - accuracy: 0.5961 - val_loss: 0.6374 - val_accuracy: 0.6633\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6647 - accuracy: 0.6068 - val_loss: 0.6317 - val_accuracy: 0.6505\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6619 - accuracy: 0.6110 - val_loss: 0.6318 - val_accuracy: 0.6620\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6615 - accuracy: 0.6069 - val_loss: 0.6316 - val_accuracy: 0.6620\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6643 - accuracy: 0.6046 - val_loss: 0.6328 - val_accuracy: 0.6607\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6614 - accuracy: 0.6031 - val_loss: 0.6300 - val_accuracy: 0.6556\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6647 - accuracy: 0.6011 - val_loss: 0.6325 - val_accuracy: 0.6633\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.6118 - val_loss: 0.6294 - val_accuracy: 0.6722\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6636 - accuracy: 0.6045 - val_loss: 0.6299 - val_accuracy: 0.6620\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6641 - accuracy: 0.6084 - val_loss: 0.6291 - val_accuracy: 0.6696\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6629 - accuracy: 0.6009 - val_loss: 0.6304 - val_accuracy: 0.6696\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6627 - accuracy: 0.6109 - val_loss: 0.6299 - val_accuracy: 0.6722\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6596 - accuracy: 0.6133 - val_loss: 0.6290 - val_accuracy: 0.6709\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6610 - accuracy: 0.6103 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6581 - accuracy: 0.6155 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6564 - accuracy: 0.6101 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6612 - accuracy: 0.6049 - val_loss: 0.6295 - val_accuracy: 0.6709\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6631 - accuracy: 0.6081 - val_loss: 0.6303 - val_accuracy: 0.6735\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6554 - accuracy: 0.6178 - val_loss: 0.6254 - val_accuracy: 0.6684\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6577 - accuracy: 0.6178 - val_loss: 0.6258 - val_accuracy: 0.6696\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6582 - accuracy: 0.6133 - val_loss: 0.6271 - val_accuracy: 0.6709\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6624 - accuracy: 0.6109 - val_loss: 0.6268 - val_accuracy: 0.6722\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6592 - accuracy: 0.6142 - val_loss: 0.6288 - val_accuracy: 0.6709\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6581 - accuracy: 0.6150 - val_loss: 0.6257 - val_accuracy: 0.6684\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6598 - accuracy: 0.6169 - val_loss: 0.6264 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6579 - accuracy: 0.6153 - val_loss: 0.6270 - val_accuracy: 0.6722\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6593 - accuracy: 0.6124 - val_loss: 0.6300 - val_accuracy: 0.6709\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6596 - accuracy: 0.6093 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.6119 - val_loss: 0.6233 - val_accuracy: 0.6696\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6582 - accuracy: 0.6084 - val_loss: 0.6235 - val_accuracy: 0.6696\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6589 - accuracy: 0.6081 - val_loss: 0.6263 - val_accuracy: 0.6735\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6570 - accuracy: 0.6164 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6588 - accuracy: 0.6139 - val_loss: 0.6241 - val_accuracy: 0.6709\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6560 - accuracy: 0.6189 - val_loss: 0.6214 - val_accuracy: 0.6735\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6536 - accuracy: 0.6183 - val_loss: 0.6194 - val_accuracy: 0.6722\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6512 - accuracy: 0.6211 - val_loss: 0.6231 - val_accuracy: 0.6722\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6551 - accuracy: 0.6169 - val_loss: 0.6225 - val_accuracy: 0.6709\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6580 - accuracy: 0.6178 - val_loss: 0.6225 - val_accuracy: 0.6722\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6596 - accuracy: 0.6154 - val_loss: 0.6258 - val_accuracy: 0.6760\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6571 - accuracy: 0.6164 - val_loss: 0.6209 - val_accuracy: 0.6722\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6575 - accuracy: 0.6211 - val_loss: 0.6228 - val_accuracy: 0.6760\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.6186 - val_loss: 0.6251 - val_accuracy: 0.6747\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6533 - accuracy: 0.6187 - val_loss: 0.6217 - val_accuracy: 0.6735\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6531 - accuracy: 0.6139 - val_loss: 0.6188 - val_accuracy: 0.6735\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6584 - accuracy: 0.6132 - val_loss: 0.6225 - val_accuracy: 0.6722\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6545 - accuracy: 0.6218 - val_loss: 0.6218 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6558 - accuracy: 0.6179 - val_loss: 0.6219 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6573 - accuracy: 0.6240 - val_loss: 0.6255 - val_accuracy: 0.6798\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6568 - accuracy: 0.6188 - val_loss: 0.6215 - val_accuracy: 0.6735\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6544 - accuracy: 0.6209 - val_loss: 0.6225 - val_accuracy: 0.6824\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6554 - accuracy: 0.6179 - val_loss: 0.6208 - val_accuracy: 0.6760\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6528 - accuracy: 0.6218 - val_loss: 0.6204 - val_accuracy: 0.6773\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6546 - accuracy: 0.6237 - val_loss: 0.6201 - val_accuracy: 0.6773\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6521 - accuracy: 0.6238 - val_loss: 0.6212 - val_accuracy: 0.6773\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.6232 - val_loss: 0.6191 - val_accuracy: 0.6798\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6506 - accuracy: 0.6261 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6546 - accuracy: 0.6216 - val_loss: 0.6210 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6544 - accuracy: 0.6274 - val_loss: 0.6164 - val_accuracy: 0.6824\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6560 - accuracy: 0.6183 - val_loss: 0.6207 - val_accuracy: 0.6798\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6521 - accuracy: 0.6232 - val_loss: 0.6187 - val_accuracy: 0.6760\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6556 - accuracy: 0.6203 - val_loss: 0.6247 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6514 - accuracy: 0.6218 - val_loss: 0.6183 - val_accuracy: 0.6798\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6514 - accuracy: 0.6213 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6534 - accuracy: 0.6216 - val_loss: 0.6235 - val_accuracy: 0.6811\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6528 - accuracy: 0.6236 - val_loss: 0.6175 - val_accuracy: 0.6786\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6483 - accuracy: 0.6275 - val_loss: 0.6163 - val_accuracy: 0.6837\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6508 - accuracy: 0.6216 - val_loss: 0.6184 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6513 - accuracy: 0.6289 - val_loss: 0.6192 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6522 - accuracy: 0.6240 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6511 - accuracy: 0.6295 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6507 - accuracy: 0.6238 - val_loss: 0.6143 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.6275 - val_loss: 0.6145 - val_accuracy: 0.6849\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6521 - accuracy: 0.6247 - val_loss: 0.6186 - val_accuracy: 0.6837\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6517 - accuracy: 0.6236 - val_loss: 0.6163 - val_accuracy: 0.6811\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6504 - accuracy: 0.6250 - val_loss: 0.6164 - val_accuracy: 0.6837\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6477 - accuracy: 0.6295 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6499 - accuracy: 0.6274 - val_loss: 0.6157 - val_accuracy: 0.6837\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6537 - accuracy: 0.6238 - val_loss: 0.6182 - val_accuracy: 0.6798\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6491 - accuracy: 0.6258 - val_loss: 0.6201 - val_accuracy: 0.6824\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6514 - accuracy: 0.6256 - val_loss: 0.6171 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6507 - accuracy: 0.6297 - val_loss: 0.6168 - val_accuracy: 0.6824\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6478 - accuracy: 0.6291 - val_loss: 0.6146 - val_accuracy: 0.6837\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6507 - accuracy: 0.6226 - val_loss: 0.6157 - val_accuracy: 0.6824\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6476 - accuracy: 0.6300 - val_loss: 0.6138 - val_accuracy: 0.6837\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6486 - accuracy: 0.6341 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6458 - accuracy: 0.6271 - val_loss: 0.6138 - val_accuracy: 0.6811\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6480 - accuracy: 0.6358 - val_loss: 0.6150 - val_accuracy: 0.6837\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6488 - accuracy: 0.6321 - val_loss: 0.6135 - val_accuracy: 0.6837\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6508 - accuracy: 0.6257 - val_loss: 0.6176 - val_accuracy: 0.6849\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6463 - accuracy: 0.6330 - val_loss: 0.6154 - val_accuracy: 0.6811\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6484 - accuracy: 0.6304 - val_loss: 0.6127 - val_accuracy: 0.6849\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6458 - accuracy: 0.6318 - val_loss: 0.6123 - val_accuracy: 0.6849\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6445 - accuracy: 0.6341 - val_loss: 0.6137 - val_accuracy: 0.6824\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6444 - accuracy: 0.6311 - val_loss: 0.6105 - val_accuracy: 0.6837\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6456 - accuracy: 0.6351 - val_loss: 0.6124 - val_accuracy: 0.6811\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6440 - accuracy: 0.6345 - val_loss: 0.6117 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6433 - accuracy: 0.6338 - val_loss: 0.6125 - val_accuracy: 0.6837\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6466 - accuracy: 0.6292 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6476 - accuracy: 0.6280 - val_loss: 0.6161 - val_accuracy: 0.6849\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6482 - accuracy: 0.6320 - val_loss: 0.6122 - val_accuracy: 0.6862\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6487 - accuracy: 0.6325 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6470 - accuracy: 0.6296 - val_loss: 0.6133 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6472 - accuracy: 0.6333 - val_loss: 0.6140 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6482 - accuracy: 0.6338 - val_loss: 0.6168 - val_accuracy: 0.6837\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6486 - accuracy: 0.6311 - val_loss: 0.6167 - val_accuracy: 0.6824\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6476 - accuracy: 0.6312 - val_loss: 0.6161 - val_accuracy: 0.6786\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6495 - accuracy: 0.6282 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6497 - accuracy: 0.6248 - val_loss: 0.6182 - val_accuracy: 0.6862\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6467 - accuracy: 0.6372 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6425 - accuracy: 0.6372 - val_loss: 0.6121 - val_accuracy: 0.6849\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6451 - accuracy: 0.6319 - val_loss: 0.6148 - val_accuracy: 0.6798\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6464 - accuracy: 0.6302 - val_loss: 0.6113 - val_accuracy: 0.6811\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6458 - accuracy: 0.6351 - val_loss: 0.6134 - val_accuracy: 0.6798\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6474 - accuracy: 0.6353 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6478 - accuracy: 0.6296 - val_loss: 0.6144 - val_accuracy: 0.6824\n",
      "Calculating for: 1000 100 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_204 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7640 - accuracy: 0.5437 - val_loss: 0.6817 - val_accuracy: 0.5574\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6961 - accuracy: 0.5775 - val_loss: 0.6628 - val_accuracy: 0.6148\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.5878 - val_loss: 0.6581 - val_accuracy: 0.6250\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5986 - val_loss: 0.6518 - val_accuracy: 0.6480\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6014 - val_loss: 0.6514 - val_accuracy: 0.6250\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6073 - val_loss: 0.6476 - val_accuracy: 0.6441\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6197 - val_loss: 0.6399 - val_accuracy: 0.6543\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6220 - val_loss: 0.6447 - val_accuracy: 0.6480\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6168 - val_loss: 0.6450 - val_accuracy: 0.6480\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6281 - val_loss: 0.6447 - val_accuracy: 0.6454\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6295 - val_loss: 0.6413 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6481 - accuracy: 0.6267 - val_loss: 0.6423 - val_accuracy: 0.6480\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6338 - val_loss: 0.6364 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6316 - val_loss: 0.6394 - val_accuracy: 0.6543\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6366 - val_loss: 0.6370 - val_accuracy: 0.6505\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6407 - val_loss: 0.6357 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6333 - val_loss: 0.6382 - val_accuracy: 0.6492\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6384 - val_loss: 0.6338 - val_accuracy: 0.6658\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6355 - val_loss: 0.6331 - val_accuracy: 0.6658\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6457 - val_loss: 0.6301 - val_accuracy: 0.6735\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6476 - val_loss: 0.6343 - val_accuracy: 0.6518\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6527 - val_loss: 0.6267 - val_accuracy: 0.6735\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6436 - val_loss: 0.6313 - val_accuracy: 0.6658\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6521 - val_loss: 0.6317 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6507 - val_loss: 0.6323 - val_accuracy: 0.6607\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6497 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6555 - val_loss: 0.6260 - val_accuracy: 0.6773\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6556 - val_loss: 0.6269 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6629 - val_loss: 0.6279 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6615 - val_loss: 0.6271 - val_accuracy: 0.6747\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6598 - val_loss: 0.6248 - val_accuracy: 0.6849\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6639 - val_loss: 0.6217 - val_accuracy: 0.6862\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6680 - val_loss: 0.6262 - val_accuracy: 0.6798\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6649 - val_loss: 0.6274 - val_accuracy: 0.6786\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6709 - val_loss: 0.6261 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6692 - val_loss: 0.6258 - val_accuracy: 0.6747\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6626 - val_loss: 0.6240 - val_accuracy: 0.6862\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6623 - val_loss: 0.6275 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6703 - val_loss: 0.6246 - val_accuracy: 0.6824\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6069 - accuracy: 0.6760 - val_loss: 0.6246 - val_accuracy: 0.6811\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6126 - accuracy: 0.6630 - val_loss: 0.6233 - val_accuracy: 0.6798\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6747 - val_loss: 0.6260 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6724 - val_loss: 0.6274 - val_accuracy: 0.6811\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6721 - val_loss: 0.6312 - val_accuracy: 0.6684\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6027 - accuracy: 0.6747 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6781 - val_loss: 0.6243 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6056 - accuracy: 0.6763 - val_loss: 0.6198 - val_accuracy: 0.6849\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5993 - accuracy: 0.6825 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5998 - accuracy: 0.6827 - val_loss: 0.6264 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5972 - accuracy: 0.6836 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5984 - accuracy: 0.6792 - val_loss: 0.6227 - val_accuracy: 0.6901\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5959 - accuracy: 0.6879 - val_loss: 0.6288 - val_accuracy: 0.6735\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5959 - accuracy: 0.6840 - val_loss: 0.6297 - val_accuracy: 0.6645\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5972 - accuracy: 0.6797 - val_loss: 0.6316 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5893 - accuracy: 0.6863 - val_loss: 0.6275 - val_accuracy: 0.6735\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5899 - accuracy: 0.6883 - val_loss: 0.6288 - val_accuracy: 0.6735\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5926 - accuracy: 0.6895 - val_loss: 0.6315 - val_accuracy: 0.6760\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5927 - accuracy: 0.6879 - val_loss: 0.6330 - val_accuracy: 0.6735\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5934 - accuracy: 0.6865 - val_loss: 0.6287 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5886 - accuracy: 0.6939 - val_loss: 0.6292 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5865 - accuracy: 0.6937 - val_loss: 0.6332 - val_accuracy: 0.6709\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5828 - accuracy: 0.6902 - val_loss: 0.6307 - val_accuracy: 0.6811\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5836 - accuracy: 0.6940 - val_loss: 0.6316 - val_accuracy: 0.6747\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5819 - accuracy: 0.6943 - val_loss: 0.6372 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5855 - accuracy: 0.6874 - val_loss: 0.6318 - val_accuracy: 0.6760\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5799 - accuracy: 0.6994 - val_loss: 0.6325 - val_accuracy: 0.6696\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5780 - accuracy: 0.6979 - val_loss: 0.6360 - val_accuracy: 0.6696\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5763 - accuracy: 0.6942 - val_loss: 0.6415 - val_accuracy: 0.6735\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5751 - accuracy: 0.7021 - val_loss: 0.6336 - val_accuracy: 0.6709\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5784 - accuracy: 0.6962 - val_loss: 0.6338 - val_accuracy: 0.6747\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5738 - accuracy: 0.6976 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5768 - accuracy: 0.6981 - val_loss: 0.6373 - val_accuracy: 0.6747\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5746 - accuracy: 0.7047 - val_loss: 0.6372 - val_accuracy: 0.6747\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5659 - accuracy: 0.7126 - val_loss: 0.6448 - val_accuracy: 0.6607\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5675 - accuracy: 0.7048 - val_loss: 0.6442 - val_accuracy: 0.6607\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5671 - accuracy: 0.7143 - val_loss: 0.6385 - val_accuracy: 0.6645\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5707 - accuracy: 0.7066 - val_loss: 0.6439 - val_accuracy: 0.6658\n",
      "Calculating for: 1000 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_208 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8118 - accuracy: 0.5320 - val_loss: 0.6590 - val_accuracy: 0.6403\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7199 - accuracy: 0.5573 - val_loss: 0.6486 - val_accuracy: 0.6416\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6927 - accuracy: 0.5610 - val_loss: 0.6466 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6765 - accuracy: 0.5803 - val_loss: 0.6427 - val_accuracy: 0.6645\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5789 - val_loss: 0.6477 - val_accuracy: 0.6492\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5787 - val_loss: 0.6522 - val_accuracy: 0.6531\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5924 - val_loss: 0.6493 - val_accuracy: 0.6531\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.6015 - val_loss: 0.6458 - val_accuracy: 0.6569\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5991 - val_loss: 0.6427 - val_accuracy: 0.6620\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6064 - val_loss: 0.6349 - val_accuracy: 0.6696\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6638 - accuracy: 0.6056 - val_loss: 0.6372 - val_accuracy: 0.6684\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6035 - val_loss: 0.6427 - val_accuracy: 0.6582\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6099 - val_loss: 0.6435 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6044 - val_loss: 0.6386 - val_accuracy: 0.6671\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6045 - val_loss: 0.6412 - val_accuracy: 0.6543\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6075 - val_loss: 0.6334 - val_accuracy: 0.6735\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6596 - accuracy: 0.6101 - val_loss: 0.6426 - val_accuracy: 0.6505\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6149 - val_loss: 0.6355 - val_accuracy: 0.6696\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6091 - val_loss: 0.6334 - val_accuracy: 0.6760\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6218 - val_loss: 0.6391 - val_accuracy: 0.6505\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6204 - val_loss: 0.6364 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6182 - val_loss: 0.6356 - val_accuracy: 0.6556\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6216 - val_loss: 0.6365 - val_accuracy: 0.6569\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6257 - val_loss: 0.6420 - val_accuracy: 0.6441\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6213 - val_loss: 0.6340 - val_accuracy: 0.6645\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6280 - val_loss: 0.6332 - val_accuracy: 0.6671\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6252 - val_loss: 0.6297 - val_accuracy: 0.6798\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6206 - val_loss: 0.6293 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6312 - val_loss: 0.6295 - val_accuracy: 0.6735\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6286 - val_loss: 0.6298 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6228 - val_loss: 0.6300 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6282 - val_loss: 0.6342 - val_accuracy: 0.6607\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6480 - accuracy: 0.6304 - val_loss: 0.6330 - val_accuracy: 0.6696\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6311 - val_loss: 0.6298 - val_accuracy: 0.6735\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6445 - accuracy: 0.6287 - val_loss: 0.6318 - val_accuracy: 0.6722\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6329 - val_loss: 0.6313 - val_accuracy: 0.6594\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6324 - val_loss: 0.6287 - val_accuracy: 0.6735\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6445 - accuracy: 0.6318 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6333 - val_loss: 0.6234 - val_accuracy: 0.6798\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6336 - val_loss: 0.6309 - val_accuracy: 0.6620\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6297 - val_loss: 0.6264 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6349 - val_loss: 0.6205 - val_accuracy: 0.6888\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6389 - val_loss: 0.6270 - val_accuracy: 0.6735\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6274 - val_loss: 0.6303 - val_accuracy: 0.6582\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6453 - val_loss: 0.6247 - val_accuracy: 0.6786\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6385 - val_loss: 0.6261 - val_accuracy: 0.6620\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6348 - val_loss: 0.6307 - val_accuracy: 0.6594\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6436 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6384 - accuracy: 0.6414 - val_loss: 0.6251 - val_accuracy: 0.6633\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6409 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6498 - val_loss: 0.6281 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6433 - val_loss: 0.6262 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6444 - val_loss: 0.6280 - val_accuracy: 0.6786\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6400 - val_loss: 0.6291 - val_accuracy: 0.6645\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6414 - val_loss: 0.6267 - val_accuracy: 0.6747\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6431 - val_loss: 0.6266 - val_accuracy: 0.6747\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6426 - val_loss: 0.6275 - val_accuracy: 0.6645\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6507 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6436 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6438 - val_loss: 0.6184 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6517 - val_loss: 0.6235 - val_accuracy: 0.6760\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6487 - val_loss: 0.6231 - val_accuracy: 0.6798\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6539 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6531 - val_loss: 0.6202 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6593 - val_loss: 0.6253 - val_accuracy: 0.6696\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6457 - val_loss: 0.6172 - val_accuracy: 0.6862\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6600 - val_loss: 0.6253 - val_accuracy: 0.6735\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6270 - accuracy: 0.6596 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6260 - accuracy: 0.6575 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6272 - accuracy: 0.6562 - val_loss: 0.6288 - val_accuracy: 0.6722\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6574 - val_loss: 0.6269 - val_accuracy: 0.6671\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6545 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6529 - val_loss: 0.6237 - val_accuracy: 0.6709\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6583 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6545 - val_loss: 0.6218 - val_accuracy: 0.6747\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6540 - val_loss: 0.6224 - val_accuracy: 0.6760\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6615 - val_loss: 0.6214 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6194 - accuracy: 0.6618 - val_loss: 0.6202 - val_accuracy: 0.6735\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6530 - val_loss: 0.6249 - val_accuracy: 0.6684\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6664 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6647 - val_loss: 0.6230 - val_accuracy: 0.6747\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6606 - val_loss: 0.6221 - val_accuracy: 0.6773\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6684 - val_loss: 0.6245 - val_accuracy: 0.6709\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6642 - val_loss: 0.6242 - val_accuracy: 0.6658\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6644 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6139 - accuracy: 0.6654 - val_loss: 0.6207 - val_accuracy: 0.6786\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6673 - val_loss: 0.6265 - val_accuracy: 0.6696\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6603 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6649 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6194 - accuracy: 0.6672 - val_loss: 0.6277 - val_accuracy: 0.6645\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6595 - val_loss: 0.6223 - val_accuracy: 0.6760\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6642 - val_loss: 0.6255 - val_accuracy: 0.6684\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6637 - val_loss: 0.6213 - val_accuracy: 0.6760\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6655 - val_loss: 0.6234 - val_accuracy: 0.6747\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6638 - val_loss: 0.6224 - val_accuracy: 0.6722\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6117 - accuracy: 0.6706 - val_loss: 0.6225 - val_accuracy: 0.6786\n",
      "Calculating for: 1000 100 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_212 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "236/249 [===========================>..] - ETA: 0s - loss: 0.8126 - accuracy: 0.5097WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8087 - accuracy: 0.5113 - val_loss: 0.6791 - val_accuracy: 0.6237\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7319 - accuracy: 0.5098 - val_loss: 0.6776 - val_accuracy: 0.6224\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7053 - accuracy: 0.5190 - val_loss: 0.6766 - val_accuracy: 0.6212\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6961 - accuracy: 0.5231 - val_loss: 0.6685 - val_accuracy: 0.6199\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5264 - val_loss: 0.6703 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5291 - val_loss: 0.6701 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5285 - val_loss: 0.6743 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5279 - val_loss: 0.6673 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5337 - val_loss: 0.6700 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5322 - val_loss: 0.6685 - val_accuracy: 0.6199\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5480 - val_loss: 0.6736 - val_accuracy: 0.6199\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6904 - accuracy: 0.5383 - val_loss: 0.6664 - val_accuracy: 0.6199\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6888 - accuracy: 0.5409 - val_loss: 0.6657 - val_accuracy: 0.6199\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.5352 - val_loss: 0.6699 - val_accuracy: 0.6224\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5333 - val_loss: 0.6703 - val_accuracy: 0.6237\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6893 - accuracy: 0.5377 - val_loss: 0.6653 - val_accuracy: 0.6212\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.5330 - val_loss: 0.6670 - val_accuracy: 0.6224\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6883 - accuracy: 0.5399 - val_loss: 0.6698 - val_accuracy: 0.6237\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6873 - accuracy: 0.5423 - val_loss: 0.6636 - val_accuracy: 0.6237\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6864 - accuracy: 0.5445 - val_loss: 0.6633 - val_accuracy: 0.6237\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5404 - val_loss: 0.6652 - val_accuracy: 0.6276\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6860 - accuracy: 0.5467 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6854 - accuracy: 0.5455 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6868 - accuracy: 0.5472 - val_loss: 0.6654 - val_accuracy: 0.6352\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6870 - accuracy: 0.5487 - val_loss: 0.6677 - val_accuracy: 0.6378\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5461 - val_loss: 0.6584 - val_accuracy: 0.6288\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5556 - val_loss: 0.6581 - val_accuracy: 0.6339\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5482 - val_loss: 0.6610 - val_accuracy: 0.6352\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6845 - accuracy: 0.5570 - val_loss: 0.6593 - val_accuracy: 0.6390\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6863 - accuracy: 0.5524 - val_loss: 0.6666 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6830 - accuracy: 0.5593 - val_loss: 0.6568 - val_accuracy: 0.6314\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.5622 - val_loss: 0.6595 - val_accuracy: 0.6378\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5555 - val_loss: 0.6612 - val_accuracy: 0.6403\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6801 - accuracy: 0.5588 - val_loss: 0.6572 - val_accuracy: 0.6403\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.5624 - val_loss: 0.6593 - val_accuracy: 0.6416\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.5633 - val_loss: 0.6566 - val_accuracy: 0.6441\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5535 - val_loss: 0.6551 - val_accuracy: 0.6365\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6814 - accuracy: 0.5593 - val_loss: 0.6543 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5608 - val_loss: 0.6593 - val_accuracy: 0.6429\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6802 - accuracy: 0.5662 - val_loss: 0.6578 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6818 - accuracy: 0.5583 - val_loss: 0.6591 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6787 - accuracy: 0.5750 - val_loss: 0.6533 - val_accuracy: 0.6429\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6823 - accuracy: 0.5683 - val_loss: 0.6555 - val_accuracy: 0.6441\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6780 - accuracy: 0.5732 - val_loss: 0.6524 - val_accuracy: 0.6429\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5683 - val_loss: 0.6514 - val_accuracy: 0.6429\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6779 - accuracy: 0.5659 - val_loss: 0.6501 - val_accuracy: 0.6416\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5791 - val_loss: 0.6512 - val_accuracy: 0.6403\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6804 - accuracy: 0.5624 - val_loss: 0.6531 - val_accuracy: 0.6429\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6799 - accuracy: 0.5623 - val_loss: 0.6527 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5756 - val_loss: 0.6500 - val_accuracy: 0.6429\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5647 - val_loss: 0.6507 - val_accuracy: 0.6429\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5742 - val_loss: 0.6496 - val_accuracy: 0.6429\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5774 - val_loss: 0.6493 - val_accuracy: 0.6441\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6771 - accuracy: 0.5745 - val_loss: 0.6500 - val_accuracy: 0.6416\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5662 - val_loss: 0.6529 - val_accuracy: 0.6416\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5735 - val_loss: 0.6531 - val_accuracy: 0.6429\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.5730 - val_loss: 0.6479 - val_accuracy: 0.6416\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5824 - val_loss: 0.6478 - val_accuracy: 0.6390\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6774 - accuracy: 0.5813 - val_loss: 0.6504 - val_accuracy: 0.6429\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.5789 - val_loss: 0.6468 - val_accuracy: 0.6416\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5800 - val_loss: 0.6499 - val_accuracy: 0.6505\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6757 - accuracy: 0.5803 - val_loss: 0.6489 - val_accuracy: 0.6429\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6774 - accuracy: 0.5730 - val_loss: 0.6466 - val_accuracy: 0.6429\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5843 - val_loss: 0.6464 - val_accuracy: 0.6492\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6747 - accuracy: 0.5785 - val_loss: 0.6446 - val_accuracy: 0.6492\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5901 - val_loss: 0.6440 - val_accuracy: 0.6480\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5838 - val_loss: 0.6457 - val_accuracy: 0.6480\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.5870 - val_loss: 0.6469 - val_accuracy: 0.6441\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5867 - val_loss: 0.6442 - val_accuracy: 0.6467\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5926 - val_loss: 0.6448 - val_accuracy: 0.6441\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.5776 - val_loss: 0.6476 - val_accuracy: 0.6492\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.5834 - val_loss: 0.6433 - val_accuracy: 0.6492\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5916 - val_loss: 0.6431 - val_accuracy: 0.6505\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5882 - val_loss: 0.6460 - val_accuracy: 0.6480\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6700 - accuracy: 0.5883 - val_loss: 0.6440 - val_accuracy: 0.6505\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5896 - val_loss: 0.6434 - val_accuracy: 0.6480\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5922 - val_loss: 0.6413 - val_accuracy: 0.6480\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5957 - val_loss: 0.6426 - val_accuracy: 0.6543\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6719 - accuracy: 0.5867 - val_loss: 0.6444 - val_accuracy: 0.6480\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6700 - accuracy: 0.5887 - val_loss: 0.6420 - val_accuracy: 0.6518\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5919 - val_loss: 0.6410 - val_accuracy: 0.6505\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5944 - val_loss: 0.6398 - val_accuracy: 0.6492\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5868 - val_loss: 0.6406 - val_accuracy: 0.6518\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5929 - val_loss: 0.6419 - val_accuracy: 0.6518\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5972 - val_loss: 0.6449 - val_accuracy: 0.6531\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5887 - val_loss: 0.6408 - val_accuracy: 0.6556\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5916 - val_loss: 0.6448 - val_accuracy: 0.6658\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5931 - val_loss: 0.6369 - val_accuracy: 0.6518\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6684 - accuracy: 0.5917 - val_loss: 0.6409 - val_accuracy: 0.6569\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5966 - val_loss: 0.6375 - val_accuracy: 0.6505\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5874 - val_loss: 0.6403 - val_accuracy: 0.6518\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6678 - accuracy: 0.5971 - val_loss: 0.6408 - val_accuracy: 0.6556\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6675 - accuracy: 0.5995 - val_loss: 0.6389 - val_accuracy: 0.6607\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.6021 - val_loss: 0.6356 - val_accuracy: 0.6582\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6007 - val_loss: 0.6403 - val_accuracy: 0.6658\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6678 - accuracy: 0.5919 - val_loss: 0.6385 - val_accuracy: 0.6620\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6004 - val_loss: 0.6351 - val_accuracy: 0.6582\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.6080 - val_loss: 0.6360 - val_accuracy: 0.6620\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.6001 - val_loss: 0.6376 - val_accuracy: 0.6594\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.5966 - val_loss: 0.6352 - val_accuracy: 0.6747\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6071 - val_loss: 0.6349 - val_accuracy: 0.6658\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6673 - accuracy: 0.6014 - val_loss: 0.6412 - val_accuracy: 0.6747\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.5981 - val_loss: 0.6382 - val_accuracy: 0.6684\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6005 - val_loss: 0.6411 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6051 - val_loss: 0.6373 - val_accuracy: 0.6684\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6656 - accuracy: 0.6024 - val_loss: 0.6379 - val_accuracy: 0.6671\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.5988 - val_loss: 0.6355 - val_accuracy: 0.6722\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6639 - accuracy: 0.6065 - val_loss: 0.6343 - val_accuracy: 0.6645\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6096 - val_loss: 0.6339 - val_accuracy: 0.6645\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6660 - accuracy: 0.5996 - val_loss: 0.6348 - val_accuracy: 0.6735\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.5950 - val_loss: 0.6370 - val_accuracy: 0.6747\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6090 - val_loss: 0.6337 - val_accuracy: 0.6696\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6014 - val_loss: 0.6338 - val_accuracy: 0.6709\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6042 - val_loss: 0.6338 - val_accuracy: 0.6735\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6021 - val_loss: 0.6341 - val_accuracy: 0.6722\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.6061 - val_loss: 0.6383 - val_accuracy: 0.6696\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6086 - val_loss: 0.6331 - val_accuracy: 0.6735\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6066 - val_loss: 0.6351 - val_accuracy: 0.6798\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6022 - val_loss: 0.6333 - val_accuracy: 0.6722\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6060 - val_loss: 0.6323 - val_accuracy: 0.6722\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6130 - val_loss: 0.6300 - val_accuracy: 0.6722\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6065 - val_loss: 0.6302 - val_accuracy: 0.6684\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6036 - val_loss: 0.6345 - val_accuracy: 0.6875\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6162 - val_loss: 0.6284 - val_accuracy: 0.6722\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6623 - accuracy: 0.6050 - val_loss: 0.6312 - val_accuracy: 0.6735\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6632 - accuracy: 0.6113 - val_loss: 0.6322 - val_accuracy: 0.6773\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6176 - val_loss: 0.6335 - val_accuracy: 0.6747\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6100 - val_loss: 0.6301 - val_accuracy: 0.6709\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6162 - val_loss: 0.6305 - val_accuracy: 0.6786\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6125 - val_loss: 0.6274 - val_accuracy: 0.6773\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6158 - val_loss: 0.6279 - val_accuracy: 0.6722\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.6120 - val_loss: 0.6291 - val_accuracy: 0.6709\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6578 - accuracy: 0.6187 - val_loss: 0.6331 - val_accuracy: 0.6849\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6130 - val_loss: 0.6330 - val_accuracy: 0.6837\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6566 - accuracy: 0.6196 - val_loss: 0.6272 - val_accuracy: 0.6811\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6109 - val_loss: 0.6312 - val_accuracy: 0.6786\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6114 - val_loss: 0.6252 - val_accuracy: 0.6837\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6118 - val_loss: 0.6270 - val_accuracy: 0.6824\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6584 - accuracy: 0.6094 - val_loss: 0.6277 - val_accuracy: 0.6747\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6117 - val_loss: 0.6284 - val_accuracy: 0.6773\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6063 - val_loss: 0.6277 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6162 - val_loss: 0.6297 - val_accuracy: 0.6760\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6125 - val_loss: 0.6270 - val_accuracy: 0.6760\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6208 - val_loss: 0.6300 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6571 - accuracy: 0.6152 - val_loss: 0.6297 - val_accuracy: 0.6798\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6129 - val_loss: 0.6299 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6145 - val_loss: 0.6284 - val_accuracy: 0.6773\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6101 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6213 - val_loss: 0.6274 - val_accuracy: 0.6773\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6178 - val_loss: 0.6227 - val_accuracy: 0.6811\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6207 - val_loss: 0.6271 - val_accuracy: 0.6798\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6189 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6242 - val_loss: 0.6237 - val_accuracy: 0.6773\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6199 - val_loss: 0.6275 - val_accuracy: 0.6837\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6262 - val_loss: 0.6235 - val_accuracy: 0.6824\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6179 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6196 - val_loss: 0.6228 - val_accuracy: 0.6786\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6237 - val_loss: 0.6224 - val_accuracy: 0.6798\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6227 - val_loss: 0.6278 - val_accuracy: 0.6798\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6241 - val_loss: 0.6217 - val_accuracy: 0.6811\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6562 - accuracy: 0.6271 - val_loss: 0.6225 - val_accuracy: 0.6811\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6215 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6167 - val_loss: 0.6281 - val_accuracy: 0.6824\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6133 - val_loss: 0.6284 - val_accuracy: 0.6811\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6215 - val_loss: 0.6282 - val_accuracy: 0.6837\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6173 - val_loss: 0.6285 - val_accuracy: 0.6849\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6169 - val_loss: 0.6241 - val_accuracy: 0.6849\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6544 - accuracy: 0.6255 - val_loss: 0.6274 - val_accuracy: 0.6849\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6166 - val_loss: 0.6305 - val_accuracy: 0.6811\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6226 - val_loss: 0.6231 - val_accuracy: 0.6811\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6242 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6246 - val_loss: 0.6261 - val_accuracy: 0.6824\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6262 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6245 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6221 - val_loss: 0.6276 - val_accuracy: 0.6824\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.6182 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6521 - accuracy: 0.6243 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6546 - accuracy: 0.6147 - val_loss: 0.6246 - val_accuracy: 0.6811\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6242 - val_loss: 0.6228 - val_accuracy: 0.6824\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6302 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6232 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6206 - val_loss: 0.6195 - val_accuracy: 0.6824\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6211 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6241 - val_loss: 0.6245 - val_accuracy: 0.6824\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6325 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6274 - val_loss: 0.6239 - val_accuracy: 0.6811\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6255 - val_loss: 0.6211 - val_accuracy: 0.6786\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6186 - val_loss: 0.6222 - val_accuracy: 0.6798\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6284 - val_loss: 0.6214 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6282 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6262 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6227 - val_loss: 0.6196 - val_accuracy: 0.6849\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6237 - val_loss: 0.6186 - val_accuracy: 0.6862\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6204 - val_loss: 0.6212 - val_accuracy: 0.6824\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6265 - val_loss: 0.6237 - val_accuracy: 0.6824\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6262 - val_loss: 0.6217 - val_accuracy: 0.6837\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6516 - accuracy: 0.6211 - val_loss: 0.6222 - val_accuracy: 0.6849\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6292 - val_loss: 0.6219 - val_accuracy: 0.6811\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6247 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6188 - val_loss: 0.6247 - val_accuracy: 0.6811\n",
      "Calculating for: 1000 250 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_216 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7943 - accuracy: 0.5633 - val_loss: 0.6410 - val_accuracy: 0.6722\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7021 - accuracy: 0.5840 - val_loss: 0.6257 - val_accuracy: 0.6786\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6703 - accuracy: 0.6049 - val_loss: 0.6279 - val_accuracy: 0.6849\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6610 - accuracy: 0.6090 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6140 - val_loss: 0.6236 - val_accuracy: 0.6862\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6204 - val_loss: 0.6241 - val_accuracy: 0.6811\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6237 - val_loss: 0.6193 - val_accuracy: 0.6888\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6505 - accuracy: 0.6197 - val_loss: 0.6188 - val_accuracy: 0.6849\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6477 - accuracy: 0.6270 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6437 - accuracy: 0.6256 - val_loss: 0.6192 - val_accuracy: 0.6901\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6411 - accuracy: 0.6364 - val_loss: 0.6155 - val_accuracy: 0.6811\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6395 - accuracy: 0.6392 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6382 - val_loss: 0.6182 - val_accuracy: 0.6964\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6320 - accuracy: 0.6481 - val_loss: 0.6145 - val_accuracy: 0.6901\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6434 - val_loss: 0.6175 - val_accuracy: 0.6926\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6353 - accuracy: 0.6483 - val_loss: 0.6190 - val_accuracy: 0.6888\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6426 - val_loss: 0.6183 - val_accuracy: 0.6913\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6446 - val_loss: 0.6151 - val_accuracy: 0.6926\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6551 - val_loss: 0.6163 - val_accuracy: 0.6888\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6461 - val_loss: 0.6138 - val_accuracy: 0.6901\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6498 - val_loss: 0.6164 - val_accuracy: 0.6977\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6525 - val_loss: 0.6158 - val_accuracy: 0.6977\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6550 - val_loss: 0.6210 - val_accuracy: 0.6811\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6569 - val_loss: 0.6186 - val_accuracy: 0.6913\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6611 - val_loss: 0.6199 - val_accuracy: 0.6837\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6265 - accuracy: 0.6518 - val_loss: 0.6150 - val_accuracy: 0.6964\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6655 - val_loss: 0.6186 - val_accuracy: 0.6977\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6198 - accuracy: 0.6615 - val_loss: 0.6191 - val_accuracy: 0.6888\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6133 - accuracy: 0.6680 - val_loss: 0.6220 - val_accuracy: 0.6901\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6196 - accuracy: 0.6613 - val_loss: 0.6195 - val_accuracy: 0.6964\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6142 - accuracy: 0.6694 - val_loss: 0.6193 - val_accuracy: 0.6964\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6098 - accuracy: 0.6733 - val_loss: 0.6199 - val_accuracy: 0.6849\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6110 - accuracy: 0.6714 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6094 - accuracy: 0.6788 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6102 - accuracy: 0.6662 - val_loss: 0.6198 - val_accuracy: 0.6901\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6077 - accuracy: 0.6707 - val_loss: 0.6202 - val_accuracy: 0.6875\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6081 - accuracy: 0.6732 - val_loss: 0.6156 - val_accuracy: 0.6926\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6042 - accuracy: 0.6729 - val_loss: 0.6246 - val_accuracy: 0.6888\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6057 - accuracy: 0.6694 - val_loss: 0.6215 - val_accuracy: 0.6888\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6025 - accuracy: 0.6800 - val_loss: 0.6299 - val_accuracy: 0.6824\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6001 - accuracy: 0.6860 - val_loss: 0.6305 - val_accuracy: 0.6888\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5971 - accuracy: 0.6851 - val_loss: 0.6273 - val_accuracy: 0.6837\n",
      "Calculating for: 1000 250 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_220 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8247 - accuracy: 0.5345 - val_loss: 0.6424 - val_accuracy: 0.6531\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7178 - accuracy: 0.5599 - val_loss: 0.6452 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6887 - accuracy: 0.5745 - val_loss: 0.6401 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6773 - accuracy: 0.5752 - val_loss: 0.6407 - val_accuracy: 0.6671\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6712 - accuracy: 0.5855 - val_loss: 0.6392 - val_accuracy: 0.6696\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6705 - accuracy: 0.5902 - val_loss: 0.6387 - val_accuracy: 0.6658\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6666 - accuracy: 0.5976 - val_loss: 0.6398 - val_accuracy: 0.6582\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6668 - accuracy: 0.5962 - val_loss: 0.6382 - val_accuracy: 0.6645\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6665 - accuracy: 0.5928 - val_loss: 0.6403 - val_accuracy: 0.6556\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6613 - accuracy: 0.6076 - val_loss: 0.6371 - val_accuracy: 0.6505\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6041 - val_loss: 0.6335 - val_accuracy: 0.6671\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6623 - accuracy: 0.6059 - val_loss: 0.6308 - val_accuracy: 0.6658\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6605 - accuracy: 0.6074 - val_loss: 0.6317 - val_accuracy: 0.6671\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6613 - accuracy: 0.6045 - val_loss: 0.6295 - val_accuracy: 0.6658\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6568 - accuracy: 0.6171 - val_loss: 0.6298 - val_accuracy: 0.6722\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6144 - val_loss: 0.6296 - val_accuracy: 0.6722\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6568 - accuracy: 0.6171 - val_loss: 0.6276 - val_accuracy: 0.6633\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6558 - accuracy: 0.6138 - val_loss: 0.6325 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6542 - accuracy: 0.6137 - val_loss: 0.6281 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6540 - accuracy: 0.6191 - val_loss: 0.6305 - val_accuracy: 0.6760\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6290 - val_loss: 0.6254 - val_accuracy: 0.6709\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6183 - val_loss: 0.6299 - val_accuracy: 0.6709\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.6194 - val_loss: 0.6260 - val_accuracy: 0.6696\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6240 - val_loss: 0.6236 - val_accuracy: 0.6747\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6290 - val_loss: 0.6224 - val_accuracy: 0.6735\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6193 - val_loss: 0.6234 - val_accuracy: 0.6747\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6264 - val_loss: 0.6239 - val_accuracy: 0.6773\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6228 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6227 - val_loss: 0.6227 - val_accuracy: 0.6773\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6343 - val_loss: 0.6275 - val_accuracy: 0.6722\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6436 - accuracy: 0.6341 - val_loss: 0.6196 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6261 - val_loss: 0.6199 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6442 - accuracy: 0.6289 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6467 - accuracy: 0.6280 - val_loss: 0.6194 - val_accuracy: 0.6875\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6448 - accuracy: 0.6280 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6306 - val_loss: 0.6197 - val_accuracy: 0.6786\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6368 - val_loss: 0.6166 - val_accuracy: 0.6875\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6433 - accuracy: 0.6329 - val_loss: 0.6181 - val_accuracy: 0.6773\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6454 - accuracy: 0.6309 - val_loss: 0.6231 - val_accuracy: 0.6760\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6387 - val_loss: 0.6179 - val_accuracy: 0.6760\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6355 - val_loss: 0.6241 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6409 - accuracy: 0.6408 - val_loss: 0.6170 - val_accuracy: 0.6849\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6389 - accuracy: 0.6429 - val_loss: 0.6189 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6385 - val_loss: 0.6198 - val_accuracy: 0.6696\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6374 - val_loss: 0.6175 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6436 - val_loss: 0.6156 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6409 - val_loss: 0.6178 - val_accuracy: 0.6811\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6434 - val_loss: 0.6158 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6383 - accuracy: 0.6423 - val_loss: 0.6182 - val_accuracy: 0.6747\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6358 - accuracy: 0.6437 - val_loss: 0.6201 - val_accuracy: 0.6773\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6419 - val_loss: 0.6184 - val_accuracy: 0.6747\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6409 - val_loss: 0.6175 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.6438 - val_loss: 0.6196 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6362 - accuracy: 0.6503 - val_loss: 0.6181 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6434 - val_loss: 0.6168 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6332 - accuracy: 0.6480 - val_loss: 0.6169 - val_accuracy: 0.6798\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6472 - val_loss: 0.6170 - val_accuracy: 0.6837\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6506 - val_loss: 0.6128 - val_accuracy: 0.6849\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6486 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6292 - accuracy: 0.6501 - val_loss: 0.6131 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6462 - val_loss: 0.6157 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6522 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6540 - val_loss: 0.6148 - val_accuracy: 0.6837\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6529 - val_loss: 0.6164 - val_accuracy: 0.6849\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6556 - val_loss: 0.6135 - val_accuracy: 0.6901\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6492 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6572 - val_loss: 0.6125 - val_accuracy: 0.6875\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6659 - val_loss: 0.6131 - val_accuracy: 0.6849\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6551 - val_loss: 0.6156 - val_accuracy: 0.6875\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6546 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6615 - val_loss: 0.6174 - val_accuracy: 0.6964\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6210 - accuracy: 0.6629 - val_loss: 0.6143 - val_accuracy: 0.6964\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6567 - val_loss: 0.6146 - val_accuracy: 0.6990\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6606 - val_loss: 0.6161 - val_accuracy: 0.6939\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6191 - accuracy: 0.6665 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6644 - val_loss: 0.6146 - val_accuracy: 0.6913\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6570 - val_loss: 0.6155 - val_accuracy: 0.6939\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6204 - accuracy: 0.6535 - val_loss: 0.6154 - val_accuracy: 0.6952\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6209 - accuracy: 0.6551 - val_loss: 0.6164 - val_accuracy: 0.6939\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6609 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6605 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6192 - accuracy: 0.6629 - val_loss: 0.6167 - val_accuracy: 0.6926\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6143 - accuracy: 0.6683 - val_loss: 0.6137 - val_accuracy: 0.6939\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6581 - val_loss: 0.6157 - val_accuracy: 0.6939\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6210 - accuracy: 0.6620 - val_loss: 0.6133 - val_accuracy: 0.6977\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6151 - accuracy: 0.6644 - val_loss: 0.6142 - val_accuracy: 0.7003\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6172 - accuracy: 0.6639 - val_loss: 0.6117 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6117 - accuracy: 0.6618 - val_loss: 0.6141 - val_accuracy: 0.6939\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6157 - accuracy: 0.6644 - val_loss: 0.6126 - val_accuracy: 0.6952\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6157 - accuracy: 0.6613 - val_loss: 0.6121 - val_accuracy: 0.6964\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6138 - accuracy: 0.6682 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6140 - accuracy: 0.6619 - val_loss: 0.6173 - val_accuracy: 0.6926\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6124 - accuracy: 0.6667 - val_loss: 0.6141 - val_accuracy: 0.6964\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6090 - accuracy: 0.6747 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6128 - accuracy: 0.6717 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6122 - accuracy: 0.6675 - val_loss: 0.6162 - val_accuracy: 0.6862\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6088 - accuracy: 0.6644 - val_loss: 0.6149 - val_accuracy: 0.6952\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6120 - accuracy: 0.6704 - val_loss: 0.6164 - val_accuracy: 0.6888\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6125 - accuracy: 0.6717 - val_loss: 0.6184 - val_accuracy: 0.6888\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6067 - accuracy: 0.6762 - val_loss: 0.6174 - val_accuracy: 0.6888\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6111 - accuracy: 0.6712 - val_loss: 0.6184 - val_accuracy: 0.6913\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6105 - accuracy: 0.6718 - val_loss: 0.6141 - val_accuracy: 0.6939\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6070 - accuracy: 0.6711 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6077 - accuracy: 0.6713 - val_loss: 0.6160 - val_accuracy: 0.6901\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6043 - accuracy: 0.6777 - val_loss: 0.6158 - val_accuracy: 0.6926\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6074 - accuracy: 0.6765 - val_loss: 0.6138 - val_accuracy: 0.6913\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6035 - accuracy: 0.6760 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6032 - accuracy: 0.6802 - val_loss: 0.6199 - val_accuracy: 0.6811\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6026 - accuracy: 0.6770 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6006 - accuracy: 0.6775 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6046 - accuracy: 0.6791 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6089 - accuracy: 0.6722 - val_loss: 0.6207 - val_accuracy: 0.6760\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5976 - accuracy: 0.6777 - val_loss: 0.6193 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6005 - accuracy: 0.6820 - val_loss: 0.6214 - val_accuracy: 0.6849\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5981 - accuracy: 0.6801 - val_loss: 0.6193 - val_accuracy: 0.6837\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6073 - accuracy: 0.6770 - val_loss: 0.6181 - val_accuracy: 0.6901\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6016 - accuracy: 0.6776 - val_loss: 0.6198 - val_accuracy: 0.6875\n",
      "Calculating for: 1000 250 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_224 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8326 - accuracy: 0.5055 - val_loss: 0.6657 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7330 - accuracy: 0.5206 - val_loss: 0.6654 - val_accuracy: 0.6212\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7068 - accuracy: 0.5092 - val_loss: 0.6653 - val_accuracy: 0.6186\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6916 - accuracy: 0.5319 - val_loss: 0.6661 - val_accuracy: 0.6186\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6936 - accuracy: 0.5229 - val_loss: 0.6691 - val_accuracy: 0.6224\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6902 - accuracy: 0.5323 - val_loss: 0.6713 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6919 - accuracy: 0.5283 - val_loss: 0.6692 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6901 - accuracy: 0.5359 - val_loss: 0.6686 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6907 - accuracy: 0.5381 - val_loss: 0.6664 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.5363 - val_loss: 0.6662 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6909 - accuracy: 0.5335 - val_loss: 0.6703 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6888 - accuracy: 0.5367 - val_loss: 0.6658 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6880 - accuracy: 0.5389 - val_loss: 0.6635 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6900 - accuracy: 0.5322 - val_loss: 0.6662 - val_accuracy: 0.6199\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6892 - accuracy: 0.5382 - val_loss: 0.6639 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6891 - accuracy: 0.5352 - val_loss: 0.6637 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6882 - accuracy: 0.5489 - val_loss: 0.6596 - val_accuracy: 0.6224\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6862 - accuracy: 0.5534 - val_loss: 0.6619 - val_accuracy: 0.6339\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6862 - accuracy: 0.5496 - val_loss: 0.6590 - val_accuracy: 0.6288\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6861 - accuracy: 0.5548 - val_loss: 0.6603 - val_accuracy: 0.6365\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6863 - accuracy: 0.5495 - val_loss: 0.6584 - val_accuracy: 0.6339\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6877 - accuracy: 0.5465 - val_loss: 0.6623 - val_accuracy: 0.6365\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6844 - accuracy: 0.5550 - val_loss: 0.6579 - val_accuracy: 0.6365\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6858 - accuracy: 0.5465 - val_loss: 0.6583 - val_accuracy: 0.6378\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6833 - accuracy: 0.5565 - val_loss: 0.6577 - val_accuracy: 0.6416\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6840 - accuracy: 0.5555 - val_loss: 0.6542 - val_accuracy: 0.6390\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6830 - accuracy: 0.5533 - val_loss: 0.6531 - val_accuracy: 0.6403\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6832 - accuracy: 0.5603 - val_loss: 0.6559 - val_accuracy: 0.6378\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6851 - accuracy: 0.5535 - val_loss: 0.6555 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6824 - accuracy: 0.5662 - val_loss: 0.6550 - val_accuracy: 0.6390\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6791 - accuracy: 0.5721 - val_loss: 0.6502 - val_accuracy: 0.6378\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.5641 - val_loss: 0.6491 - val_accuracy: 0.6416\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6812 - accuracy: 0.5643 - val_loss: 0.6504 - val_accuracy: 0.6429\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6820 - accuracy: 0.5638 - val_loss: 0.6518 - val_accuracy: 0.6441\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6781 - accuracy: 0.5752 - val_loss: 0.6498 - val_accuracy: 0.6441\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6778 - accuracy: 0.5678 - val_loss: 0.6508 - val_accuracy: 0.6390\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6773 - accuracy: 0.5771 - val_loss: 0.6485 - val_accuracy: 0.6429\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.5700 - val_loss: 0.6492 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6773 - accuracy: 0.5723 - val_loss: 0.6466 - val_accuracy: 0.6416\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6790 - accuracy: 0.5701 - val_loss: 0.6447 - val_accuracy: 0.6454\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6784 - accuracy: 0.5737 - val_loss: 0.6488 - val_accuracy: 0.6390\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6784 - accuracy: 0.5681 - val_loss: 0.6481 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6786 - accuracy: 0.5718 - val_loss: 0.6475 - val_accuracy: 0.6403\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6778 - accuracy: 0.5767 - val_loss: 0.6464 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6746 - accuracy: 0.5793 - val_loss: 0.6466 - val_accuracy: 0.6403\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6753 - accuracy: 0.5823 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.5795 - val_loss: 0.6458 - val_accuracy: 0.6429\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6744 - accuracy: 0.5730 - val_loss: 0.6434 - val_accuracy: 0.6403\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6767 - accuracy: 0.5742 - val_loss: 0.6449 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6728 - accuracy: 0.5859 - val_loss: 0.6423 - val_accuracy: 0.6416\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6758 - accuracy: 0.5760 - val_loss: 0.6446 - val_accuracy: 0.6429\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6747 - accuracy: 0.5774 - val_loss: 0.6459 - val_accuracy: 0.6480\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6718 - accuracy: 0.5824 - val_loss: 0.6398 - val_accuracy: 0.6416\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6700 - accuracy: 0.5879 - val_loss: 0.6372 - val_accuracy: 0.6429\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6750 - accuracy: 0.5860 - val_loss: 0.6420 - val_accuracy: 0.6467\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6730 - accuracy: 0.5870 - val_loss: 0.6418 - val_accuracy: 0.6480\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6747 - accuracy: 0.5809 - val_loss: 0.6396 - val_accuracy: 0.6467\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6708 - accuracy: 0.5909 - val_loss: 0.6401 - val_accuracy: 0.6505\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6687 - accuracy: 0.5864 - val_loss: 0.6355 - val_accuracy: 0.6467\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6713 - accuracy: 0.5883 - val_loss: 0.6387 - val_accuracy: 0.6454\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6710 - accuracy: 0.5873 - val_loss: 0.6420 - val_accuracy: 0.6543\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5927 - val_loss: 0.6380 - val_accuracy: 0.6480\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6703 - accuracy: 0.5873 - val_loss: 0.6368 - val_accuracy: 0.6467\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.5938 - val_loss: 0.6388 - val_accuracy: 0.6492\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6727 - accuracy: 0.5898 - val_loss: 0.6378 - val_accuracy: 0.6480\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5937 - val_loss: 0.6376 - val_accuracy: 0.6518\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6706 - accuracy: 0.5916 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.5914 - val_loss: 0.6364 - val_accuracy: 0.6480\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.5950 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6684 - accuracy: 0.5908 - val_loss: 0.6390 - val_accuracy: 0.6684\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6670 - accuracy: 0.5944 - val_loss: 0.6342 - val_accuracy: 0.6684\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.5943 - val_loss: 0.6369 - val_accuracy: 0.6684\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6685 - accuracy: 0.5917 - val_loss: 0.6359 - val_accuracy: 0.6658\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6653 - accuracy: 0.5923 - val_loss: 0.6332 - val_accuracy: 0.6696\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6688 - accuracy: 0.5946 - val_loss: 0.6345 - val_accuracy: 0.6671\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6673 - accuracy: 0.5913 - val_loss: 0.6359 - val_accuracy: 0.6684\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5946 - val_loss: 0.6364 - val_accuracy: 0.6684\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6046 - val_loss: 0.6320 - val_accuracy: 0.6722\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6655 - accuracy: 0.6012 - val_loss: 0.6350 - val_accuracy: 0.6658\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6675 - accuracy: 0.5967 - val_loss: 0.6321 - val_accuracy: 0.6633\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6689 - accuracy: 0.5941 - val_loss: 0.6360 - val_accuracy: 0.6658\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5962 - val_loss: 0.6331 - val_accuracy: 0.6696\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6622 - accuracy: 0.6010 - val_loss: 0.6303 - val_accuracy: 0.6696\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6606 - accuracy: 0.6032 - val_loss: 0.6314 - val_accuracy: 0.6684\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6066 - val_loss: 0.6300 - val_accuracy: 0.6696\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6648 - accuracy: 0.6014 - val_loss: 0.6310 - val_accuracy: 0.6696\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.6095 - val_loss: 0.6314 - val_accuracy: 0.6671\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6627 - accuracy: 0.6083 - val_loss: 0.6286 - val_accuracy: 0.6709\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6623 - accuracy: 0.6002 - val_loss: 0.6287 - val_accuracy: 0.6684\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6059 - val_loss: 0.6285 - val_accuracy: 0.6735\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6664 - accuracy: 0.6006 - val_loss: 0.6305 - val_accuracy: 0.6722\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6617 - accuracy: 0.6096 - val_loss: 0.6284 - val_accuracy: 0.6696\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6609 - accuracy: 0.6137 - val_loss: 0.6275 - val_accuracy: 0.6696\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6608 - accuracy: 0.6049 - val_loss: 0.6262 - val_accuracy: 0.6658\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6656 - accuracy: 0.6024 - val_loss: 0.6286 - val_accuracy: 0.6735\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6089 - val_loss: 0.6294 - val_accuracy: 0.6658\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6034 - val_loss: 0.6278 - val_accuracy: 0.6735\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6084 - val_loss: 0.6280 - val_accuracy: 0.6684\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6615 - accuracy: 0.6128 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6587 - accuracy: 0.6070 - val_loss: 0.6255 - val_accuracy: 0.6684\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6086 - val_loss: 0.6275 - val_accuracy: 0.6786\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6649 - accuracy: 0.6047 - val_loss: 0.6282 - val_accuracy: 0.6684\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6593 - accuracy: 0.6135 - val_loss: 0.6276 - val_accuracy: 0.6773\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6113 - val_loss: 0.6276 - val_accuracy: 0.6684\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6623 - accuracy: 0.6059 - val_loss: 0.6274 - val_accuracy: 0.6696\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6636 - accuracy: 0.6070 - val_loss: 0.6304 - val_accuracy: 0.6671\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.6113 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.6056 - val_loss: 0.6273 - val_accuracy: 0.6747\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6612 - accuracy: 0.6088 - val_loss: 0.6257 - val_accuracy: 0.6684\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6604 - accuracy: 0.6140 - val_loss: 0.6249 - val_accuracy: 0.6684\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6559 - accuracy: 0.6174 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6573 - accuracy: 0.6211 - val_loss: 0.6256 - val_accuracy: 0.6735\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6573 - accuracy: 0.6167 - val_loss: 0.6211 - val_accuracy: 0.6709\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6169 - val_loss: 0.6268 - val_accuracy: 0.6824\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6600 - accuracy: 0.6093 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6157 - val_loss: 0.6252 - val_accuracy: 0.6760\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6576 - accuracy: 0.6139 - val_loss: 0.6258 - val_accuracy: 0.6722\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6578 - accuracy: 0.6164 - val_loss: 0.6251 - val_accuracy: 0.6722\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6148 - val_loss: 0.6250 - val_accuracy: 0.6798\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6610 - accuracy: 0.6083 - val_loss: 0.6265 - val_accuracy: 0.6786\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6563 - accuracy: 0.6221 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6545 - accuracy: 0.6161 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6194 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6581 - accuracy: 0.6140 - val_loss: 0.6287 - val_accuracy: 0.6798\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6565 - accuracy: 0.6173 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6587 - accuracy: 0.6090 - val_loss: 0.6243 - val_accuracy: 0.6837\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6544 - accuracy: 0.6191 - val_loss: 0.6207 - val_accuracy: 0.6798\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6167 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6549 - accuracy: 0.6179 - val_loss: 0.6227 - val_accuracy: 0.6773\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6591 - accuracy: 0.6132 - val_loss: 0.6240 - val_accuracy: 0.6824\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6570 - accuracy: 0.6130 - val_loss: 0.6200 - val_accuracy: 0.6824\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6573 - accuracy: 0.6177 - val_loss: 0.6221 - val_accuracy: 0.6837\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6557 - accuracy: 0.6192 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6535 - accuracy: 0.6163 - val_loss: 0.6233 - val_accuracy: 0.6798\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6540 - accuracy: 0.6218 - val_loss: 0.6189 - val_accuracy: 0.6798\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6230 - val_loss: 0.6215 - val_accuracy: 0.6837\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6560 - accuracy: 0.6168 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6237 - val_loss: 0.6201 - val_accuracy: 0.6849\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6560 - accuracy: 0.6159 - val_loss: 0.6205 - val_accuracy: 0.6760\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6526 - accuracy: 0.6184 - val_loss: 0.6186 - val_accuracy: 0.6862\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6555 - accuracy: 0.6173 - val_loss: 0.6237 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6235 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6564 - accuracy: 0.6163 - val_loss: 0.6230 - val_accuracy: 0.6811\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6247 - val_loss: 0.6218 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6544 - accuracy: 0.6242 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6585 - accuracy: 0.6193 - val_loss: 0.6219 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6237 - val_loss: 0.6205 - val_accuracy: 0.6798\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6271 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6159 - val_loss: 0.6216 - val_accuracy: 0.6798\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6213 - val_loss: 0.6190 - val_accuracy: 0.6811\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6208 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.6291 - val_loss: 0.6190 - val_accuracy: 0.6824\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6539 - accuracy: 0.6269 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6547 - accuracy: 0.6192 - val_loss: 0.6182 - val_accuracy: 0.6824\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6209 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6231 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6270 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6242 - val_loss: 0.6161 - val_accuracy: 0.6849\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6253 - val_loss: 0.6205 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6302 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6539 - accuracy: 0.6215 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6319 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6508 - accuracy: 0.6300 - val_loss: 0.6137 - val_accuracy: 0.6824\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6236 - val_loss: 0.6163 - val_accuracy: 0.6837\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6267 - val_loss: 0.6190 - val_accuracy: 0.6824\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6532 - accuracy: 0.6282 - val_loss: 0.6196 - val_accuracy: 0.6798\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6531 - accuracy: 0.6318 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6284 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6501 - accuracy: 0.6316 - val_loss: 0.6158 - val_accuracy: 0.6786\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6282 - val_loss: 0.6157 - val_accuracy: 0.6862\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6295 - val_loss: 0.6195 - val_accuracy: 0.6798\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6265 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6184 - val_loss: 0.6144 - val_accuracy: 0.6811\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6531 - accuracy: 0.6275 - val_loss: 0.6214 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6541 - accuracy: 0.6196 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6505 - accuracy: 0.6232 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6252 - val_loss: 0.6180 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6282 - val_loss: 0.6150 - val_accuracy: 0.6824\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6449 - accuracy: 0.6340 - val_loss: 0.6126 - val_accuracy: 0.6862\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6483 - accuracy: 0.6326 - val_loss: 0.6157 - val_accuracy: 0.6837\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6350 - val_loss: 0.6165 - val_accuracy: 0.6862\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6267 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6484 - accuracy: 0.6297 - val_loss: 0.6158 - val_accuracy: 0.6862\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6496 - accuracy: 0.6297 - val_loss: 0.6133 - val_accuracy: 0.6824\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6366 - val_loss: 0.6160 - val_accuracy: 0.6862\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6487 - accuracy: 0.6247 - val_loss: 0.6171 - val_accuracy: 0.6849\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6508 - accuracy: 0.6262 - val_loss: 0.6141 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6501 - accuracy: 0.6241 - val_loss: 0.6147 - val_accuracy: 0.6901\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6487 - accuracy: 0.6261 - val_loss: 0.6163 - val_accuracy: 0.6888\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6252 - val_loss: 0.6155 - val_accuracy: 0.6875\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6470 - accuracy: 0.6355 - val_loss: 0.6144 - val_accuracy: 0.6875\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6281 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6478 - accuracy: 0.6279 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6478 - accuracy: 0.6304 - val_loss: 0.6149 - val_accuracy: 0.6875\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6458 - accuracy: 0.6316 - val_loss: 0.6130 - val_accuracy: 0.6862\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6465 - accuracy: 0.6331 - val_loss: 0.6121 - val_accuracy: 0.6875\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6464 - accuracy: 0.6302 - val_loss: 0.6145 - val_accuracy: 0.6875\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6377 - val_loss: 0.6115 - val_accuracy: 0.6901\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6325 - val_loss: 0.6172 - val_accuracy: 0.6888\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6454 - accuracy: 0.6389 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Calculating for: 1000 400 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_228 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.8030 - accuracy: 0.5504 - val_loss: 0.6900 - val_accuracy: 0.5536\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7069 - accuracy: 0.5772 - val_loss: 0.6645 - val_accuracy: 0.6288\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6761 - accuracy: 0.5946 - val_loss: 0.6571 - val_accuracy: 0.6390\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6656 - accuracy: 0.6083 - val_loss: 0.6472 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6576 - accuracy: 0.6204 - val_loss: 0.6441 - val_accuracy: 0.6633\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6509 - accuracy: 0.6238 - val_loss: 0.6493 - val_accuracy: 0.6403\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6527 - accuracy: 0.6233 - val_loss: 0.6443 - val_accuracy: 0.6403\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6493 - accuracy: 0.6286 - val_loss: 0.6378 - val_accuracy: 0.6582\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6494 - accuracy: 0.6292 - val_loss: 0.6433 - val_accuracy: 0.6403\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6420 - accuracy: 0.6417 - val_loss: 0.6347 - val_accuracy: 0.6594\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6408 - accuracy: 0.6368 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6418 - accuracy: 0.6364 - val_loss: 0.6410 - val_accuracy: 0.6480\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6355 - accuracy: 0.6420 - val_loss: 0.6367 - val_accuracy: 0.6569\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6351 - accuracy: 0.6417 - val_loss: 0.6343 - val_accuracy: 0.6543\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6343 - accuracy: 0.6469 - val_loss: 0.6364 - val_accuracy: 0.6429\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6354 - accuracy: 0.6423 - val_loss: 0.6327 - val_accuracy: 0.6594\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6319 - accuracy: 0.6502 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6262 - accuracy: 0.6555 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6309 - accuracy: 0.6472 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6289 - accuracy: 0.6513 - val_loss: 0.6352 - val_accuracy: 0.6518\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6194 - accuracy: 0.6669 - val_loss: 0.6323 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6201 - accuracy: 0.6579 - val_loss: 0.6327 - val_accuracy: 0.6582\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6233 - accuracy: 0.6599 - val_loss: 0.6292 - val_accuracy: 0.6582\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6154 - accuracy: 0.6639 - val_loss: 0.6303 - val_accuracy: 0.6620\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6187 - accuracy: 0.6621 - val_loss: 0.6322 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6202 - accuracy: 0.6594 - val_loss: 0.6331 - val_accuracy: 0.6582\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6132 - accuracy: 0.6696 - val_loss: 0.6327 - val_accuracy: 0.6582\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6146 - accuracy: 0.6655 - val_loss: 0.6365 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6137 - accuracy: 0.6625 - val_loss: 0.6318 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6141 - accuracy: 0.6693 - val_loss: 0.6315 - val_accuracy: 0.6658\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6122 - accuracy: 0.6719 - val_loss: 0.6359 - val_accuracy: 0.6582\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6077 - accuracy: 0.6707 - val_loss: 0.6322 - val_accuracy: 0.6658\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6038 - accuracy: 0.6785 - val_loss: 0.6357 - val_accuracy: 0.6620\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6012 - accuracy: 0.6797 - val_loss: 0.6432 - val_accuracy: 0.6556\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6062 - accuracy: 0.6733 - val_loss: 0.6364 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5997 - accuracy: 0.6796 - val_loss: 0.6398 - val_accuracy: 0.6518\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6013 - accuracy: 0.6801 - val_loss: 0.6423 - val_accuracy: 0.6505\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5986 - accuracy: 0.6821 - val_loss: 0.6360 - val_accuracy: 0.6684\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5955 - accuracy: 0.6855 - val_loss: 0.6446 - val_accuracy: 0.6492\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6026 - accuracy: 0.6752 - val_loss: 0.6380 - val_accuracy: 0.6620\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5943 - accuracy: 0.6844 - val_loss: 0.6412 - val_accuracy: 0.6620\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5975 - accuracy: 0.6815 - val_loss: 0.6345 - val_accuracy: 0.6709\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5939 - accuracy: 0.6850 - val_loss: 0.6397 - val_accuracy: 0.6620\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5966 - accuracy: 0.6810 - val_loss: 0.6435 - val_accuracy: 0.6569\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5901 - accuracy: 0.6870 - val_loss: 0.6451 - val_accuracy: 0.6645\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5961 - accuracy: 0.6854 - val_loss: 0.6442 - val_accuracy: 0.6543\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5904 - accuracy: 0.6866 - val_loss: 0.6353 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5872 - accuracy: 0.6913 - val_loss: 0.6469 - val_accuracy: 0.6480\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5860 - accuracy: 0.6884 - val_loss: 0.6417 - val_accuracy: 0.6594\n",
      "Calculating for: 1000 400 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_232 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.8244 - accuracy: 0.5402 - val_loss: 0.6595 - val_accuracy: 0.6505\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7160 - accuracy: 0.5681 - val_loss: 0.6510 - val_accuracy: 0.6633\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6868 - accuracy: 0.5762 - val_loss: 0.6418 - val_accuracy: 0.6684\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6750 - accuracy: 0.5844 - val_loss: 0.6523 - val_accuracy: 0.6556\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6739 - accuracy: 0.5870 - val_loss: 0.6486 - val_accuracy: 0.6722\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6685 - accuracy: 0.5951 - val_loss: 0.6467 - val_accuracy: 0.6747\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6668 - accuracy: 0.5966 - val_loss: 0.6431 - val_accuracy: 0.6735\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6608 - accuracy: 0.6056 - val_loss: 0.6399 - val_accuracy: 0.6722\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6634 - accuracy: 0.6039 - val_loss: 0.6471 - val_accuracy: 0.6607\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6628 - accuracy: 0.6049 - val_loss: 0.6428 - val_accuracy: 0.6620\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6590 - accuracy: 0.6135 - val_loss: 0.6434 - val_accuracy: 0.6543\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6602 - accuracy: 0.6094 - val_loss: 0.6396 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6551 - accuracy: 0.6173 - val_loss: 0.6415 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6580 - accuracy: 0.6060 - val_loss: 0.6399 - val_accuracy: 0.6607\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6551 - accuracy: 0.6143 - val_loss: 0.6369 - val_accuracy: 0.6607\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6552 - accuracy: 0.6216 - val_loss: 0.6420 - val_accuracy: 0.6492\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6558 - accuracy: 0.6122 - val_loss: 0.6362 - val_accuracy: 0.6620\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6576 - accuracy: 0.6142 - val_loss: 0.6360 - val_accuracy: 0.6582\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6557 - accuracy: 0.6221 - val_loss: 0.6362 - val_accuracy: 0.6582\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.6218 - val_loss: 0.6368 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6489 - accuracy: 0.6242 - val_loss: 0.6377 - val_accuracy: 0.6480\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6503 - accuracy: 0.6267 - val_loss: 0.6409 - val_accuracy: 0.6441\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6521 - accuracy: 0.6253 - val_loss: 0.6355 - val_accuracy: 0.6620\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6506 - accuracy: 0.6242 - val_loss: 0.6365 - val_accuracy: 0.6492\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6484 - accuracy: 0.6247 - val_loss: 0.6330 - val_accuracy: 0.6607\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6466 - accuracy: 0.6368 - val_loss: 0.6369 - val_accuracy: 0.6403\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6468 - accuracy: 0.6240 - val_loss: 0.6359 - val_accuracy: 0.6480\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6518 - accuracy: 0.6240 - val_loss: 0.6403 - val_accuracy: 0.6390\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6454 - accuracy: 0.6300 - val_loss: 0.6287 - val_accuracy: 0.6747\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6447 - accuracy: 0.6323 - val_loss: 0.6321 - val_accuracy: 0.6658\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6449 - accuracy: 0.6312 - val_loss: 0.6263 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6437 - accuracy: 0.6318 - val_loss: 0.6280 - val_accuracy: 0.6658\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6414 - accuracy: 0.6373 - val_loss: 0.6263 - val_accuracy: 0.6671\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6391 - accuracy: 0.6439 - val_loss: 0.6295 - val_accuracy: 0.6569\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6435 - accuracy: 0.6361 - val_loss: 0.6295 - val_accuracy: 0.6633\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6428 - accuracy: 0.6353 - val_loss: 0.6294 - val_accuracy: 0.6594\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6390 - accuracy: 0.6383 - val_loss: 0.6266 - val_accuracy: 0.6633\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6396 - accuracy: 0.6351 - val_loss: 0.6304 - val_accuracy: 0.6531\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6416 - accuracy: 0.6389 - val_loss: 0.6299 - val_accuracy: 0.6620\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6386 - accuracy: 0.6389 - val_loss: 0.6273 - val_accuracy: 0.6747\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6377 - accuracy: 0.6388 - val_loss: 0.6275 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6357 - accuracy: 0.6473 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6384 - accuracy: 0.6364 - val_loss: 0.6282 - val_accuracy: 0.6633\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6347 - accuracy: 0.6419 - val_loss: 0.6273 - val_accuracy: 0.6684\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6327 - accuracy: 0.6443 - val_loss: 0.6301 - val_accuracy: 0.6620\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6362 - accuracy: 0.6395 - val_loss: 0.6278 - val_accuracy: 0.6658\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6358 - accuracy: 0.6448 - val_loss: 0.6283 - val_accuracy: 0.6709\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6354 - accuracy: 0.6429 - val_loss: 0.6343 - val_accuracy: 0.6569\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6315 - accuracy: 0.6438 - val_loss: 0.6253 - val_accuracy: 0.6747\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6328 - accuracy: 0.6478 - val_loss: 0.6308 - val_accuracy: 0.6658\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6319 - accuracy: 0.6492 - val_loss: 0.6292 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6334 - accuracy: 0.6530 - val_loss: 0.6292 - val_accuracy: 0.6696\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6323 - accuracy: 0.6534 - val_loss: 0.6244 - val_accuracy: 0.6696\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6300 - accuracy: 0.6511 - val_loss: 0.6275 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6293 - accuracy: 0.6426 - val_loss: 0.6278 - val_accuracy: 0.6658\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6293 - accuracy: 0.6511 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6285 - accuracy: 0.6549 - val_loss: 0.6260 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6266 - accuracy: 0.6574 - val_loss: 0.6243 - val_accuracy: 0.6709\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6275 - accuracy: 0.6500 - val_loss: 0.6289 - val_accuracy: 0.6684\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6267 - accuracy: 0.6531 - val_loss: 0.6309 - val_accuracy: 0.6671\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6256 - accuracy: 0.6513 - val_loss: 0.6276 - val_accuracy: 0.6735\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6224 - accuracy: 0.6562 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6263 - accuracy: 0.6523 - val_loss: 0.6272 - val_accuracy: 0.6735\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6271 - accuracy: 0.6574 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6260 - accuracy: 0.6495 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6223 - accuracy: 0.6536 - val_loss: 0.6219 - val_accuracy: 0.6773\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6205 - accuracy: 0.6596 - val_loss: 0.6280 - val_accuracy: 0.6684\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6228 - accuracy: 0.6596 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6218 - accuracy: 0.6574 - val_loss: 0.6177 - val_accuracy: 0.6837\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6232 - accuracy: 0.6603 - val_loss: 0.6244 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6180 - accuracy: 0.6638 - val_loss: 0.6277 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6198 - accuracy: 0.6594 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6186 - accuracy: 0.6585 - val_loss: 0.6256 - val_accuracy: 0.6786\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6210 - accuracy: 0.6625 - val_loss: 0.6271 - val_accuracy: 0.6798\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6167 - accuracy: 0.6669 - val_loss: 0.6229 - val_accuracy: 0.6837\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6177 - accuracy: 0.6620 - val_loss: 0.6226 - val_accuracy: 0.6811\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6638 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6633 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6178 - accuracy: 0.6629 - val_loss: 0.6350 - val_accuracy: 0.6696\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6165 - accuracy: 0.6691 - val_loss: 0.6335 - val_accuracy: 0.6684\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6149 - accuracy: 0.6616 - val_loss: 0.6264 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6164 - accuracy: 0.6653 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6144 - accuracy: 0.6708 - val_loss: 0.6309 - val_accuracy: 0.6684\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6147 - accuracy: 0.6694 - val_loss: 0.6314 - val_accuracy: 0.6696\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6102 - accuracy: 0.6734 - val_loss: 0.6304 - val_accuracy: 0.6709\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6114 - accuracy: 0.6743 - val_loss: 0.6285 - val_accuracy: 0.6747\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6093 - accuracy: 0.6728 - val_loss: 0.6302 - val_accuracy: 0.6760\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6125 - accuracy: 0.6689 - val_loss: 0.6274 - val_accuracy: 0.6798\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6105 - accuracy: 0.6673 - val_loss: 0.6303 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6101 - accuracy: 0.6706 - val_loss: 0.6320 - val_accuracy: 0.6735\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6093 - accuracy: 0.6703 - val_loss: 0.6330 - val_accuracy: 0.6607\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6058 - accuracy: 0.6718 - val_loss: 0.6369 - val_accuracy: 0.6556\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6084 - accuracy: 0.6765 - val_loss: 0.6312 - val_accuracy: 0.6633\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6061 - accuracy: 0.6772 - val_loss: 0.6271 - val_accuracy: 0.6747\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6041 - accuracy: 0.6805 - val_loss: 0.6234 - val_accuracy: 0.6811\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6099 - accuracy: 0.6721 - val_loss: 0.6283 - val_accuracy: 0.6722\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6106 - accuracy: 0.6712 - val_loss: 0.6347 - val_accuracy: 0.6671\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6020 - accuracy: 0.6757 - val_loss: 0.6358 - val_accuracy: 0.6658\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6076 - accuracy: 0.6736 - val_loss: 0.6301 - val_accuracy: 0.6709\n",
      "Calculating for: 1000 400 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_236 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.8560 - accuracy: 0.5046 - val_loss: 0.6628 - val_accuracy: 0.6339\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.7370 - accuracy: 0.5106 - val_loss: 0.6602 - val_accuracy: 0.6365\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.7042 - accuracy: 0.5269 - val_loss: 0.6610 - val_accuracy: 0.6339\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6957 - accuracy: 0.5289 - val_loss: 0.6620 - val_accuracy: 0.6212\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6915 - accuracy: 0.5288 - val_loss: 0.6669 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6888 - accuracy: 0.5409 - val_loss: 0.6623 - val_accuracy: 0.6224\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6912 - accuracy: 0.5262 - val_loss: 0.6691 - val_accuracy: 0.6224\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6883 - accuracy: 0.5460 - val_loss: 0.6627 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6899 - accuracy: 0.5392 - val_loss: 0.6641 - val_accuracy: 0.6224\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6883 - accuracy: 0.5397 - val_loss: 0.6606 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6875 - accuracy: 0.5467 - val_loss: 0.6581 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6896 - accuracy: 0.5350 - val_loss: 0.6608 - val_accuracy: 0.6224\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6861 - accuracy: 0.5555 - val_loss: 0.6563 - val_accuracy: 0.6237\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6851 - accuracy: 0.5509 - val_loss: 0.6568 - val_accuracy: 0.6237\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6860 - accuracy: 0.5489 - val_loss: 0.6569 - val_accuracy: 0.6301\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6864 - accuracy: 0.5484 - val_loss: 0.6575 - val_accuracy: 0.6314\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6870 - accuracy: 0.5495 - val_loss: 0.6561 - val_accuracy: 0.6276\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6840 - accuracy: 0.5528 - val_loss: 0.6537 - val_accuracy: 0.6352\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6846 - accuracy: 0.5489 - val_loss: 0.6536 - val_accuracy: 0.6378\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6853 - accuracy: 0.5569 - val_loss: 0.6523 - val_accuracy: 0.6365\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6844 - accuracy: 0.5536 - val_loss: 0.6534 - val_accuracy: 0.6365\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6845 - accuracy: 0.5579 - val_loss: 0.6533 - val_accuracy: 0.6365\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6822 - accuracy: 0.5629 - val_loss: 0.6507 - val_accuracy: 0.6390\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6845 - accuracy: 0.5536 - val_loss: 0.6534 - val_accuracy: 0.6390\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6829 - accuracy: 0.5623 - val_loss: 0.6529 - val_accuracy: 0.6441\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6802 - accuracy: 0.5671 - val_loss: 0.6491 - val_accuracy: 0.6416\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6812 - accuracy: 0.5620 - val_loss: 0.6495 - val_accuracy: 0.6441\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6800 - accuracy: 0.5597 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6807 - accuracy: 0.5630 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6783 - accuracy: 0.5736 - val_loss: 0.6460 - val_accuracy: 0.6429\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6780 - accuracy: 0.5739 - val_loss: 0.6459 - val_accuracy: 0.6429\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6796 - accuracy: 0.5647 - val_loss: 0.6464 - val_accuracy: 0.6416\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6789 - accuracy: 0.5672 - val_loss: 0.6447 - val_accuracy: 0.6416\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6765 - accuracy: 0.5728 - val_loss: 0.6437 - val_accuracy: 0.6390\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6759 - accuracy: 0.5806 - val_loss: 0.6442 - val_accuracy: 0.6441\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6782 - accuracy: 0.5702 - val_loss: 0.6432 - val_accuracy: 0.6429\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6730 - accuracy: 0.5793 - val_loss: 0.6411 - val_accuracy: 0.6429\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6738 - accuracy: 0.5887 - val_loss: 0.6418 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6750 - accuracy: 0.5801 - val_loss: 0.6413 - val_accuracy: 0.6390\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6759 - accuracy: 0.5803 - val_loss: 0.6414 - val_accuracy: 0.6390\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6749 - accuracy: 0.5752 - val_loss: 0.6422 - val_accuracy: 0.6390\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6738 - accuracy: 0.5772 - val_loss: 0.6421 - val_accuracy: 0.6416\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6753 - accuracy: 0.5787 - val_loss: 0.6423 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6757 - accuracy: 0.5825 - val_loss: 0.6416 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6755 - accuracy: 0.5805 - val_loss: 0.6402 - val_accuracy: 0.6480\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6732 - accuracy: 0.5854 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6711 - accuracy: 0.5923 - val_loss: 0.6389 - val_accuracy: 0.6492\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6723 - accuracy: 0.5864 - val_loss: 0.6387 - val_accuracy: 0.6492\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6705 - accuracy: 0.5868 - val_loss: 0.6394 - val_accuracy: 0.6492\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6731 - accuracy: 0.5860 - val_loss: 0.6385 - val_accuracy: 0.6492\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6668 - accuracy: 0.5942 - val_loss: 0.6348 - val_accuracy: 0.6505\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6737 - accuracy: 0.5885 - val_loss: 0.6375 - val_accuracy: 0.6518\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6708 - accuracy: 0.5887 - val_loss: 0.6391 - val_accuracy: 0.6531\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6734 - accuracy: 0.5855 - val_loss: 0.6404 - val_accuracy: 0.6492\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6707 - accuracy: 0.5942 - val_loss: 0.6395 - val_accuracy: 0.6505\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6661 - accuracy: 0.5944 - val_loss: 0.6345 - val_accuracy: 0.6480\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6722 - accuracy: 0.5901 - val_loss: 0.6370 - val_accuracy: 0.6556\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6663 - accuracy: 0.6007 - val_loss: 0.6349 - val_accuracy: 0.6531\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6713 - accuracy: 0.5893 - val_loss: 0.6381 - val_accuracy: 0.6531\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6719 - accuracy: 0.5967 - val_loss: 0.6358 - val_accuracy: 0.6556\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6716 - accuracy: 0.5888 - val_loss: 0.6392 - val_accuracy: 0.6531\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6689 - accuracy: 0.5885 - val_loss: 0.6355 - val_accuracy: 0.6531\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6702 - accuracy: 0.5873 - val_loss: 0.6364 - val_accuracy: 0.6518\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6645 - accuracy: 0.5995 - val_loss: 0.6340 - val_accuracy: 0.6531\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6690 - accuracy: 0.5947 - val_loss: 0.6346 - val_accuracy: 0.6658\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6682 - accuracy: 0.5936 - val_loss: 0.6348 - val_accuracy: 0.6747\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6648 - accuracy: 0.5992 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6626 - accuracy: 0.6093 - val_loss: 0.6319 - val_accuracy: 0.6582\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6653 - accuracy: 0.6001 - val_loss: 0.6345 - val_accuracy: 0.6607\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6644 - accuracy: 0.6035 - val_loss: 0.6322 - val_accuracy: 0.6671\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6665 - accuracy: 0.5963 - val_loss: 0.6332 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6647 - accuracy: 0.5992 - val_loss: 0.6314 - val_accuracy: 0.6709\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6636 - accuracy: 0.6066 - val_loss: 0.6296 - val_accuracy: 0.6658\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6658 - accuracy: 0.6000 - val_loss: 0.6304 - val_accuracy: 0.6671\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6654 - accuracy: 0.6021 - val_loss: 0.6315 - val_accuracy: 0.6645\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6687 - accuracy: 0.6000 - val_loss: 0.6363 - val_accuracy: 0.6658\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6655 - accuracy: 0.6036 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6654 - accuracy: 0.5997 - val_loss: 0.6297 - val_accuracy: 0.6760\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6653 - accuracy: 0.6021 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6623 - accuracy: 0.6064 - val_loss: 0.6285 - val_accuracy: 0.6709\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6602 - accuracy: 0.6083 - val_loss: 0.6275 - val_accuracy: 0.6709\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6629 - accuracy: 0.6063 - val_loss: 0.6303 - val_accuracy: 0.6735\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6649 - accuracy: 0.6019 - val_loss: 0.6291 - val_accuracy: 0.6722\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6604 - accuracy: 0.6083 - val_loss: 0.6257 - val_accuracy: 0.6722\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6620 - accuracy: 0.6040 - val_loss: 0.6273 - val_accuracy: 0.6735\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6614 - accuracy: 0.6084 - val_loss: 0.6274 - val_accuracy: 0.6722\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6624 - accuracy: 0.6084 - val_loss: 0.6281 - val_accuracy: 0.6696\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6628 - accuracy: 0.6055 - val_loss: 0.6265 - val_accuracy: 0.6735\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6617 - accuracy: 0.6114 - val_loss: 0.6256 - val_accuracy: 0.6735\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6628 - accuracy: 0.6005 - val_loss: 0.6276 - val_accuracy: 0.6709\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6572 - accuracy: 0.6177 - val_loss: 0.6258 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6579 - accuracy: 0.6093 - val_loss: 0.6253 - val_accuracy: 0.6760\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6581 - accuracy: 0.6127 - val_loss: 0.6259 - val_accuracy: 0.6709\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6619 - accuracy: 0.6094 - val_loss: 0.6272 - val_accuracy: 0.6722\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6631 - accuracy: 0.6069 - val_loss: 0.6321 - val_accuracy: 0.6722\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6613 - accuracy: 0.6107 - val_loss: 0.6258 - val_accuracy: 0.6722\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6590 - accuracy: 0.6117 - val_loss: 0.6245 - val_accuracy: 0.6773\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6617 - accuracy: 0.6078 - val_loss: 0.6255 - val_accuracy: 0.6735\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6616 - accuracy: 0.6188 - val_loss: 0.6242 - val_accuracy: 0.6747\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6613 - accuracy: 0.6073 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6615 - accuracy: 0.6076 - val_loss: 0.6277 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6571 - accuracy: 0.6191 - val_loss: 0.6198 - val_accuracy: 0.6760\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6592 - accuracy: 0.6228 - val_loss: 0.6223 - val_accuracy: 0.6773\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6605 - accuracy: 0.6119 - val_loss: 0.6277 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6572 - accuracy: 0.6109 - val_loss: 0.6197 - val_accuracy: 0.6760\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6573 - accuracy: 0.6208 - val_loss: 0.6213 - val_accuracy: 0.6747\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6581 - accuracy: 0.6117 - val_loss: 0.6238 - val_accuracy: 0.6786\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6576 - accuracy: 0.6183 - val_loss: 0.6235 - val_accuracy: 0.6798\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6598 - accuracy: 0.6117 - val_loss: 0.6231 - val_accuracy: 0.6760\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6561 - accuracy: 0.6110 - val_loss: 0.6207 - val_accuracy: 0.6773\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6584 - accuracy: 0.6091 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6558 - accuracy: 0.6163 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6584 - accuracy: 0.6132 - val_loss: 0.6187 - val_accuracy: 0.6811\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6600 - accuracy: 0.6094 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6566 - accuracy: 0.6202 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6602 - accuracy: 0.6137 - val_loss: 0.6200 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6587 - accuracy: 0.6128 - val_loss: 0.6263 - val_accuracy: 0.6837\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6534 - accuracy: 0.6217 - val_loss: 0.6229 - val_accuracy: 0.6811\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6568 - accuracy: 0.6217 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6536 - accuracy: 0.6207 - val_loss: 0.6181 - val_accuracy: 0.6798\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6562 - accuracy: 0.6164 - val_loss: 0.6226 - val_accuracy: 0.6786\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6537 - accuracy: 0.6172 - val_loss: 0.6190 - val_accuracy: 0.6811\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6539 - accuracy: 0.6173 - val_loss: 0.6161 - val_accuracy: 0.6798\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6564 - accuracy: 0.6135 - val_loss: 0.6182 - val_accuracy: 0.6811\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6560 - accuracy: 0.6217 - val_loss: 0.6239 - val_accuracy: 0.6837\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6538 - accuracy: 0.6197 - val_loss: 0.6211 - val_accuracy: 0.6798\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6529 - accuracy: 0.6198 - val_loss: 0.6162 - val_accuracy: 0.6773\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6553 - accuracy: 0.6144 - val_loss: 0.6173 - val_accuracy: 0.6811\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6550 - accuracy: 0.6183 - val_loss: 0.6171 - val_accuracy: 0.6773\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6541 - accuracy: 0.6206 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6559 - accuracy: 0.6149 - val_loss: 0.6163 - val_accuracy: 0.6786\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6528 - accuracy: 0.6287 - val_loss: 0.6158 - val_accuracy: 0.6798\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6554 - accuracy: 0.6217 - val_loss: 0.6156 - val_accuracy: 0.6773\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6552 - accuracy: 0.6174 - val_loss: 0.6166 - val_accuracy: 0.6811\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6519 - accuracy: 0.6202 - val_loss: 0.6161 - val_accuracy: 0.6798\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6528 - accuracy: 0.6209 - val_loss: 0.6183 - val_accuracy: 0.6824\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6528 - accuracy: 0.6179 - val_loss: 0.6183 - val_accuracy: 0.6824\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6543 - accuracy: 0.6220 - val_loss: 0.6172 - val_accuracy: 0.6824\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6528 - accuracy: 0.6202 - val_loss: 0.6151 - val_accuracy: 0.6824\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6527 - accuracy: 0.6246 - val_loss: 0.6227 - val_accuracy: 0.6811\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6523 - accuracy: 0.6230 - val_loss: 0.6157 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6543 - accuracy: 0.6191 - val_loss: 0.6149 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6544 - accuracy: 0.6188 - val_loss: 0.6175 - val_accuracy: 0.6824\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6545 - accuracy: 0.6227 - val_loss: 0.6177 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6533 - accuracy: 0.6215 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6484 - accuracy: 0.6206 - val_loss: 0.6147 - val_accuracy: 0.6824\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6500 - accuracy: 0.6281 - val_loss: 0.6144 - val_accuracy: 0.6786\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6512 - accuracy: 0.6231 - val_loss: 0.6148 - val_accuracy: 0.6811\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6507 - accuracy: 0.6246 - val_loss: 0.6151 - val_accuracy: 0.6837\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6489 - accuracy: 0.6236 - val_loss: 0.6124 - val_accuracy: 0.6824\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6482 - accuracy: 0.6261 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6527 - accuracy: 0.6211 - val_loss: 0.6122 - val_accuracy: 0.6824\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6487 - accuracy: 0.6261 - val_loss: 0.6131 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6521 - accuracy: 0.6279 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6498 - accuracy: 0.6305 - val_loss: 0.6132 - val_accuracy: 0.6798\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6493 - accuracy: 0.6300 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6498 - accuracy: 0.6276 - val_loss: 0.6148 - val_accuracy: 0.6824\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6496 - accuracy: 0.6260 - val_loss: 0.6148 - val_accuracy: 0.6824\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6492 - accuracy: 0.6264 - val_loss: 0.6119 - val_accuracy: 0.6849\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6482 - accuracy: 0.6326 - val_loss: 0.6118 - val_accuracy: 0.6849\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6517 - accuracy: 0.6290 - val_loss: 0.6117 - val_accuracy: 0.6824\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6499 - accuracy: 0.6279 - val_loss: 0.6116 - val_accuracy: 0.6837\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6495 - accuracy: 0.6309 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6508 - accuracy: 0.6250 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6509 - accuracy: 0.6264 - val_loss: 0.6157 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6477 - accuracy: 0.6335 - val_loss: 0.6129 - val_accuracy: 0.6811\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6486 - accuracy: 0.6272 - val_loss: 0.6129 - val_accuracy: 0.6773\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6466 - accuracy: 0.6326 - val_loss: 0.6103 - val_accuracy: 0.6811\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6429 - accuracy: 0.6368 - val_loss: 0.6094 - val_accuracy: 0.6811\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6430 - accuracy: 0.6334 - val_loss: 0.6119 - val_accuracy: 0.6786\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6477 - accuracy: 0.6266 - val_loss: 0.6156 - val_accuracy: 0.6824\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6483 - accuracy: 0.6246 - val_loss: 0.6130 - val_accuracy: 0.6811\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6475 - accuracy: 0.6356 - val_loss: 0.6113 - val_accuracy: 0.6798\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6495 - accuracy: 0.6246 - val_loss: 0.6153 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6470 - accuracy: 0.6309 - val_loss: 0.6111 - val_accuracy: 0.6837\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6440 - accuracy: 0.6309 - val_loss: 0.6124 - val_accuracy: 0.6811\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6462 - accuracy: 0.6324 - val_loss: 0.6113 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6479 - accuracy: 0.6323 - val_loss: 0.6152 - val_accuracy: 0.6849\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6457 - accuracy: 0.6295 - val_loss: 0.6124 - val_accuracy: 0.6862\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6442 - accuracy: 0.6318 - val_loss: 0.6112 - val_accuracy: 0.6849\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6459 - accuracy: 0.6345 - val_loss: 0.6113 - val_accuracy: 0.6837\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6461 - accuracy: 0.6291 - val_loss: 0.6092 - val_accuracy: 0.6875\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6438 - accuracy: 0.6348 - val_loss: 0.6107 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6447 - accuracy: 0.6380 - val_loss: 0.6118 - val_accuracy: 0.6837\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6425 - accuracy: 0.6339 - val_loss: 0.6101 - val_accuracy: 0.6824\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6494 - accuracy: 0.6237 - val_loss: 0.6123 - val_accuracy: 0.6837\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6427 - accuracy: 0.6405 - val_loss: 0.6117 - val_accuracy: 0.6837\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6484 - accuracy: 0.6358 - val_loss: 0.6118 - val_accuracy: 0.6811\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6439 - accuracy: 0.6368 - val_loss: 0.6089 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6437 - accuracy: 0.6344 - val_loss: 0.6086 - val_accuracy: 0.6862\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6452 - accuracy: 0.6392 - val_loss: 0.6094 - val_accuracy: 0.6888\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6434 - accuracy: 0.6408 - val_loss: 0.6098 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6464 - accuracy: 0.6346 - val_loss: 0.6118 - val_accuracy: 0.6875\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6434 - accuracy: 0.6361 - val_loss: 0.6104 - val_accuracy: 0.6849\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6415 - accuracy: 0.6368 - val_loss: 0.6110 - val_accuracy: 0.6888\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6436 - accuracy: 0.6331 - val_loss: 0.6090 - val_accuracy: 0.6875\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6452 - accuracy: 0.6356 - val_loss: 0.6096 - val_accuracy: 0.6862\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6442 - accuracy: 0.6372 - val_loss: 0.6117 - val_accuracy: 0.6811\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6479 - accuracy: 0.6318 - val_loss: 0.6114 - val_accuracy: 0.6875\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6420 - accuracy: 0.6355 - val_loss: 0.6085 - val_accuracy: 0.6901\n"
     ]
    }
   ],
   "source": [
    "# Lets explore some parameters:\n",
    "scores = {}\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30) # Early stop will stop training when the model has failed to improve\n",
    "\n",
    "dense1 = [700, 850, 1000]\n",
    "dense2 = [100, 250, 400]\n",
    "dropout = [0.5, 0.7, 0.9]\n",
    "optimizer = ['sgd']\n",
    "\n",
    "best_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "for d1 in dense1:\n",
    "    for d2 in dense2:\n",
    "        for dr in dropout:\n",
    "            for opt in optimizer:\n",
    "                print('Calculating for:', d1, d2, dr, opt)\n",
    "                keras_model = init_keras_model(dense1=d1, dense2=d2, dropout=dr, optimizer=opt)\n",
    "                keras_model.fit(X_train_tensor, \n",
    "                                y_train_tensor, \n",
    "                                epochs=n_epochs, \n",
    "                                validation_data=(X_val_tensor, y_val_tensor),\n",
    "                                callbacks=[es] \n",
    "                                )\n",
    "                score = keras_model.evaluate(X_test_tensor, y_test_tensor, verbose=0)[1]\n",
    "                scores[(d1, d2, dr, opt)] = score\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'dense1':d1, 'dense2':d2, 'dropout':dr, 'optimizer':opt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(700, 100, 0.5, 'sgd'): 0.6743295192718506, (700, 100, 0.7, 'sgd'): 0.6858237385749817, (700, 100, 0.9, 'sgd'): 0.6692209243774414, (700, 250, 0.5, 'sgd'): 0.671775221824646, (700, 250, 0.7, 'sgd'): 0.6615580916404724, (700, 250, 0.9, 'sgd'): 0.6704980731010437, (700, 400, 0.5, 'sgd'): 0.6730523705482483, (700, 400, 0.7, 'sgd'): 0.6628352403640747, (700, 400, 0.9, 'sgd'): 0.6692209243774414, (850, 100, 0.5, 'sgd'): 0.6743295192718506, (850, 100, 0.7, 'sgd'): 0.6602809429168701, (850, 100, 0.9, 'sgd'): 0.6743295192718506, (850, 250, 0.5, 'sgd'): 0.6513410210609436, (850, 250, 0.7, 'sgd'): 0.679438054561615, (850, 250, 0.9, 'sgd'): 0.671775221824646, (850, 400, 0.5, 'sgd'): 0.6602809429168701, (850, 400, 0.7, 'sgd'): 0.6704980731010437, (850, 400, 0.9, 'sgd'): 0.6756066679954529, (1000, 100, 0.5, 'sgd'): 0.6679438352584839, (1000, 100, 0.7, 'sgd'): 0.6666666865348816, (1000, 100, 0.9, 'sgd'): 0.6730523705482483, (1000, 250, 0.5, 'sgd'): 0.679438054561615, (1000, 250, 0.7, 'sgd'): 0.6768837571144104, (1000, 250, 0.9, 'sgd'): 0.679438054561615, (1000, 400, 0.5, 'sgd'): 0.656449556350708, (1000, 400, 0.7, 'sgd'): 0.6832695007324219, (1000, 400, 0.9, 'sgd'): 0.6768837571144104}\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'dense1': 700, 'dense2': 100, 'dropout': 0.7, 'optimizer': 'sgd'}, best_score: 0.6858237385749817\n"
     ]
    }
   ],
   "source": [
    "print(f'best_params: {best_params}, best_score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for: 650 50 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_240 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8221 - accuracy: 0.5309 - val_loss: 0.6546 - val_accuracy: 0.6467\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7168 - accuracy: 0.5504 - val_loss: 0.6543 - val_accuracy: 0.6454\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5605 - val_loss: 0.6454 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6806 - accuracy: 0.5835 - val_loss: 0.6437 - val_accuracy: 0.6607\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5816 - val_loss: 0.6476 - val_accuracy: 0.6543\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5867 - val_loss: 0.6417 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6681 - accuracy: 0.5956 - val_loss: 0.6451 - val_accuracy: 0.6645\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5944 - val_loss: 0.6425 - val_accuracy: 0.6633\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6081 - val_loss: 0.6371 - val_accuracy: 0.6671\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6084 - val_loss: 0.6369 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6094 - val_loss: 0.6402 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6042 - val_loss: 0.6389 - val_accuracy: 0.6658\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6564 - accuracy: 0.6110 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6076 - val_loss: 0.6358 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6207 - val_loss: 0.6359 - val_accuracy: 0.6620\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6149 - val_loss: 0.6328 - val_accuracy: 0.6760\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6222 - val_loss: 0.6268 - val_accuracy: 0.6747\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6248 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6207 - val_loss: 0.6339 - val_accuracy: 0.6531\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6306 - val_loss: 0.6329 - val_accuracy: 0.6786\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6257 - val_loss: 0.6329 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6483 - accuracy: 0.6284 - val_loss: 0.6305 - val_accuracy: 0.6747\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6266 - val_loss: 0.6280 - val_accuracy: 0.6671\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6277 - val_loss: 0.6280 - val_accuracy: 0.6824\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6307 - val_loss: 0.6270 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6237 - val_loss: 0.6319 - val_accuracy: 0.6760\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.6315 - val_loss: 0.6264 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6442 - accuracy: 0.6324 - val_loss: 0.6304 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6427 - accuracy: 0.6363 - val_loss: 0.6289 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6407 - accuracy: 0.6335 - val_loss: 0.6212 - val_accuracy: 0.6913\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6410 - accuracy: 0.6413 - val_loss: 0.6224 - val_accuracy: 0.6901\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6393 - val_loss: 0.6287 - val_accuracy: 0.6684\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6396 - accuracy: 0.6309 - val_loss: 0.6268 - val_accuracy: 0.6684\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6358 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6427 - val_loss: 0.6207 - val_accuracy: 0.6964\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6423 - val_loss: 0.6150 - val_accuracy: 0.6977\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6394 - accuracy: 0.6382 - val_loss: 0.6176 - val_accuracy: 0.6913\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6367 - accuracy: 0.6402 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6325 - accuracy: 0.6508 - val_loss: 0.6179 - val_accuracy: 0.6849\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6462 - val_loss: 0.6192 - val_accuracy: 0.6773\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6347 - accuracy: 0.6454 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6388 - accuracy: 0.6417 - val_loss: 0.6253 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6492 - val_loss: 0.6253 - val_accuracy: 0.6735\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6361 - accuracy: 0.6456 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6344 - accuracy: 0.6472 - val_loss: 0.6220 - val_accuracy: 0.6837\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6306 - accuracy: 0.6483 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6534 - val_loss: 0.6193 - val_accuracy: 0.6875\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6523 - val_loss: 0.6185 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6454 - val_loss: 0.6195 - val_accuracy: 0.6901\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6316 - accuracy: 0.6477 - val_loss: 0.6218 - val_accuracy: 0.6824\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6523 - val_loss: 0.6228 - val_accuracy: 0.6773\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6295 - accuracy: 0.6532 - val_loss: 0.6237 - val_accuracy: 0.6849\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6308 - accuracy: 0.6531 - val_loss: 0.6232 - val_accuracy: 0.6747\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6557 - val_loss: 0.6164 - val_accuracy: 0.6901\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6539 - val_loss: 0.6188 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6294 - accuracy: 0.6466 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6576 - val_loss: 0.6256 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6557 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6231 - accuracy: 0.6616 - val_loss: 0.6144 - val_accuracy: 0.6926\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6270 - accuracy: 0.6546 - val_loss: 0.6215 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6593 - val_loss: 0.6271 - val_accuracy: 0.6837\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6510 - val_loss: 0.6246 - val_accuracy: 0.6849\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6222 - accuracy: 0.6606 - val_loss: 0.6233 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6185 - accuracy: 0.6643 - val_loss: 0.6236 - val_accuracy: 0.6926\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6166 - accuracy: 0.6658 - val_loss: 0.6212 - val_accuracy: 0.6939\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6559 - val_loss: 0.6234 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6213 - accuracy: 0.6591 - val_loss: 0.6230 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6139 - accuracy: 0.6605 - val_loss: 0.6251 - val_accuracy: 0.6735\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.6704 - val_loss: 0.6260 - val_accuracy: 0.6849\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6173 - accuracy: 0.6682 - val_loss: 0.6219 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6608 - val_loss: 0.6206 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6129 - accuracy: 0.6647 - val_loss: 0.6223 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6706 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6118 - accuracy: 0.6694 - val_loss: 0.6240 - val_accuracy: 0.6849\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6709 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.6678 - val_loss: 0.6199 - val_accuracy: 0.6939\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6648 - val_loss: 0.6254 - val_accuracy: 0.6786\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6093 - accuracy: 0.6645 - val_loss: 0.6230 - val_accuracy: 0.6837\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6682 - val_loss: 0.6258 - val_accuracy: 0.6735\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6611 - val_loss: 0.6249 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6722 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6068 - accuracy: 0.6723 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6709 - val_loss: 0.6241 - val_accuracy: 0.6786\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6109 - accuracy: 0.6702 - val_loss: 0.6255 - val_accuracy: 0.6811\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6017 - accuracy: 0.6792 - val_loss: 0.6249 - val_accuracy: 0.6798\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6037 - accuracy: 0.6746 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6827 - val_loss: 0.6238 - val_accuracy: 0.6888\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6044 - accuracy: 0.6728 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6041 - accuracy: 0.6738 - val_loss: 0.6236 - val_accuracy: 0.6837\n",
      "Calculating for: 650 50 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_244 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8211 - accuracy: 0.5214 - val_loss: 0.6578 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7249 - accuracy: 0.5441 - val_loss: 0.6572 - val_accuracy: 0.6543\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6974 - accuracy: 0.5516 - val_loss: 0.6519 - val_accuracy: 0.6633\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6854 - accuracy: 0.5625 - val_loss: 0.6523 - val_accuracy: 0.6633\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6813 - accuracy: 0.5620 - val_loss: 0.6480 - val_accuracy: 0.6671\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5883 - val_loss: 0.6416 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6763 - accuracy: 0.5823 - val_loss: 0.6462 - val_accuracy: 0.6658\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.5853 - val_loss: 0.6451 - val_accuracy: 0.6709\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5918 - val_loss: 0.6471 - val_accuracy: 0.6760\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5936 - val_loss: 0.6398 - val_accuracy: 0.6722\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5888 - val_loss: 0.6386 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5934 - val_loss: 0.6390 - val_accuracy: 0.6760\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.5987 - val_loss: 0.6347 - val_accuracy: 0.6798\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6652 - accuracy: 0.6035 - val_loss: 0.6358 - val_accuracy: 0.6786\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.5958 - val_loss: 0.6401 - val_accuracy: 0.6760\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6011 - val_loss: 0.6417 - val_accuracy: 0.6722\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6600 - accuracy: 0.6076 - val_loss: 0.6322 - val_accuracy: 0.6862\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6093 - val_loss: 0.6344 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6107 - val_loss: 0.6421 - val_accuracy: 0.6684\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6152 - val_loss: 0.6347 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6203 - val_loss: 0.6291 - val_accuracy: 0.6875\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6591 - accuracy: 0.6078 - val_loss: 0.6325 - val_accuracy: 0.6798\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6194 - val_loss: 0.6284 - val_accuracy: 0.6837\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6619 - accuracy: 0.6145 - val_loss: 0.6336 - val_accuracy: 0.6862\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6554 - accuracy: 0.6168 - val_loss: 0.6338 - val_accuracy: 0.6811\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6553 - accuracy: 0.6144 - val_loss: 0.6302 - val_accuracy: 0.6837\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6155 - val_loss: 0.6322 - val_accuracy: 0.6849\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6568 - accuracy: 0.6138 - val_loss: 0.6318 - val_accuracy: 0.6913\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6124 - val_loss: 0.6334 - val_accuracy: 0.6875\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6559 - accuracy: 0.6095 - val_loss: 0.6311 - val_accuracy: 0.6849\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6230 - val_loss: 0.6275 - val_accuracy: 0.6837\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6220 - val_loss: 0.6250 - val_accuracy: 0.6824\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6518 - accuracy: 0.6264 - val_loss: 0.6290 - val_accuracy: 0.6901\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6252 - val_loss: 0.6283 - val_accuracy: 0.6888\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6526 - accuracy: 0.6238 - val_loss: 0.6263 - val_accuracy: 0.6913\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6276 - val_loss: 0.6295 - val_accuracy: 0.6901\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6508 - accuracy: 0.6265 - val_loss: 0.6233 - val_accuracy: 0.6913\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6236 - val_loss: 0.6315 - val_accuracy: 0.6888\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6515 - accuracy: 0.6296 - val_loss: 0.6221 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6493 - accuracy: 0.6252 - val_loss: 0.6206 - val_accuracy: 0.6913\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6467 - accuracy: 0.6346 - val_loss: 0.6157 - val_accuracy: 0.6952\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6306 - val_loss: 0.6220 - val_accuracy: 0.6926\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6346 - val_loss: 0.6239 - val_accuracy: 0.6926\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6496 - accuracy: 0.6243 - val_loss: 0.6257 - val_accuracy: 0.6939\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.6296 - val_loss: 0.6221 - val_accuracy: 0.6913\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6309 - val_loss: 0.6216 - val_accuracy: 0.6926\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6462 - accuracy: 0.6338 - val_loss: 0.6251 - val_accuracy: 0.6926\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6474 - accuracy: 0.6299 - val_loss: 0.6223 - val_accuracy: 0.6901\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6389 - val_loss: 0.6190 - val_accuracy: 0.7015\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6333 - val_loss: 0.6208 - val_accuracy: 0.6901\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6453 - accuracy: 0.6289 - val_loss: 0.6188 - val_accuracy: 0.7003\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6463 - accuracy: 0.6343 - val_loss: 0.6204 - val_accuracy: 0.7041\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6379 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6370 - val_loss: 0.6175 - val_accuracy: 0.7003\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6419 - accuracy: 0.6442 - val_loss: 0.6178 - val_accuracy: 0.6990\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6377 - val_loss: 0.6251 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6397 - val_loss: 0.6248 - val_accuracy: 0.6862\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6404 - accuracy: 0.6359 - val_loss: 0.6208 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6431 - accuracy: 0.6356 - val_loss: 0.6240 - val_accuracy: 0.6888\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6481 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6412 - accuracy: 0.6456 - val_loss: 0.6179 - val_accuracy: 0.6977\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6422 - val_loss: 0.6206 - val_accuracy: 0.6888\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6373 - accuracy: 0.6423 - val_loss: 0.6218 - val_accuracy: 0.6875\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6439 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6330 - accuracy: 0.6433 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6385 - accuracy: 0.6453 - val_loss: 0.6218 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6347 - accuracy: 0.6437 - val_loss: 0.6177 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6451 - val_loss: 0.6166 - val_accuracy: 0.6952\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6360 - accuracy: 0.6488 - val_loss: 0.6186 - val_accuracy: 0.6964\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6527 - val_loss: 0.6148 - val_accuracy: 0.6977\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6320 - accuracy: 0.6501 - val_loss: 0.6159 - val_accuracy: 0.6926\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6491 - val_loss: 0.6164 - val_accuracy: 0.6926\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6300 - accuracy: 0.6544 - val_loss: 0.6177 - val_accuracy: 0.6926\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6518 - val_loss: 0.6164 - val_accuracy: 0.6939\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6529 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6311 - accuracy: 0.6486 - val_loss: 0.6214 - val_accuracy: 0.6875\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6323 - accuracy: 0.6550 - val_loss: 0.6169 - val_accuracy: 0.6926\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6506 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6508 - val_loss: 0.6161 - val_accuracy: 0.6939\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6319 - accuracy: 0.6565 - val_loss: 0.6217 - val_accuracy: 0.6913\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6274 - accuracy: 0.6546 - val_loss: 0.6119 - val_accuracy: 0.6913\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6338 - accuracy: 0.6444 - val_loss: 0.6167 - val_accuracy: 0.6939\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6510 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6333 - accuracy: 0.6498 - val_loss: 0.6180 - val_accuracy: 0.6901\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6540 - val_loss: 0.6192 - val_accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6567 - val_loss: 0.6161 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6251 - accuracy: 0.6588 - val_loss: 0.6178 - val_accuracy: 0.6824\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6240 - accuracy: 0.6545 - val_loss: 0.6178 - val_accuracy: 0.6811\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6303 - accuracy: 0.6589 - val_loss: 0.6166 - val_accuracy: 0.6824\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6525 - val_loss: 0.6143 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6257 - accuracy: 0.6603 - val_loss: 0.6141 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6575 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6560 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6576 - val_loss: 0.6143 - val_accuracy: 0.6837\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6556 - val_loss: 0.6140 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6256 - accuracy: 0.6545 - val_loss: 0.6167 - val_accuracy: 0.6888\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6248 - accuracy: 0.6583 - val_loss: 0.6174 - val_accuracy: 0.6837\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6241 - accuracy: 0.6579 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6674 - val_loss: 0.6131 - val_accuracy: 0.6977\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6561 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6649 - val_loss: 0.6156 - val_accuracy: 0.6926\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6204 - accuracy: 0.6628 - val_loss: 0.6140 - val_accuracy: 0.6913\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6625 - val_loss: 0.6137 - val_accuracy: 0.6901\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6194 - accuracy: 0.6728 - val_loss: 0.6106 - val_accuracy: 0.6926\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6654 - val_loss: 0.6142 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6626 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6536 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6176 - accuracy: 0.6600 - val_loss: 0.6148 - val_accuracy: 0.6875\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6638 - val_loss: 0.6139 - val_accuracy: 0.6926\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6232 - accuracy: 0.6611 - val_loss: 0.6129 - val_accuracy: 0.6990\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6174 - accuracy: 0.6647 - val_loss: 0.6135 - val_accuracy: 0.6977\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6188 - accuracy: 0.6620 - val_loss: 0.6120 - val_accuracy: 0.6913\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6680 - val_loss: 0.6178 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6694 - val_loss: 0.6125 - val_accuracy: 0.6952\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6674 - val_loss: 0.6154 - val_accuracy: 0.6952\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6655 - val_loss: 0.6124 - val_accuracy: 0.6964\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6166 - accuracy: 0.6652 - val_loss: 0.6188 - val_accuracy: 0.6913\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6687 - val_loss: 0.6156 - val_accuracy: 0.6913\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6704 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6659 - val_loss: 0.6149 - val_accuracy: 0.6939\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6624 - val_loss: 0.6171 - val_accuracy: 0.6926\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6120 - accuracy: 0.6689 - val_loss: 0.6147 - val_accuracy: 0.6901\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6138 - accuracy: 0.6721 - val_loss: 0.6127 - val_accuracy: 0.6862\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6139 - accuracy: 0.6734 - val_loss: 0.6181 - val_accuracy: 0.6901\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6119 - accuracy: 0.6738 - val_loss: 0.6129 - val_accuracy: 0.6888\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6113 - accuracy: 0.6738 - val_loss: 0.6152 - val_accuracy: 0.6837\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6097 - accuracy: 0.6702 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6123 - accuracy: 0.6672 - val_loss: 0.6159 - val_accuracy: 0.6798\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6152 - accuracy: 0.6717 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6101 - accuracy: 0.6742 - val_loss: 0.6184 - val_accuracy: 0.6862\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6111 - accuracy: 0.6731 - val_loss: 0.6147 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6131 - accuracy: 0.6654 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6094 - accuracy: 0.6799 - val_loss: 0.6201 - val_accuracy: 0.6837\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6089 - accuracy: 0.6703 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Calculating for: 650 50 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_248 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8191 - accuracy: 0.5112 - val_loss: 0.6613 - val_accuracy: 0.6237\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7258 - accuracy: 0.5280 - val_loss: 0.6625 - val_accuracy: 0.6276\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7045 - accuracy: 0.5295 - val_loss: 0.6638 - val_accuracy: 0.6327\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5383 - val_loss: 0.6589 - val_accuracy: 0.6429\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5497 - val_loss: 0.6620 - val_accuracy: 0.6441\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6865 - accuracy: 0.5475 - val_loss: 0.6624 - val_accuracy: 0.6505\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6851 - accuracy: 0.5481 - val_loss: 0.6601 - val_accuracy: 0.6454\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6833 - accuracy: 0.5582 - val_loss: 0.6605 - val_accuracy: 0.6429\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6829 - accuracy: 0.5540 - val_loss: 0.6644 - val_accuracy: 0.6620\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.5578 - val_loss: 0.6613 - val_accuracy: 0.6582\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5604 - val_loss: 0.6557 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6798 - accuracy: 0.5668 - val_loss: 0.6527 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.5673 - val_loss: 0.6524 - val_accuracy: 0.6569\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5766 - val_loss: 0.6531 - val_accuracy: 0.6607\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5764 - val_loss: 0.6496 - val_accuracy: 0.6492\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.5772 - val_loss: 0.6510 - val_accuracy: 0.6582\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6751 - accuracy: 0.5787 - val_loss: 0.6525 - val_accuracy: 0.6594\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6732 - accuracy: 0.5864 - val_loss: 0.6501 - val_accuracy: 0.6594\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6750 - accuracy: 0.5815 - val_loss: 0.6537 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5774 - val_loss: 0.6448 - val_accuracy: 0.6582\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5801 - val_loss: 0.6448 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6710 - accuracy: 0.5863 - val_loss: 0.6500 - val_accuracy: 0.6658\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5880 - val_loss: 0.6377 - val_accuracy: 0.6569\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6702 - accuracy: 0.5907 - val_loss: 0.6423 - val_accuracy: 0.6671\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6708 - accuracy: 0.5934 - val_loss: 0.6440 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6704 - accuracy: 0.5864 - val_loss: 0.6422 - val_accuracy: 0.6709\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5960 - val_loss: 0.6484 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6705 - accuracy: 0.5956 - val_loss: 0.6423 - val_accuracy: 0.6722\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5841 - val_loss: 0.6471 - val_accuracy: 0.6760\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.5985 - val_loss: 0.6454 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6679 - accuracy: 0.5892 - val_loss: 0.6425 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6677 - accuracy: 0.5968 - val_loss: 0.6435 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5968 - val_loss: 0.6382 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5918 - val_loss: 0.6369 - val_accuracy: 0.6722\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6675 - accuracy: 0.5957 - val_loss: 0.6418 - val_accuracy: 0.6773\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6629 - accuracy: 0.6032 - val_loss: 0.6403 - val_accuracy: 0.6735\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6658 - accuracy: 0.6065 - val_loss: 0.6438 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.5978 - val_loss: 0.6373 - val_accuracy: 0.6709\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6069 - val_loss: 0.6395 - val_accuracy: 0.6747\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6017 - val_loss: 0.6388 - val_accuracy: 0.6786\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6622 - accuracy: 0.6035 - val_loss: 0.6396 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6649 - accuracy: 0.5990 - val_loss: 0.6393 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6054 - val_loss: 0.6412 - val_accuracy: 0.6722\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6047 - val_loss: 0.6354 - val_accuracy: 0.6837\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6086 - val_loss: 0.6352 - val_accuracy: 0.6786\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6125 - val_loss: 0.6358 - val_accuracy: 0.6747\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6088 - val_loss: 0.6385 - val_accuracy: 0.6824\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6623 - accuracy: 0.6107 - val_loss: 0.6411 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6124 - val_loss: 0.6328 - val_accuracy: 0.6747\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6610 - accuracy: 0.6150 - val_loss: 0.6386 - val_accuracy: 0.6811\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6130 - val_loss: 0.6341 - val_accuracy: 0.6824\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.6115 - val_loss: 0.6332 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6061 - val_loss: 0.6363 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6147 - val_loss: 0.6383 - val_accuracy: 0.6722\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6589 - accuracy: 0.6158 - val_loss: 0.6337 - val_accuracy: 0.6786\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6089 - val_loss: 0.6326 - val_accuracy: 0.6735\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6613 - accuracy: 0.6027 - val_loss: 0.6339 - val_accuracy: 0.6773\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6173 - val_loss: 0.6336 - val_accuracy: 0.6837\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6130 - val_loss: 0.6334 - val_accuracy: 0.6837\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6564 - accuracy: 0.6167 - val_loss: 0.6298 - val_accuracy: 0.6747\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6601 - accuracy: 0.6099 - val_loss: 0.6380 - val_accuracy: 0.6862\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6110 - val_loss: 0.6378 - val_accuracy: 0.6875\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6144 - val_loss: 0.6360 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6572 - accuracy: 0.6118 - val_loss: 0.6356 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6113 - val_loss: 0.6304 - val_accuracy: 0.6811\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6294 - val_loss: 0.6328 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6600 - accuracy: 0.6153 - val_loss: 0.6328 - val_accuracy: 0.6875\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6137 - val_loss: 0.6286 - val_accuracy: 0.6798\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6230 - val_loss: 0.6302 - val_accuracy: 0.6849\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6218 - val_loss: 0.6264 - val_accuracy: 0.6862\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6550 - accuracy: 0.6245 - val_loss: 0.6272 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6208 - val_loss: 0.6334 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6184 - val_loss: 0.6408 - val_accuracy: 0.6531\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6558 - accuracy: 0.6243 - val_loss: 0.6341 - val_accuracy: 0.6824\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6212 - val_loss: 0.6350 - val_accuracy: 0.6875\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6201 - val_loss: 0.6293 - val_accuracy: 0.6862\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6232 - val_loss: 0.6301 - val_accuracy: 0.6798\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6188 - val_loss: 0.6270 - val_accuracy: 0.6798\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6196 - val_loss: 0.6305 - val_accuracy: 0.6888\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6226 - val_loss: 0.6258 - val_accuracy: 0.6837\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6546 - accuracy: 0.6230 - val_loss: 0.6348 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6221 - val_loss: 0.6334 - val_accuracy: 0.6875\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6553 - accuracy: 0.6204 - val_loss: 0.6323 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6199 - val_loss: 0.6261 - val_accuracy: 0.6849\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6246 - val_loss: 0.6295 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6503 - accuracy: 0.6264 - val_loss: 0.6298 - val_accuracy: 0.6837\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6510 - accuracy: 0.6251 - val_loss: 0.6272 - val_accuracy: 0.6837\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6504 - accuracy: 0.6262 - val_loss: 0.6230 - val_accuracy: 0.6888\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6255 - val_loss: 0.6251 - val_accuracy: 0.6875\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6513 - accuracy: 0.6246 - val_loss: 0.6317 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6265 - val_loss: 0.6308 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6307 - val_loss: 0.6264 - val_accuracy: 0.6875\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6513 - accuracy: 0.6247 - val_loss: 0.6331 - val_accuracy: 0.6888\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6312 - val_loss: 0.6252 - val_accuracy: 0.6811\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6299 - val_loss: 0.6251 - val_accuracy: 0.6824\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6502 - accuracy: 0.6284 - val_loss: 0.6229 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6487 - accuracy: 0.6363 - val_loss: 0.6251 - val_accuracy: 0.6862\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6487 - accuracy: 0.6295 - val_loss: 0.6268 - val_accuracy: 0.6849\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6272 - val_loss: 0.6256 - val_accuracy: 0.6862\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6492 - accuracy: 0.6335 - val_loss: 0.6284 - val_accuracy: 0.6875\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6479 - accuracy: 0.6311 - val_loss: 0.6270 - val_accuracy: 0.6901\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6499 - accuracy: 0.6315 - val_loss: 0.6273 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6328 - val_loss: 0.6308 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6294 - val_loss: 0.6290 - val_accuracy: 0.6913\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6473 - accuracy: 0.6314 - val_loss: 0.6273 - val_accuracy: 0.6888\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6294 - val_loss: 0.6270 - val_accuracy: 0.6939\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6316 - val_loss: 0.6205 - val_accuracy: 0.6824\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6277 - val_loss: 0.6276 - val_accuracy: 0.6837\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6457 - accuracy: 0.6296 - val_loss: 0.6230 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6349 - val_loss: 0.6302 - val_accuracy: 0.6849\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6393 - val_loss: 0.6264 - val_accuracy: 0.6849\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6339 - val_loss: 0.6269 - val_accuracy: 0.6849\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6305 - val_loss: 0.6201 - val_accuracy: 0.6773\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6282 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6370 - val_loss: 0.6232 - val_accuracy: 0.6913\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6323 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6370 - val_loss: 0.6264 - val_accuracy: 0.6888\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6448 - accuracy: 0.6340 - val_loss: 0.6266 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6349 - val_loss: 0.6184 - val_accuracy: 0.6811\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6389 - val_loss: 0.6233 - val_accuracy: 0.6811\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6420 - accuracy: 0.6382 - val_loss: 0.6244 - val_accuracy: 0.6862\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6330 - val_loss: 0.6304 - val_accuracy: 0.6849\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6331 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6436 - accuracy: 0.6387 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6385 - val_loss: 0.6251 - val_accuracy: 0.6862\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6378 - val_loss: 0.6153 - val_accuracy: 0.6811\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6393 - val_loss: 0.6251 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6335 - val_loss: 0.6340 - val_accuracy: 0.6773\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6379 - val_loss: 0.6206 - val_accuracy: 0.6837\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6354 - val_loss: 0.6249 - val_accuracy: 0.6837\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6393 - val_loss: 0.6274 - val_accuracy: 0.6901\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6414 - val_loss: 0.6165 - val_accuracy: 0.6849\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6270 - val_loss: 0.6193 - val_accuracy: 0.6875\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6408 - val_loss: 0.6205 - val_accuracy: 0.6862\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6339 - val_loss: 0.6220 - val_accuracy: 0.6849\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6446 - val_loss: 0.6234 - val_accuracy: 0.6837\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6418 - val_loss: 0.6191 - val_accuracy: 0.6849\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6407 - val_loss: 0.6271 - val_accuracy: 0.6837\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6375 - val_loss: 0.6235 - val_accuracy: 0.6862\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6392 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6389 - val_loss: 0.6193 - val_accuracy: 0.6901\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6458 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6380 - val_loss: 0.6211 - val_accuracy: 0.6964\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6512 - val_loss: 0.6174 - val_accuracy: 0.6952\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6375 - val_loss: 0.6190 - val_accuracy: 0.6913\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6356 - accuracy: 0.6485 - val_loss: 0.6181 - val_accuracy: 0.6888\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6387 - accuracy: 0.6468 - val_loss: 0.6142 - val_accuracy: 0.6913\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6462 - val_loss: 0.6153 - val_accuracy: 0.6875\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6453 - val_loss: 0.6201 - val_accuracy: 0.6875\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.6477 - val_loss: 0.6230 - val_accuracy: 0.6875\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6391 - accuracy: 0.6473 - val_loss: 0.6158 - val_accuracy: 0.6952\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6393 - accuracy: 0.6463 - val_loss: 0.6219 - val_accuracy: 0.6926\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6453 - val_loss: 0.6202 - val_accuracy: 0.6926\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6476 - val_loss: 0.6176 - val_accuracy: 0.6977\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6449 - val_loss: 0.6192 - val_accuracy: 0.6939\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6472 - val_loss: 0.6215 - val_accuracy: 0.6939\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6438 - accuracy: 0.6412 - val_loss: 0.6178 - val_accuracy: 0.6964\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6462 - val_loss: 0.6150 - val_accuracy: 0.7003\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6444 - val_loss: 0.6299 - val_accuracy: 0.6773\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6531 - val_loss: 0.6195 - val_accuracy: 0.6952\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6343 - accuracy: 0.6516 - val_loss: 0.6198 - val_accuracy: 0.6952\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6363 - accuracy: 0.6466 - val_loss: 0.6174 - val_accuracy: 0.7054\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6409 - val_loss: 0.6151 - val_accuracy: 0.7028\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6378 - accuracy: 0.6423 - val_loss: 0.6161 - val_accuracy: 0.6977\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6531 - val_loss: 0.6205 - val_accuracy: 0.6926\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6356 - accuracy: 0.6500 - val_loss: 0.6178 - val_accuracy: 0.6913\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6510 - val_loss: 0.6155 - val_accuracy: 0.7041\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6300 - accuracy: 0.6545 - val_loss: 0.6118 - val_accuracy: 0.7066\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6590 - val_loss: 0.6123 - val_accuracy: 0.7028\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6338 - accuracy: 0.6520 - val_loss: 0.6158 - val_accuracy: 0.7003\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6532 - val_loss: 0.6141 - val_accuracy: 0.7041\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6349 - accuracy: 0.6449 - val_loss: 0.6189 - val_accuracy: 0.6977\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6338 - accuracy: 0.6501 - val_loss: 0.6160 - val_accuracy: 0.7028\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6506 - val_loss: 0.6144 - val_accuracy: 0.6977\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6574 - val_loss: 0.6164 - val_accuracy: 0.6990\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6473 - val_loss: 0.6154 - val_accuracy: 0.6939\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6304 - accuracy: 0.6544 - val_loss: 0.6152 - val_accuracy: 0.6926\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6614 - val_loss: 0.6158 - val_accuracy: 0.6964\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6567 - val_loss: 0.6124 - val_accuracy: 0.6926\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6332 - accuracy: 0.6529 - val_loss: 0.6143 - val_accuracy: 0.6952\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6550 - val_loss: 0.6149 - val_accuracy: 0.6977\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6459 - val_loss: 0.6129 - val_accuracy: 0.7028\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6515 - val_loss: 0.6177 - val_accuracy: 0.6977\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6279 - accuracy: 0.6600 - val_loss: 0.6156 - val_accuracy: 0.6913\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6576 - val_loss: 0.6113 - val_accuracy: 0.6990\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6523 - val_loss: 0.6207 - val_accuracy: 0.6849\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6505 - val_loss: 0.6200 - val_accuracy: 0.6849\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6585 - val_loss: 0.6116 - val_accuracy: 0.7015\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6554 - val_loss: 0.6183 - val_accuracy: 0.7003\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6567 - val_loss: 0.6145 - val_accuracy: 0.6977\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6556 - val_loss: 0.6150 - val_accuracy: 0.7015\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6596 - val_loss: 0.6129 - val_accuracy: 0.6977\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6600 - val_loss: 0.6121 - val_accuracy: 0.6939\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6511 - val_loss: 0.6151 - val_accuracy: 0.6964\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6534 - val_loss: 0.6192 - val_accuracy: 0.6964\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6520 - val_loss: 0.6153 - val_accuracy: 0.7003\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6595 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6255 - accuracy: 0.6601 - val_loss: 0.6130 - val_accuracy: 0.6977\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6585 - val_loss: 0.6150 - val_accuracy: 0.6977\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6610 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Calculating for: 650 100 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_252 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8317 - accuracy: 0.5348 - val_loss: 0.7068 - val_accuracy: 0.4694\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7164 - accuracy: 0.5653 - val_loss: 0.6667 - val_accuracy: 0.6276\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6852 - accuracy: 0.5769 - val_loss: 0.6486 - val_accuracy: 0.6633\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6790 - accuracy: 0.5760 - val_loss: 0.6503 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5855 - val_loss: 0.6494 - val_accuracy: 0.6722\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5933 - val_loss: 0.6470 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6056 - val_loss: 0.6449 - val_accuracy: 0.6696\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6022 - val_loss: 0.6407 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6069 - val_loss: 0.6448 - val_accuracy: 0.6556\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6065 - val_loss: 0.6444 - val_accuracy: 0.6607\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6118 - val_loss: 0.6396 - val_accuracy: 0.6722\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6167 - val_loss: 0.6384 - val_accuracy: 0.6696\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6220 - val_loss: 0.6377 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6201 - val_loss: 0.6392 - val_accuracy: 0.6620\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6197 - val_loss: 0.6358 - val_accuracy: 0.6645\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6183 - val_loss: 0.6341 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6197 - val_loss: 0.6390 - val_accuracy: 0.6480\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6276 - val_loss: 0.6346 - val_accuracy: 0.6696\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6250 - val_loss: 0.6358 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6265 - val_loss: 0.6340 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6326 - val_loss: 0.6365 - val_accuracy: 0.6671\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6282 - val_loss: 0.6344 - val_accuracy: 0.6620\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6276 - val_loss: 0.6275 - val_accuracy: 0.6786\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6449 - accuracy: 0.6380 - val_loss: 0.6275 - val_accuracy: 0.6926\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6320 - val_loss: 0.6298 - val_accuracy: 0.6811\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6340 - val_loss: 0.6311 - val_accuracy: 0.6798\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6316 - val_loss: 0.6354 - val_accuracy: 0.6735\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6370 - val_loss: 0.6319 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6309 - val_loss: 0.6293 - val_accuracy: 0.6760\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6383 - val_loss: 0.6290 - val_accuracy: 0.6837\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6379 - val_loss: 0.6253 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6378 - val_loss: 0.6257 - val_accuracy: 0.6811\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6419 - val_loss: 0.6256 - val_accuracy: 0.6786\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6408 - val_loss: 0.6226 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6459 - val_loss: 0.6256 - val_accuracy: 0.6722\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6375 - val_loss: 0.6312 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6408 - val_loss: 0.6291 - val_accuracy: 0.6735\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6417 - val_loss: 0.6227 - val_accuracy: 0.6786\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6449 - val_loss: 0.6272 - val_accuracy: 0.6658\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6522 - val_loss: 0.6299 - val_accuracy: 0.6543\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6503 - val_loss: 0.6295 - val_accuracy: 0.6633\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6510 - val_loss: 0.6198 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6513 - val_loss: 0.6197 - val_accuracy: 0.6773\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6487 - val_loss: 0.6217 - val_accuracy: 0.6773\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6478 - val_loss: 0.6216 - val_accuracy: 0.6747\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6520 - val_loss: 0.6228 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6487 - val_loss: 0.6251 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6502 - val_loss: 0.6210 - val_accuracy: 0.6824\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6530 - val_loss: 0.6220 - val_accuracy: 0.6849\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6598 - val_loss: 0.6220 - val_accuracy: 0.6888\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6523 - val_loss: 0.6185 - val_accuracy: 0.6926\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6521 - val_loss: 0.6214 - val_accuracy: 0.6747\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6567 - val_loss: 0.6236 - val_accuracy: 0.6722\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6571 - val_loss: 0.6239 - val_accuracy: 0.6696\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6210 - accuracy: 0.6590 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6593 - val_loss: 0.6283 - val_accuracy: 0.6658\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6603 - val_loss: 0.6248 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6628 - val_loss: 0.6233 - val_accuracy: 0.6696\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6642 - val_loss: 0.6310 - val_accuracy: 0.6543\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6580 - val_loss: 0.6226 - val_accuracy: 0.6684\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6614 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6574 - val_loss: 0.6214 - val_accuracy: 0.6735\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6633 - val_loss: 0.6210 - val_accuracy: 0.6760\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6642 - val_loss: 0.6219 - val_accuracy: 0.6722\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6125 - accuracy: 0.6677 - val_loss: 0.6212 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6120 - accuracy: 0.6653 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6667 - val_loss: 0.6254 - val_accuracy: 0.6658\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6121 - accuracy: 0.6707 - val_loss: 0.6257 - val_accuracy: 0.6658\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6134 - accuracy: 0.6698 - val_loss: 0.6280 - val_accuracy: 0.6620\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6726 - val_loss: 0.6249 - val_accuracy: 0.6684\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6678 - val_loss: 0.6267 - val_accuracy: 0.6633\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6643 - val_loss: 0.6343 - val_accuracy: 0.6518\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6111 - accuracy: 0.6732 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.6698 - val_loss: 0.6309 - val_accuracy: 0.6569\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6104 - accuracy: 0.6745 - val_loss: 0.6246 - val_accuracy: 0.6671\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6076 - accuracy: 0.6780 - val_loss: 0.6280 - val_accuracy: 0.6556\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6747 - val_loss: 0.6270 - val_accuracy: 0.6633\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6691 - val_loss: 0.6273 - val_accuracy: 0.6645\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6727 - val_loss: 0.6293 - val_accuracy: 0.6594\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6076 - accuracy: 0.6736 - val_loss: 0.6305 - val_accuracy: 0.6645\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6087 - accuracy: 0.6702 - val_loss: 0.6323 - val_accuracy: 0.6645\n",
      "Calculating for: 650 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_256 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8219 - accuracy: 0.5234 - val_loss: 0.6943 - val_accuracy: 0.5013\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7247 - accuracy: 0.5485 - val_loss: 0.6612 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5578 - val_loss: 0.6571 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6818 - accuracy: 0.5747 - val_loss: 0.6543 - val_accuracy: 0.6518\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6782 - accuracy: 0.5678 - val_loss: 0.6519 - val_accuracy: 0.6620\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6763 - accuracy: 0.5804 - val_loss: 0.6497 - val_accuracy: 0.6633\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.5863 - val_loss: 0.6521 - val_accuracy: 0.6696\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5849 - val_loss: 0.6532 - val_accuracy: 0.6582\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6712 - accuracy: 0.5874 - val_loss: 0.6500 - val_accuracy: 0.6594\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5960 - val_loss: 0.6468 - val_accuracy: 0.6633\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6015 - val_loss: 0.6432 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6007 - val_loss: 0.6502 - val_accuracy: 0.6594\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6016 - val_loss: 0.6446 - val_accuracy: 0.6671\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6066 - val_loss: 0.6449 - val_accuracy: 0.6607\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6055 - val_loss: 0.6473 - val_accuracy: 0.6645\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5987 - val_loss: 0.6390 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6626 - accuracy: 0.6034 - val_loss: 0.6431 - val_accuracy: 0.6582\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6068 - val_loss: 0.6418 - val_accuracy: 0.6594\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6112 - val_loss: 0.6416 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6153 - val_loss: 0.6387 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6187 - val_loss: 0.6389 - val_accuracy: 0.6518\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6107 - val_loss: 0.6334 - val_accuracy: 0.6671\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6142 - val_loss: 0.6402 - val_accuracy: 0.6556\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6171 - val_loss: 0.6412 - val_accuracy: 0.6531\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6148 - val_loss: 0.6409 - val_accuracy: 0.6505\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6226 - val_loss: 0.6365 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6252 - val_loss: 0.6326 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6206 - val_loss: 0.6351 - val_accuracy: 0.6556\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6223 - val_loss: 0.6378 - val_accuracy: 0.6594\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6197 - val_loss: 0.6396 - val_accuracy: 0.6569\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6236 - val_loss: 0.6343 - val_accuracy: 0.6620\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6223 - val_loss: 0.6366 - val_accuracy: 0.6492\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6197 - val_loss: 0.6330 - val_accuracy: 0.6518\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6512 - accuracy: 0.6232 - val_loss: 0.6357 - val_accuracy: 0.6620\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6233 - val_loss: 0.6334 - val_accuracy: 0.6607\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6256 - val_loss: 0.6323 - val_accuracy: 0.6633\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6492 - accuracy: 0.6274 - val_loss: 0.6311 - val_accuracy: 0.6594\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6309 - val_loss: 0.6361 - val_accuracy: 0.6569\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6301 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6291 - val_loss: 0.6323 - val_accuracy: 0.6607\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6294 - val_loss: 0.6330 - val_accuracy: 0.6454\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6361 - val_loss: 0.6304 - val_accuracy: 0.6645\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6267 - val_loss: 0.6325 - val_accuracy: 0.6556\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6304 - val_loss: 0.6336 - val_accuracy: 0.6594\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6330 - val_loss: 0.6343 - val_accuracy: 0.6582\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6363 - val_loss: 0.6237 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6360 - val_loss: 0.6249 - val_accuracy: 0.6696\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6349 - val_loss: 0.6234 - val_accuracy: 0.6747\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6380 - val_loss: 0.6287 - val_accuracy: 0.6671\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6382 - val_loss: 0.6266 - val_accuracy: 0.6645\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6349 - val_loss: 0.6321 - val_accuracy: 0.6594\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6414 - accuracy: 0.6373 - val_loss: 0.6270 - val_accuracy: 0.6569\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6387 - val_loss: 0.6273 - val_accuracy: 0.6505\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6375 - val_loss: 0.6290 - val_accuracy: 0.6543\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6407 - val_loss: 0.6250 - val_accuracy: 0.6531\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6408 - val_loss: 0.6250 - val_accuracy: 0.6658\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6355 - val_loss: 0.6226 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6393 - val_loss: 0.6332 - val_accuracy: 0.6594\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6449 - val_loss: 0.6286 - val_accuracy: 0.6569\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6453 - val_loss: 0.6306 - val_accuracy: 0.6569\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6485 - val_loss: 0.6231 - val_accuracy: 0.6684\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6420 - val_loss: 0.6200 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6458 - val_loss: 0.6288 - val_accuracy: 0.6594\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6497 - val_loss: 0.6287 - val_accuracy: 0.6543\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6467 - val_loss: 0.6257 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6427 - val_loss: 0.6286 - val_accuracy: 0.6607\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6395 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6447 - val_loss: 0.6269 - val_accuracy: 0.6556\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6335 - accuracy: 0.6508 - val_loss: 0.6186 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6507 - val_loss: 0.6249 - val_accuracy: 0.6709\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6497 - val_loss: 0.6329 - val_accuracy: 0.6582\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6534 - val_loss: 0.6233 - val_accuracy: 0.6760\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6501 - val_loss: 0.6236 - val_accuracy: 0.6684\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6559 - val_loss: 0.6201 - val_accuracy: 0.6722\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6488 - val_loss: 0.6221 - val_accuracy: 0.6747\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6523 - val_loss: 0.6260 - val_accuracy: 0.6671\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6496 - val_loss: 0.6206 - val_accuracy: 0.6696\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6515 - val_loss: 0.6288 - val_accuracy: 0.6543\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6555 - val_loss: 0.6243 - val_accuracy: 0.6658\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6498 - val_loss: 0.6259 - val_accuracy: 0.6620\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6526 - val_loss: 0.6257 - val_accuracy: 0.6684\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6516 - val_loss: 0.6242 - val_accuracy: 0.6722\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6522 - val_loss: 0.6246 - val_accuracy: 0.6735\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6610 - val_loss: 0.6221 - val_accuracy: 0.6760\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6560 - val_loss: 0.6218 - val_accuracy: 0.6735\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6620 - val_loss: 0.6228 - val_accuracy: 0.6709\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6561 - val_loss: 0.6254 - val_accuracy: 0.6671\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6544 - val_loss: 0.6266 - val_accuracy: 0.6645\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6224 - accuracy: 0.6596 - val_loss: 0.6309 - val_accuracy: 0.6569\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6603 - val_loss: 0.6220 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6593 - val_loss: 0.6219 - val_accuracy: 0.6722\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6530 - val_loss: 0.6274 - val_accuracy: 0.6722\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6531 - val_loss: 0.6266 - val_accuracy: 0.6722\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6243 - accuracy: 0.6579 - val_loss: 0.6248 - val_accuracy: 0.6684\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6614 - val_loss: 0.6247 - val_accuracy: 0.6696\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6566 - val_loss: 0.6213 - val_accuracy: 0.6811\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6217 - accuracy: 0.6576 - val_loss: 0.6228 - val_accuracy: 0.6773\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6613 - val_loss: 0.6230 - val_accuracy: 0.6658\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6701 - val_loss: 0.6244 - val_accuracy: 0.6658\n",
      "Calculating for: 650 100 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_260 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8143 - accuracy: 0.5225 - val_loss: 0.6581 - val_accuracy: 0.6339\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7274 - accuracy: 0.5261 - val_loss: 0.6553 - val_accuracy: 0.6390\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7005 - accuracy: 0.5226 - val_loss: 0.6565 - val_accuracy: 0.6352\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.5369 - val_loss: 0.6601 - val_accuracy: 0.6429\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6888 - accuracy: 0.5426 - val_loss: 0.6616 - val_accuracy: 0.6467\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6851 - accuracy: 0.5482 - val_loss: 0.6574 - val_accuracy: 0.6480\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.5561 - val_loss: 0.6531 - val_accuracy: 0.6454\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.5662 - val_loss: 0.6579 - val_accuracy: 0.6543\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5688 - val_loss: 0.6504 - val_accuracy: 0.6441\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.5653 - val_loss: 0.6542 - val_accuracy: 0.6505\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6807 - accuracy: 0.5587 - val_loss: 0.6509 - val_accuracy: 0.6441\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5643 - val_loss: 0.6508 - val_accuracy: 0.6556\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6790 - accuracy: 0.5703 - val_loss: 0.6453 - val_accuracy: 0.6492\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5787 - val_loss: 0.6455 - val_accuracy: 0.6633\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5826 - val_loss: 0.6439 - val_accuracy: 0.6645\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6764 - accuracy: 0.5764 - val_loss: 0.6419 - val_accuracy: 0.6607\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6738 - accuracy: 0.5804 - val_loss: 0.6431 - val_accuracy: 0.6607\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6723 - accuracy: 0.5809 - val_loss: 0.6398 - val_accuracy: 0.6658\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5833 - val_loss: 0.6363 - val_accuracy: 0.6633\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5947 - val_loss: 0.6373 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5845 - val_loss: 0.6389 - val_accuracy: 0.6684\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5977 - val_loss: 0.6356 - val_accuracy: 0.6696\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6728 - accuracy: 0.5816 - val_loss: 0.6374 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5878 - val_loss: 0.6420 - val_accuracy: 0.6671\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6717 - accuracy: 0.5923 - val_loss: 0.6392 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6030 - val_loss: 0.6330 - val_accuracy: 0.6747\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.6016 - val_loss: 0.6347 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.5998 - val_loss: 0.6343 - val_accuracy: 0.6696\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5957 - val_loss: 0.6341 - val_accuracy: 0.6658\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6702 - accuracy: 0.5926 - val_loss: 0.6401 - val_accuracy: 0.6684\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5997 - val_loss: 0.6387 - val_accuracy: 0.6684\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6010 - val_loss: 0.6345 - val_accuracy: 0.6735\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.6001 - val_loss: 0.6368 - val_accuracy: 0.6671\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6639 - accuracy: 0.5993 - val_loss: 0.6305 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.6014 - val_loss: 0.6356 - val_accuracy: 0.6709\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.5985 - val_loss: 0.6362 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6009 - val_loss: 0.6315 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6063 - val_loss: 0.6334 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6068 - val_loss: 0.6297 - val_accuracy: 0.6735\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6030 - val_loss: 0.6306 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6085 - val_loss: 0.6274 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6064 - val_loss: 0.6258 - val_accuracy: 0.6696\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.6075 - val_loss: 0.6309 - val_accuracy: 0.6709\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6076 - val_loss: 0.6280 - val_accuracy: 0.6722\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6004 - val_loss: 0.6275 - val_accuracy: 0.6735\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6117 - val_loss: 0.6301 - val_accuracy: 0.6747\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6196 - val_loss: 0.6267 - val_accuracy: 0.6798\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6124 - val_loss: 0.6306 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6091 - val_loss: 0.6330 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6148 - val_loss: 0.6251 - val_accuracy: 0.6798\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6124 - val_loss: 0.6296 - val_accuracy: 0.6824\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6567 - accuracy: 0.6172 - val_loss: 0.6285 - val_accuracy: 0.6837\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6235 - val_loss: 0.6293 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6187 - val_loss: 0.6233 - val_accuracy: 0.6849\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6251 - val_loss: 0.6237 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6110 - val_loss: 0.6272 - val_accuracy: 0.6824\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6182 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6137 - val_loss: 0.6266 - val_accuracy: 0.6773\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6242 - val_loss: 0.6269 - val_accuracy: 0.6849\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6177 - val_loss: 0.6244 - val_accuracy: 0.6888\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6125 - val_loss: 0.6278 - val_accuracy: 0.6811\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6172 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6248 - val_loss: 0.6197 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6186 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6306 - val_loss: 0.6252 - val_accuracy: 0.6913\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6264 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6164 - val_loss: 0.6219 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6199 - val_loss: 0.6218 - val_accuracy: 0.6939\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6225 - val_accuracy: 0.6849\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6216 - val_loss: 0.6224 - val_accuracy: 0.6837\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6267 - val_loss: 0.6252 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6242 - val_loss: 0.6211 - val_accuracy: 0.6849\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6252 - val_loss: 0.6258 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6208 - val_loss: 0.6240 - val_accuracy: 0.6786\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6240 - val_loss: 0.6250 - val_accuracy: 0.6798\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6230 - val_loss: 0.6235 - val_accuracy: 0.6875\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6547 - accuracy: 0.6248 - val_loss: 0.6208 - val_accuracy: 0.6862\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6295 - val_loss: 0.6233 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6238 - val_loss: 0.6210 - val_accuracy: 0.6837\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6243 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6305 - val_loss: 0.6182 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6134 - val_loss: 0.6204 - val_accuracy: 0.6913\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6301 - val_loss: 0.6226 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6284 - val_loss: 0.6197 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6311 - val_loss: 0.6229 - val_accuracy: 0.6888\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6296 - val_loss: 0.6208 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6310 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6474 - accuracy: 0.6331 - val_loss: 0.6219 - val_accuracy: 0.6862\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6294 - val_loss: 0.6174 - val_accuracy: 0.6888\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6331 - val_loss: 0.6160 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6226 - val_loss: 0.6169 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6355 - val_loss: 0.6199 - val_accuracy: 0.6811\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6365 - val_loss: 0.6184 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6299 - val_loss: 0.6189 - val_accuracy: 0.6849\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6315 - val_loss: 0.6217 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6315 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6284 - val_loss: 0.6197 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6402 - val_loss: 0.6183 - val_accuracy: 0.6913\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6323 - val_loss: 0.6150 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6329 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6400 - val_loss: 0.6205 - val_accuracy: 0.6849\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6402 - val_loss: 0.6209 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6353 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6413 - val_loss: 0.6185 - val_accuracy: 0.6875\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6355 - val_loss: 0.6213 - val_accuracy: 0.6786\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6438 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6312 - val_loss: 0.6211 - val_accuracy: 0.6786\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6364 - val_loss: 0.6226 - val_accuracy: 0.6837\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6409 - val_loss: 0.6175 - val_accuracy: 0.6875\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6330 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6424 - val_loss: 0.6165 - val_accuracy: 0.6837\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6447 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6385 - val_loss: 0.6188 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6426 - val_loss: 0.6190 - val_accuracy: 0.6849\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6452 - val_loss: 0.6141 - val_accuracy: 0.6849\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6372 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6416 - accuracy: 0.6353 - val_loss: 0.6164 - val_accuracy: 0.6837\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6495 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6382 - val_loss: 0.6139 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6417 - val_loss: 0.6149 - val_accuracy: 0.6824\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6375 - val_loss: 0.6147 - val_accuracy: 0.6837\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6475 - val_loss: 0.6188 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6414 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6388 - accuracy: 0.6429 - val_loss: 0.6211 - val_accuracy: 0.6798\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6436 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6429 - val_loss: 0.6142 - val_accuracy: 0.6811\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6436 - val_loss: 0.6188 - val_accuracy: 0.6837\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6438 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6363 - accuracy: 0.6482 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6467 - val_loss: 0.6107 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6390 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6444 - val_loss: 0.6157 - val_accuracy: 0.6849\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6486 - val_loss: 0.6196 - val_accuracy: 0.6811\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6518 - val_loss: 0.6155 - val_accuracy: 0.6824\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6371 - accuracy: 0.6477 - val_loss: 0.6158 - val_accuracy: 0.6875\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6428 - val_loss: 0.6182 - val_accuracy: 0.6888\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6540 - val_loss: 0.6119 - val_accuracy: 0.6939\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6472 - val_loss: 0.6129 - val_accuracy: 0.6939\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6493 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6449 - val_loss: 0.6135 - val_accuracy: 0.6875\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6495 - val_loss: 0.6167 - val_accuracy: 0.6913\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6459 - val_loss: 0.6166 - val_accuracy: 0.6862\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6517 - val_loss: 0.6125 - val_accuracy: 0.6875\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6471 - val_loss: 0.6122 - val_accuracy: 0.6888\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6336 - accuracy: 0.6461 - val_loss: 0.6120 - val_accuracy: 0.6875\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6516 - val_loss: 0.6139 - val_accuracy: 0.6862\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6588 - val_loss: 0.6120 - val_accuracy: 0.6862\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6475 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6468 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6486 - val_loss: 0.6132 - val_accuracy: 0.6862\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6550 - val_loss: 0.6160 - val_accuracy: 0.6849\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6525 - val_loss: 0.6140 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6526 - val_loss: 0.6132 - val_accuracy: 0.6888\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6488 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6495 - val_loss: 0.6157 - val_accuracy: 0.6888\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6472 - val_loss: 0.6136 - val_accuracy: 0.6888\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6579 - val_loss: 0.6108 - val_accuracy: 0.6849\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6510 - val_loss: 0.6128 - val_accuracy: 0.6849\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6536 - val_loss: 0.6135 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6541 - val_loss: 0.6112 - val_accuracy: 0.6837\n",
      "Calculating for: 650 150 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_264 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8317 - accuracy: 0.5373 - val_loss: 0.6446 - val_accuracy: 0.6531\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7177 - accuracy: 0.5574 - val_loss: 0.6401 - val_accuracy: 0.6671\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6846 - accuracy: 0.5808 - val_loss: 0.6405 - val_accuracy: 0.6556\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5831 - val_loss: 0.6409 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5911 - val_loss: 0.6389 - val_accuracy: 0.6658\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5933 - val_loss: 0.6390 - val_accuracy: 0.6773\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.5970 - val_loss: 0.6360 - val_accuracy: 0.6709\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6049 - val_loss: 0.6351 - val_accuracy: 0.6773\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6606 - accuracy: 0.6074 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6148 - val_loss: 0.6310 - val_accuracy: 0.6849\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6066 - val_loss: 0.6358 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6169 - val_loss: 0.6314 - val_accuracy: 0.6798\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6147 - val_loss: 0.6295 - val_accuracy: 0.6798\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6153 - val_loss: 0.6289 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6207 - val_loss: 0.6324 - val_accuracy: 0.6684\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6218 - val_loss: 0.6310 - val_accuracy: 0.6671\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6504 - accuracy: 0.6220 - val_loss: 0.6314 - val_accuracy: 0.6722\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6208 - val_loss: 0.6289 - val_accuracy: 0.6645\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6238 - val_loss: 0.6272 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6258 - val_loss: 0.6280 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6454 - accuracy: 0.6329 - val_loss: 0.6237 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6270 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6250 - val_loss: 0.6275 - val_accuracy: 0.6875\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6260 - val_loss: 0.6301 - val_accuracy: 0.6722\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6272 - val_loss: 0.6281 - val_accuracy: 0.6709\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6373 - val_loss: 0.6284 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6372 - val_loss: 0.6253 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6356 - val_loss: 0.6216 - val_accuracy: 0.6798\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6315 - val_loss: 0.6265 - val_accuracy: 0.6684\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6331 - val_loss: 0.6237 - val_accuracy: 0.6594\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6400 - val_loss: 0.6221 - val_accuracy: 0.6696\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6355 - val_loss: 0.6259 - val_accuracy: 0.6696\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6356 - val_loss: 0.6191 - val_accuracy: 0.6735\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6374 - val_loss: 0.6291 - val_accuracy: 0.6658\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6379 - val_loss: 0.6272 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6478 - val_loss: 0.6209 - val_accuracy: 0.6684\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6390 - val_loss: 0.6244 - val_accuracy: 0.6645\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6478 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6409 - val_loss: 0.6221 - val_accuracy: 0.6671\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6493 - val_loss: 0.6201 - val_accuracy: 0.6735\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6486 - val_loss: 0.6191 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6478 - val_loss: 0.6152 - val_accuracy: 0.6811\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6443 - val_loss: 0.6179 - val_accuracy: 0.6786\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6418 - val_loss: 0.6252 - val_accuracy: 0.6722\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6426 - val_loss: 0.6198 - val_accuracy: 0.6747\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6546 - val_loss: 0.6162 - val_accuracy: 0.6888\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6478 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6591 - val_loss: 0.6182 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6532 - val_loss: 0.6193 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6539 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6614 - val_loss: 0.6186 - val_accuracy: 0.6849\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6555 - val_loss: 0.6182 - val_accuracy: 0.6862\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6556 - val_loss: 0.6208 - val_accuracy: 0.6849\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6521 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6569 - val_loss: 0.6219 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6603 - val_loss: 0.6154 - val_accuracy: 0.6952\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6653 - val_loss: 0.6151 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6606 - val_loss: 0.6173 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6561 - val_loss: 0.6160 - val_accuracy: 0.6862\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6611 - val_loss: 0.6129 - val_accuracy: 0.6990\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6572 - val_loss: 0.6187 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6616 - val_loss: 0.6158 - val_accuracy: 0.6888\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6611 - val_loss: 0.6189 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6598 - val_loss: 0.6204 - val_accuracy: 0.6773\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6166 - accuracy: 0.6648 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6652 - val_loss: 0.6222 - val_accuracy: 0.6735\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6126 - accuracy: 0.6655 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6642 - val_loss: 0.6239 - val_accuracy: 0.6709\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6166 - accuracy: 0.6640 - val_loss: 0.6212 - val_accuracy: 0.6786\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6112 - accuracy: 0.6673 - val_loss: 0.6181 - val_accuracy: 0.6786\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6642 - val_loss: 0.6227 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6649 - val_loss: 0.6204 - val_accuracy: 0.6760\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6075 - accuracy: 0.6733 - val_loss: 0.6199 - val_accuracy: 0.6773\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6724 - val_loss: 0.6223 - val_accuracy: 0.6747\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6084 - accuracy: 0.6686 - val_loss: 0.6211 - val_accuracy: 0.6760\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5995 - accuracy: 0.6834 - val_loss: 0.6227 - val_accuracy: 0.6760\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6062 - accuracy: 0.6763 - val_loss: 0.6210 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6745 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6771 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6712 - val_loss: 0.6209 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6060 - accuracy: 0.6772 - val_loss: 0.6209 - val_accuracy: 0.6709\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6000 - accuracy: 0.6811 - val_loss: 0.6230 - val_accuracy: 0.6709\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6035 - accuracy: 0.6726 - val_loss: 0.6237 - val_accuracy: 0.6696\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6811 - val_loss: 0.6205 - val_accuracy: 0.6786\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6810 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5997 - accuracy: 0.6829 - val_loss: 0.6215 - val_accuracy: 0.6786\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6049 - accuracy: 0.6760 - val_loss: 0.6235 - val_accuracy: 0.6722\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6020 - accuracy: 0.6806 - val_loss: 0.6239 - val_accuracy: 0.6747\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6001 - accuracy: 0.6767 - val_loss: 0.6230 - val_accuracy: 0.6773\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5978 - accuracy: 0.6805 - val_loss: 0.6285 - val_accuracy: 0.6709\n",
      "Calculating for: 650 150 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_268 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8212 - accuracy: 0.5259 - val_loss: 0.6860 - val_accuracy: 0.5446\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7199 - accuracy: 0.5446 - val_loss: 0.6692 - val_accuracy: 0.6148\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5540 - val_loss: 0.6551 - val_accuracy: 0.6543\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6799 - accuracy: 0.5678 - val_loss: 0.6526 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.5761 - val_loss: 0.6495 - val_accuracy: 0.6722\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5844 - val_loss: 0.6504 - val_accuracy: 0.6658\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5896 - val_loss: 0.6476 - val_accuracy: 0.6684\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5814 - val_loss: 0.6504 - val_accuracy: 0.6722\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6700 - accuracy: 0.5960 - val_loss: 0.6432 - val_accuracy: 0.6760\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5967 - val_loss: 0.6450 - val_accuracy: 0.6633\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5853 - val_loss: 0.6439 - val_accuracy: 0.6684\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5987 - val_loss: 0.6481 - val_accuracy: 0.6696\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6029 - val_loss: 0.6464 - val_accuracy: 0.6620\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.6029 - val_loss: 0.6461 - val_accuracy: 0.6633\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6017 - val_loss: 0.6387 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6103 - val_loss: 0.6387 - val_accuracy: 0.6709\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6045 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6584 - accuracy: 0.6179 - val_loss: 0.6477 - val_accuracy: 0.6454\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6133 - val_loss: 0.6398 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6108 - val_loss: 0.6443 - val_accuracy: 0.6531\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6184 - val_loss: 0.6363 - val_accuracy: 0.6709\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6085 - val_loss: 0.6360 - val_accuracy: 0.6709\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6187 - val_loss: 0.6352 - val_accuracy: 0.6645\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6216 - val_loss: 0.6368 - val_accuracy: 0.6684\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6208 - val_loss: 0.6317 - val_accuracy: 0.6709\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6133 - val_loss: 0.6422 - val_accuracy: 0.6633\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6191 - val_loss: 0.6393 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6240 - val_loss: 0.6355 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6291 - val_loss: 0.6384 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6150 - val_loss: 0.6431 - val_accuracy: 0.6454\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6265 - val_loss: 0.6308 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6189 - val_loss: 0.6357 - val_accuracy: 0.6671\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6235 - val_loss: 0.6410 - val_accuracy: 0.6531\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6237 - val_loss: 0.6340 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6248 - val_loss: 0.6321 - val_accuracy: 0.6747\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6173 - val_loss: 0.6338 - val_accuracy: 0.6722\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6307 - val_loss: 0.6378 - val_accuracy: 0.6492\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6297 - val_loss: 0.6317 - val_accuracy: 0.6722\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6251 - val_loss: 0.6335 - val_accuracy: 0.6773\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6271 - val_loss: 0.6306 - val_accuracy: 0.6709\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6305 - val_loss: 0.6310 - val_accuracy: 0.6709\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6232 - val_loss: 0.6308 - val_accuracy: 0.6773\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6439 - val_loss: 0.6266 - val_accuracy: 0.6798\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6280 - val_loss: 0.6263 - val_accuracy: 0.6760\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6294 - val_loss: 0.6299 - val_accuracy: 0.6696\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6426 - val_loss: 0.6263 - val_accuracy: 0.6747\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6384 - val_loss: 0.6307 - val_accuracy: 0.6709\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6306 - val_loss: 0.6300 - val_accuracy: 0.6633\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6359 - val_loss: 0.6282 - val_accuracy: 0.6735\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6345 - val_loss: 0.6225 - val_accuracy: 0.6747\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6366 - val_loss: 0.6269 - val_accuracy: 0.6747\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6338 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6354 - val_loss: 0.6286 - val_accuracy: 0.6620\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6302 - val_loss: 0.6254 - val_accuracy: 0.6735\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6405 - val_loss: 0.6248 - val_accuracy: 0.6696\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6454 - val_loss: 0.6274 - val_accuracy: 0.6633\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6359 - val_loss: 0.6253 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6360 - val_loss: 0.6239 - val_accuracy: 0.6760\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6394 - val_loss: 0.6231 - val_accuracy: 0.6862\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6372 - val_loss: 0.6305 - val_accuracy: 0.6645\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6400 - val_loss: 0.6262 - val_accuracy: 0.6709\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6398 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6384 - val_loss: 0.6265 - val_accuracy: 0.6722\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6473 - val_loss: 0.6287 - val_accuracy: 0.6607\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6419 - val_loss: 0.6292 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6384 - val_loss: 0.6255 - val_accuracy: 0.6798\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6350 - val_loss: 0.6317 - val_accuracy: 0.6709\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6478 - val_loss: 0.6265 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6443 - val_loss: 0.6193 - val_accuracy: 0.6811\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6415 - val_loss: 0.6235 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6487 - val_loss: 0.6189 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6486 - val_loss: 0.6190 - val_accuracy: 0.6747\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6483 - val_loss: 0.6248 - val_accuracy: 0.6773\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6547 - val_loss: 0.6221 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6446 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6547 - val_loss: 0.6164 - val_accuracy: 0.6735\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6426 - val_loss: 0.6194 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6501 - val_loss: 0.6250 - val_accuracy: 0.6824\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6518 - val_loss: 0.6224 - val_accuracy: 0.6786\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6517 - val_loss: 0.6198 - val_accuracy: 0.6773\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6549 - val_loss: 0.6263 - val_accuracy: 0.6888\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6522 - val_loss: 0.6254 - val_accuracy: 0.6888\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6574 - val_loss: 0.6227 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6512 - val_loss: 0.6272 - val_accuracy: 0.6824\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6515 - val_loss: 0.6230 - val_accuracy: 0.6837\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6575 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6537 - val_loss: 0.6200 - val_accuracy: 0.6888\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6527 - val_loss: 0.6186 - val_accuracy: 0.6862\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6510 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6541 - val_loss: 0.6231 - val_accuracy: 0.6888\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6551 - val_loss: 0.6188 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6540 - val_loss: 0.6196 - val_accuracy: 0.6952\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6596 - val_loss: 0.6204 - val_accuracy: 0.6862\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6570 - val_loss: 0.6202 - val_accuracy: 0.6862\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6570 - val_loss: 0.6224 - val_accuracy: 0.6862\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6616 - val_loss: 0.6201 - val_accuracy: 0.6862\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6660 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6680 - val_loss: 0.6212 - val_accuracy: 0.6901\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6620 - val_loss: 0.6213 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6658 - val_loss: 0.6214 - val_accuracy: 0.6875\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6598 - val_loss: 0.6215 - val_accuracy: 0.6901\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6556 - val_loss: 0.6185 - val_accuracy: 0.6862\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6593 - val_loss: 0.6145 - val_accuracy: 0.6939\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6560 - val_loss: 0.6174 - val_accuracy: 0.6875\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6613 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6645 - val_loss: 0.6231 - val_accuracy: 0.6798\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6624 - val_loss: 0.6186 - val_accuracy: 0.6888\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6588 - val_loss: 0.6231 - val_accuracy: 0.6709\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6689 - val_loss: 0.6209 - val_accuracy: 0.6862\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6615 - val_loss: 0.6225 - val_accuracy: 0.6786\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6604 - val_loss: 0.6212 - val_accuracy: 0.6837\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6583 - val_loss: 0.6235 - val_accuracy: 0.6722\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6694 - val_loss: 0.6202 - val_accuracy: 0.6837\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6113 - accuracy: 0.6702 - val_loss: 0.6257 - val_accuracy: 0.6709\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6140 - accuracy: 0.6714 - val_loss: 0.6230 - val_accuracy: 0.6747\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6182 - accuracy: 0.6561 - val_loss: 0.6184 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6655 - val_loss: 0.6183 - val_accuracy: 0.6888\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6645 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6114 - accuracy: 0.6653 - val_loss: 0.6212 - val_accuracy: 0.6824\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6109 - accuracy: 0.6682 - val_loss: 0.6290 - val_accuracy: 0.6671\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6101 - accuracy: 0.6752 - val_loss: 0.6248 - val_accuracy: 0.6722\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6674 - val_loss: 0.6198 - val_accuracy: 0.6862\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6729 - val_loss: 0.6237 - val_accuracy: 0.6773\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6091 - accuracy: 0.6699 - val_loss: 0.6243 - val_accuracy: 0.6735\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6089 - accuracy: 0.6726 - val_loss: 0.6224 - val_accuracy: 0.6760\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6112 - accuracy: 0.6677 - val_loss: 0.6233 - val_accuracy: 0.6747\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6733 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6091 - accuracy: 0.6734 - val_loss: 0.6268 - val_accuracy: 0.6645\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6708 - val_loss: 0.6233 - val_accuracy: 0.6671\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6680 - val_loss: 0.6228 - val_accuracy: 0.6684\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6075 - accuracy: 0.6718 - val_loss: 0.6226 - val_accuracy: 0.6760\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6092 - accuracy: 0.6763 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6734 - val_loss: 0.6261 - val_accuracy: 0.6620\n",
      "Calculating for: 650 150 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_272 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8116 - accuracy: 0.5207 - val_loss: 0.6662 - val_accuracy: 0.6505\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7272 - accuracy: 0.5261 - val_loss: 0.6593 - val_accuracy: 0.6492\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5466 - val_loss: 0.6674 - val_accuracy: 0.6556\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5453 - val_loss: 0.6637 - val_accuracy: 0.6492\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5540 - val_loss: 0.6638 - val_accuracy: 0.6454\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6823 - accuracy: 0.5566 - val_loss: 0.6614 - val_accuracy: 0.6480\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.5544 - val_loss: 0.6611 - val_accuracy: 0.6582\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6802 - accuracy: 0.5668 - val_loss: 0.6624 - val_accuracy: 0.6658\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6776 - accuracy: 0.5713 - val_loss: 0.6503 - val_accuracy: 0.6454\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5692 - val_loss: 0.6530 - val_accuracy: 0.6531\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5757 - val_loss: 0.6532 - val_accuracy: 0.6684\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6797 - accuracy: 0.5686 - val_loss: 0.6583 - val_accuracy: 0.6556\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5808 - val_loss: 0.6532 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5799 - val_loss: 0.6557 - val_accuracy: 0.6441\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6719 - accuracy: 0.5872 - val_loss: 0.6446 - val_accuracy: 0.6696\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5759 - val_loss: 0.6423 - val_accuracy: 0.6658\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5850 - val_loss: 0.6506 - val_accuracy: 0.6531\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6728 - accuracy: 0.5865 - val_loss: 0.6498 - val_accuracy: 0.6543\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5978 - val_loss: 0.6419 - val_accuracy: 0.6709\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5841 - val_loss: 0.6475 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5907 - val_loss: 0.6471 - val_accuracy: 0.6607\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5923 - val_loss: 0.6473 - val_accuracy: 0.6454\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5850 - val_loss: 0.6434 - val_accuracy: 0.6633\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6019 - val_loss: 0.6409 - val_accuracy: 0.6658\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5922 - val_loss: 0.6422 - val_accuracy: 0.6671\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.6009 - val_loss: 0.6398 - val_accuracy: 0.6645\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5952 - val_loss: 0.6399 - val_accuracy: 0.6671\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5960 - val_loss: 0.6432 - val_accuracy: 0.6658\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5981 - val_loss: 0.6438 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6010 - val_loss: 0.6423 - val_accuracy: 0.6620\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6022 - val_loss: 0.6395 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6026 - val_loss: 0.6395 - val_accuracy: 0.6645\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6104 - val_loss: 0.6403 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6000 - val_loss: 0.6348 - val_accuracy: 0.6696\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6035 - val_loss: 0.6431 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6089 - val_loss: 0.6365 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.5996 - val_loss: 0.6359 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6118 - val_loss: 0.6482 - val_accuracy: 0.6339\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6140 - val_loss: 0.6365 - val_accuracy: 0.6645\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6108 - val_loss: 0.6400 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6074 - val_loss: 0.6338 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6100 - val_loss: 0.6392 - val_accuracy: 0.6607\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6066 - val_loss: 0.6397 - val_accuracy: 0.6594\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6172 - val_loss: 0.6382 - val_accuracy: 0.6633\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6060 - val_loss: 0.6346 - val_accuracy: 0.6696\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6153 - val_loss: 0.6361 - val_accuracy: 0.6658\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6182 - val_loss: 0.6322 - val_accuracy: 0.6645\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6140 - val_loss: 0.6341 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6150 - val_loss: 0.6353 - val_accuracy: 0.6658\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6173 - val_loss: 0.6369 - val_accuracy: 0.6709\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6138 - val_loss: 0.6344 - val_accuracy: 0.6684\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6128 - val_loss: 0.6366 - val_accuracy: 0.6696\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6216 - val_loss: 0.6352 - val_accuracy: 0.6709\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6289 - val_loss: 0.6305 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6159 - val_loss: 0.6347 - val_accuracy: 0.6696\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6230 - val_loss: 0.6360 - val_accuracy: 0.6620\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6184 - val_loss: 0.6386 - val_accuracy: 0.6582\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6237 - val_loss: 0.6390 - val_accuracy: 0.6429\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6282 - val_loss: 0.6320 - val_accuracy: 0.6709\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6220 - val_loss: 0.6255 - val_accuracy: 0.6824\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6194 - val_loss: 0.6318 - val_accuracy: 0.6747\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6228 - val_loss: 0.6281 - val_accuracy: 0.6671\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6221 - val_loss: 0.6270 - val_accuracy: 0.6811\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6211 - val_loss: 0.6287 - val_accuracy: 0.6798\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6218 - val_loss: 0.6306 - val_accuracy: 0.6709\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6271 - val_loss: 0.6352 - val_accuracy: 0.6543\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6235 - val_loss: 0.6330 - val_accuracy: 0.6671\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6339 - val_loss: 0.6275 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6218 - val_loss: 0.6328 - val_accuracy: 0.6671\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6276 - val_loss: 0.6315 - val_accuracy: 0.6569\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6324 - val_loss: 0.6288 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6296 - val_loss: 0.6288 - val_accuracy: 0.6696\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6202 - val_loss: 0.6367 - val_accuracy: 0.6518\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6256 - val_loss: 0.6309 - val_accuracy: 0.6696\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6304 - val_loss: 0.6394 - val_accuracy: 0.6390\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6218 - val_loss: 0.6329 - val_accuracy: 0.6607\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6349 - val_loss: 0.6314 - val_accuracy: 0.6671\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6280 - val_loss: 0.6252 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6331 - val_loss: 0.6282 - val_accuracy: 0.6709\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6241 - val_loss: 0.6264 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6344 - val_loss: 0.6271 - val_accuracy: 0.6824\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6297 - val_loss: 0.6299 - val_accuracy: 0.6696\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6272 - val_loss: 0.6317 - val_accuracy: 0.6594\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6328 - val_loss: 0.6264 - val_accuracy: 0.6786\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6316 - val_loss: 0.6288 - val_accuracy: 0.6773\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6306 - val_loss: 0.6243 - val_accuracy: 0.6849\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6279 - val_loss: 0.6287 - val_accuracy: 0.6786\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6292 - val_loss: 0.6306 - val_accuracy: 0.6696\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6324 - val_loss: 0.6323 - val_accuracy: 0.6684\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6351 - val_loss: 0.6269 - val_accuracy: 0.6773\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6418 - val_loss: 0.6337 - val_accuracy: 0.6429\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6335 - val_loss: 0.6320 - val_accuracy: 0.6645\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6354 - val_loss: 0.6269 - val_accuracy: 0.6735\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6330 - val_loss: 0.6298 - val_accuracy: 0.6735\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6311 - val_loss: 0.6262 - val_accuracy: 0.6824\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6315 - val_loss: 0.6201 - val_accuracy: 0.6913\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6360 - val_loss: 0.6227 - val_accuracy: 0.6875\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6297 - val_loss: 0.6269 - val_accuracy: 0.6620\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6350 - val_loss: 0.6267 - val_accuracy: 0.6747\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6350 - val_loss: 0.6281 - val_accuracy: 0.6684\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6429 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6452 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6380 - val_loss: 0.6214 - val_accuracy: 0.6862\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6379 - val_loss: 0.6235 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6383 - val_loss: 0.6254 - val_accuracy: 0.6824\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6377 - val_loss: 0.6199 - val_accuracy: 0.6926\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6439 - val_loss: 0.6298 - val_accuracy: 0.6518\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6453 - val_loss: 0.6275 - val_accuracy: 0.6684\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6404 - val_loss: 0.6327 - val_accuracy: 0.6403\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6492 - val_loss: 0.6262 - val_accuracy: 0.6633\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6394 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6412 - val_loss: 0.6231 - val_accuracy: 0.6786\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6431 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6426 - val_loss: 0.6238 - val_accuracy: 0.6760\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6400 - val_loss: 0.6276 - val_accuracy: 0.6760\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6472 - val_loss: 0.6265 - val_accuracy: 0.6696\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6404 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6369 - val_loss: 0.6253 - val_accuracy: 0.6837\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6456 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6480 - val_loss: 0.6246 - val_accuracy: 0.6684\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6437 - val_loss: 0.6258 - val_accuracy: 0.6696\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6438 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6420 - accuracy: 0.6392 - val_loss: 0.6262 - val_accuracy: 0.6747\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6409 - val_loss: 0.6229 - val_accuracy: 0.6798\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6459 - val_loss: 0.6265 - val_accuracy: 0.6722\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6545 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6454 - val_loss: 0.6194 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6449 - val_loss: 0.6216 - val_accuracy: 0.6773\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6505 - val_loss: 0.6219 - val_accuracy: 0.6747\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6537 - val_loss: 0.6217 - val_accuracy: 0.6773\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6443 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6555 - val_loss: 0.6237 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6485 - val_loss: 0.6225 - val_accuracy: 0.6849\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6446 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6472 - val_loss: 0.6190 - val_accuracy: 0.6888\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6505 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6507 - val_loss: 0.6157 - val_accuracy: 0.6939\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6503 - val_loss: 0.6190 - val_accuracy: 0.6837\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6518 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6532 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6464 - val_loss: 0.6201 - val_accuracy: 0.6747\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6456 - val_loss: 0.6245 - val_accuracy: 0.6709\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6444 - val_loss: 0.6260 - val_accuracy: 0.6671\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6491 - val_loss: 0.6189 - val_accuracy: 0.6824\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6555 - val_loss: 0.6185 - val_accuracy: 0.6862\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6513 - val_loss: 0.6248 - val_accuracy: 0.6760\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6497 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6461 - val_loss: 0.6284 - val_accuracy: 0.6671\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6472 - val_loss: 0.6222 - val_accuracy: 0.6773\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6555 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6534 - val_loss: 0.6243 - val_accuracy: 0.6722\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6463 - val_loss: 0.6205 - val_accuracy: 0.6875\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6525 - val_loss: 0.6174 - val_accuracy: 0.6952\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6542 - val_loss: 0.6202 - val_accuracy: 0.6862\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6576 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6575 - val_loss: 0.6211 - val_accuracy: 0.6824\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6594 - val_loss: 0.6192 - val_accuracy: 0.6837\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6510 - val_loss: 0.6205 - val_accuracy: 0.6875\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6570 - val_loss: 0.6185 - val_accuracy: 0.6862\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6554 - val_loss: 0.6207 - val_accuracy: 0.6849\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6551 - val_loss: 0.6229 - val_accuracy: 0.6786\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6551 - val_loss: 0.6237 - val_accuracy: 0.6798\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6571 - val_loss: 0.6180 - val_accuracy: 0.6837\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6571 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6619 - val_loss: 0.6224 - val_accuracy: 0.6747\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6576 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6596 - val_loss: 0.6239 - val_accuracy: 0.6696\n",
      "Calculating for: 700 50 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_276 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8237 - accuracy: 0.5201 - val_loss: 0.6837 - val_accuracy: 0.5663\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7357 - accuracy: 0.5324 - val_loss: 0.6550 - val_accuracy: 0.6339\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7012 - accuracy: 0.5588 - val_loss: 0.6547 - val_accuracy: 0.6288\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6807 - accuracy: 0.5809 - val_loss: 0.6514 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5806 - val_loss: 0.6553 - val_accuracy: 0.6327\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5801 - val_loss: 0.6486 - val_accuracy: 0.6531\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5868 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6652 - accuracy: 0.5944 - val_loss: 0.6417 - val_accuracy: 0.6569\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6060 - val_loss: 0.6474 - val_accuracy: 0.6429\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6032 - val_loss: 0.6363 - val_accuracy: 0.6671\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6046 - val_loss: 0.6362 - val_accuracy: 0.6645\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6606 - accuracy: 0.6104 - val_loss: 0.6380 - val_accuracy: 0.6582\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6095 - val_loss: 0.6350 - val_accuracy: 0.6773\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6168 - val_loss: 0.6326 - val_accuracy: 0.6645\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6602 - accuracy: 0.6128 - val_loss: 0.6357 - val_accuracy: 0.6671\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6105 - val_loss: 0.6323 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6155 - val_loss: 0.6320 - val_accuracy: 0.6671\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6186 - val_loss: 0.6327 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6560 - accuracy: 0.6153 - val_loss: 0.6330 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6191 - val_loss: 0.6268 - val_accuracy: 0.6786\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6197 - val_loss: 0.6344 - val_accuracy: 0.6594\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6515 - accuracy: 0.6217 - val_loss: 0.6304 - val_accuracy: 0.6811\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6505 - accuracy: 0.6237 - val_loss: 0.6294 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6285 - val_loss: 0.6322 - val_accuracy: 0.6556\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6449 - accuracy: 0.6255 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6281 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6480 - accuracy: 0.6294 - val_loss: 0.6304 - val_accuracy: 0.6773\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6256 - val_loss: 0.6301 - val_accuracy: 0.6760\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6453 - accuracy: 0.6321 - val_loss: 0.6283 - val_accuracy: 0.6773\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6436 - accuracy: 0.6316 - val_loss: 0.6239 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6445 - accuracy: 0.6335 - val_loss: 0.6237 - val_accuracy: 0.6811\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6410 - val_loss: 0.6244 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6385 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6408 - accuracy: 0.6373 - val_loss: 0.6223 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6368 - val_loss: 0.6234 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6370 - accuracy: 0.6389 - val_loss: 0.6173 - val_accuracy: 0.6798\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6431 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6420 - accuracy: 0.6358 - val_loss: 0.6213 - val_accuracy: 0.6786\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6356 - val_loss: 0.6201 - val_accuracy: 0.6798\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6434 - val_loss: 0.6164 - val_accuracy: 0.6824\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6367 - accuracy: 0.6395 - val_loss: 0.6154 - val_accuracy: 0.6849\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6426 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6437 - val_loss: 0.6152 - val_accuracy: 0.6952\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6433 - val_loss: 0.6175 - val_accuracy: 0.6849\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6478 - val_loss: 0.6164 - val_accuracy: 0.6913\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6434 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6500 - val_loss: 0.6159 - val_accuracy: 0.6888\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6490 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6518 - val_loss: 0.6143 - val_accuracy: 0.6901\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6261 - accuracy: 0.6556 - val_loss: 0.6188 - val_accuracy: 0.6773\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6464 - val_loss: 0.6158 - val_accuracy: 0.6939\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6261 - accuracy: 0.6581 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6529 - val_loss: 0.6108 - val_accuracy: 0.6990\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6517 - val_loss: 0.6151 - val_accuracy: 0.6901\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6539 - val_loss: 0.6118 - val_accuracy: 0.6964\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6277 - accuracy: 0.6488 - val_loss: 0.6086 - val_accuracy: 0.6964\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6562 - val_loss: 0.6110 - val_accuracy: 0.6926\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6277 - accuracy: 0.6526 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6583 - val_loss: 0.6120 - val_accuracy: 0.6952\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6222 - accuracy: 0.6534 - val_loss: 0.6090 - val_accuracy: 0.6939\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6577 - val_loss: 0.6131 - val_accuracy: 0.6926\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6536 - val_loss: 0.6129 - val_accuracy: 0.6939\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6249 - accuracy: 0.6614 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6626 - val_loss: 0.6127 - val_accuracy: 0.6913\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6195 - accuracy: 0.6596 - val_loss: 0.6093 - val_accuracy: 0.6990\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6189 - accuracy: 0.6650 - val_loss: 0.6111 - val_accuracy: 0.6952\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6585 - val_loss: 0.6103 - val_accuracy: 0.6939\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6589 - val_loss: 0.6100 - val_accuracy: 0.6977\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6616 - val_loss: 0.6119 - val_accuracy: 0.6913\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6139 - accuracy: 0.6693 - val_loss: 0.6098 - val_accuracy: 0.6952\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6643 - val_loss: 0.6107 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6173 - accuracy: 0.6678 - val_loss: 0.6127 - val_accuracy: 0.6888\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6650 - val_loss: 0.6083 - val_accuracy: 0.6990\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6125 - accuracy: 0.6714 - val_loss: 0.6122 - val_accuracy: 0.6875\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6167 - accuracy: 0.6655 - val_loss: 0.6116 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6082 - accuracy: 0.6755 - val_loss: 0.6120 - val_accuracy: 0.6862\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6756 - val_loss: 0.6106 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6116 - accuracy: 0.6686 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6722 - val_loss: 0.6150 - val_accuracy: 0.6811\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6109 - accuracy: 0.6743 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6083 - accuracy: 0.6718 - val_loss: 0.6141 - val_accuracy: 0.6849\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6126 - accuracy: 0.6663 - val_loss: 0.6112 - val_accuracy: 0.6849\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6084 - accuracy: 0.6753 - val_loss: 0.6113 - val_accuracy: 0.6849\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6047 - accuracy: 0.6755 - val_loss: 0.6131 - val_accuracy: 0.6811\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6046 - accuracy: 0.6837 - val_loss: 0.6155 - val_accuracy: 0.6760\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6751 - val_loss: 0.6134 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6047 - accuracy: 0.6723 - val_loss: 0.6168 - val_accuracy: 0.6773\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6016 - accuracy: 0.6795 - val_loss: 0.6166 - val_accuracy: 0.6798\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6078 - accuracy: 0.6733 - val_loss: 0.6129 - val_accuracy: 0.6862\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6026 - accuracy: 0.6781 - val_loss: 0.6157 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6025 - accuracy: 0.6816 - val_loss: 0.6125 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6071 - accuracy: 0.6743 - val_loss: 0.6118 - val_accuracy: 0.6811\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6073 - accuracy: 0.6745 - val_loss: 0.6141 - val_accuracy: 0.6811\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6044 - accuracy: 0.6794 - val_loss: 0.6153 - val_accuracy: 0.6798\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6001 - accuracy: 0.6859 - val_loss: 0.6160 - val_accuracy: 0.6747\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6015 - accuracy: 0.6799 - val_loss: 0.6199 - val_accuracy: 0.6760\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6008 - accuracy: 0.6794 - val_loss: 0.6156 - val_accuracy: 0.6811\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6015 - accuracy: 0.6815 - val_loss: 0.6180 - val_accuracy: 0.6824\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6020 - accuracy: 0.6778 - val_loss: 0.6147 - val_accuracy: 0.6837\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.6850 - val_loss: 0.6172 - val_accuracy: 0.6824\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6002 - accuracy: 0.6819 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5945 - accuracy: 0.6874 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5943 - accuracy: 0.6855 - val_loss: 0.6177 - val_accuracy: 0.6786\n",
      "Calculating for: 700 50 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_280 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7986 - accuracy: 0.5325 - val_loss: 0.6468 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7241 - accuracy: 0.5383 - val_loss: 0.6496 - val_accuracy: 0.6467\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6976 - accuracy: 0.5534 - val_loss: 0.6521 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5510 - val_loss: 0.6510 - val_accuracy: 0.6594\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6801 - accuracy: 0.5745 - val_loss: 0.6499 - val_accuracy: 0.6620\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.5736 - val_loss: 0.6466 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5833 - val_loss: 0.6511 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5872 - val_loss: 0.6426 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6727 - accuracy: 0.5872 - val_loss: 0.6421 - val_accuracy: 0.6633\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5854 - val_loss: 0.6432 - val_accuracy: 0.6633\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5824 - val_loss: 0.6442 - val_accuracy: 0.6722\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5973 - val_loss: 0.6410 - val_accuracy: 0.6633\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6667 - accuracy: 0.5958 - val_loss: 0.6423 - val_accuracy: 0.6722\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6060 - val_loss: 0.6405 - val_accuracy: 0.6658\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6039 - val_loss: 0.6432 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6080 - val_loss: 0.6359 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6046 - val_loss: 0.6389 - val_accuracy: 0.6709\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6031 - val_loss: 0.6396 - val_accuracy: 0.6747\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6071 - val_loss: 0.6346 - val_accuracy: 0.6735\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6049 - val_loss: 0.6345 - val_accuracy: 0.6722\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6098 - val_loss: 0.6328 - val_accuracy: 0.6709\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6035 - val_loss: 0.6399 - val_accuracy: 0.6786\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6138 - val_loss: 0.6343 - val_accuracy: 0.6798\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6105 - val_loss: 0.6351 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6130 - val_loss: 0.6346 - val_accuracy: 0.6837\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6187 - val_loss: 0.6303 - val_accuracy: 0.6811\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6103 - val_loss: 0.6309 - val_accuracy: 0.6862\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6154 - val_loss: 0.6346 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6218 - val_loss: 0.6284 - val_accuracy: 0.6849\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6203 - val_loss: 0.6288 - val_accuracy: 0.6837\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6119 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6217 - val_loss: 0.6326 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6266 - val_loss: 0.6332 - val_accuracy: 0.6671\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6172 - val_loss: 0.6313 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6238 - val_loss: 0.6320 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6280 - val_loss: 0.6289 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6265 - val_loss: 0.6290 - val_accuracy: 0.6901\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6266 - val_loss: 0.6311 - val_accuracy: 0.6875\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6212 - val_loss: 0.6282 - val_accuracy: 0.6862\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6216 - val_loss: 0.6238 - val_accuracy: 0.6875\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6265 - val_loss: 0.6317 - val_accuracy: 0.6696\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6274 - val_loss: 0.6266 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6326 - val_loss: 0.6273 - val_accuracy: 0.6824\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6300 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6361 - val_loss: 0.6196 - val_accuracy: 0.6888\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6374 - val_loss: 0.6237 - val_accuracy: 0.6862\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6361 - val_loss: 0.6195 - val_accuracy: 0.6862\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6296 - val_loss: 0.6300 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6343 - val_loss: 0.6247 - val_accuracy: 0.6837\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6368 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6302 - val_loss: 0.6237 - val_accuracy: 0.6837\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6436 - val_loss: 0.6291 - val_accuracy: 0.6760\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6427 - val_loss: 0.6218 - val_accuracy: 0.6849\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6369 - val_loss: 0.6198 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6409 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6353 - val_loss: 0.6197 - val_accuracy: 0.6888\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6350 - val_loss: 0.6241 - val_accuracy: 0.6786\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6364 - val_loss: 0.6209 - val_accuracy: 0.6875\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6441 - val_loss: 0.6191 - val_accuracy: 0.6888\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6410 - val_loss: 0.6223 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6384 - accuracy: 0.6441 - val_loss: 0.6179 - val_accuracy: 0.6862\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6365 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6372 - val_loss: 0.6159 - val_accuracy: 0.6875\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6443 - val_loss: 0.6185 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6456 - val_loss: 0.6228 - val_accuracy: 0.6837\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6420 - val_loss: 0.6209 - val_accuracy: 0.6760\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6458 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6434 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6373 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6413 - val_loss: 0.6181 - val_accuracy: 0.6875\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6477 - val_loss: 0.6188 - val_accuracy: 0.6901\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6457 - val_loss: 0.6207 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6415 - val_loss: 0.6201 - val_accuracy: 0.6875\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6466 - val_loss: 0.6224 - val_accuracy: 0.6862\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6527 - val_loss: 0.6194 - val_accuracy: 0.6901\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6522 - val_loss: 0.6180 - val_accuracy: 0.6913\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6451 - val_loss: 0.6205 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6480 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6546 - val_loss: 0.6207 - val_accuracy: 0.6926\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6511 - val_loss: 0.6201 - val_accuracy: 0.6939\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6476 - val_loss: 0.6199 - val_accuracy: 0.6964\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6477 - val_loss: 0.6209 - val_accuracy: 0.6939\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6521 - val_loss: 0.6195 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6599 - val_loss: 0.6215 - val_accuracy: 0.6875\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6532 - val_loss: 0.6215 - val_accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6554 - val_loss: 0.6158 - val_accuracy: 0.6901\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6520 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6536 - val_loss: 0.6168 - val_accuracy: 0.6952\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6497 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6603 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6600 - val_loss: 0.6195 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6581 - val_loss: 0.6138 - val_accuracy: 0.6888\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6559 - val_loss: 0.6166 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6606 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6588 - val_loss: 0.6163 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6541 - val_loss: 0.6193 - val_accuracy: 0.6939\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6629 - val_loss: 0.6147 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6598 - val_loss: 0.6175 - val_accuracy: 0.6837\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6580 - val_loss: 0.6147 - val_accuracy: 0.6913\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6557 - val_loss: 0.6138 - val_accuracy: 0.6913\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6628 - val_loss: 0.6181 - val_accuracy: 0.6888\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6626 - val_loss: 0.6150 - val_accuracy: 0.6888\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6616 - val_loss: 0.6175 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6576 - val_loss: 0.6199 - val_accuracy: 0.6888\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6590 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6623 - val_loss: 0.6197 - val_accuracy: 0.6824\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6583 - val_loss: 0.6155 - val_accuracy: 0.6901\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6146 - accuracy: 0.6683 - val_loss: 0.6197 - val_accuracy: 0.6888\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6571 - val_loss: 0.6152 - val_accuracy: 0.6913\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6731 - val_loss: 0.6132 - val_accuracy: 0.6888\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6650 - val_loss: 0.6133 - val_accuracy: 0.6901\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6635 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6623 - val_loss: 0.6160 - val_accuracy: 0.6888\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6717 - val_loss: 0.6186 - val_accuracy: 0.6888\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6669 - val_loss: 0.6129 - val_accuracy: 0.6901\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6136 - accuracy: 0.6664 - val_loss: 0.6130 - val_accuracy: 0.6888\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6128 - accuracy: 0.6729 - val_loss: 0.6155 - val_accuracy: 0.6901\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6603 - val_loss: 0.6175 - val_accuracy: 0.6926\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6703 - val_loss: 0.6190 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6648 - val_loss: 0.6201 - val_accuracy: 0.6862\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6117 - accuracy: 0.6697 - val_loss: 0.6176 - val_accuracy: 0.6926\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.6703 - val_loss: 0.6190 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6630 - val_loss: 0.6212 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6683 - val_loss: 0.6214 - val_accuracy: 0.6862\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6631 - val_loss: 0.6151 - val_accuracy: 0.6862\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6689 - val_loss: 0.6160 - val_accuracy: 0.6901\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6721 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6699 - val_loss: 0.6205 - val_accuracy: 0.6901\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6650 - val_loss: 0.6166 - val_accuracy: 0.6888\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6122 - accuracy: 0.6706 - val_loss: 0.6185 - val_accuracy: 0.6798\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6131 - accuracy: 0.6723 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6139 - accuracy: 0.6644 - val_loss: 0.6195 - val_accuracy: 0.6837\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6768 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6100 - accuracy: 0.6756 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6063 - accuracy: 0.6727 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6755 - val_loss: 0.6170 - val_accuracy: 0.6849\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6737 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6047 - accuracy: 0.6763 - val_loss: 0.6169 - val_accuracy: 0.6888\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6751 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6736 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6822 - val_loss: 0.6192 - val_accuracy: 0.6862\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6087 - accuracy: 0.6708 - val_loss: 0.6202 - val_accuracy: 0.6837\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6083 - accuracy: 0.6791 - val_loss: 0.6160 - val_accuracy: 0.6913\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6804 - val_loss: 0.6225 - val_accuracy: 0.6849\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6048 - accuracy: 0.6791 - val_loss: 0.6212 - val_accuracy: 0.6875\n",
      "Calculating for: 700 50 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_284 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8244 - accuracy: 0.5211 - val_loss: 0.6552 - val_accuracy: 0.6250\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7430 - accuracy: 0.5225 - val_loss: 0.6606 - val_accuracy: 0.6390\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7099 - accuracy: 0.5252 - val_loss: 0.6643 - val_accuracy: 0.6339\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6995 - accuracy: 0.5275 - val_loss: 0.6620 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5315 - val_loss: 0.6621 - val_accuracy: 0.6403\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5453 - val_loss: 0.6576 - val_accuracy: 0.6429\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6841 - accuracy: 0.5472 - val_loss: 0.6606 - val_accuracy: 0.6467\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5549 - val_loss: 0.6545 - val_accuracy: 0.6378\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5649 - val_loss: 0.6592 - val_accuracy: 0.6454\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6797 - accuracy: 0.5697 - val_loss: 0.6569 - val_accuracy: 0.6531\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5710 - val_loss: 0.6582 - val_accuracy: 0.6518\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6797 - accuracy: 0.5752 - val_loss: 0.6540 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6803 - accuracy: 0.5722 - val_loss: 0.6545 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6798 - accuracy: 0.5739 - val_loss: 0.6569 - val_accuracy: 0.6556\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5764 - val_loss: 0.6484 - val_accuracy: 0.6505\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5849 - val_loss: 0.6521 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.5819 - val_loss: 0.6496 - val_accuracy: 0.6543\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.5804 - val_loss: 0.6483 - val_accuracy: 0.6531\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6757 - accuracy: 0.5784 - val_loss: 0.6475 - val_accuracy: 0.6543\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6769 - accuracy: 0.5715 - val_loss: 0.6553 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5844 - val_loss: 0.6474 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5850 - val_loss: 0.6513 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5854 - val_loss: 0.6431 - val_accuracy: 0.6684\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5919 - val_loss: 0.6444 - val_accuracy: 0.6709\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6691 - accuracy: 0.5956 - val_loss: 0.6454 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5867 - val_loss: 0.6423 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6684 - accuracy: 0.5943 - val_loss: 0.6429 - val_accuracy: 0.6709\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5913 - val_loss: 0.6436 - val_accuracy: 0.6671\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5941 - val_loss: 0.6517 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.6034 - val_loss: 0.6393 - val_accuracy: 0.6633\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6021 - val_loss: 0.6391 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5936 - val_loss: 0.6428 - val_accuracy: 0.6684\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5996 - val_loss: 0.6434 - val_accuracy: 0.6684\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5977 - val_loss: 0.6410 - val_accuracy: 0.6747\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6001 - val_loss: 0.6364 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.5976 - val_loss: 0.6384 - val_accuracy: 0.6696\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6006 - val_loss: 0.6373 - val_accuracy: 0.6773\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6024 - val_loss: 0.6342 - val_accuracy: 0.6658\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.5992 - val_loss: 0.6400 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.5996 - val_loss: 0.6407 - val_accuracy: 0.6735\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6039 - val_loss: 0.6383 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6036 - val_loss: 0.6402 - val_accuracy: 0.6735\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6056 - val_loss: 0.6370 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6049 - val_loss: 0.6362 - val_accuracy: 0.6722\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6045 - val_loss: 0.6411 - val_accuracy: 0.6709\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6010 - val_loss: 0.6385 - val_accuracy: 0.6709\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6107 - val_loss: 0.6365 - val_accuracy: 0.6747\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6099 - val_loss: 0.6343 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6039 - val_loss: 0.6395 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6118 - val_loss: 0.6333 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6012 - val_loss: 0.6372 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6133 - val_loss: 0.6344 - val_accuracy: 0.6773\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6113 - val_loss: 0.6354 - val_accuracy: 0.6747\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6134 - val_loss: 0.6404 - val_accuracy: 0.6645\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6138 - val_loss: 0.6361 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6149 - val_loss: 0.6303 - val_accuracy: 0.6786\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6113 - val_loss: 0.6305 - val_accuracy: 0.6798\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6230 - val_loss: 0.6280 - val_accuracy: 0.6786\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6271 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6099 - val_loss: 0.6302 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6150 - val_loss: 0.6335 - val_accuracy: 0.6735\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6144 - val_loss: 0.6296 - val_accuracy: 0.6824\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6236 - val_loss: 0.6295 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6114 - val_loss: 0.6340 - val_accuracy: 0.6837\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6203 - val_loss: 0.6301 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6218 - val_loss: 0.6314 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6222 - val_loss: 0.6264 - val_accuracy: 0.6824\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6245 - val_loss: 0.6281 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6184 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6183 - val_loss: 0.6305 - val_accuracy: 0.6786\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6242 - val_loss: 0.6293 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6245 - val_loss: 0.6284 - val_accuracy: 0.6862\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6174 - val_loss: 0.6260 - val_accuracy: 0.6811\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6222 - val_loss: 0.6301 - val_accuracy: 0.6798\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6245 - val_loss: 0.6282 - val_accuracy: 0.6837\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6162 - val_loss: 0.6278 - val_accuracy: 0.6849\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6193 - val_loss: 0.6280 - val_accuracy: 0.6875\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6261 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6261 - val_loss: 0.6356 - val_accuracy: 0.6760\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6251 - val_loss: 0.6269 - val_accuracy: 0.6862\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6211 - val_loss: 0.6290 - val_accuracy: 0.6888\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6250 - val_loss: 0.6267 - val_accuracy: 0.6901\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6227 - val_loss: 0.6247 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6228 - val_loss: 0.6259 - val_accuracy: 0.6888\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6481 - accuracy: 0.6302 - val_loss: 0.6256 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6275 - val_loss: 0.6269 - val_accuracy: 0.6862\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6301 - val_loss: 0.6236 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6277 - val_loss: 0.6244 - val_accuracy: 0.6913\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6266 - val_loss: 0.6278 - val_accuracy: 0.6824\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6260 - val_loss: 0.6313 - val_accuracy: 0.6901\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6297 - val_loss: 0.6238 - val_accuracy: 0.6862\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6272 - val_loss: 0.6237 - val_accuracy: 0.6926\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6294 - val_loss: 0.6226 - val_accuracy: 0.6875\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6258 - val_loss: 0.6190 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6355 - val_loss: 0.6233 - val_accuracy: 0.6901\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6504 - accuracy: 0.6319 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6415 - val_loss: 0.6179 - val_accuracy: 0.6952\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6321 - val_loss: 0.6226 - val_accuracy: 0.6837\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6280 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6380 - val_loss: 0.6174 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6296 - val_loss: 0.6215 - val_accuracy: 0.6824\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6353 - val_loss: 0.6220 - val_accuracy: 0.6862\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6378 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6272 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6377 - val_loss: 0.6214 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6319 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.6284 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6349 - val_loss: 0.6195 - val_accuracy: 0.6888\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6422 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6310 - val_loss: 0.6268 - val_accuracy: 0.6837\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6304 - val_loss: 0.6283 - val_accuracy: 0.6888\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6350 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6390 - val_loss: 0.6226 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6321 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6374 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6344 - val_loss: 0.6217 - val_accuracy: 0.6849\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6417 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6373 - val_loss: 0.6204 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6428 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6395 - val_loss: 0.6205 - val_accuracy: 0.6862\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6400 - val_loss: 0.6208 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.6310 - val_loss: 0.6238 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6392 - val_loss: 0.6196 - val_accuracy: 0.6875\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6321 - val_loss: 0.6178 - val_accuracy: 0.6875\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6412 - val_loss: 0.6191 - val_accuracy: 0.6862\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6379 - val_loss: 0.6181 - val_accuracy: 0.6875\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6420 - val_loss: 0.6171 - val_accuracy: 0.6888\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6358 - val_loss: 0.6163 - val_accuracy: 0.6913\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6413 - val_loss: 0.6220 - val_accuracy: 0.6913\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6424 - val_loss: 0.6190 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6423 - val_loss: 0.6152 - val_accuracy: 0.6926\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6433 - val_loss: 0.6178 - val_accuracy: 0.6913\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6417 - accuracy: 0.6387 - val_loss: 0.6229 - val_accuracy: 0.6888\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6417 - val_loss: 0.6152 - val_accuracy: 0.6939\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6374 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6387 - val_loss: 0.6195 - val_accuracy: 0.6901\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6516 - val_loss: 0.6142 - val_accuracy: 0.6888\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6433 - val_loss: 0.6155 - val_accuracy: 0.6952\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6461 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6418 - val_loss: 0.6182 - val_accuracy: 0.6913\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6431 - val_loss: 0.6185 - val_accuracy: 0.6901\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6429 - val_loss: 0.6151 - val_accuracy: 0.6913\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6473 - val_loss: 0.6166 - val_accuracy: 0.6888\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6471 - val_loss: 0.6135 - val_accuracy: 0.6888\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6487 - val_loss: 0.6168 - val_accuracy: 0.6926\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6508 - val_loss: 0.6163 - val_accuracy: 0.6926\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6475 - val_loss: 0.6164 - val_accuracy: 0.6926\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6501 - val_loss: 0.6166 - val_accuracy: 0.6952\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6408 - val_loss: 0.6158 - val_accuracy: 0.6913\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6468 - val_loss: 0.6146 - val_accuracy: 0.7003\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6530 - val_loss: 0.6168 - val_accuracy: 0.6964\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6427 - val_loss: 0.6183 - val_accuracy: 0.6862\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6535 - val_loss: 0.6118 - val_accuracy: 0.7015\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6516 - val_loss: 0.6132 - val_accuracy: 0.6977\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6512 - val_loss: 0.6144 - val_accuracy: 0.6977\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6518 - val_loss: 0.6137 - val_accuracy: 0.6990\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6518 - val_loss: 0.6136 - val_accuracy: 0.6952\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6507 - val_loss: 0.6104 - val_accuracy: 0.6964\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6544 - val_loss: 0.6125 - val_accuracy: 0.7015\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6472 - val_loss: 0.6148 - val_accuracy: 0.7015\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6534 - val_loss: 0.6130 - val_accuracy: 0.7015\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6517 - val_loss: 0.6147 - val_accuracy: 0.6952\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6593 - val_loss: 0.6219 - val_accuracy: 0.6952\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6527 - val_loss: 0.6169 - val_accuracy: 0.7015\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6410 - val_loss: 0.6150 - val_accuracy: 0.6964\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6525 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6462 - val_loss: 0.6116 - val_accuracy: 0.6926\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6595 - val_loss: 0.6108 - val_accuracy: 0.6964\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6575 - val_loss: 0.6147 - val_accuracy: 0.7003\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6541 - val_loss: 0.6113 - val_accuracy: 0.7003\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6497 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6600 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6515 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6603 - val_loss: 0.6123 - val_accuracy: 0.6939\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6600 - val_loss: 0.6187 - val_accuracy: 0.6849\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6604 - val_loss: 0.6147 - val_accuracy: 0.6862\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6606 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6532 - val_loss: 0.6116 - val_accuracy: 0.6849\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6561 - val_loss: 0.6126 - val_accuracy: 0.6913\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6580 - val_loss: 0.6096 - val_accuracy: 0.6913\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6551 - val_loss: 0.6085 - val_accuracy: 0.6913\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6556 - val_loss: 0.6153 - val_accuracy: 0.6901\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6565 - val_loss: 0.6161 - val_accuracy: 0.6888\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6626 - val_loss: 0.6155 - val_accuracy: 0.6875\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6594 - val_loss: 0.6094 - val_accuracy: 0.6888\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6559 - val_loss: 0.6163 - val_accuracy: 0.6926\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6552 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6579 - val_loss: 0.6104 - val_accuracy: 0.6901\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6637 - val_loss: 0.6133 - val_accuracy: 0.6875\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6605 - val_loss: 0.6132 - val_accuracy: 0.6875\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6586 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6229 - accuracy: 0.6614 - val_loss: 0.6145 - val_accuracy: 0.6913\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6596 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6611 - val_loss: 0.6184 - val_accuracy: 0.6875\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6567 - val_loss: 0.6129 - val_accuracy: 0.6888\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6516 - val_loss: 0.6159 - val_accuracy: 0.6926\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6606 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6613 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6583 - val_loss: 0.6179 - val_accuracy: 0.6913\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6604 - val_loss: 0.6165 - val_accuracy: 0.6875\n",
      "Calculating for: 700 100 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_288 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8017 - accuracy: 0.5308 - val_loss: 0.6415 - val_accuracy: 0.6480\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7085 - accuracy: 0.5605 - val_loss: 0.6405 - val_accuracy: 0.6696\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.5718 - val_loss: 0.6413 - val_accuracy: 0.6658\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5823 - val_loss: 0.6432 - val_accuracy: 0.6671\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.5824 - val_loss: 0.6390 - val_accuracy: 0.6709\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.5987 - val_loss: 0.6398 - val_accuracy: 0.6786\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.5901 - val_loss: 0.6462 - val_accuracy: 0.6747\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6025 - val_loss: 0.6360 - val_accuracy: 0.6773\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6017 - val_loss: 0.6360 - val_accuracy: 0.6747\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6616 - accuracy: 0.6074 - val_loss: 0.6364 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6070 - val_loss: 0.6388 - val_accuracy: 0.6747\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6098 - val_loss: 0.6361 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6184 - val_loss: 0.6289 - val_accuracy: 0.6773\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6179 - val_loss: 0.6326 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6147 - val_loss: 0.6330 - val_accuracy: 0.6722\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6184 - val_loss: 0.6305 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6193 - val_loss: 0.6327 - val_accuracy: 0.6760\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6152 - val_loss: 0.6303 - val_accuracy: 0.6747\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6253 - val_loss: 0.6208 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6228 - val_loss: 0.6290 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6233 - val_loss: 0.6291 - val_accuracy: 0.6811\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6251 - val_loss: 0.6242 - val_accuracy: 0.6824\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6287 - val_loss: 0.6259 - val_accuracy: 0.6811\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6261 - val_loss: 0.6273 - val_accuracy: 0.6837\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6304 - val_loss: 0.6253 - val_accuracy: 0.6862\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6353 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6246 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6384 - val_loss: 0.6185 - val_accuracy: 0.6811\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6305 - val_loss: 0.6158 - val_accuracy: 0.6798\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6383 - val_loss: 0.6239 - val_accuracy: 0.6824\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6419 - val_loss: 0.6273 - val_accuracy: 0.6658\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6365 - accuracy: 0.6400 - val_loss: 0.6223 - val_accuracy: 0.6684\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6309 - val_loss: 0.6248 - val_accuracy: 0.6735\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6356 - val_loss: 0.6175 - val_accuracy: 0.6798\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6379 - val_loss: 0.6223 - val_accuracy: 0.6811\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6387 - val_loss: 0.6214 - val_accuracy: 0.6824\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6418 - val_loss: 0.6206 - val_accuracy: 0.6811\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6500 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6497 - val_loss: 0.6168 - val_accuracy: 0.6811\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6451 - val_loss: 0.6178 - val_accuracy: 0.6760\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6461 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6518 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6506 - val_loss: 0.6168 - val_accuracy: 0.6811\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6555 - val_loss: 0.6183 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6525 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6541 - val_loss: 0.6182 - val_accuracy: 0.6786\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6570 - val_loss: 0.6129 - val_accuracy: 0.6901\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6485 - val_loss: 0.6143 - val_accuracy: 0.6849\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6462 - val_loss: 0.6135 - val_accuracy: 0.6811\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6569 - val_loss: 0.6170 - val_accuracy: 0.6824\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6571 - val_loss: 0.6183 - val_accuracy: 0.6760\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6614 - val_loss: 0.6150 - val_accuracy: 0.6837\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6580 - val_loss: 0.6165 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6521 - val_loss: 0.6125 - val_accuracy: 0.6824\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6645 - val_loss: 0.6162 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6621 - val_loss: 0.6132 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6654 - val_loss: 0.6139 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6610 - val_loss: 0.6142 - val_accuracy: 0.6888\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6630 - val_loss: 0.6161 - val_accuracy: 0.6798\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6600 - val_loss: 0.6164 - val_accuracy: 0.6824\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6621 - val_loss: 0.6156 - val_accuracy: 0.6773\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6679 - val_loss: 0.6147 - val_accuracy: 0.6811\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6672 - val_loss: 0.6179 - val_accuracy: 0.6837\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6166 - accuracy: 0.6640 - val_loss: 0.6184 - val_accuracy: 0.6849\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6154 - accuracy: 0.6668 - val_loss: 0.6175 - val_accuracy: 0.6862\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6621 - val_loss: 0.6150 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6643 - val_loss: 0.6193 - val_accuracy: 0.6798\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6137 - accuracy: 0.6689 - val_loss: 0.6204 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6729 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6097 - accuracy: 0.6736 - val_loss: 0.6201 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6067 - accuracy: 0.6760 - val_loss: 0.6207 - val_accuracy: 0.6786\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6704 - val_loss: 0.6195 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6103 - accuracy: 0.6675 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6079 - accuracy: 0.6757 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6111 - accuracy: 0.6697 - val_loss: 0.6163 - val_accuracy: 0.6824\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6112 - accuracy: 0.6741 - val_loss: 0.6124 - val_accuracy: 0.6875\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6144 - accuracy: 0.6782 - val_loss: 0.6104 - val_accuracy: 0.6901\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6760 - val_loss: 0.6139 - val_accuracy: 0.6811\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6113 - accuracy: 0.6719 - val_loss: 0.6157 - val_accuracy: 0.6862\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6040 - accuracy: 0.6845 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5971 - accuracy: 0.6858 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6030 - accuracy: 0.6861 - val_loss: 0.6182 - val_accuracy: 0.6862\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5971 - accuracy: 0.6806 - val_loss: 0.6144 - val_accuracy: 0.6901\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6052 - accuracy: 0.6775 - val_loss: 0.6156 - val_accuracy: 0.6837\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6009 - accuracy: 0.6830 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6035 - accuracy: 0.6732 - val_loss: 0.6170 - val_accuracy: 0.6837\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5969 - accuracy: 0.6858 - val_loss: 0.6175 - val_accuracy: 0.6824\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6827 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5980 - accuracy: 0.6767 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5992 - accuracy: 0.6786 - val_loss: 0.6206 - val_accuracy: 0.6837\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5967 - accuracy: 0.6874 - val_loss: 0.6180 - val_accuracy: 0.6875\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6009 - accuracy: 0.6868 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5954 - accuracy: 0.6868 - val_loss: 0.6231 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5974 - accuracy: 0.6820 - val_loss: 0.6201 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5924 - accuracy: 0.6893 - val_loss: 0.6240 - val_accuracy: 0.6786\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5919 - accuracy: 0.6869 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5952 - accuracy: 0.6904 - val_loss: 0.6215 - val_accuracy: 0.6824\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5966 - accuracy: 0.6863 - val_loss: 0.6151 - val_accuracy: 0.6888\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5950 - accuracy: 0.6952 - val_loss: 0.6182 - val_accuracy: 0.6849\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5942 - accuracy: 0.6908 - val_loss: 0.6189 - val_accuracy: 0.6862\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5883 - accuracy: 0.6958 - val_loss: 0.6183 - val_accuracy: 0.6849\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5890 - accuracy: 0.6930 - val_loss: 0.6193 - val_accuracy: 0.6824\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5926 - accuracy: 0.6898 - val_loss: 0.6233 - val_accuracy: 0.6811\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5862 - accuracy: 0.6958 - val_loss: 0.6265 - val_accuracy: 0.6786\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5961 - accuracy: 0.6874 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5819 - accuracy: 0.6971 - val_loss: 0.6248 - val_accuracy: 0.6773\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5856 - accuracy: 0.6962 - val_loss: 0.6228 - val_accuracy: 0.6875\n",
      "Calculating for: 700 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_292 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8087 - accuracy: 0.5252 - val_loss: 0.6748 - val_accuracy: 0.6148\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7191 - accuracy: 0.5455 - val_loss: 0.6628 - val_accuracy: 0.6352\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6921 - accuracy: 0.5491 - val_loss: 0.6558 - val_accuracy: 0.6467\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.5649 - val_loss: 0.6525 - val_accuracy: 0.6658\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6767 - accuracy: 0.5696 - val_loss: 0.6507 - val_accuracy: 0.6709\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5820 - val_loss: 0.6483 - val_accuracy: 0.6696\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5840 - val_loss: 0.6486 - val_accuracy: 0.6607\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5860 - val_loss: 0.6448 - val_accuracy: 0.6645\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.5985 - val_loss: 0.6452 - val_accuracy: 0.6467\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5899 - val_loss: 0.6447 - val_accuracy: 0.6531\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5973 - val_loss: 0.6473 - val_accuracy: 0.6416\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6703 - accuracy: 0.5906 - val_loss: 0.6470 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5916 - val_loss: 0.6378 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6042 - val_loss: 0.6399 - val_accuracy: 0.6492\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5971 - val_loss: 0.6425 - val_accuracy: 0.6518\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6004 - val_loss: 0.6427 - val_accuracy: 0.6518\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.5987 - val_loss: 0.6363 - val_accuracy: 0.6607\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6040 - val_loss: 0.6420 - val_accuracy: 0.6429\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6598 - accuracy: 0.6114 - val_loss: 0.6401 - val_accuracy: 0.6518\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6105 - val_loss: 0.6373 - val_accuracy: 0.6543\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6124 - val_loss: 0.6376 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6135 - val_loss: 0.6418 - val_accuracy: 0.6480\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6112 - val_loss: 0.6395 - val_accuracy: 0.6645\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6099 - val_loss: 0.6395 - val_accuracy: 0.6556\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6184 - val_loss: 0.6389 - val_accuracy: 0.6633\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6149 - val_loss: 0.6346 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6194 - val_loss: 0.6347 - val_accuracy: 0.6645\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6198 - val_loss: 0.6332 - val_accuracy: 0.6582\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6148 - val_loss: 0.6308 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6204 - val_loss: 0.6368 - val_accuracy: 0.6518\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6289 - val_loss: 0.6429 - val_accuracy: 0.6365\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6204 - val_loss: 0.6382 - val_accuracy: 0.6531\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6202 - val_loss: 0.6326 - val_accuracy: 0.6658\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6222 - val_loss: 0.6343 - val_accuracy: 0.6620\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6281 - val_loss: 0.6325 - val_accuracy: 0.6607\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6207 - val_loss: 0.6303 - val_accuracy: 0.6773\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6267 - val_loss: 0.6284 - val_accuracy: 0.6722\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6216 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6476 - accuracy: 0.6282 - val_loss: 0.6322 - val_accuracy: 0.6607\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6285 - val_loss: 0.6352 - val_accuracy: 0.6569\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6246 - val_loss: 0.6298 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6474 - accuracy: 0.6256 - val_loss: 0.6274 - val_accuracy: 0.6594\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6515 - accuracy: 0.6235 - val_loss: 0.6355 - val_accuracy: 0.6684\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6428 - accuracy: 0.6330 - val_loss: 0.6307 - val_accuracy: 0.6696\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6481 - accuracy: 0.6335 - val_loss: 0.6248 - val_accuracy: 0.6773\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6282 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6333 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6356 - val_loss: 0.6291 - val_accuracy: 0.6658\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6448 - accuracy: 0.6377 - val_loss: 0.6280 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6334 - val_loss: 0.6304 - val_accuracy: 0.6607\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6356 - val_loss: 0.6274 - val_accuracy: 0.6645\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6379 - val_loss: 0.6327 - val_accuracy: 0.6671\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.6353 - val_loss: 0.6271 - val_accuracy: 0.6684\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6412 - accuracy: 0.6321 - val_loss: 0.6289 - val_accuracy: 0.6696\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6420 - accuracy: 0.6368 - val_loss: 0.6225 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6414 - accuracy: 0.6392 - val_loss: 0.6262 - val_accuracy: 0.6684\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6400 - accuracy: 0.6432 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6417 - accuracy: 0.6395 - val_loss: 0.6240 - val_accuracy: 0.6735\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6400 - accuracy: 0.6387 - val_loss: 0.6249 - val_accuracy: 0.6735\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6388 - accuracy: 0.6402 - val_loss: 0.6273 - val_accuracy: 0.6722\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6390 - accuracy: 0.6413 - val_loss: 0.6214 - val_accuracy: 0.6760\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6369 - accuracy: 0.6429 - val_loss: 0.6267 - val_accuracy: 0.6633\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6418 - accuracy: 0.6368 - val_loss: 0.6278 - val_accuracy: 0.6735\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6401 - accuracy: 0.6377 - val_loss: 0.6315 - val_accuracy: 0.6671\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6316 - accuracy: 0.6464 - val_loss: 0.6272 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6361 - accuracy: 0.6434 - val_loss: 0.6246 - val_accuracy: 0.6786\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6343 - accuracy: 0.6461 - val_loss: 0.6259 - val_accuracy: 0.6773\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6377 - accuracy: 0.6438 - val_loss: 0.6264 - val_accuracy: 0.6760\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6358 - accuracy: 0.6454 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6372 - accuracy: 0.6442 - val_loss: 0.6211 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6487 - val_loss: 0.6235 - val_accuracy: 0.6722\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6447 - val_loss: 0.6264 - val_accuracy: 0.6786\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6393 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6512 - val_loss: 0.6152 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6332 - accuracy: 0.6490 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6313 - accuracy: 0.6486 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6335 - accuracy: 0.6490 - val_loss: 0.6244 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6306 - accuracy: 0.6473 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6327 - accuracy: 0.6466 - val_loss: 0.6246 - val_accuracy: 0.6773\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6310 - accuracy: 0.6569 - val_loss: 0.6261 - val_accuracy: 0.6798\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6273 - accuracy: 0.6566 - val_loss: 0.6224 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6274 - accuracy: 0.6531 - val_loss: 0.6256 - val_accuracy: 0.6773\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6596 - val_loss: 0.6247 - val_accuracy: 0.6786\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6559 - val_loss: 0.6275 - val_accuracy: 0.6709\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6527 - val_loss: 0.6241 - val_accuracy: 0.6824\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6505 - val_loss: 0.6219 - val_accuracy: 0.6862\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6547 - val_loss: 0.6212 - val_accuracy: 0.6901\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6238 - accuracy: 0.6572 - val_loss: 0.6224 - val_accuracy: 0.6901\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6577 - val_loss: 0.6282 - val_accuracy: 0.6811\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6567 - val_loss: 0.6235 - val_accuracy: 0.6875\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6552 - val_loss: 0.6251 - val_accuracy: 0.6786\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6610 - val_loss: 0.6197 - val_accuracy: 0.6901\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6193 - accuracy: 0.6664 - val_loss: 0.6251 - val_accuracy: 0.6811\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6187 - accuracy: 0.6606 - val_loss: 0.6228 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6267 - accuracy: 0.6547 - val_loss: 0.6194 - val_accuracy: 0.6875\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6583 - val_loss: 0.6159 - val_accuracy: 0.6952\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6643 - val_loss: 0.6213 - val_accuracy: 0.6888\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6610 - val_loss: 0.6209 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6634 - val_loss: 0.6216 - val_accuracy: 0.6837\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6580 - val_loss: 0.6255 - val_accuracy: 0.6798\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6631 - val_loss: 0.6273 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6576 - val_loss: 0.6203 - val_accuracy: 0.6913\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6645 - val_loss: 0.6240 - val_accuracy: 0.6837\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6165 - accuracy: 0.6613 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Calculating for: 700 100 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_296 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.8482 - accuracy: 0.5014 - val_loss: 0.6598 - val_accuracy: 0.6390\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.7272 - accuracy: 0.5387 - val_loss: 0.6581 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7087 - accuracy: 0.5203 - val_loss: 0.6567 - val_accuracy: 0.6416\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5296 - val_loss: 0.6654 - val_accuracy: 0.6518\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6914 - accuracy: 0.5396 - val_loss: 0.6673 - val_accuracy: 0.6492\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6894 - accuracy: 0.5389 - val_loss: 0.6618 - val_accuracy: 0.6403\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6841 - accuracy: 0.5558 - val_loss: 0.6534 - val_accuracy: 0.6390\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6849 - accuracy: 0.5551 - val_loss: 0.6569 - val_accuracy: 0.6403\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6849 - accuracy: 0.5494 - val_loss: 0.6592 - val_accuracy: 0.6441\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.5664 - val_loss: 0.6529 - val_accuracy: 0.6390\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.5649 - val_loss: 0.6532 - val_accuracy: 0.6441\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.5575 - val_loss: 0.6547 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6800 - accuracy: 0.5707 - val_loss: 0.6538 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5622 - val_loss: 0.6568 - val_accuracy: 0.6645\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6784 - accuracy: 0.5732 - val_loss: 0.6486 - val_accuracy: 0.6492\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.5678 - val_loss: 0.6547 - val_accuracy: 0.6645\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6750 - accuracy: 0.5726 - val_loss: 0.6500 - val_accuracy: 0.6594\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6790 - accuracy: 0.5723 - val_loss: 0.6498 - val_accuracy: 0.6620\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5779 - val_loss: 0.6486 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6752 - accuracy: 0.5794 - val_loss: 0.6458 - val_accuracy: 0.6671\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5865 - val_loss: 0.6456 - val_accuracy: 0.6671\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5907 - val_loss: 0.6430 - val_accuracy: 0.6671\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5815 - val_loss: 0.6404 - val_accuracy: 0.6645\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5921 - val_loss: 0.6428 - val_accuracy: 0.6696\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5917 - val_loss: 0.6420 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.5907 - val_loss: 0.6450 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5927 - val_loss: 0.6391 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5971 - val_loss: 0.6365 - val_accuracy: 0.6684\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6684 - accuracy: 0.5961 - val_loss: 0.6375 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6657 - accuracy: 0.5980 - val_loss: 0.6378 - val_accuracy: 0.6671\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6099 - val_loss: 0.6387 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6639 - accuracy: 0.6011 - val_loss: 0.6344 - val_accuracy: 0.6709\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.6017 - val_loss: 0.6380 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.6035 - val_loss: 0.6431 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5955 - val_loss: 0.6404 - val_accuracy: 0.6735\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.6024 - val_loss: 0.6434 - val_accuracy: 0.6684\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.5992 - val_loss: 0.6400 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6017 - val_loss: 0.6353 - val_accuracy: 0.6760\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5967 - val_loss: 0.6382 - val_accuracy: 0.6722\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6081 - val_loss: 0.6457 - val_accuracy: 0.6658\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6617 - accuracy: 0.6107 - val_loss: 0.6386 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6604 - accuracy: 0.6118 - val_loss: 0.6370 - val_accuracy: 0.6811\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6042 - val_loss: 0.6362 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6110 - val_loss: 0.6326 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6091 - val_loss: 0.6343 - val_accuracy: 0.6837\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6569 - accuracy: 0.6162 - val_loss: 0.6321 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6115 - val_loss: 0.6399 - val_accuracy: 0.6696\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6041 - val_loss: 0.6394 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6091 - val_loss: 0.6355 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6168 - val_loss: 0.6331 - val_accuracy: 0.6862\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6587 - accuracy: 0.6113 - val_loss: 0.6305 - val_accuracy: 0.6798\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6123 - val_loss: 0.6323 - val_accuracy: 0.6862\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6212 - val_loss: 0.6303 - val_accuracy: 0.6773\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6153 - val_loss: 0.6278 - val_accuracy: 0.6824\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6152 - val_loss: 0.6387 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6125 - val_loss: 0.6289 - val_accuracy: 0.6837\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6564 - accuracy: 0.6158 - val_loss: 0.6349 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6162 - val_loss: 0.6358 - val_accuracy: 0.6798\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6541 - accuracy: 0.6194 - val_loss: 0.6334 - val_accuracy: 0.6798\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6163 - val_loss: 0.6362 - val_accuracy: 0.6773\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6173 - val_loss: 0.6281 - val_accuracy: 0.6875\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6232 - val_loss: 0.6306 - val_accuracy: 0.6773\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6194 - val_loss: 0.6297 - val_accuracy: 0.6786\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6222 - val_loss: 0.6275 - val_accuracy: 0.6773\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6204 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6238 - val_loss: 0.6335 - val_accuracy: 0.6849\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6184 - val_loss: 0.6370 - val_accuracy: 0.6747\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.6228 - val_loss: 0.6348 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6267 - val_loss: 0.6319 - val_accuracy: 0.6811\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6252 - val_loss: 0.6289 - val_accuracy: 0.6901\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6232 - val_loss: 0.6315 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6256 - val_loss: 0.6271 - val_accuracy: 0.6862\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6543 - accuracy: 0.6251 - val_loss: 0.6277 - val_accuracy: 0.6888\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.6216 - val_loss: 0.6280 - val_accuracy: 0.6824\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6495 - accuracy: 0.6266 - val_loss: 0.6257 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6509 - accuracy: 0.6324 - val_loss: 0.6263 - val_accuracy: 0.6939\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6285 - val_loss: 0.6301 - val_accuracy: 0.6939\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6247 - val_loss: 0.6300 - val_accuracy: 0.6901\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6260 - val_loss: 0.6246 - val_accuracy: 0.6875\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6248 - val_loss: 0.6310 - val_accuracy: 0.6658\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6280 - val_loss: 0.6251 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6285 - val_loss: 0.6304 - val_accuracy: 0.6747\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6295 - val_loss: 0.6268 - val_accuracy: 0.6862\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6292 - val_loss: 0.6261 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6296 - val_loss: 0.6238 - val_accuracy: 0.6964\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6294 - val_loss: 0.6220 - val_accuracy: 0.6901\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6368 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6356 - val_loss: 0.6236 - val_accuracy: 0.6926\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6369 - val_loss: 0.6226 - val_accuracy: 0.6901\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6344 - val_loss: 0.6236 - val_accuracy: 0.6773\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6331 - val_loss: 0.6251 - val_accuracy: 0.6773\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6349 - val_loss: 0.6203 - val_accuracy: 0.6901\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6349 - val_loss: 0.6252 - val_accuracy: 0.6888\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6340 - val_loss: 0.6226 - val_accuracy: 0.6888\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6346 - val_loss: 0.6261 - val_accuracy: 0.6862\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6331 - val_loss: 0.6275 - val_accuracy: 0.6875\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6323 - val_loss: 0.6250 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6441 - accuracy: 0.6353 - val_loss: 0.6251 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6432 - accuracy: 0.6358 - val_loss: 0.6237 - val_accuracy: 0.6862\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6437 - accuracy: 0.6372 - val_loss: 0.6213 - val_accuracy: 0.6901\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6369 - val_loss: 0.6210 - val_accuracy: 0.6952\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6452 - accuracy: 0.6290 - val_loss: 0.6267 - val_accuracy: 0.6875\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6424 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6419 - val_loss: 0.6212 - val_accuracy: 0.6862\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6414 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6363 - val_loss: 0.6216 - val_accuracy: 0.6837\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6418 - val_loss: 0.6168 - val_accuracy: 0.6901\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6328 - val_loss: 0.6198 - val_accuracy: 0.6862\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6405 - val_loss: 0.6231 - val_accuracy: 0.6824\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6378 - val_loss: 0.6235 - val_accuracy: 0.6811\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6404 - accuracy: 0.6428 - val_loss: 0.6189 - val_accuracy: 0.6901\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6382 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6413 - val_loss: 0.6247 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6392 - accuracy: 0.6427 - val_loss: 0.6151 - val_accuracy: 0.6913\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6452 - val_loss: 0.6198 - val_accuracy: 0.6824\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6355 - val_loss: 0.6195 - val_accuracy: 0.6875\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6427 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6446 - val_loss: 0.6250 - val_accuracy: 0.6811\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6448 - val_loss: 0.6266 - val_accuracy: 0.6760\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6393 - accuracy: 0.6429 - val_loss: 0.6176 - val_accuracy: 0.6926\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6461 - val_loss: 0.6173 - val_accuracy: 0.6913\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6428 - val_loss: 0.6186 - val_accuracy: 0.6901\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6463 - val_loss: 0.6174 - val_accuracy: 0.6849\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6417 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6446 - val_loss: 0.6179 - val_accuracy: 0.6888\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6399 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6437 - val_loss: 0.6163 - val_accuracy: 0.6901\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6431 - val_loss: 0.6197 - val_accuracy: 0.6926\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6339 - accuracy: 0.6442 - val_loss: 0.6203 - val_accuracy: 0.6862\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6469 - val_loss: 0.6192 - val_accuracy: 0.6901\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6496 - val_loss: 0.6160 - val_accuracy: 0.6913\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6529 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6442 - val_loss: 0.6192 - val_accuracy: 0.6888\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6410 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6373 - accuracy: 0.6467 - val_loss: 0.6172 - val_accuracy: 0.6913\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6351 - accuracy: 0.6497 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6327 - accuracy: 0.6493 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6427 - val_loss: 0.6184 - val_accuracy: 0.6913\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6463 - val_loss: 0.6133 - val_accuracy: 0.6939\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6467 - val_loss: 0.6145 - val_accuracy: 0.6952\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6506 - val_loss: 0.6141 - val_accuracy: 0.6939\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6497 - val_loss: 0.6137 - val_accuracy: 0.6913\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6541 - val_loss: 0.6155 - val_accuracy: 0.6913\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6547 - val_loss: 0.6172 - val_accuracy: 0.6939\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6498 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6503 - val_loss: 0.6137 - val_accuracy: 0.6952\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6502 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6518 - val_loss: 0.6149 - val_accuracy: 0.6939\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6559 - val_loss: 0.6167 - val_accuracy: 0.6964\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6541 - val_loss: 0.6191 - val_accuracy: 0.6875\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6488 - val_loss: 0.6184 - val_accuracy: 0.6875\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6505 - val_loss: 0.6155 - val_accuracy: 0.7003\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6513 - val_loss: 0.6135 - val_accuracy: 0.6977\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6561 - val_loss: 0.6119 - val_accuracy: 0.6926\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6550 - val_loss: 0.6169 - val_accuracy: 0.6888\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6522 - val_loss: 0.6152 - val_accuracy: 0.6913\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6311 - accuracy: 0.6556 - val_loss: 0.6165 - val_accuracy: 0.6901\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6536 - val_loss: 0.6166 - val_accuracy: 0.6926\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6279 - accuracy: 0.6567 - val_loss: 0.6137 - val_accuracy: 0.6939\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6232 - accuracy: 0.6624 - val_loss: 0.6104 - val_accuracy: 0.6913\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6277 - accuracy: 0.6554 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6546 - val_loss: 0.6122 - val_accuracy: 0.6888\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6541 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6593 - val_loss: 0.6105 - val_accuracy: 0.6926\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6572 - val_loss: 0.6182 - val_accuracy: 0.6901\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6235 - accuracy: 0.6640 - val_loss: 0.6126 - val_accuracy: 0.6964\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6536 - val_loss: 0.6143 - val_accuracy: 0.6913\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6248 - accuracy: 0.6606 - val_loss: 0.6121 - val_accuracy: 0.6901\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6262 - accuracy: 0.6598 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6562 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6532 - val_loss: 0.6174 - val_accuracy: 0.6913\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6626 - val_loss: 0.6162 - val_accuracy: 0.6926\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6545 - val_loss: 0.6141 - val_accuracy: 0.6990\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6608 - val_loss: 0.6130 - val_accuracy: 0.6939\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6657 - val_loss: 0.6098 - val_accuracy: 0.7015\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6619 - val_loss: 0.6150 - val_accuracy: 0.6901\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6594 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6577 - val_loss: 0.6213 - val_accuracy: 0.6888\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6618 - val_loss: 0.6211 - val_accuracy: 0.6888\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6257 - accuracy: 0.6577 - val_loss: 0.6170 - val_accuracy: 0.6875\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6182 - accuracy: 0.6686 - val_loss: 0.6185 - val_accuracy: 0.6875\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6644 - val_loss: 0.6177 - val_accuracy: 0.6862\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6216 - accuracy: 0.6609 - val_loss: 0.6193 - val_accuracy: 0.6875\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6174 - accuracy: 0.6689 - val_loss: 0.6121 - val_accuracy: 0.6862\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6619 - val_loss: 0.6168 - val_accuracy: 0.6875\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6240 - accuracy: 0.6586 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6649 - val_loss: 0.6128 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6613 - val_loss: 0.6173 - val_accuracy: 0.6837\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6583 - val_loss: 0.6182 - val_accuracy: 0.6837\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6570 - val_loss: 0.6176 - val_accuracy: 0.6913\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6223 - accuracy: 0.6628 - val_loss: 0.6192 - val_accuracy: 0.6901\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6191 - accuracy: 0.6668 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6630 - val_loss: 0.6180 - val_accuracy: 0.6837\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6180 - accuracy: 0.6699 - val_loss: 0.6189 - val_accuracy: 0.6862\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6686 - val_loss: 0.6162 - val_accuracy: 0.6849\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6201 - accuracy: 0.6648 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6624 - val_loss: 0.6139 - val_accuracy: 0.6875\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6148 - accuracy: 0.6609 - val_loss: 0.6148 - val_accuracy: 0.6862\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6586 - val_loss: 0.6148 - val_accuracy: 0.6875\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6200 - accuracy: 0.6652 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Calculating for: 700 150 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_300 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7952 - accuracy: 0.5344 - val_loss: 0.6661 - val_accuracy: 0.6301\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7031 - accuracy: 0.5698 - val_loss: 0.6623 - val_accuracy: 0.6237\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6816 - accuracy: 0.5740 - val_loss: 0.6517 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6753 - accuracy: 0.5841 - val_loss: 0.6435 - val_accuracy: 0.6645\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6679 - accuracy: 0.5968 - val_loss: 0.6424 - val_accuracy: 0.6658\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6642 - accuracy: 0.6034 - val_loss: 0.6431 - val_accuracy: 0.6454\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6654 - accuracy: 0.6002 - val_loss: 0.6393 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6621 - accuracy: 0.6040 - val_loss: 0.6412 - val_accuracy: 0.6480\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6045 - val_loss: 0.6395 - val_accuracy: 0.6543\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6597 - accuracy: 0.6088 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6568 - accuracy: 0.6194 - val_loss: 0.6365 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6162 - val_loss: 0.6318 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6577 - accuracy: 0.6143 - val_loss: 0.6314 - val_accuracy: 0.6658\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.6186 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6228 - val_loss: 0.6287 - val_accuracy: 0.6747\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6535 - accuracy: 0.6221 - val_loss: 0.6333 - val_accuracy: 0.6518\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6199 - val_loss: 0.6333 - val_accuracy: 0.6505\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6255 - val_loss: 0.6308 - val_accuracy: 0.6594\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6490 - accuracy: 0.6226 - val_loss: 0.6304 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6470 - accuracy: 0.6260 - val_loss: 0.6219 - val_accuracy: 0.6824\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6500 - accuracy: 0.6291 - val_loss: 0.6294 - val_accuracy: 0.6594\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6442 - accuracy: 0.6297 - val_loss: 0.6262 - val_accuracy: 0.6620\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6455 - accuracy: 0.6281 - val_loss: 0.6257 - val_accuracy: 0.6607\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6361 - val_loss: 0.6266 - val_accuracy: 0.6607\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6402 - accuracy: 0.6408 - val_loss: 0.6268 - val_accuracy: 0.6633\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6457 - accuracy: 0.6305 - val_loss: 0.6285 - val_accuracy: 0.6620\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6377 - val_loss: 0.6276 - val_accuracy: 0.6607\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6394 - val_loss: 0.6270 - val_accuracy: 0.6607\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6373 - accuracy: 0.6442 - val_loss: 0.6250 - val_accuracy: 0.6658\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6388 - val_loss: 0.6254 - val_accuracy: 0.6735\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6390 - accuracy: 0.6374 - val_loss: 0.6262 - val_accuracy: 0.6594\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6380 - accuracy: 0.6426 - val_loss: 0.6278 - val_accuracy: 0.6594\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6414 - accuracy: 0.6385 - val_loss: 0.6254 - val_accuracy: 0.6620\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6369 - val_loss: 0.6256 - val_accuracy: 0.6696\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6466 - val_loss: 0.6252 - val_accuracy: 0.6658\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6523 - val_loss: 0.6239 - val_accuracy: 0.6735\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6377 - accuracy: 0.6368 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6394 - val_loss: 0.6203 - val_accuracy: 0.6760\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6290 - accuracy: 0.6481 - val_loss: 0.6176 - val_accuracy: 0.6747\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6448 - val_loss: 0.6209 - val_accuracy: 0.6671\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6448 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6307 - accuracy: 0.6500 - val_loss: 0.6227 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6269 - accuracy: 0.6486 - val_loss: 0.6258 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6535 - val_loss: 0.6293 - val_accuracy: 0.6569\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6280 - accuracy: 0.6477 - val_loss: 0.6214 - val_accuracy: 0.6760\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6545 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6537 - val_loss: 0.6247 - val_accuracy: 0.6747\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6501 - val_loss: 0.6254 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6236 - accuracy: 0.6537 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6215 - accuracy: 0.6580 - val_loss: 0.6206 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6216 - accuracy: 0.6560 - val_loss: 0.6226 - val_accuracy: 0.6709\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6230 - accuracy: 0.6555 - val_loss: 0.6222 - val_accuracy: 0.6735\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6221 - accuracy: 0.6566 - val_loss: 0.6205 - val_accuracy: 0.6760\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6537 - val_loss: 0.6242 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6628 - val_loss: 0.6208 - val_accuracy: 0.6696\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6186 - accuracy: 0.6579 - val_loss: 0.6219 - val_accuracy: 0.6747\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6644 - val_loss: 0.6255 - val_accuracy: 0.6722\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6626 - val_loss: 0.6263 - val_accuracy: 0.6747\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6163 - accuracy: 0.6644 - val_loss: 0.6227 - val_accuracy: 0.6709\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6158 - accuracy: 0.6673 - val_loss: 0.6225 - val_accuracy: 0.6798\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6668 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6664 - val_loss: 0.6254 - val_accuracy: 0.6773\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6114 - accuracy: 0.6721 - val_loss: 0.6264 - val_accuracy: 0.6722\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6144 - accuracy: 0.6678 - val_loss: 0.6238 - val_accuracy: 0.6798\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6142 - accuracy: 0.6703 - val_loss: 0.6239 - val_accuracy: 0.6786\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6682 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6654 - val_loss: 0.6216 - val_accuracy: 0.6901\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6625 - val_loss: 0.6222 - val_accuracy: 0.6811\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6109 - accuracy: 0.6652 - val_loss: 0.6218 - val_accuracy: 0.6888\n",
      "Calculating for: 700 150 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_304 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8170 - accuracy: 0.5280 - val_loss: 0.6689 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7183 - accuracy: 0.5659 - val_loss: 0.6428 - val_accuracy: 0.6633\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6892 - accuracy: 0.5732 - val_loss: 0.6450 - val_accuracy: 0.6645\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6794 - accuracy: 0.5723 - val_loss: 0.6477 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6762 - accuracy: 0.5737 - val_loss: 0.6410 - val_accuracy: 0.6633\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6722 - accuracy: 0.5855 - val_loss: 0.6425 - val_accuracy: 0.6556\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5962 - val_loss: 0.6426 - val_accuracy: 0.6645\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5883 - val_loss: 0.6398 - val_accuracy: 0.6645\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.5967 - val_loss: 0.6373 - val_accuracy: 0.6696\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6031 - val_loss: 0.6334 - val_accuracy: 0.6684\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6052 - val_loss: 0.6377 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6039 - val_loss: 0.6389 - val_accuracy: 0.6582\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.6095 - val_loss: 0.6311 - val_accuracy: 0.6760\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6638 - accuracy: 0.6058 - val_loss: 0.6359 - val_accuracy: 0.6696\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6140 - val_loss: 0.6331 - val_accuracy: 0.6696\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.5958 - val_loss: 0.6331 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6611 - accuracy: 0.6020 - val_loss: 0.6362 - val_accuracy: 0.6747\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6096 - val_loss: 0.6305 - val_accuracy: 0.6811\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6601 - accuracy: 0.6107 - val_loss: 0.6281 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6582 - accuracy: 0.6078 - val_loss: 0.6269 - val_accuracy: 0.6760\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6179 - val_loss: 0.6283 - val_accuracy: 0.6773\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6570 - accuracy: 0.6076 - val_loss: 0.6297 - val_accuracy: 0.6798\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6093 - val_loss: 0.6289 - val_accuracy: 0.6773\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6552 - accuracy: 0.6202 - val_loss: 0.6290 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6174 - val_loss: 0.6260 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6546 - accuracy: 0.6178 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6218 - val_loss: 0.6243 - val_accuracy: 0.6798\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6187 - val_loss: 0.6260 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6258 - val_loss: 0.6267 - val_accuracy: 0.6824\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6498 - accuracy: 0.6242 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6473 - accuracy: 0.6321 - val_loss: 0.6198 - val_accuracy: 0.6798\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6289 - val_loss: 0.6301 - val_accuracy: 0.6837\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6236 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6235 - val_loss: 0.6226 - val_accuracy: 0.6875\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6227 - val_loss: 0.6247 - val_accuracy: 0.6837\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.6299 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6471 - accuracy: 0.6299 - val_loss: 0.6212 - val_accuracy: 0.6888\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6453 - accuracy: 0.6312 - val_loss: 0.6240 - val_accuracy: 0.6888\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6486 - accuracy: 0.6301 - val_loss: 0.6217 - val_accuracy: 0.6913\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6439 - accuracy: 0.6316 - val_loss: 0.6196 - val_accuracy: 0.6952\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6277 - val_loss: 0.6249 - val_accuracy: 0.6760\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6385 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6290 - val_loss: 0.6199 - val_accuracy: 0.6888\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6320 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6450 - accuracy: 0.6319 - val_loss: 0.6216 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6459 - accuracy: 0.6335 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6341 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6354 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6422 - accuracy: 0.6387 - val_loss: 0.6168 - val_accuracy: 0.6849\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6394 - val_loss: 0.6195 - val_accuracy: 0.6837\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6382 - val_loss: 0.6189 - val_accuracy: 0.6875\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6404 - val_loss: 0.6183 - val_accuracy: 0.6888\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6389 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6395 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6388 - val_loss: 0.6207 - val_accuracy: 0.6862\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6409 - val_loss: 0.6172 - val_accuracy: 0.6901\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6373 - val_loss: 0.6214 - val_accuracy: 0.6875\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6468 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6399 - val_loss: 0.6157 - val_accuracy: 0.6862\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6386 - accuracy: 0.6418 - val_loss: 0.6134 - val_accuracy: 0.6888\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6486 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6383 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6463 - val_loss: 0.6162 - val_accuracy: 0.6875\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6468 - val_loss: 0.6196 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6495 - val_loss: 0.6201 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6476 - val_loss: 0.6174 - val_accuracy: 0.6811\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6510 - val_loss: 0.6125 - val_accuracy: 0.6926\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6382 - accuracy: 0.6443 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6463 - val_loss: 0.6155 - val_accuracy: 0.6926\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6502 - val_loss: 0.6116 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6443 - val_loss: 0.6133 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6492 - val_loss: 0.6187 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6544 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6463 - val_loss: 0.6157 - val_accuracy: 0.6901\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6526 - val_loss: 0.6134 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6521 - val_loss: 0.6179 - val_accuracy: 0.6837\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6539 - val_loss: 0.6137 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6505 - val_loss: 0.6144 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6570 - val_loss: 0.6172 - val_accuracy: 0.6862\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6510 - val_loss: 0.6126 - val_accuracy: 0.6901\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6557 - val_loss: 0.6097 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6539 - val_loss: 0.6155 - val_accuracy: 0.6888\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6572 - val_loss: 0.6122 - val_accuracy: 0.6964\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6546 - val_loss: 0.6143 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6536 - val_loss: 0.6176 - val_accuracy: 0.6824\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6628 - val_loss: 0.6137 - val_accuracy: 0.6811\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6503 - val_loss: 0.6125 - val_accuracy: 0.6913\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6598 - val_loss: 0.6153 - val_accuracy: 0.6837\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6642 - val_loss: 0.6135 - val_accuracy: 0.6926\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6556 - val_loss: 0.6132 - val_accuracy: 0.6888\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6550 - val_loss: 0.6124 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6539 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6583 - val_loss: 0.6167 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6624 - val_loss: 0.6146 - val_accuracy: 0.6811\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6620 - val_loss: 0.6131 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6611 - val_loss: 0.6126 - val_accuracy: 0.6798\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6605 - val_loss: 0.6163 - val_accuracy: 0.6786\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6585 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6637 - val_loss: 0.6143 - val_accuracy: 0.6811\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6590 - val_loss: 0.6143 - val_accuracy: 0.6760\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6168 - accuracy: 0.6613 - val_loss: 0.6134 - val_accuracy: 0.6798\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6176 - accuracy: 0.6650 - val_loss: 0.6138 - val_accuracy: 0.6798\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6644 - val_loss: 0.6149 - val_accuracy: 0.6773\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6192 - accuracy: 0.6556 - val_loss: 0.6131 - val_accuracy: 0.6824\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6629 - val_loss: 0.6161 - val_accuracy: 0.6811\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6140 - accuracy: 0.6663 - val_loss: 0.6163 - val_accuracy: 0.6760\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6669 - val_loss: 0.6166 - val_accuracy: 0.6747\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6625 - val_loss: 0.6167 - val_accuracy: 0.6747\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6669 - val_loss: 0.6162 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6134 - accuracy: 0.6729 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6672 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Calculating for: 700 150 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_308 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8167 - accuracy: 0.5200 - val_loss: 0.6532 - val_accuracy: 0.6378\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7285 - accuracy: 0.5359 - val_loss: 0.6523 - val_accuracy: 0.6390\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7020 - accuracy: 0.5418 - val_loss: 0.6525 - val_accuracy: 0.6429\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5446 - val_loss: 0.6553 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6789 - accuracy: 0.5667 - val_loss: 0.6480 - val_accuracy: 0.6480\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6840 - accuracy: 0.5609 - val_loss: 0.6531 - val_accuracy: 0.6480\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.5676 - val_loss: 0.6524 - val_accuracy: 0.6492\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6787 - accuracy: 0.5656 - val_loss: 0.6466 - val_accuracy: 0.6556\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5666 - val_loss: 0.6479 - val_accuracy: 0.6582\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6805 - accuracy: 0.5623 - val_loss: 0.6471 - val_accuracy: 0.6582\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6774 - accuracy: 0.5725 - val_loss: 0.6436 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5782 - val_loss: 0.6435 - val_accuracy: 0.6594\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6756 - accuracy: 0.5759 - val_loss: 0.6423 - val_accuracy: 0.6556\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6735 - accuracy: 0.5775 - val_loss: 0.6399 - val_accuracy: 0.6582\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5946 - val_loss: 0.6369 - val_accuracy: 0.6607\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5830 - val_loss: 0.6422 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6717 - accuracy: 0.5823 - val_loss: 0.6441 - val_accuracy: 0.6696\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5879 - val_loss: 0.6461 - val_accuracy: 0.6658\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5889 - val_loss: 0.6390 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5962 - val_loss: 0.6390 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5909 - val_loss: 0.6349 - val_accuracy: 0.6696\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6693 - accuracy: 0.5941 - val_loss: 0.6369 - val_accuracy: 0.6709\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5981 - val_loss: 0.6372 - val_accuracy: 0.6747\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.5948 - val_loss: 0.6336 - val_accuracy: 0.6735\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6029 - val_loss: 0.6340 - val_accuracy: 0.6773\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5990 - val_loss: 0.6326 - val_accuracy: 0.6747\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6061 - val_loss: 0.6335 - val_accuracy: 0.6786\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.6022 - val_loss: 0.6336 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5937 - val_loss: 0.6325 - val_accuracy: 0.6747\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6026 - val_loss: 0.6356 - val_accuracy: 0.6735\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6009 - val_loss: 0.6312 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6636 - accuracy: 0.6065 - val_loss: 0.6332 - val_accuracy: 0.6658\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.6039 - val_loss: 0.6307 - val_accuracy: 0.6773\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6597 - accuracy: 0.6085 - val_loss: 0.6278 - val_accuracy: 0.6747\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.6010 - val_loss: 0.6308 - val_accuracy: 0.6760\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6060 - val_loss: 0.6301 - val_accuracy: 0.6735\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6078 - val_loss: 0.6284 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6115 - val_loss: 0.6320 - val_accuracy: 0.6811\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6622 - accuracy: 0.6058 - val_loss: 0.6312 - val_accuracy: 0.6811\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6598 - accuracy: 0.6085 - val_loss: 0.6284 - val_accuracy: 0.6811\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6602 - accuracy: 0.6114 - val_loss: 0.6276 - val_accuracy: 0.6811\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6122 - val_loss: 0.6289 - val_accuracy: 0.6709\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6133 - val_loss: 0.6258 - val_accuracy: 0.6824\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6100 - val_loss: 0.6262 - val_accuracy: 0.6837\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6117 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6117 - val_loss: 0.6260 - val_accuracy: 0.6849\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6161 - val_loss: 0.6236 - val_accuracy: 0.6798\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6143 - val_loss: 0.6285 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6166 - val_loss: 0.6279 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6588 - accuracy: 0.6058 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6150 - val_loss: 0.6250 - val_accuracy: 0.6849\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6562 - accuracy: 0.6167 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6222 - val_loss: 0.6264 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6551 - accuracy: 0.6172 - val_loss: 0.6308 - val_accuracy: 0.6824\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6117 - val_loss: 0.6239 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6216 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6191 - val_loss: 0.6232 - val_accuracy: 0.6837\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6184 - val_loss: 0.6223 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6153 - val_loss: 0.6255 - val_accuracy: 0.6875\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6177 - val_loss: 0.6273 - val_accuracy: 0.6786\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6233 - val_loss: 0.6254 - val_accuracy: 0.6837\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6196 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6548 - accuracy: 0.6194 - val_loss: 0.6210 - val_accuracy: 0.6837\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6235 - val_loss: 0.6217 - val_accuracy: 0.6811\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.6266 - val_loss: 0.6231 - val_accuracy: 0.6849\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6305 - val_loss: 0.6270 - val_accuracy: 0.6735\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6275 - val_loss: 0.6225 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6269 - val_loss: 0.6225 - val_accuracy: 0.6875\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6226 - val_loss: 0.6265 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6294 - val_loss: 0.6224 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6482 - accuracy: 0.6272 - val_loss: 0.6228 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6271 - val_loss: 0.6227 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6307 - val_loss: 0.6239 - val_accuracy: 0.6849\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6266 - val_loss: 0.6241 - val_accuracy: 0.6862\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6257 - val_loss: 0.6188 - val_accuracy: 0.6824\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6483 - accuracy: 0.6257 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6323 - val_loss: 0.6232 - val_accuracy: 0.6849\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6252 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6499 - accuracy: 0.6270 - val_loss: 0.6209 - val_accuracy: 0.6824\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6320 - val_loss: 0.6204 - val_accuracy: 0.6875\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6287 - val_loss: 0.6218 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6279 - val_loss: 0.6215 - val_accuracy: 0.6849\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6336 - val_loss: 0.6145 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6436 - accuracy: 0.6349 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6477 - accuracy: 0.6285 - val_loss: 0.6250 - val_accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6445 - accuracy: 0.6295 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6370 - val_loss: 0.6223 - val_accuracy: 0.6849\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6295 - val_loss: 0.6207 - val_accuracy: 0.6837\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6393 - val_loss: 0.6176 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6339 - val_loss: 0.6191 - val_accuracy: 0.6798\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6484 - accuracy: 0.6300 - val_loss: 0.6170 - val_accuracy: 0.6862\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6325 - val_loss: 0.6137 - val_accuracy: 0.6888\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6442 - accuracy: 0.6324 - val_loss: 0.6155 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6379 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6388 - val_loss: 0.6155 - val_accuracy: 0.6837\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6400 - val_loss: 0.6184 - val_accuracy: 0.6888\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6432 - accuracy: 0.6346 - val_loss: 0.6144 - val_accuracy: 0.6837\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6452 - accuracy: 0.6385 - val_loss: 0.6166 - val_accuracy: 0.6862\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6415 - accuracy: 0.6407 - val_loss: 0.6170 - val_accuracy: 0.6862\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6418 - accuracy: 0.6390 - val_loss: 0.6153 - val_accuracy: 0.6837\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6361 - val_loss: 0.6157 - val_accuracy: 0.6875\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6449 - val_loss: 0.6192 - val_accuracy: 0.6888\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6370 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6389 - val_loss: 0.6173 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6427 - val_loss: 0.6162 - val_accuracy: 0.6888\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6437 - val_loss: 0.6172 - val_accuracy: 0.6811\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6441 - accuracy: 0.6387 - val_loss: 0.6203 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6381 - accuracy: 0.6404 - val_loss: 0.6138 - val_accuracy: 0.6875\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6439 - val_loss: 0.6167 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6372 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6387 - accuracy: 0.6432 - val_loss: 0.6176 - val_accuracy: 0.6875\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6448 - val_loss: 0.6148 - val_accuracy: 0.6862\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.6384 - val_loss: 0.6181 - val_accuracy: 0.6862\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6353 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6375 - val_loss: 0.6157 - val_accuracy: 0.6875\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6399 - val_loss: 0.6181 - val_accuracy: 0.6875\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6377 - val_loss: 0.6167 - val_accuracy: 0.6901\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6437 - val_loss: 0.6226 - val_accuracy: 0.6786\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6457 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6382 - val_loss: 0.6178 - val_accuracy: 0.6824\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6429 - val_loss: 0.6132 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6432 - val_loss: 0.6152 - val_accuracy: 0.6888\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6462 - val_loss: 0.6184 - val_accuracy: 0.6849\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6467 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6461 - val_loss: 0.6118 - val_accuracy: 0.6926\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6544 - val_loss: 0.6137 - val_accuracy: 0.6888\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6467 - val_loss: 0.6136 - val_accuracy: 0.6901\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6408 - val_loss: 0.6152 - val_accuracy: 0.6939\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6516 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6367 - accuracy: 0.6476 - val_loss: 0.6151 - val_accuracy: 0.6926\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6463 - val_loss: 0.6160 - val_accuracy: 0.6849\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6468 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6368 - accuracy: 0.6477 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.6500 - val_loss: 0.6137 - val_accuracy: 0.6875\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6560 - val_loss: 0.6146 - val_accuracy: 0.6901\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6510 - val_loss: 0.6145 - val_accuracy: 0.6939\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6439 - val_loss: 0.6209 - val_accuracy: 0.6824\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6350 - accuracy: 0.6448 - val_loss: 0.6142 - val_accuracy: 0.6939\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6546 - val_loss: 0.6092 - val_accuracy: 0.6990\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6525 - val_loss: 0.6118 - val_accuracy: 0.6939\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6511 - val_loss: 0.6140 - val_accuracy: 0.6875\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6541 - val_loss: 0.6074 - val_accuracy: 0.6990\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6442 - val_loss: 0.6149 - val_accuracy: 0.6875\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6491 - val_loss: 0.6119 - val_accuracy: 0.7054\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6463 - val_loss: 0.6149 - val_accuracy: 0.6964\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6502 - val_loss: 0.6119 - val_accuracy: 0.6952\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6500 - val_loss: 0.6116 - val_accuracy: 0.6952\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6530 - val_loss: 0.6152 - val_accuracy: 0.6901\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6529 - val_loss: 0.6145 - val_accuracy: 0.6862\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6601 - val_loss: 0.6131 - val_accuracy: 0.6901\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6480 - val_loss: 0.6102 - val_accuracy: 0.6990\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6595 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6523 - val_loss: 0.6135 - val_accuracy: 0.6888\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6510 - val_loss: 0.6115 - val_accuracy: 0.7028\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6550 - val_loss: 0.6101 - val_accuracy: 0.6990\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6626 - val_loss: 0.6117 - val_accuracy: 0.6977\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6581 - val_loss: 0.6134 - val_accuracy: 0.6875\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6541 - val_loss: 0.6158 - val_accuracy: 0.6875\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6629 - val_loss: 0.6117 - val_accuracy: 0.6926\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6601 - val_loss: 0.6145 - val_accuracy: 0.6888\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6537 - val_loss: 0.6127 - val_accuracy: 0.6990\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6588 - val_loss: 0.6113 - val_accuracy: 0.6901\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6566 - val_loss: 0.6112 - val_accuracy: 0.6862\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6620 - val_loss: 0.6096 - val_accuracy: 0.7003\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6614 - val_loss: 0.6100 - val_accuracy: 0.6926\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6588 - val_loss: 0.6108 - val_accuracy: 0.6926\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6531 - val_loss: 0.6148 - val_accuracy: 0.6939\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6593 - val_loss: 0.6120 - val_accuracy: 0.6901\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6275 - accuracy: 0.6584 - val_loss: 0.6121 - val_accuracy: 0.6939\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6523 - val_loss: 0.6100 - val_accuracy: 0.6913\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.6615 - val_loss: 0.6085 - val_accuracy: 0.6990\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6589 - val_loss: 0.6144 - val_accuracy: 0.6888\n",
      "Calculating for: 750 50 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_312 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7893 - accuracy: 0.5373 - val_loss: 0.6534 - val_accuracy: 0.6429\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7096 - accuracy: 0.5528 - val_loss: 0.6440 - val_accuracy: 0.6531\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5705 - val_loss: 0.6411 - val_accuracy: 0.6696\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.5811 - val_loss: 0.6435 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6741 - accuracy: 0.5909 - val_loss: 0.6415 - val_accuracy: 0.6760\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.5958 - val_loss: 0.6428 - val_accuracy: 0.6735\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.5911 - val_loss: 0.6365 - val_accuracy: 0.6722\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6015 - val_loss: 0.6378 - val_accuracy: 0.6760\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6088 - val_loss: 0.6364 - val_accuracy: 0.6722\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6090 - val_loss: 0.6333 - val_accuracy: 0.6747\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6133 - val_loss: 0.6361 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6047 - val_loss: 0.6360 - val_accuracy: 0.6735\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6596 - accuracy: 0.6075 - val_loss: 0.6325 - val_accuracy: 0.6837\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6152 - val_loss: 0.6290 - val_accuracy: 0.6811\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6159 - val_loss: 0.6269 - val_accuracy: 0.6798\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6177 - val_loss: 0.6306 - val_accuracy: 0.6760\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6261 - val_loss: 0.6299 - val_accuracy: 0.6837\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6236 - val_loss: 0.6257 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6204 - val_loss: 0.6314 - val_accuracy: 0.6735\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6260 - val_loss: 0.6270 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6256 - val_loss: 0.6275 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6243 - val_loss: 0.6272 - val_accuracy: 0.6875\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6276 - val_loss: 0.6239 - val_accuracy: 0.6913\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6311 - val_loss: 0.6274 - val_accuracy: 0.6888\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6282 - val_loss: 0.6236 - val_accuracy: 0.6862\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6330 - val_loss: 0.6258 - val_accuracy: 0.6875\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6325 - val_loss: 0.6202 - val_accuracy: 0.6875\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6343 - val_loss: 0.6232 - val_accuracy: 0.6837\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6325 - val_loss: 0.6263 - val_accuracy: 0.6875\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6359 - val_loss: 0.6229 - val_accuracy: 0.6849\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6334 - val_loss: 0.6267 - val_accuracy: 0.6837\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6438 - val_loss: 0.6232 - val_accuracy: 0.6875\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6405 - accuracy: 0.6385 - val_loss: 0.6223 - val_accuracy: 0.6888\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6415 - val_loss: 0.6213 - val_accuracy: 0.6862\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6427 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6457 - val_loss: 0.6241 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6461 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6532 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6535 - val_loss: 0.6118 - val_accuracy: 0.6901\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6545 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6561 - val_loss: 0.6172 - val_accuracy: 0.6849\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6488 - val_loss: 0.6172 - val_accuracy: 0.6849\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6496 - val_loss: 0.6200 - val_accuracy: 0.6862\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6507 - val_loss: 0.6217 - val_accuracy: 0.6888\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6339 - accuracy: 0.6469 - val_loss: 0.6194 - val_accuracy: 0.6735\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6466 - val_loss: 0.6173 - val_accuracy: 0.6786\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6512 - val_loss: 0.6236 - val_accuracy: 0.6709\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6567 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6561 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6501 - val_loss: 0.6197 - val_accuracy: 0.6824\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6557 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6520 - val_loss: 0.6197 - val_accuracy: 0.6901\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6529 - val_loss: 0.6193 - val_accuracy: 0.6901\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6662 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6550 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6549 - val_loss: 0.6189 - val_accuracy: 0.6849\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6213 - accuracy: 0.6640 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6601 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6668 - val_loss: 0.6144 - val_accuracy: 0.6913\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6584 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6199 - accuracy: 0.6619 - val_loss: 0.6219 - val_accuracy: 0.6798\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6619 - val_loss: 0.6151 - val_accuracy: 0.6913\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6643 - val_loss: 0.6171 - val_accuracy: 0.6824\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6166 - accuracy: 0.6683 - val_loss: 0.6227 - val_accuracy: 0.6760\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6616 - val_loss: 0.6168 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.6672 - val_loss: 0.6181 - val_accuracy: 0.6849\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6645 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6203 - accuracy: 0.6630 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6074 - accuracy: 0.6776 - val_loss: 0.6189 - val_accuracy: 0.6824\n",
      "Calculating for: 750 50 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_316 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8143 - accuracy: 0.5136 - val_loss: 0.6771 - val_accuracy: 0.6365\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7340 - accuracy: 0.5286 - val_loss: 0.6625 - val_accuracy: 0.6569\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7002 - accuracy: 0.5504 - val_loss: 0.6564 - val_accuracy: 0.6543\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5671 - val_loss: 0.6543 - val_accuracy: 0.6658\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6815 - accuracy: 0.5703 - val_loss: 0.6532 - val_accuracy: 0.6607\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6781 - accuracy: 0.5777 - val_loss: 0.6512 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6723 - accuracy: 0.5829 - val_loss: 0.6521 - val_accuracy: 0.6569\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5814 - val_loss: 0.6501 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5923 - val_loss: 0.6457 - val_accuracy: 0.6607\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5845 - val_loss: 0.6475 - val_accuracy: 0.6556\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6005 - val_loss: 0.6432 - val_accuracy: 0.6620\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5916 - val_loss: 0.6459 - val_accuracy: 0.6594\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5988 - val_loss: 0.6425 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5917 - val_loss: 0.6374 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6022 - val_loss: 0.6344 - val_accuracy: 0.6862\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5985 - val_loss: 0.6396 - val_accuracy: 0.6684\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6045 - val_loss: 0.6358 - val_accuracy: 0.6837\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6029 - val_loss: 0.6379 - val_accuracy: 0.6773\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.6000 - val_loss: 0.6319 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6640 - accuracy: 0.6027 - val_loss: 0.6428 - val_accuracy: 0.6620\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6154 - val_loss: 0.6423 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6137 - val_loss: 0.6353 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6107 - val_loss: 0.6363 - val_accuracy: 0.6684\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6591 - accuracy: 0.6154 - val_loss: 0.6342 - val_accuracy: 0.6811\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6047 - val_loss: 0.6362 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6158 - val_loss: 0.6428 - val_accuracy: 0.6620\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6150 - val_loss: 0.6385 - val_accuracy: 0.6658\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6182 - val_loss: 0.6351 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6183 - val_loss: 0.6325 - val_accuracy: 0.6722\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6223 - val_loss: 0.6326 - val_accuracy: 0.6773\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6207 - val_loss: 0.6303 - val_accuracy: 0.6811\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6269 - val_loss: 0.6303 - val_accuracy: 0.6811\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6555 - accuracy: 0.6196 - val_loss: 0.6278 - val_accuracy: 0.6837\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6171 - val_loss: 0.6338 - val_accuracy: 0.6658\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6201 - val_loss: 0.6317 - val_accuracy: 0.6760\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6211 - val_loss: 0.6287 - val_accuracy: 0.6862\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6323 - val_accuracy: 0.6773\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6262 - val_loss: 0.6289 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6281 - val_loss: 0.6266 - val_accuracy: 0.6849\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6262 - val_loss: 0.6279 - val_accuracy: 0.6875\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6233 - val_loss: 0.6265 - val_accuracy: 0.6901\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6482 - accuracy: 0.6285 - val_loss: 0.6287 - val_accuracy: 0.6862\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6329 - val_loss: 0.6276 - val_accuracy: 0.6773\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6500 - accuracy: 0.6297 - val_loss: 0.6271 - val_accuracy: 0.6901\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6306 - val_loss: 0.6338 - val_accuracy: 0.6798\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6304 - val_loss: 0.6280 - val_accuracy: 0.6849\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6402 - val_loss: 0.6270 - val_accuracy: 0.6862\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6320 - val_loss: 0.6254 - val_accuracy: 0.6875\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6326 - val_loss: 0.6271 - val_accuracy: 0.6990\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6358 - val_loss: 0.6243 - val_accuracy: 0.6913\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6271 - val_loss: 0.6240 - val_accuracy: 0.6811\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6404 - val_loss: 0.6216 - val_accuracy: 0.6875\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6326 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6366 - val_loss: 0.6209 - val_accuracy: 0.6811\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6366 - val_loss: 0.6210 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6319 - val_loss: 0.6229 - val_accuracy: 0.6837\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6351 - val_loss: 0.6194 - val_accuracy: 0.6926\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6393 - val_loss: 0.6243 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6408 - accuracy: 0.6323 - val_loss: 0.6242 - val_accuracy: 0.6837\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6420 - val_loss: 0.6205 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6377 - val_loss: 0.6215 - val_accuracy: 0.6811\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6403 - val_loss: 0.6235 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6482 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6487 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6393 - accuracy: 0.6402 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6448 - val_loss: 0.6288 - val_accuracy: 0.6645\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6390 - val_loss: 0.6194 - val_accuracy: 0.6875\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6319 - accuracy: 0.6517 - val_loss: 0.6157 - val_accuracy: 0.6926\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6458 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6469 - val_loss: 0.6209 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6480 - val_loss: 0.6153 - val_accuracy: 0.6926\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6447 - val_loss: 0.6189 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6518 - val_loss: 0.6204 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6487 - val_loss: 0.6195 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6550 - val_loss: 0.6179 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6546 - val_loss: 0.6150 - val_accuracy: 0.6952\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6522 - val_loss: 0.6180 - val_accuracy: 0.6990\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6500 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6501 - val_loss: 0.6175 - val_accuracy: 0.6926\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6535 - val_loss: 0.6196 - val_accuracy: 0.6837\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6545 - val_loss: 0.6185 - val_accuracy: 0.6926\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6523 - val_loss: 0.6130 - val_accuracy: 0.6977\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6541 - val_loss: 0.6188 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6560 - val_loss: 0.6159 - val_accuracy: 0.6875\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6508 - val_loss: 0.6180 - val_accuracy: 0.6747\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6259 - accuracy: 0.6518 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6557 - val_loss: 0.6150 - val_accuracy: 0.6939\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6552 - val_loss: 0.6159 - val_accuracy: 0.6901\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6520 - val_loss: 0.6132 - val_accuracy: 0.6964\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6644 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6575 - val_loss: 0.6153 - val_accuracy: 0.6939\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6604 - val_loss: 0.6149 - val_accuracy: 0.6939\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6611 - val_loss: 0.6161 - val_accuracy: 0.6926\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6628 - val_loss: 0.6178 - val_accuracy: 0.6837\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6613 - val_loss: 0.6177 - val_accuracy: 0.6837\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6629 - val_loss: 0.6145 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6631 - val_loss: 0.6155 - val_accuracy: 0.6888\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6554 - val_loss: 0.6154 - val_accuracy: 0.6849\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6225 - accuracy: 0.6647 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6640 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6577 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6179 - accuracy: 0.6683 - val_loss: 0.6148 - val_accuracy: 0.6901\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6228 - accuracy: 0.6633 - val_loss: 0.6156 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6658 - val_loss: 0.6180 - val_accuracy: 0.6875\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6667 - val_loss: 0.6152 - val_accuracy: 0.6811\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6717 - val_loss: 0.6155 - val_accuracy: 0.6849\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6693 - val_loss: 0.6168 - val_accuracy: 0.6849\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6624 - val_loss: 0.6174 - val_accuracy: 0.6837\n",
      "Calculating for: 750 50 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_320 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8224 - accuracy: 0.5231 - val_loss: 0.6602 - val_accuracy: 0.6390\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7400 - accuracy: 0.5262 - val_loss: 0.6585 - val_accuracy: 0.6441\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7060 - accuracy: 0.5377 - val_loss: 0.6631 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5456 - val_loss: 0.6551 - val_accuracy: 0.6454\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5522 - val_loss: 0.6593 - val_accuracy: 0.6352\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5496 - val_loss: 0.6585 - val_accuracy: 0.6467\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.5666 - val_loss: 0.6572 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5676 - val_loss: 0.6502 - val_accuracy: 0.6416\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6795 - accuracy: 0.5667 - val_loss: 0.6545 - val_accuracy: 0.6531\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6792 - accuracy: 0.5688 - val_loss: 0.6520 - val_accuracy: 0.6505\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6793 - accuracy: 0.5656 - val_loss: 0.6557 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5776 - val_loss: 0.6454 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6817 - accuracy: 0.5695 - val_loss: 0.6523 - val_accuracy: 0.6543\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6746 - accuracy: 0.5845 - val_loss: 0.6466 - val_accuracy: 0.6556\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5756 - val_loss: 0.6457 - val_accuracy: 0.6531\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6722 - accuracy: 0.5813 - val_loss: 0.6442 - val_accuracy: 0.6709\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5737 - val_loss: 0.6449 - val_accuracy: 0.6696\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5838 - val_loss: 0.6485 - val_accuracy: 0.6607\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6712 - accuracy: 0.5904 - val_loss: 0.6421 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6717 - accuracy: 0.5928 - val_loss: 0.6426 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.5894 - val_loss: 0.6485 - val_accuracy: 0.6505\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6705 - accuracy: 0.5884 - val_loss: 0.6455 - val_accuracy: 0.6633\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5869 - val_loss: 0.6434 - val_accuracy: 0.6671\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.5944 - val_loss: 0.6404 - val_accuracy: 0.6696\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6691 - accuracy: 0.5943 - val_loss: 0.6391 - val_accuracy: 0.6684\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5922 - val_loss: 0.6436 - val_accuracy: 0.6658\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5929 - val_loss: 0.6422 - val_accuracy: 0.6671\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5990 - val_loss: 0.6447 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5948 - val_loss: 0.6388 - val_accuracy: 0.6722\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6693 - accuracy: 0.5929 - val_loss: 0.6406 - val_accuracy: 0.6722\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.6012 - val_loss: 0.6399 - val_accuracy: 0.6633\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.6009 - val_loss: 0.6391 - val_accuracy: 0.6607\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6683 - accuracy: 0.5968 - val_loss: 0.6372 - val_accuracy: 0.6658\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.5941 - val_loss: 0.6379 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6015 - val_loss: 0.6353 - val_accuracy: 0.6658\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6078 - val_loss: 0.6398 - val_accuracy: 0.6658\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6093 - val_loss: 0.6358 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6034 - val_loss: 0.6384 - val_accuracy: 0.6658\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6630 - accuracy: 0.6109 - val_loss: 0.6343 - val_accuracy: 0.6722\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6638 - accuracy: 0.5997 - val_loss: 0.6386 - val_accuracy: 0.6620\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6631 - accuracy: 0.6068 - val_loss: 0.6337 - val_accuracy: 0.6671\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6084 - val_loss: 0.6341 - val_accuracy: 0.6645\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6004 - val_loss: 0.6342 - val_accuracy: 0.6658\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6599 - accuracy: 0.6120 - val_loss: 0.6358 - val_accuracy: 0.6671\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6576 - accuracy: 0.6181 - val_loss: 0.6354 - val_accuracy: 0.6696\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6576 - accuracy: 0.6138 - val_loss: 0.6336 - val_accuracy: 0.6645\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6076 - val_loss: 0.6346 - val_accuracy: 0.6722\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6108 - val_loss: 0.6358 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6583 - accuracy: 0.6177 - val_loss: 0.6334 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6606 - accuracy: 0.6129 - val_loss: 0.6313 - val_accuracy: 0.6747\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6117 - val_loss: 0.6339 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6600 - accuracy: 0.6098 - val_loss: 0.6316 - val_accuracy: 0.6633\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6159 - val_loss: 0.6289 - val_accuracy: 0.6722\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6571 - accuracy: 0.6127 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6267 - val_loss: 0.6303 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6071 - val_loss: 0.6359 - val_accuracy: 0.6696\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6186 - val_loss: 0.6268 - val_accuracy: 0.6709\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6112 - val_loss: 0.6344 - val_accuracy: 0.6760\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6187 - val_loss: 0.6255 - val_accuracy: 0.6735\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6162 - val_loss: 0.6296 - val_accuracy: 0.6773\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6242 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6197 - val_loss: 0.6301 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6230 - val_loss: 0.6283 - val_accuracy: 0.6735\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6149 - val_loss: 0.6338 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6133 - val_loss: 0.6322 - val_accuracy: 0.6811\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6208 - val_loss: 0.6290 - val_accuracy: 0.6824\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6257 - val_loss: 0.6255 - val_accuracy: 0.6709\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6262 - val_loss: 0.6297 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6196 - val_loss: 0.6292 - val_accuracy: 0.6837\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6238 - val_loss: 0.6273 - val_accuracy: 0.6760\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6183 - val_loss: 0.6338 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6194 - val_loss: 0.6309 - val_accuracy: 0.6722\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6230 - val_loss: 0.6283 - val_accuracy: 0.6811\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6262 - val_loss: 0.6233 - val_accuracy: 0.6773\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6201 - val_loss: 0.6256 - val_accuracy: 0.6875\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6247 - val_loss: 0.6306 - val_accuracy: 0.6773\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6267 - val_loss: 0.6277 - val_accuracy: 0.6824\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6264 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6287 - val_loss: 0.6239 - val_accuracy: 0.6824\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6257 - val_loss: 0.6251 - val_accuracy: 0.6824\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6305 - val_loss: 0.6251 - val_accuracy: 0.6837\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6335 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6506 - accuracy: 0.6305 - val_loss: 0.6253 - val_accuracy: 0.6837\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6336 - val_loss: 0.6221 - val_accuracy: 0.6849\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6296 - val_loss: 0.6221 - val_accuracy: 0.6849\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6286 - val_loss: 0.6267 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6296 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6275 - val_loss: 0.6237 - val_accuracy: 0.6849\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6334 - val_loss: 0.6214 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6265 - val_loss: 0.6235 - val_accuracy: 0.6837\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6325 - val_loss: 0.6256 - val_accuracy: 0.6798\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6306 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6338 - val_loss: 0.6248 - val_accuracy: 0.6786\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6407 - val_loss: 0.6238 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6364 - val_loss: 0.6209 - val_accuracy: 0.6913\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6494 - accuracy: 0.6330 - val_loss: 0.6229 - val_accuracy: 0.6862\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6384 - val_loss: 0.6203 - val_accuracy: 0.6901\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6324 - val_loss: 0.6212 - val_accuracy: 0.6901\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6339 - val_loss: 0.6192 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6445 - accuracy: 0.6321 - val_loss: 0.6250 - val_accuracy: 0.6824\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6355 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6355 - val_loss: 0.6199 - val_accuracy: 0.6760\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6410 - accuracy: 0.6379 - val_loss: 0.6160 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6479 - accuracy: 0.6333 - val_loss: 0.6218 - val_accuracy: 0.6849\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6418 - val_loss: 0.6236 - val_accuracy: 0.6684\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6405 - val_loss: 0.6222 - val_accuracy: 0.6862\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6319 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6390 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6453 - val_loss: 0.6166 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6370 - val_loss: 0.6208 - val_accuracy: 0.6811\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6427 - accuracy: 0.6366 - val_loss: 0.6197 - val_accuracy: 0.6811\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6403 - val_loss: 0.6201 - val_accuracy: 0.6798\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6338 - val_loss: 0.6188 - val_accuracy: 0.6837\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6395 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6452 - val_loss: 0.6172 - val_accuracy: 0.6773\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6404 - val_loss: 0.6196 - val_accuracy: 0.6849\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6463 - val_loss: 0.6212 - val_accuracy: 0.6862\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6428 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6477 - val_loss: 0.6177 - val_accuracy: 0.6824\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6443 - val_loss: 0.6200 - val_accuracy: 0.6824\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6420 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6452 - val_loss: 0.6172 - val_accuracy: 0.6875\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6366 - val_loss: 0.6192 - val_accuracy: 0.6849\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6511 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6441 - val_loss: 0.6147 - val_accuracy: 0.6849\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6452 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6439 - val_loss: 0.6147 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6481 - val_loss: 0.6128 - val_accuracy: 0.6888\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6531 - val_loss: 0.6143 - val_accuracy: 0.6837\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6478 - val_loss: 0.6166 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6453 - val_loss: 0.6170 - val_accuracy: 0.6913\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6375 - val_loss: 0.6150 - val_accuracy: 0.6875\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6490 - val_loss: 0.6133 - val_accuracy: 0.6837\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6413 - val_loss: 0.6148 - val_accuracy: 0.6913\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6536 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6459 - val_loss: 0.6163 - val_accuracy: 0.6913\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6483 - val_loss: 0.6141 - val_accuracy: 0.6901\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6456 - val_loss: 0.6181 - val_accuracy: 0.6964\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6501 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6354 - accuracy: 0.6488 - val_loss: 0.6135 - val_accuracy: 0.6901\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6500 - val_loss: 0.6141 - val_accuracy: 0.6952\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6468 - val_loss: 0.6134 - val_accuracy: 0.6990\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6513 - val_loss: 0.6167 - val_accuracy: 0.6913\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6486 - val_loss: 0.6201 - val_accuracy: 0.6901\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6472 - val_loss: 0.6157 - val_accuracy: 0.6990\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6463 - val_loss: 0.6118 - val_accuracy: 0.6913\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6492 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6503 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6510 - val_loss: 0.6114 - val_accuracy: 0.6939\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6506 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6560 - val_loss: 0.6103 - val_accuracy: 0.6977\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6531 - val_loss: 0.6166 - val_accuracy: 0.6926\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6555 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6535 - val_loss: 0.6135 - val_accuracy: 0.6939\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6590 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6567 - val_loss: 0.6104 - val_accuracy: 0.6913\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6595 - val_loss: 0.6117 - val_accuracy: 0.6901\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6544 - val_loss: 0.6088 - val_accuracy: 0.6888\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6527 - val_loss: 0.6154 - val_accuracy: 0.6837\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6554 - val_loss: 0.6106 - val_accuracy: 0.6888\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6544 - val_loss: 0.6143 - val_accuracy: 0.6888\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6546 - val_loss: 0.6107 - val_accuracy: 0.6913\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6527 - val_loss: 0.6133 - val_accuracy: 0.6888\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6605 - val_loss: 0.6098 - val_accuracy: 0.6875\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6588 - val_loss: 0.6108 - val_accuracy: 0.6926\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6585 - val_loss: 0.6117 - val_accuracy: 0.6901\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6589 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6536 - val_loss: 0.6151 - val_accuracy: 0.6939\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6645 - val_loss: 0.6080 - val_accuracy: 0.6888\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6569 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6625 - val_loss: 0.6114 - val_accuracy: 0.6939\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6633 - val_loss: 0.6107 - val_accuracy: 0.6913\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6511 - val_loss: 0.6087 - val_accuracy: 0.6952\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6603 - val_loss: 0.6101 - val_accuracy: 0.6875\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6539 - val_loss: 0.6127 - val_accuracy: 0.6888\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6575 - val_loss: 0.6100 - val_accuracy: 0.6913\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6541 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6614 - val_loss: 0.6111 - val_accuracy: 0.6926\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6491 - val_loss: 0.6143 - val_accuracy: 0.6862\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6574 - val_loss: 0.6135 - val_accuracy: 0.6849\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6667 - val_loss: 0.6111 - val_accuracy: 0.6913\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6571 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6605 - val_loss: 0.6144 - val_accuracy: 0.6849\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6562 - val_loss: 0.6151 - val_accuracy: 0.6901\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6546 - val_loss: 0.6125 - val_accuracy: 0.6952\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6576 - val_loss: 0.6116 - val_accuracy: 0.6901\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6584 - val_loss: 0.6082 - val_accuracy: 0.6901\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6667 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6658 - val_loss: 0.6115 - val_accuracy: 0.6901\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6585 - val_loss: 0.6131 - val_accuracy: 0.6926\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6677 - val_loss: 0.6135 - val_accuracy: 0.6862\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6635 - val_loss: 0.6124 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6640 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6702 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6652 - val_loss: 0.6087 - val_accuracy: 0.6875\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6581 - val_loss: 0.6115 - val_accuracy: 0.6913\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6594 - val_loss: 0.6118 - val_accuracy: 0.6939\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6601 - val_loss: 0.6096 - val_accuracy: 0.6926\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6584 - val_loss: 0.6118 - val_accuracy: 0.7028\n",
      "Calculating for: 750 100 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_324 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8121 - accuracy: 0.5428 - val_loss: 0.6473 - val_accuracy: 0.6518\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7294 - accuracy: 0.5534 - val_loss: 0.6378 - val_accuracy: 0.6607\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.5672 - val_loss: 0.6415 - val_accuracy: 0.6671\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5786 - val_loss: 0.6410 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5836 - val_loss: 0.6384 - val_accuracy: 0.6633\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5955 - val_loss: 0.6428 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6656 - accuracy: 0.5978 - val_loss: 0.6379 - val_accuracy: 0.6658\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6086 - val_loss: 0.6423 - val_accuracy: 0.6658\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6040 - val_loss: 0.6360 - val_accuracy: 0.6671\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6091 - val_loss: 0.6357 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6598 - accuracy: 0.6086 - val_loss: 0.6374 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6578 - accuracy: 0.6133 - val_loss: 0.6359 - val_accuracy: 0.6709\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6150 - val_loss: 0.6370 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6207 - val_loss: 0.6310 - val_accuracy: 0.6735\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6144 - val_loss: 0.6267 - val_accuracy: 0.6824\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6157 - val_loss: 0.6302 - val_accuracy: 0.6824\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6479 - accuracy: 0.6236 - val_loss: 0.6278 - val_accuracy: 0.6849\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6168 - val_loss: 0.6304 - val_accuracy: 0.6798\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6261 - val_loss: 0.6314 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6289 - val_loss: 0.6295 - val_accuracy: 0.6747\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6248 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6274 - val_loss: 0.6247 - val_accuracy: 0.6773\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6286 - val_loss: 0.6256 - val_accuracy: 0.6837\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6320 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6290 - val_loss: 0.6279 - val_accuracy: 0.6735\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6305 - val_loss: 0.6271 - val_accuracy: 0.6760\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6378 - val_loss: 0.6251 - val_accuracy: 0.6811\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6419 - accuracy: 0.6316 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6392 - accuracy: 0.6378 - val_loss: 0.6224 - val_accuracy: 0.6824\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6390 - val_loss: 0.6228 - val_accuracy: 0.6837\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6304 - val_loss: 0.6225 - val_accuracy: 0.6837\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6377 - val_loss: 0.6207 - val_accuracy: 0.6798\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6394 - val_loss: 0.6225 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6510 - val_loss: 0.6194 - val_accuracy: 0.6837\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6466 - val_loss: 0.6221 - val_accuracy: 0.6735\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6395 - val_loss: 0.6227 - val_accuracy: 0.6786\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6424 - val_loss: 0.6216 - val_accuracy: 0.6798\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6477 - val_loss: 0.6189 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6433 - val_loss: 0.6179 - val_accuracy: 0.6773\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6482 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6545 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6487 - val_loss: 0.6245 - val_accuracy: 0.6747\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6547 - val_loss: 0.6223 - val_accuracy: 0.6735\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6535 - val_loss: 0.6188 - val_accuracy: 0.6786\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6547 - val_loss: 0.6182 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6520 - val_loss: 0.6193 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6495 - val_loss: 0.6210 - val_accuracy: 0.6773\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6520 - val_loss: 0.6185 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6590 - val_loss: 0.6190 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6647 - val_loss: 0.6217 - val_accuracy: 0.6735\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6581 - val_loss: 0.6168 - val_accuracy: 0.6786\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6660 - val_loss: 0.6220 - val_accuracy: 0.6773\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6609 - val_loss: 0.6239 - val_accuracy: 0.6747\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6625 - val_loss: 0.6197 - val_accuracy: 0.6773\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6177 - accuracy: 0.6576 - val_loss: 0.6248 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6137 - accuracy: 0.6590 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6615 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6621 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6637 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6724 - val_loss: 0.6253 - val_accuracy: 0.6735\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6693 - val_loss: 0.6181 - val_accuracy: 0.6735\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6106 - accuracy: 0.6691 - val_loss: 0.6189 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6120 - accuracy: 0.6652 - val_loss: 0.6207 - val_accuracy: 0.6747\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6090 - accuracy: 0.6747 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6083 - accuracy: 0.6768 - val_loss: 0.6245 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6083 - accuracy: 0.6740 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6092 - accuracy: 0.6740 - val_loss: 0.6221 - val_accuracy: 0.6735\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6716 - val_loss: 0.6205 - val_accuracy: 0.6773\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6746 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6093 - accuracy: 0.6707 - val_loss: 0.6265 - val_accuracy: 0.6594\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6025 - accuracy: 0.6762 - val_loss: 0.6226 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6038 - accuracy: 0.6768 - val_loss: 0.6252 - val_accuracy: 0.6696\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6066 - accuracy: 0.6707 - val_loss: 0.6218 - val_accuracy: 0.6722\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6796 - val_loss: 0.6233 - val_accuracy: 0.6773\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5997 - accuracy: 0.6822 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5982 - accuracy: 0.6807 - val_loss: 0.6288 - val_accuracy: 0.6633\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5986 - accuracy: 0.6806 - val_loss: 0.6276 - val_accuracy: 0.6658\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6032 - accuracy: 0.6804 - val_loss: 0.6265 - val_accuracy: 0.6722\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6008 - accuracy: 0.6817 - val_loss: 0.6243 - val_accuracy: 0.6760\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6001 - accuracy: 0.6855 - val_loss: 0.6291 - val_accuracy: 0.6760\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6022 - accuracy: 0.6796 - val_loss: 0.6253 - val_accuracy: 0.6824\n",
      "Calculating for: 750 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_328 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8021 - accuracy: 0.5197 - val_loss: 0.6681 - val_accuracy: 0.6416\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7135 - accuracy: 0.5467 - val_loss: 0.6592 - val_accuracy: 0.6518\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5574 - val_loss: 0.6618 - val_accuracy: 0.6390\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6837 - accuracy: 0.5673 - val_loss: 0.6586 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.5671 - val_loss: 0.6620 - val_accuracy: 0.6390\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6765 - accuracy: 0.5850 - val_loss: 0.6543 - val_accuracy: 0.6543\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5841 - val_loss: 0.6507 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5814 - val_loss: 0.6519 - val_accuracy: 0.6454\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5893 - val_loss: 0.6546 - val_accuracy: 0.6543\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5899 - val_loss: 0.6538 - val_accuracy: 0.6594\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5948 - val_loss: 0.6539 - val_accuracy: 0.6492\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.5927 - val_loss: 0.6502 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5953 - val_loss: 0.6492 - val_accuracy: 0.6492\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.5986 - val_loss: 0.6444 - val_accuracy: 0.6671\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6616 - accuracy: 0.6042 - val_loss: 0.6403 - val_accuracy: 0.6735\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6143 - val_loss: 0.6469 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6099 - val_loss: 0.6493 - val_accuracy: 0.6518\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6063 - val_loss: 0.6463 - val_accuracy: 0.6582\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6052 - val_loss: 0.6456 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6607 - accuracy: 0.6039 - val_loss: 0.6400 - val_accuracy: 0.6684\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6591 - accuracy: 0.6109 - val_loss: 0.6472 - val_accuracy: 0.6492\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6178 - val_loss: 0.6457 - val_accuracy: 0.6531\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6132 - val_loss: 0.6427 - val_accuracy: 0.6582\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6129 - val_loss: 0.6435 - val_accuracy: 0.6607\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6177 - val_loss: 0.6416 - val_accuracy: 0.6645\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6139 - val_loss: 0.6346 - val_accuracy: 0.6773\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6144 - val_loss: 0.6424 - val_accuracy: 0.6518\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6138 - val_loss: 0.6363 - val_accuracy: 0.6684\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6173 - val_loss: 0.6379 - val_accuracy: 0.6645\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6235 - val_loss: 0.6352 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6198 - val_loss: 0.6332 - val_accuracy: 0.6786\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6243 - val_loss: 0.6414 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6150 - val_loss: 0.6386 - val_accuracy: 0.6633\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6250 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6207 - val_loss: 0.6310 - val_accuracy: 0.6773\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6222 - val_loss: 0.6316 - val_accuracy: 0.6747\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6284 - val_loss: 0.6345 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6262 - val_loss: 0.6293 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6310 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6368 - val_loss: 0.6354 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6240 - val_loss: 0.6306 - val_accuracy: 0.6798\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6302 - val_loss: 0.6357 - val_accuracy: 0.6658\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6291 - val_loss: 0.6343 - val_accuracy: 0.6722\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6272 - val_loss: 0.6350 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6354 - val_loss: 0.6371 - val_accuracy: 0.6582\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6351 - val_loss: 0.6317 - val_accuracy: 0.6760\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6310 - val_loss: 0.6329 - val_accuracy: 0.6760\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6359 - val_loss: 0.6326 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6336 - val_loss: 0.6320 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6306 - val_loss: 0.6264 - val_accuracy: 0.6798\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6359 - val_loss: 0.6293 - val_accuracy: 0.6747\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6412 - val_loss: 0.6304 - val_accuracy: 0.6760\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6292 - val_loss: 0.6249 - val_accuracy: 0.6837\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6404 - accuracy: 0.6398 - val_loss: 0.6305 - val_accuracy: 0.6747\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6301 - val_loss: 0.6345 - val_accuracy: 0.6709\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6372 - val_loss: 0.6322 - val_accuracy: 0.6684\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6384 - val_loss: 0.6270 - val_accuracy: 0.6696\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6389 - val_loss: 0.6233 - val_accuracy: 0.6786\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6355 - val_loss: 0.6294 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6423 - val_loss: 0.6217 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6365 - val_loss: 0.6238 - val_accuracy: 0.6747\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6429 - val_loss: 0.6278 - val_accuracy: 0.6709\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6354 - val_loss: 0.6244 - val_accuracy: 0.6773\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6433 - val_loss: 0.6314 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6433 - val_loss: 0.6276 - val_accuracy: 0.6709\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6442 - val_loss: 0.6313 - val_accuracy: 0.6709\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6463 - val_loss: 0.6277 - val_accuracy: 0.6722\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6517 - val_loss: 0.6233 - val_accuracy: 0.6722\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6481 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6345 - accuracy: 0.6477 - val_loss: 0.6282 - val_accuracy: 0.6735\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6448 - val_loss: 0.6285 - val_accuracy: 0.6735\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6518 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6473 - val_loss: 0.6239 - val_accuracy: 0.6735\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6520 - val_loss: 0.6219 - val_accuracy: 0.6709\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6550 - val_loss: 0.6230 - val_accuracy: 0.6760\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6510 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6476 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6506 - val_loss: 0.6269 - val_accuracy: 0.6773\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6575 - val_loss: 0.6179 - val_accuracy: 0.6824\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6539 - val_loss: 0.6228 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6508 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6531 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6611 - val_loss: 0.6252 - val_accuracy: 0.6773\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6559 - val_loss: 0.6238 - val_accuracy: 0.6760\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6549 - val_loss: 0.6236 - val_accuracy: 0.6722\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6551 - val_loss: 0.6296 - val_accuracy: 0.6735\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6566 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6522 - val_loss: 0.6256 - val_accuracy: 0.6747\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6566 - val_loss: 0.6251 - val_accuracy: 0.6722\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6541 - val_loss: 0.6207 - val_accuracy: 0.6722\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6520 - val_loss: 0.6279 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6680 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6188 - accuracy: 0.6555 - val_loss: 0.6246 - val_accuracy: 0.6798\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6626 - val_loss: 0.6194 - val_accuracy: 0.6747\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6529 - val_loss: 0.6283 - val_accuracy: 0.6798\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6583 - val_loss: 0.6209 - val_accuracy: 0.6786\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6590 - val_loss: 0.6221 - val_accuracy: 0.6786\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6588 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6643 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6629 - val_loss: 0.6235 - val_accuracy: 0.6798\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6648 - val_loss: 0.6221 - val_accuracy: 0.6773\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6598 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6615 - val_loss: 0.6215 - val_accuracy: 0.6760\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6219 - accuracy: 0.6613 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6214 - accuracy: 0.6631 - val_loss: 0.6235 - val_accuracy: 0.6773\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6609 - val_loss: 0.6240 - val_accuracy: 0.6824\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6618 - val_loss: 0.6174 - val_accuracy: 0.6837\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6623 - val_loss: 0.6266 - val_accuracy: 0.6773\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6598 - val_loss: 0.6220 - val_accuracy: 0.6798\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6614 - val_loss: 0.6232 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6618 - val_loss: 0.6261 - val_accuracy: 0.6747\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6663 - val_loss: 0.6242 - val_accuracy: 0.6773\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6117 - accuracy: 0.6728 - val_loss: 0.6323 - val_accuracy: 0.6760\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6134 - accuracy: 0.6677 - val_loss: 0.6290 - val_accuracy: 0.6709\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6122 - accuracy: 0.6729 - val_loss: 0.6258 - val_accuracy: 0.6735\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6731 - val_loss: 0.6322 - val_accuracy: 0.6735\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6121 - accuracy: 0.6719 - val_loss: 0.6296 - val_accuracy: 0.6696\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6152 - accuracy: 0.6688 - val_loss: 0.6272 - val_accuracy: 0.6773\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6084 - accuracy: 0.6740 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6148 - accuracy: 0.6670 - val_loss: 0.6263 - val_accuracy: 0.6786\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6757 - val_loss: 0.6280 - val_accuracy: 0.6773\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6649 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6775 - val_loss: 0.6282 - val_accuracy: 0.6722\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6108 - accuracy: 0.6729 - val_loss: 0.6295 - val_accuracy: 0.6786\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6084 - accuracy: 0.6734 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6086 - accuracy: 0.6763 - val_loss: 0.6242 - val_accuracy: 0.6798\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6740 - val_loss: 0.6276 - val_accuracy: 0.6798\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6072 - accuracy: 0.6762 - val_loss: 0.6287 - val_accuracy: 0.6837\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6083 - accuracy: 0.6738 - val_loss: 0.6303 - val_accuracy: 0.6684\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6057 - accuracy: 0.6772 - val_loss: 0.6261 - val_accuracy: 0.6824\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6018 - accuracy: 0.6826 - val_loss: 0.6247 - val_accuracy: 0.6837\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6094 - accuracy: 0.6726 - val_loss: 0.6235 - val_accuracy: 0.6849\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6067 - accuracy: 0.6746 - val_loss: 0.6232 - val_accuracy: 0.6862\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6041 - accuracy: 0.6809 - val_loss: 0.6283 - val_accuracy: 0.6824\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6056 - accuracy: 0.6787 - val_loss: 0.6248 - val_accuracy: 0.6824\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6080 - accuracy: 0.6766 - val_loss: 0.6248 - val_accuracy: 0.6824\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6037 - accuracy: 0.6766 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Calculating for: 750 100 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_332 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8436 - accuracy: 0.5099 - val_loss: 0.6710 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7321 - accuracy: 0.5328 - val_loss: 0.6658 - val_accuracy: 0.6454\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6997 - accuracy: 0.5349 - val_loss: 0.6622 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6882 - accuracy: 0.5525 - val_loss: 0.6644 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.5593 - val_loss: 0.6621 - val_accuracy: 0.6645\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.5568 - val_loss: 0.6627 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5657 - val_loss: 0.6628 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6844 - accuracy: 0.5587 - val_loss: 0.6604 - val_accuracy: 0.6569\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5629 - val_loss: 0.6609 - val_accuracy: 0.6594\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6808 - accuracy: 0.5687 - val_loss: 0.6564 - val_accuracy: 0.6594\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5732 - val_loss: 0.6533 - val_accuracy: 0.6671\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.5723 - val_loss: 0.6529 - val_accuracy: 0.6620\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.5767 - val_loss: 0.6493 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.5869 - val_loss: 0.6519 - val_accuracy: 0.6658\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5782 - val_loss: 0.6512 - val_accuracy: 0.6671\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5796 - val_loss: 0.6513 - val_accuracy: 0.6569\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5809 - val_loss: 0.6464 - val_accuracy: 0.6658\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6730 - accuracy: 0.5831 - val_loss: 0.6491 - val_accuracy: 0.6556\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6710 - accuracy: 0.5970 - val_loss: 0.6526 - val_accuracy: 0.6531\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5904 - val_loss: 0.6424 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5864 - val_loss: 0.6468 - val_accuracy: 0.6543\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.6010 - val_loss: 0.6445 - val_accuracy: 0.6543\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5948 - val_loss: 0.6484 - val_accuracy: 0.6518\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6667 - accuracy: 0.5972 - val_loss: 0.6433 - val_accuracy: 0.6607\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5950 - val_loss: 0.6466 - val_accuracy: 0.6569\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6017 - val_loss: 0.6462 - val_accuracy: 0.6633\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5955 - val_loss: 0.6407 - val_accuracy: 0.6633\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6032 - val_loss: 0.6477 - val_accuracy: 0.6607\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6069 - val_loss: 0.6419 - val_accuracy: 0.6594\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5966 - val_loss: 0.6411 - val_accuracy: 0.6633\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6027 - val_loss: 0.6430 - val_accuracy: 0.6620\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6010 - val_loss: 0.6445 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6086 - val_loss: 0.6364 - val_accuracy: 0.6658\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6073 - val_loss: 0.6401 - val_accuracy: 0.6684\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6105 - val_loss: 0.6350 - val_accuracy: 0.6735\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6054 - val_loss: 0.6408 - val_accuracy: 0.6645\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6027 - val_loss: 0.6412 - val_accuracy: 0.6760\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6089 - val_loss: 0.6412 - val_accuracy: 0.6645\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6105 - val_loss: 0.6393 - val_accuracy: 0.6671\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6138 - val_loss: 0.6362 - val_accuracy: 0.6709\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6104 - val_loss: 0.6350 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6164 - val_loss: 0.6351 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6595 - accuracy: 0.6172 - val_loss: 0.6382 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6095 - val_loss: 0.6361 - val_accuracy: 0.6786\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6154 - val_loss: 0.6403 - val_accuracy: 0.6722\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6129 - val_loss: 0.6383 - val_accuracy: 0.6722\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6166 - val_loss: 0.6358 - val_accuracy: 0.6747\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6118 - val_loss: 0.6479 - val_accuracy: 0.6276\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6135 - val_loss: 0.6366 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6133 - val_loss: 0.6340 - val_accuracy: 0.6837\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6198 - val_loss: 0.6433 - val_accuracy: 0.6416\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6139 - val_loss: 0.6323 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6235 - val_loss: 0.6375 - val_accuracy: 0.6658\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6162 - val_loss: 0.6313 - val_accuracy: 0.6773\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6178 - val_loss: 0.6323 - val_accuracy: 0.6798\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6109 - val_loss: 0.6342 - val_accuracy: 0.6811\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6212 - val_loss: 0.6292 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6179 - val_loss: 0.6363 - val_accuracy: 0.6620\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6257 - val_loss: 0.6307 - val_accuracy: 0.6811\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6245 - val_loss: 0.6315 - val_accuracy: 0.6837\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6206 - val_loss: 0.6431 - val_accuracy: 0.6416\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6199 - val_loss: 0.6369 - val_accuracy: 0.6620\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6246 - val_loss: 0.6315 - val_accuracy: 0.6773\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6227 - val_loss: 0.6358 - val_accuracy: 0.6531\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6230 - val_loss: 0.6313 - val_accuracy: 0.6786\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6258 - val_loss: 0.6250 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6267 - val_loss: 0.6288 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6290 - val_loss: 0.6264 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6250 - val_loss: 0.6284 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6285 - val_loss: 0.6284 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6192 - val_loss: 0.6296 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6198 - val_loss: 0.6339 - val_accuracy: 0.6684\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6284 - val_loss: 0.6332 - val_accuracy: 0.6684\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6290 - val_loss: 0.6284 - val_accuracy: 0.6811\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6285 - val_loss: 0.6325 - val_accuracy: 0.6722\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.6329 - val_loss: 0.6322 - val_accuracy: 0.6760\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6328 - val_loss: 0.6284 - val_accuracy: 0.6798\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6292 - val_loss: 0.6323 - val_accuracy: 0.6684\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6339 - val_loss: 0.6249 - val_accuracy: 0.6811\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6307 - val_loss: 0.6268 - val_accuracy: 0.6735\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6282 - val_loss: 0.6286 - val_accuracy: 0.6773\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6272 - val_loss: 0.6284 - val_accuracy: 0.6760\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6306 - val_loss: 0.6273 - val_accuracy: 0.6760\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6363 - val_loss: 0.6307 - val_accuracy: 0.6722\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6257 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6330 - val_loss: 0.6252 - val_accuracy: 0.6798\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6312 - val_loss: 0.6319 - val_accuracy: 0.6620\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6351 - val_loss: 0.6300 - val_accuracy: 0.6645\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6462 - accuracy: 0.6326 - val_loss: 0.6277 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6330 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6348 - val_loss: 0.6297 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6360 - val_loss: 0.6270 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6311 - val_loss: 0.6310 - val_accuracy: 0.6671\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6274 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6331 - val_loss: 0.6290 - val_accuracy: 0.6709\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.6315 - val_loss: 0.6254 - val_accuracy: 0.6709\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6392 - val_loss: 0.6305 - val_accuracy: 0.6645\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6353 - val_loss: 0.6285 - val_accuracy: 0.6684\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6393 - accuracy: 0.6410 - val_loss: 0.6208 - val_accuracy: 0.6735\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6411 - accuracy: 0.6359 - val_loss: 0.6276 - val_accuracy: 0.6658\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6296 - val_loss: 0.6295 - val_accuracy: 0.6684\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6493 - val_loss: 0.6307 - val_accuracy: 0.6607\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6378 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6374 - val_loss: 0.6234 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6372 - val_loss: 0.6237 - val_accuracy: 0.6747\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6421 - accuracy: 0.6387 - val_loss: 0.6270 - val_accuracy: 0.6671\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6432 - val_loss: 0.6227 - val_accuracy: 0.6824\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6456 - val_loss: 0.6253 - val_accuracy: 0.6786\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6444 - val_loss: 0.6250 - val_accuracy: 0.6747\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6451 - val_loss: 0.6214 - val_accuracy: 0.6798\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6491 - val_loss: 0.6242 - val_accuracy: 0.6773\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6410 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6380 - accuracy: 0.6437 - val_loss: 0.6195 - val_accuracy: 0.6773\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6371 - accuracy: 0.6412 - val_loss: 0.6234 - val_accuracy: 0.6773\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6449 - val_loss: 0.6226 - val_accuracy: 0.6773\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6451 - val_loss: 0.6193 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6418 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6434 - val_loss: 0.6186 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6404 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6405 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6510 - val_loss: 0.6234 - val_accuracy: 0.6798\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6471 - val_loss: 0.6202 - val_accuracy: 0.6798\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6402 - val_loss: 0.6183 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6409 - val_loss: 0.6206 - val_accuracy: 0.6811\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6478 - val_loss: 0.6168 - val_accuracy: 0.6875\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6517 - val_loss: 0.6178 - val_accuracy: 0.6875\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6449 - val_loss: 0.6180 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6466 - val_loss: 0.6210 - val_accuracy: 0.6811\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6457 - val_loss: 0.6233 - val_accuracy: 0.6811\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6457 - val_loss: 0.6171 - val_accuracy: 0.6837\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6525 - val_loss: 0.6180 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6466 - val_loss: 0.6196 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6505 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6478 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6513 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6555 - val_loss: 0.6203 - val_accuracy: 0.6798\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6507 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6493 - val_loss: 0.6197 - val_accuracy: 0.6811\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6493 - val_loss: 0.6129 - val_accuracy: 0.6913\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6467 - val_loss: 0.6212 - val_accuracy: 0.6773\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6505 - val_loss: 0.6180 - val_accuracy: 0.6824\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6467 - val_loss: 0.6156 - val_accuracy: 0.6798\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6503 - val_loss: 0.6173 - val_accuracy: 0.6798\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6512 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6515 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6574 - val_loss: 0.6221 - val_accuracy: 0.6837\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6556 - val_loss: 0.6181 - val_accuracy: 0.6811\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6566 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6537 - val_loss: 0.6207 - val_accuracy: 0.6849\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6537 - val_loss: 0.6183 - val_accuracy: 0.6875\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6540 - val_loss: 0.6156 - val_accuracy: 0.6888\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6549 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6565 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6498 - val_loss: 0.6233 - val_accuracy: 0.6786\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.6611 - val_loss: 0.6168 - val_accuracy: 0.6862\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6591 - val_loss: 0.6233 - val_accuracy: 0.6773\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6576 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6575 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6601 - val_loss: 0.6168 - val_accuracy: 0.6888\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6536 - val_loss: 0.6174 - val_accuracy: 0.6901\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6559 - val_loss: 0.6149 - val_accuracy: 0.6913\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6559 - val_loss: 0.6199 - val_accuracy: 0.6875\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6580 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6595 - val_loss: 0.6205 - val_accuracy: 0.6849\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6620 - val_loss: 0.6173 - val_accuracy: 0.6913\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6487 - val_loss: 0.6166 - val_accuracy: 0.6862\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6561 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6616 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6642 - val_loss: 0.6156 - val_accuracy: 0.6913\n",
      "Calculating for: 750 150 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_336 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8099 - accuracy: 0.5481 - val_loss: 0.6704 - val_accuracy: 0.6224\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7152 - accuracy: 0.5565 - val_loss: 0.6458 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5642 - val_loss: 0.6404 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5977 - val_loss: 0.6449 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6692 - accuracy: 0.5883 - val_loss: 0.6400 - val_accuracy: 0.6658\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6677 - accuracy: 0.6002 - val_loss: 0.6372 - val_accuracy: 0.6671\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6058 - val_loss: 0.6390 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6068 - val_loss: 0.6379 - val_accuracy: 0.6633\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6135 - val_loss: 0.6357 - val_accuracy: 0.6633\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6171 - val_loss: 0.6336 - val_accuracy: 0.6607\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6124 - val_loss: 0.6354 - val_accuracy: 0.6658\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6066 - val_loss: 0.6359 - val_accuracy: 0.6722\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6140 - val_loss: 0.6365 - val_accuracy: 0.6684\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6142 - val_loss: 0.6318 - val_accuracy: 0.6633\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6238 - val_loss: 0.6324 - val_accuracy: 0.6696\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6226 - val_loss: 0.6345 - val_accuracy: 0.6620\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6270 - val_loss: 0.6297 - val_accuracy: 0.6633\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6359 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6338 - val_loss: 0.6270 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6296 - val_loss: 0.6249 - val_accuracy: 0.6735\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6334 - val_loss: 0.6301 - val_accuracy: 0.6633\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6436 - accuracy: 0.6364 - val_loss: 0.6306 - val_accuracy: 0.6607\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6356 - val_loss: 0.6314 - val_accuracy: 0.6582\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6265 - val_loss: 0.6294 - val_accuracy: 0.6658\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6392 - val_loss: 0.6269 - val_accuracy: 0.6684\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6375 - val_loss: 0.6253 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6395 - val_loss: 0.6254 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6409 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.6420 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6334 - accuracy: 0.6461 - val_loss: 0.6229 - val_accuracy: 0.6696\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6372 - accuracy: 0.6378 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6334 - accuracy: 0.6462 - val_loss: 0.6261 - val_accuracy: 0.6735\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6490 - val_loss: 0.6266 - val_accuracy: 0.6722\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6330 - accuracy: 0.6448 - val_loss: 0.6291 - val_accuracy: 0.6607\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6471 - val_loss: 0.6247 - val_accuracy: 0.6747\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6441 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6303 - accuracy: 0.6515 - val_loss: 0.6236 - val_accuracy: 0.6684\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6521 - val_loss: 0.6244 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6518 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6530 - val_loss: 0.6307 - val_accuracy: 0.6543\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6556 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6591 - val_loss: 0.6245 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6570 - val_loss: 0.6242 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6599 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6564 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6591 - val_loss: 0.6281 - val_accuracy: 0.6684\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6206 - accuracy: 0.6589 - val_loss: 0.6252 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6630 - val_loss: 0.6223 - val_accuracy: 0.6824\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6591 - val_loss: 0.6199 - val_accuracy: 0.6760\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6614 - val_loss: 0.6239 - val_accuracy: 0.6747\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6541 - val_loss: 0.6247 - val_accuracy: 0.6709\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6659 - val_loss: 0.6223 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6570 - val_loss: 0.6236 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6590 - val_loss: 0.6248 - val_accuracy: 0.6747\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6665 - val_loss: 0.6236 - val_accuracy: 0.6798\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6668 - val_loss: 0.6252 - val_accuracy: 0.6722\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6674 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6072 - accuracy: 0.6727 - val_loss: 0.6251 - val_accuracy: 0.6709\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6667 - val_loss: 0.6310 - val_accuracy: 0.6658\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6767 - val_loss: 0.6271 - val_accuracy: 0.6684\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6108 - accuracy: 0.6688 - val_loss: 0.6238 - val_accuracy: 0.6747\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6140 - accuracy: 0.6688 - val_loss: 0.6255 - val_accuracy: 0.6671\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6659 - val_loss: 0.6229 - val_accuracy: 0.6747\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6679 - val_loss: 0.6235 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6665 - val_loss: 0.6277 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6118 - accuracy: 0.6680 - val_loss: 0.6252 - val_accuracy: 0.6696\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6096 - accuracy: 0.6721 - val_loss: 0.6264 - val_accuracy: 0.6722\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6018 - accuracy: 0.6758 - val_loss: 0.6291 - val_accuracy: 0.6684\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6041 - accuracy: 0.6783 - val_loss: 0.6283 - val_accuracy: 0.6696\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6063 - accuracy: 0.6805 - val_loss: 0.6278 - val_accuracy: 0.6696\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6726 - val_loss: 0.6247 - val_accuracy: 0.6709\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6068 - accuracy: 0.6714 - val_loss: 0.6301 - val_accuracy: 0.6620\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6085 - accuracy: 0.6742 - val_loss: 0.6293 - val_accuracy: 0.6709\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6003 - accuracy: 0.6788 - val_loss: 0.6335 - val_accuracy: 0.6671\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6029 - accuracy: 0.6863 - val_loss: 0.6278 - val_accuracy: 0.6735\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6031 - accuracy: 0.6786 - val_loss: 0.6274 - val_accuracy: 0.6849\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6017 - accuracy: 0.6786 - val_loss: 0.6316 - val_accuracy: 0.6633\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6025 - accuracy: 0.6802 - val_loss: 0.6336 - val_accuracy: 0.6696\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5973 - accuracy: 0.6809 - val_loss: 0.6359 - val_accuracy: 0.6645\n",
      "Calculating for: 750 150 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_340 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8125 - accuracy: 0.5309 - val_loss: 0.6549 - val_accuracy: 0.6339\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7156 - accuracy: 0.5501 - val_loss: 0.6424 - val_accuracy: 0.6416\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5522 - val_loss: 0.6460 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5736 - val_loss: 0.6452 - val_accuracy: 0.6492\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5772 - val_loss: 0.6451 - val_accuracy: 0.6556\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6730 - accuracy: 0.5786 - val_loss: 0.6419 - val_accuracy: 0.6505\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.5791 - val_loss: 0.6431 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5903 - val_loss: 0.6393 - val_accuracy: 0.6531\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5880 - val_loss: 0.6384 - val_accuracy: 0.6556\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6693 - accuracy: 0.5904 - val_loss: 0.6357 - val_accuracy: 0.6518\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6632 - accuracy: 0.5976 - val_loss: 0.6334 - val_accuracy: 0.6607\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6650 - accuracy: 0.5990 - val_loss: 0.6374 - val_accuracy: 0.6798\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.5996 - val_loss: 0.6337 - val_accuracy: 0.6760\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.5982 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6634 - accuracy: 0.5997 - val_loss: 0.6332 - val_accuracy: 0.6760\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6071 - val_loss: 0.6286 - val_accuracy: 0.6773\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6039 - val_loss: 0.6303 - val_accuracy: 0.6798\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6101 - val_loss: 0.6299 - val_accuracy: 0.6760\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6181 - val_loss: 0.6255 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6155 - val_loss: 0.6275 - val_accuracy: 0.6773\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6112 - val_loss: 0.6264 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6174 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6098 - val_loss: 0.6280 - val_accuracy: 0.6811\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6211 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6198 - val_loss: 0.6262 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6197 - val_loss: 0.6264 - val_accuracy: 0.6760\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6242 - val_loss: 0.6214 - val_accuracy: 0.6786\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6237 - val_loss: 0.6267 - val_accuracy: 0.6798\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6233 - val_loss: 0.6260 - val_accuracy: 0.6786\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6551 - accuracy: 0.6179 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6204 - val_loss: 0.6221 - val_accuracy: 0.6747\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6497 - accuracy: 0.6226 - val_loss: 0.6212 - val_accuracy: 0.6747\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6255 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6348 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6475 - accuracy: 0.6255 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6500 - accuracy: 0.6230 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6252 - val_loss: 0.6221 - val_accuracy: 0.6824\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6301 - val_loss: 0.6228 - val_accuracy: 0.6837\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6479 - accuracy: 0.6294 - val_loss: 0.6214 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6289 - val_loss: 0.6204 - val_accuracy: 0.6849\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6255 - val_loss: 0.6223 - val_accuracy: 0.6849\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6445 - accuracy: 0.6320 - val_loss: 0.6216 - val_accuracy: 0.6913\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6364 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6344 - val_loss: 0.6188 - val_accuracy: 0.6862\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6374 - val_loss: 0.6158 - val_accuracy: 0.6888\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6360 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6349 - val_loss: 0.6143 - val_accuracy: 0.6888\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6404 - accuracy: 0.6413 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6436 - val_loss: 0.6193 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6399 - val_loss: 0.6153 - val_accuracy: 0.6875\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6394 - accuracy: 0.6363 - val_loss: 0.6125 - val_accuracy: 0.6849\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6382 - val_loss: 0.6120 - val_accuracy: 0.6913\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6350 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6335 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6389 - val_loss: 0.6123 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6380 - val_loss: 0.6133 - val_accuracy: 0.6888\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6444 - val_loss: 0.6173 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6436 - val_loss: 0.6150 - val_accuracy: 0.6913\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6412 - val_loss: 0.6155 - val_accuracy: 0.6926\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6457 - val_loss: 0.6157 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6472 - val_loss: 0.6111 - val_accuracy: 0.6939\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6423 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6448 - val_loss: 0.6115 - val_accuracy: 0.6926\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6372 - accuracy: 0.6451 - val_loss: 0.6166 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6448 - val_loss: 0.6127 - val_accuracy: 0.6888\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6426 - val_loss: 0.6121 - val_accuracy: 0.6952\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6537 - val_loss: 0.6084 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6452 - val_loss: 0.6109 - val_accuracy: 0.6926\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6393 - val_loss: 0.6136 - val_accuracy: 0.6990\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6451 - val_loss: 0.6083 - val_accuracy: 0.6939\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6555 - val_loss: 0.6080 - val_accuracy: 0.6977\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6525 - val_loss: 0.6110 - val_accuracy: 0.7015\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6485 - val_loss: 0.6112 - val_accuracy: 0.6977\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6527 - val_loss: 0.6115 - val_accuracy: 0.6990\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6517 - val_loss: 0.6138 - val_accuracy: 0.6977\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6537 - val_loss: 0.6121 - val_accuracy: 0.6964\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6539 - val_loss: 0.6112 - val_accuracy: 0.6964\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6604 - val_loss: 0.6138 - val_accuracy: 0.6990\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6551 - val_loss: 0.6122 - val_accuracy: 0.6990\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6564 - val_loss: 0.6123 - val_accuracy: 0.6926\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6224 - accuracy: 0.6511 - val_loss: 0.6114 - val_accuracy: 0.6926\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6495 - val_loss: 0.6111 - val_accuracy: 0.6926\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6561 - val_loss: 0.6145 - val_accuracy: 0.6952\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6596 - val_loss: 0.6152 - val_accuracy: 0.6964\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6584 - val_loss: 0.6126 - val_accuracy: 0.6952\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6570 - val_loss: 0.6108 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6567 - val_loss: 0.6105 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6171 - accuracy: 0.6665 - val_loss: 0.6086 - val_accuracy: 0.6939\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6570 - val_loss: 0.6115 - val_accuracy: 0.6939\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6619 - val_loss: 0.6122 - val_accuracy: 0.6913\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6220 - accuracy: 0.6598 - val_loss: 0.6112 - val_accuracy: 0.6964\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6624 - val_loss: 0.6137 - val_accuracy: 0.6913\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6624 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6596 - val_loss: 0.6135 - val_accuracy: 0.6926\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6173 - accuracy: 0.6649 - val_loss: 0.6159 - val_accuracy: 0.6913\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6640 - val_loss: 0.6125 - val_accuracy: 0.6964\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6197 - accuracy: 0.6684 - val_loss: 0.6158 - val_accuracy: 0.6913\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6635 - val_loss: 0.6127 - val_accuracy: 0.6926\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6693 - val_loss: 0.6142 - val_accuracy: 0.6952\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6668 - val_loss: 0.6154 - val_accuracy: 0.6926\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6689 - val_loss: 0.6173 - val_accuracy: 0.6849\n",
      "Calculating for: 750 150 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_344 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8212 - accuracy: 0.5134 - val_loss: 0.6767 - val_accuracy: 0.6288\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7347 - accuracy: 0.5226 - val_loss: 0.6676 - val_accuracy: 0.6480\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7010 - accuracy: 0.5398 - val_loss: 0.6620 - val_accuracy: 0.6441\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5408 - val_loss: 0.6577 - val_accuracy: 0.6441\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.5534 - val_loss: 0.6565 - val_accuracy: 0.6441\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5525 - val_loss: 0.6553 - val_accuracy: 0.6429\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6844 - accuracy: 0.5561 - val_loss: 0.6573 - val_accuracy: 0.6467\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6817 - accuracy: 0.5641 - val_loss: 0.6551 - val_accuracy: 0.6454\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6834 - accuracy: 0.5580 - val_loss: 0.6574 - val_accuracy: 0.6441\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5674 - val_loss: 0.6524 - val_accuracy: 0.6480\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5623 - val_loss: 0.6532 - val_accuracy: 0.6467\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6804 - accuracy: 0.5564 - val_loss: 0.6516 - val_accuracy: 0.6492\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6780 - accuracy: 0.5766 - val_loss: 0.6500 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5776 - val_loss: 0.6495 - val_accuracy: 0.6492\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5785 - val_loss: 0.6436 - val_accuracy: 0.6531\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.5833 - val_loss: 0.6409 - val_accuracy: 0.6531\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5870 - val_loss: 0.6474 - val_accuracy: 0.6569\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6726 - accuracy: 0.5838 - val_loss: 0.6423 - val_accuracy: 0.6518\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.5800 - val_loss: 0.6450 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5898 - val_loss: 0.6465 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5894 - val_loss: 0.6443 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5904 - val_loss: 0.6443 - val_accuracy: 0.6645\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5852 - val_loss: 0.6450 - val_accuracy: 0.6684\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5873 - val_loss: 0.6404 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.5977 - val_loss: 0.6348 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6683 - accuracy: 0.5944 - val_loss: 0.6352 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5963 - val_loss: 0.6356 - val_accuracy: 0.6671\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5967 - val_loss: 0.6393 - val_accuracy: 0.6633\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5967 - val_loss: 0.6352 - val_accuracy: 0.6735\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6069 - val_loss: 0.6374 - val_accuracy: 0.6747\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.5976 - val_loss: 0.6386 - val_accuracy: 0.6760\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.6016 - val_loss: 0.6369 - val_accuracy: 0.6747\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6025 - val_loss: 0.6345 - val_accuracy: 0.6722\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5966 - val_loss: 0.6358 - val_accuracy: 0.6709\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6063 - val_loss: 0.6333 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.5939 - val_loss: 0.6335 - val_accuracy: 0.6709\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6052 - val_loss: 0.6342 - val_accuracy: 0.6735\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6110 - val_loss: 0.6359 - val_accuracy: 0.6735\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6098 - val_loss: 0.6308 - val_accuracy: 0.6811\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6061 - val_loss: 0.6329 - val_accuracy: 0.6773\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6050 - val_loss: 0.6391 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6155 - val_loss: 0.6364 - val_accuracy: 0.6824\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6093 - val_loss: 0.6343 - val_accuracy: 0.6786\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6036 - val_loss: 0.6345 - val_accuracy: 0.6798\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6162 - val_loss: 0.6295 - val_accuracy: 0.6773\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6174 - val_loss: 0.6337 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6079 - val_loss: 0.6385 - val_accuracy: 0.6760\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6088 - val_loss: 0.6417 - val_accuracy: 0.6645\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6163 - val_loss: 0.6357 - val_accuracy: 0.6786\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6134 - val_loss: 0.6329 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6101 - val_loss: 0.6297 - val_accuracy: 0.6837\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6145 - val_loss: 0.6286 - val_accuracy: 0.6811\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6130 - val_loss: 0.6342 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6183 - val_loss: 0.6279 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6193 - val_loss: 0.6302 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6228 - val_loss: 0.6312 - val_accuracy: 0.6811\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6238 - val_loss: 0.6305 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6227 - val_loss: 0.6325 - val_accuracy: 0.6811\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6226 - val_loss: 0.6312 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6545 - accuracy: 0.6189 - val_loss: 0.6337 - val_accuracy: 0.6811\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6538 - accuracy: 0.6237 - val_loss: 0.6326 - val_accuracy: 0.6786\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6218 - val_loss: 0.6286 - val_accuracy: 0.6837\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6543 - accuracy: 0.6150 - val_loss: 0.6293 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6520 - accuracy: 0.6212 - val_loss: 0.6313 - val_accuracy: 0.6798\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6212 - val_loss: 0.6299 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6529 - accuracy: 0.6178 - val_loss: 0.6242 - val_accuracy: 0.6849\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6286 - val_loss: 0.6276 - val_accuracy: 0.6824\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6238 - val_loss: 0.6245 - val_accuracy: 0.6849\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6264 - val_loss: 0.6260 - val_accuracy: 0.6862\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6528 - accuracy: 0.6237 - val_loss: 0.6262 - val_accuracy: 0.6862\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6488 - accuracy: 0.6247 - val_loss: 0.6279 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.6168 - val_loss: 0.6237 - val_accuracy: 0.6824\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6161 - val_loss: 0.6264 - val_accuracy: 0.6811\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6274 - val_accuracy: 0.6875\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6242 - val_loss: 0.6241 - val_accuracy: 0.6888\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6186 - val_loss: 0.6258 - val_accuracy: 0.6862\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6267 - val_loss: 0.6215 - val_accuracy: 0.6875\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6325 - val_loss: 0.6213 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6291 - val_loss: 0.6228 - val_accuracy: 0.6849\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6304 - val_loss: 0.6251 - val_accuracy: 0.6837\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6318 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6366 - val_loss: 0.6276 - val_accuracy: 0.6811\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6304 - val_loss: 0.6226 - val_accuracy: 0.6862\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6338 - val_loss: 0.6217 - val_accuracy: 0.6837\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6350 - val_loss: 0.6209 - val_accuracy: 0.6811\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6390 - val_loss: 0.6192 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6326 - val_loss: 0.6181 - val_accuracy: 0.6939\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6316 - val_loss: 0.6276 - val_accuracy: 0.6798\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6488 - accuracy: 0.6311 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6447 - accuracy: 0.6311 - val_loss: 0.6159 - val_accuracy: 0.6837\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6359 - val_loss: 0.6199 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6366 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6266 - val_loss: 0.6248 - val_accuracy: 0.6760\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6356 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6280 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6331 - val_loss: 0.6216 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6446 - accuracy: 0.6341 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6394 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6439 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6338 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6323 - val_loss: 0.6230 - val_accuracy: 0.6760\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6358 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6338 - val_loss: 0.6216 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6461 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6441 - accuracy: 0.6364 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6404 - accuracy: 0.6382 - val_loss: 0.6187 - val_accuracy: 0.6837\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6420 - val_loss: 0.6222 - val_accuracy: 0.6849\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6429 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6346 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6426 - val_loss: 0.6193 - val_accuracy: 0.6849\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6372 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6402 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6431 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6423 - val_loss: 0.6193 - val_accuracy: 0.6786\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6344 - val_loss: 0.6175 - val_accuracy: 0.6798\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6459 - val_loss: 0.6139 - val_accuracy: 0.6888\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6382 - val_loss: 0.6171 - val_accuracy: 0.6901\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6417 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6350 - val_loss: 0.6148 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6447 - val_loss: 0.6179 - val_accuracy: 0.6811\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6457 - val_loss: 0.6153 - val_accuracy: 0.6862\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6447 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6357 - accuracy: 0.6477 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6492 - val_loss: 0.6110 - val_accuracy: 0.6926\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6458 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6530 - val_loss: 0.6171 - val_accuracy: 0.6849\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6462 - val_loss: 0.6167 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6564 - val_loss: 0.6181 - val_accuracy: 0.6862\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6500 - val_loss: 0.6165 - val_accuracy: 0.6837\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6444 - val_loss: 0.6181 - val_accuracy: 0.6811\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6542 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6381 - accuracy: 0.6443 - val_loss: 0.6146 - val_accuracy: 0.6913\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6482 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6478 - val_loss: 0.6141 - val_accuracy: 0.6862\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6510 - val_loss: 0.6199 - val_accuracy: 0.6837\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6483 - val_loss: 0.6178 - val_accuracy: 0.6913\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6491 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6467 - val_loss: 0.6105 - val_accuracy: 0.6888\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6458 - val_loss: 0.6136 - val_accuracy: 0.6888\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6477 - val_loss: 0.6124 - val_accuracy: 0.6926\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6485 - val_loss: 0.6145 - val_accuracy: 0.6888\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6464 - val_loss: 0.6124 - val_accuracy: 0.6939\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6527 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6545 - val_loss: 0.6164 - val_accuracy: 0.6849\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6503 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6497 - val_loss: 0.6167 - val_accuracy: 0.6875\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6594 - val_loss: 0.6106 - val_accuracy: 0.6939\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6496 - val_loss: 0.6199 - val_accuracy: 0.6875\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6590 - val_loss: 0.6159 - val_accuracy: 0.6913\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6508 - val_loss: 0.6121 - val_accuracy: 0.6926\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6502 - val_loss: 0.6139 - val_accuracy: 0.6952\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6545 - val_loss: 0.6131 - val_accuracy: 0.6939\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6574 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6608 - val_loss: 0.6113 - val_accuracy: 0.6849\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6541 - val_loss: 0.6101 - val_accuracy: 0.6875\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6542 - val_loss: 0.6097 - val_accuracy: 0.6875\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6540 - val_loss: 0.6118 - val_accuracy: 0.6888\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6634 - val_loss: 0.6114 - val_accuracy: 0.6875\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6536 - val_loss: 0.6136 - val_accuracy: 0.6849\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6472 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6532 - val_loss: 0.6138 - val_accuracy: 0.6901\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6546 - val_loss: 0.6110 - val_accuracy: 0.6875\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6542 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6541 - val_loss: 0.6109 - val_accuracy: 0.6901\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6517 - val_loss: 0.6114 - val_accuracy: 0.6952\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6609 - val_loss: 0.6140 - val_accuracy: 0.6888\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6593 - val_loss: 0.6068 - val_accuracy: 0.6939\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6561 - val_loss: 0.6124 - val_accuracy: 0.6862\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6569 - val_loss: 0.6134 - val_accuracy: 0.6849\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6579 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6565 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6701 - val_loss: 0.6126 - val_accuracy: 0.6849\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6570 - val_loss: 0.6138 - val_accuracy: 0.6862\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6589 - val_loss: 0.6122 - val_accuracy: 0.6875\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6586 - val_loss: 0.6151 - val_accuracy: 0.6862\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6670 - val_loss: 0.6144 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6569 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6624 - val_loss: 0.6181 - val_accuracy: 0.6837\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6645 - val_loss: 0.6133 - val_accuracy: 0.6849\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6638 - val_loss: 0.6105 - val_accuracy: 0.6849\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6590 - val_loss: 0.6098 - val_accuracy: 0.6862\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6653 - val_loss: 0.6121 - val_accuracy: 0.6888\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6628 - val_loss: 0.6113 - val_accuracy: 0.6875\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6644 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6618 - val_loss: 0.6138 - val_accuracy: 0.6798\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6639 - val_loss: 0.6110 - val_accuracy: 0.6875\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6591 - val_loss: 0.6157 - val_accuracy: 0.6786\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6619 - val_loss: 0.6130 - val_accuracy: 0.6786\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6187 - accuracy: 0.6697 - val_loss: 0.6135 - val_accuracy: 0.6798\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6644 - val_loss: 0.6134 - val_accuracy: 0.6849\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6193 - accuracy: 0.6624 - val_loss: 0.6136 - val_accuracy: 0.6824\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6703 - val_loss: 0.6101 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6599 - val_loss: 0.6089 - val_accuracy: 0.6837\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6694 - val_loss: 0.6127 - val_accuracy: 0.6837\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6689 - val_loss: 0.6092 - val_accuracy: 0.6824\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6701 - val_loss: 0.6090 - val_accuracy: 0.6862\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6649 - val_loss: 0.6129 - val_accuracy: 0.6824\n"
     ]
    }
   ],
   "source": [
    "dense1 = [650, 700, 750]\n",
    "dense2 = [50, 100, 150]\n",
    "dropout = [0.6, 0.7, 0.8]\n",
    "\n",
    "\n",
    "for d1 in dense1:\n",
    "    for d2 in dense2:\n",
    "        for dr in dropout:\n",
    "            print('Calculating for:', d1, d2, dr, opt)\n",
    "            keras_model = init_keras_model(dense1=d1, dense2=d2, dropout=dr, optimizer=opt)\n",
    "            keras_model.fit(X_train_tensor, \n",
    "                            y_train_tensor, \n",
    "                            epochs=n_epochs, \n",
    "                            validation_data=(X_val_tensor, y_val_tensor),\n",
    "                            callbacks=[es] \n",
    "                            )\n",
    "            score = keras_model.evaluate(X_test_tensor, y_test_tensor, verbose=0)[1]\n",
    "            scores[(d1, d2, dr, opt)] = score\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'dense1':d1, 'dense2':d2, 'dropout':dr, 'optimizer':opt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'dense1': 750, 'dense2': 50, 'dropout': 0.8, 'optimizer': 'sgd'}, best_score: 0.6883780360221863\n"
     ]
    }
   ],
   "source": [
    "print(f'best_params: {best_params}, best_score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "246/246 [==============================] - 2s 4ms/step - loss: 0.8144 - accuracy: 0.5198 - val_loss: 0.6391 - val_accuracy: 0.6569\n",
      "Epoch 2/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.7195 - accuracy: 0.5375 - val_loss: 0.6370 - val_accuracy: 0.6645\n",
      "Epoch 3/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6941 - accuracy: 0.5496 - val_loss: 0.6375 - val_accuracy: 0.6722\n",
      "Epoch 4/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6857 - accuracy: 0.5615 - val_loss: 0.6358 - val_accuracy: 0.6684\n",
      "Epoch 5/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6792 - accuracy: 0.5710 - val_loss: 0.6355 - val_accuracy: 0.6760\n",
      "Epoch 6/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6770 - accuracy: 0.5795 - val_loss: 0.6281 - val_accuracy: 0.6786\n",
      "Epoch 7/500\n",
      "246/246 [==============================] - 2s 7ms/step - loss: 0.6764 - accuracy: 0.5831 - val_loss: 0.6268 - val_accuracy: 0.6760\n",
      "Epoch 8/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6741 - accuracy: 0.5812 - val_loss: 0.6338 - val_accuracy: 0.6849\n",
      "Epoch 9/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6742 - accuracy: 0.5818 - val_loss: 0.6307 - val_accuracy: 0.6837\n",
      "Epoch 10/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.5879 - val_loss: 0.6287 - val_accuracy: 0.6811\n",
      "Epoch 11/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5884 - val_loss: 0.6265 - val_accuracy: 0.6926\n",
      "Epoch 12/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5987 - val_loss: 0.6236 - val_accuracy: 0.6952\n",
      "Epoch 13/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5914 - val_loss: 0.6257 - val_accuracy: 0.6875\n",
      "Epoch 14/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5992 - val_loss: 0.6271 - val_accuracy: 0.6837\n",
      "Epoch 15/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5964 - val_loss: 0.6214 - val_accuracy: 0.6888\n",
      "Epoch 16/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6099 - val_loss: 0.6259 - val_accuracy: 0.6811\n",
      "Epoch 17/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6105 - val_loss: 0.6234 - val_accuracy: 0.6811\n",
      "Epoch 18/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6015 - val_loss: 0.6274 - val_accuracy: 0.6786\n",
      "Epoch 19/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6107 - val_loss: 0.6249 - val_accuracy: 0.6798\n",
      "Epoch 20/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6603 - accuracy: 0.6114 - val_loss: 0.6192 - val_accuracy: 0.6837\n",
      "Epoch 21/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6613 - accuracy: 0.6164 - val_loss: 0.6268 - val_accuracy: 0.6798\n",
      "Epoch 22/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6618 - accuracy: 0.6070 - val_loss: 0.6297 - val_accuracy: 0.6786\n",
      "Epoch 23/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6208 - val_loss: 0.6275 - val_accuracy: 0.6773\n",
      "Epoch 24/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6567 - accuracy: 0.6174 - val_loss: 0.6188 - val_accuracy: 0.6786\n",
      "Epoch 25/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6139 - val_loss: 0.6233 - val_accuracy: 0.6811\n",
      "Epoch 26/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6139 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 27/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6170 - val_loss: 0.6124 - val_accuracy: 0.6798\n",
      "Epoch 28/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6249 - val_loss: 0.6213 - val_accuracy: 0.6798\n",
      "Epoch 29/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6227 - val_loss: 0.6221 - val_accuracy: 0.6824\n",
      "Epoch 30/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6268 - val_loss: 0.6151 - val_accuracy: 0.6786\n",
      "Epoch 31/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6259 - val_loss: 0.6180 - val_accuracy: 0.6862\n",
      "Epoch 32/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6506 - accuracy: 0.6230 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 33/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6484 - accuracy: 0.6305 - val_loss: 0.6204 - val_accuracy: 0.6862\n",
      "Epoch 34/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6496 - accuracy: 0.6263 - val_loss: 0.6189 - val_accuracy: 0.6849\n",
      "Epoch 35/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6296 - val_loss: 0.6218 - val_accuracy: 0.6888\n",
      "Epoch 36/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6509 - accuracy: 0.6255 - val_loss: 0.6200 - val_accuracy: 0.6875\n",
      "Epoch 37/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6541 - accuracy: 0.6235 - val_loss: 0.6186 - val_accuracy: 0.6901\n",
      "Epoch 38/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6491 - accuracy: 0.6259 - val_loss: 0.6144 - val_accuracy: 0.6901\n",
      "Epoch 39/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6267 - val_loss: 0.6161 - val_accuracy: 0.6862\n",
      "Epoch 40/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6297 - val_loss: 0.6145 - val_accuracy: 0.6888\n",
      "Epoch 41/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6479 - accuracy: 0.6294 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 42/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6263 - val_loss: 0.6207 - val_accuracy: 0.6926\n",
      "Epoch 43/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6496 - accuracy: 0.6276 - val_loss: 0.6193 - val_accuracy: 0.6888\n",
      "Epoch 44/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6368 - val_loss: 0.6145 - val_accuracy: 0.6913\n",
      "Epoch 45/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6470 - accuracy: 0.6370 - val_loss: 0.6218 - val_accuracy: 0.6926\n",
      "Epoch 46/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6465 - accuracy: 0.6291 - val_loss: 0.6207 - val_accuracy: 0.6862\n",
      "Epoch 47/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6370 - val_loss: 0.6233 - val_accuracy: 0.6747\n",
      "Epoch 48/500\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.6471 - accuracy: 0.6300 - val_loss: 0.6183 - val_accuracy: 0.6913\n",
      "Epoch 49/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6353 - val_loss: 0.6198 - val_accuracy: 0.6811\n",
      "Epoch 50/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6407 - val_loss: 0.6191 - val_accuracy: 0.6862\n",
      "Epoch 51/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6435 - accuracy: 0.6441 - val_loss: 0.6172 - val_accuracy: 0.6849\n",
      "Epoch 52/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6374 - val_loss: 0.6145 - val_accuracy: 0.6888\n",
      "Epoch 53/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6442 - accuracy: 0.6368 - val_loss: 0.6151 - val_accuracy: 0.6901\n",
      "Epoch 54/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6436 - accuracy: 0.6419 - val_loss: 0.6160 - val_accuracy: 0.6913\n",
      "Epoch 55/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6413 - val_loss: 0.6198 - val_accuracy: 0.6798\n",
      "Epoch 56/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6296 - val_loss: 0.6158 - val_accuracy: 0.6901\n",
      "Epoch 57/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6447 - accuracy: 0.6410 - val_loss: 0.6125 - val_accuracy: 0.6926\n",
      "Epoch 58/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6389 - accuracy: 0.6409 - val_loss: 0.6205 - val_accuracy: 0.6811\n",
      "Epoch 59/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6395 - val_loss: 0.6168 - val_accuracy: 0.6837\n",
      "Epoch 60/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6404 - val_loss: 0.6108 - val_accuracy: 0.6939\n",
      "Epoch 61/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6407 - accuracy: 0.6405 - val_loss: 0.6097 - val_accuracy: 0.6939\n",
      "Epoch 62/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6326 - accuracy: 0.6480 - val_loss: 0.6116 - val_accuracy: 0.6875\n",
      "Epoch 63/500\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.6402 - accuracy: 0.6437 - val_loss: 0.6149 - val_accuracy: 0.6875\n",
      "Epoch 64/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6502 - val_loss: 0.6112 - val_accuracy: 0.6926\n",
      "Epoch 65/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6360 - accuracy: 0.6452 - val_loss: 0.6136 - val_accuracy: 0.6849\n",
      "Epoch 66/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6485 - val_loss: 0.6148 - val_accuracy: 0.6901\n",
      "Epoch 67/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6418 - val_loss: 0.6130 - val_accuracy: 0.6901\n",
      "Epoch 68/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6446 - val_loss: 0.6222 - val_accuracy: 0.6786\n",
      "Epoch 69/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6427 - val_loss: 0.6131 - val_accuracy: 0.6901\n",
      "Epoch 70/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6424 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Epoch 71/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6484 - val_loss: 0.6139 - val_accuracy: 0.6926\n",
      "Epoch 72/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6536 - val_loss: 0.6158 - val_accuracy: 0.6849\n",
      "Epoch 73/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6387 - accuracy: 0.6377 - val_loss: 0.6182 - val_accuracy: 0.6849\n",
      "Epoch 74/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6403 - val_loss: 0.6212 - val_accuracy: 0.6773\n",
      "Epoch 75/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6544 - val_loss: 0.6208 - val_accuracy: 0.6696\n",
      "Epoch 76/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6332 - accuracy: 0.6531 - val_loss: 0.6157 - val_accuracy: 0.6862\n",
      "Epoch 77/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6488 - val_loss: 0.6177 - val_accuracy: 0.6849\n",
      "Epoch 78/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6371 - val_loss: 0.6090 - val_accuracy: 0.6939\n",
      "Epoch 79/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6482 - val_loss: 0.6111 - val_accuracy: 0.6964\n",
      "Epoch 80/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6476 - val_loss: 0.6119 - val_accuracy: 0.6939\n",
      "Epoch 81/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6428 - val_loss: 0.6120 - val_accuracy: 0.6926\n",
      "Epoch 82/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6480 - val_loss: 0.6185 - val_accuracy: 0.6798\n",
      "Epoch 83/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6485 - val_loss: 0.6108 - val_accuracy: 0.6964\n",
      "Epoch 84/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6524 - val_loss: 0.6110 - val_accuracy: 0.6913\n",
      "Epoch 85/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6471 - val_loss: 0.6142 - val_accuracy: 0.6901\n",
      "Epoch 86/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6517 - val_loss: 0.6093 - val_accuracy: 0.6939\n",
      "Epoch 87/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6342 - accuracy: 0.6548 - val_loss: 0.6132 - val_accuracy: 0.6913\n",
      "Epoch 88/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6541 - val_loss: 0.6110 - val_accuracy: 0.6824\n",
      "Epoch 89/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6562 - val_loss: 0.6057 - val_accuracy: 0.6964\n",
      "Epoch 90/500\n",
      "246/246 [==============================] - 2s 6ms/step - loss: 0.6320 - accuracy: 0.6521 - val_loss: 0.6072 - val_accuracy: 0.6964\n",
      "Epoch 91/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6555 - val_loss: 0.6075 - val_accuracy: 0.6964\n",
      "Epoch 92/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6525 - val_loss: 0.6089 - val_accuracy: 0.6926\n",
      "Epoch 93/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6569 - val_loss: 0.6119 - val_accuracy: 0.6939\n",
      "Epoch 94/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6431 - val_loss: 0.6100 - val_accuracy: 0.6913\n",
      "Epoch 95/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6530 - val_loss: 0.6130 - val_accuracy: 0.6786\n",
      "Epoch 96/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6600 - val_loss: 0.6169 - val_accuracy: 0.6747\n",
      "Epoch 97/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6642 - val_loss: 0.6165 - val_accuracy: 0.6709\n",
      "Epoch 98/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6515 - val_loss: 0.6078 - val_accuracy: 0.6901\n",
      "Epoch 99/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6590 - val_loss: 0.6057 - val_accuracy: 0.6952\n",
      "Epoch 100/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6550 - val_loss: 0.6097 - val_accuracy: 0.6735\n",
      "Epoch 101/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6543 - val_loss: 0.6149 - val_accuracy: 0.6760\n",
      "Epoch 102/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6590 - val_loss: 0.6126 - val_accuracy: 0.6760\n",
      "Epoch 103/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6651 - val_loss: 0.6121 - val_accuracy: 0.6811\n",
      "Epoch 104/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6544 - val_loss: 0.6108 - val_accuracy: 0.6862\n",
      "Epoch 105/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.6576 - val_loss: 0.6093 - val_accuracy: 0.6862\n",
      "Epoch 106/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6609 - val_loss: 0.6156 - val_accuracy: 0.6709\n",
      "Epoch 107/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6259 - accuracy: 0.6634 - val_loss: 0.6066 - val_accuracy: 0.6875\n",
      "Epoch 108/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6527 - val_loss: 0.6112 - val_accuracy: 0.6837\n",
      "Epoch 109/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6563 - val_loss: 0.6157 - val_accuracy: 0.6658\n",
      "Epoch 110/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6600 - val_loss: 0.6178 - val_accuracy: 0.6645\n",
      "Epoch 111/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6260 - accuracy: 0.6604 - val_loss: 0.6196 - val_accuracy: 0.6658\n",
      "Epoch 112/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6258 - accuracy: 0.6628 - val_loss: 0.6046 - val_accuracy: 0.6964\n",
      "Epoch 113/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6657 - val_loss: 0.6074 - val_accuracy: 0.6888\n",
      "Epoch 114/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6606 - val_loss: 0.6061 - val_accuracy: 0.6837\n",
      "Epoch 115/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6217 - accuracy: 0.6633 - val_loss: 0.6022 - val_accuracy: 0.6913\n",
      "Epoch 116/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6615 - val_loss: 0.6101 - val_accuracy: 0.6837\n",
      "Epoch 117/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6245 - accuracy: 0.6659 - val_loss: 0.6144 - val_accuracy: 0.6747\n",
      "Epoch 118/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6596 - val_loss: 0.6075 - val_accuracy: 0.6901\n",
      "Epoch 119/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6626 - val_loss: 0.5983 - val_accuracy: 0.7041\n",
      "Epoch 120/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6638 - val_loss: 0.6125 - val_accuracy: 0.6747\n",
      "Epoch 121/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6223 - accuracy: 0.6623 - val_loss: 0.6015 - val_accuracy: 0.6939\n",
      "Epoch 122/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6524 - val_loss: 0.6114 - val_accuracy: 0.6875\n",
      "Epoch 123/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6670 - val_loss: 0.6056 - val_accuracy: 0.6888\n",
      "Epoch 124/500\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.6250 - accuracy: 0.6606 - val_loss: 0.6053 - val_accuracy: 0.6913\n",
      "Epoch 125/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6224 - accuracy: 0.6592 - val_loss: 0.6093 - val_accuracy: 0.6913\n",
      "Epoch 126/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6626 - val_loss: 0.5975 - val_accuracy: 0.6977\n",
      "Epoch 127/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6630 - val_loss: 0.6106 - val_accuracy: 0.6811\n",
      "Epoch 128/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6233 - accuracy: 0.6651 - val_loss: 0.6119 - val_accuracy: 0.6811\n",
      "Epoch 129/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6670 - val_loss: 0.6088 - val_accuracy: 0.6824\n",
      "Epoch 130/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6170 - accuracy: 0.6689 - val_loss: 0.6027 - val_accuracy: 0.6913\n",
      "Epoch 131/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6202 - accuracy: 0.6633 - val_loss: 0.6024 - val_accuracy: 0.6901\n",
      "Epoch 132/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6214 - accuracy: 0.6653 - val_loss: 0.6034 - val_accuracy: 0.6913\n",
      "Epoch 133/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6728 - val_loss: 0.6070 - val_accuracy: 0.6913\n",
      "Epoch 134/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6224 - accuracy: 0.6637 - val_loss: 0.6103 - val_accuracy: 0.6837\n",
      "Epoch 135/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6198 - accuracy: 0.6658 - val_loss: 0.6052 - val_accuracy: 0.6875\n",
      "Epoch 136/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6180 - accuracy: 0.6661 - val_loss: 0.6059 - val_accuracy: 0.6849\n",
      "Epoch 137/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6226 - accuracy: 0.6628 - val_loss: 0.6070 - val_accuracy: 0.6875\n",
      "Epoch 138/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6638 - val_loss: 0.6135 - val_accuracy: 0.6786\n",
      "Epoch 139/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6687 - val_loss: 0.6044 - val_accuracy: 0.6926\n",
      "Epoch 140/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6154 - accuracy: 0.6679 - val_loss: 0.6066 - val_accuracy: 0.6837\n",
      "Epoch 141/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6686 - val_loss: 0.6055 - val_accuracy: 0.6849\n",
      "Epoch 142/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6686 - val_loss: 0.6086 - val_accuracy: 0.6849\n",
      "Epoch 143/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6722 - val_loss: 0.6069 - val_accuracy: 0.6837\n",
      "Epoch 144/500\n",
      "246/246 [==============================] - 1s 6ms/step - loss: 0.6138 - accuracy: 0.6741 - val_loss: 0.6093 - val_accuracy: 0.6824\n",
      "Epoch 145/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6136 - accuracy: 0.6701 - val_loss: 0.6048 - val_accuracy: 0.6952\n",
      "Epoch 146/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6094 - accuracy: 0.6748 - val_loss: 0.6027 - val_accuracy: 0.6913\n",
      "Epoch 147/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6681 - val_loss: 0.6032 - val_accuracy: 0.6888\n",
      "Epoch 148/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6128 - accuracy: 0.6710 - val_loss: 0.6037 - val_accuracy: 0.6875\n",
      "Epoch 149/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6142 - accuracy: 0.6748 - val_loss: 0.6045 - val_accuracy: 0.6849\n",
      "Epoch 150/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.6680 - val_loss: 0.6066 - val_accuracy: 0.6849\n",
      "Epoch 151/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6147 - accuracy: 0.6699 - val_loss: 0.6030 - val_accuracy: 0.6926\n",
      "Epoch 152/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6143 - accuracy: 0.6694 - val_loss: 0.6054 - val_accuracy: 0.6964\n",
      "Epoch 153/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6129 - accuracy: 0.6682 - val_loss: 0.6056 - val_accuracy: 0.6862\n",
      "Epoch 154/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6171 - accuracy: 0.6658 - val_loss: 0.6035 - val_accuracy: 0.6913\n",
      "Epoch 155/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6130 - accuracy: 0.6696 - val_loss: 0.6044 - val_accuracy: 0.6901\n",
      "Epoch 156/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6122 - accuracy: 0.6761 - val_loss: 0.6079 - val_accuracy: 0.6849\n",
      "Epoch 157/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6092 - accuracy: 0.6756 - val_loss: 0.5971 - val_accuracy: 0.6964\n",
      "Epoch 158/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6147 - accuracy: 0.6690 - val_loss: 0.6024 - val_accuracy: 0.6952\n",
      "Epoch 159/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6729 - val_loss: 0.6054 - val_accuracy: 0.6888\n",
      "Epoch 160/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6779 - val_loss: 0.6083 - val_accuracy: 0.6862\n",
      "Epoch 161/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6111 - accuracy: 0.6731 - val_loss: 0.6055 - val_accuracy: 0.6913\n",
      "Epoch 162/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6089 - accuracy: 0.6793 - val_loss: 0.6072 - val_accuracy: 0.6888\n",
      "Epoch 163/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6134 - accuracy: 0.6715 - val_loss: 0.6137 - val_accuracy: 0.6837\n",
      "Epoch 164/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6094 - accuracy: 0.6753 - val_loss: 0.6030 - val_accuracy: 0.6888\n",
      "Epoch 165/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6130 - accuracy: 0.6699 - val_loss: 0.5999 - val_accuracy: 0.6952\n",
      "Epoch 166/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6097 - accuracy: 0.6799 - val_loss: 0.6057 - val_accuracy: 0.6888\n",
      "Epoch 167/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6121 - accuracy: 0.6740 - val_loss: 0.6050 - val_accuracy: 0.6913\n",
      "Epoch 168/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6762 - val_loss: 0.6035 - val_accuracy: 0.6875\n",
      "Epoch 169/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6087 - accuracy: 0.6799 - val_loss: 0.6013 - val_accuracy: 0.6926\n",
      "Epoch 170/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6110 - accuracy: 0.6748 - val_loss: 0.6034 - val_accuracy: 0.6901\n",
      "Epoch 171/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6080 - accuracy: 0.6747 - val_loss: 0.5971 - val_accuracy: 0.7003\n",
      "Epoch 172/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6074 - accuracy: 0.6756 - val_loss: 0.6012 - val_accuracy: 0.6964\n",
      "Epoch 173/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.6100 - accuracy: 0.6787 - val_loss: 0.6023 - val_accuracy: 0.6901\n",
      "Epoch 174/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6093 - accuracy: 0.6789 - val_loss: 0.6053 - val_accuracy: 0.6901\n",
      "Epoch 175/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6027 - accuracy: 0.6901 - val_loss: 0.6094 - val_accuracy: 0.6747\n",
      "Epoch 176/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6757 - val_loss: 0.6068 - val_accuracy: 0.6773\n",
      "Epoch 177/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6751 - val_loss: 0.6004 - val_accuracy: 0.6977\n",
      "Epoch 178/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6104 - accuracy: 0.6732 - val_loss: 0.6075 - val_accuracy: 0.6786\n",
      "Epoch 179/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6053 - accuracy: 0.6878 - val_loss: 0.6011 - val_accuracy: 0.6939\n",
      "Epoch 180/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6051 - accuracy: 0.6797 - val_loss: 0.6050 - val_accuracy: 0.6862\n",
      "Epoch 181/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6051 - accuracy: 0.6855 - val_loss: 0.5964 - val_accuracy: 0.7003\n",
      "Epoch 182/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6057 - accuracy: 0.6807 - val_loss: 0.6005 - val_accuracy: 0.6977\n",
      "Epoch 183/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6023 - accuracy: 0.6806 - val_loss: 0.6010 - val_accuracy: 0.7015\n",
      "Epoch 184/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6064 - accuracy: 0.6817 - val_loss: 0.6052 - val_accuracy: 0.6862\n",
      "Epoch 185/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6050 - accuracy: 0.6797 - val_loss: 0.5977 - val_accuracy: 0.6964\n",
      "Epoch 186/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6080 - accuracy: 0.6788 - val_loss: 0.6030 - val_accuracy: 0.6913\n",
      "Epoch 187/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6057 - accuracy: 0.6788 - val_loss: 0.5980 - val_accuracy: 0.7003\n",
      "Epoch 188/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6058 - accuracy: 0.6794 - val_loss: 0.5968 - val_accuracy: 0.6990\n",
      "Epoch 189/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6064 - accuracy: 0.6835 - val_loss: 0.6015 - val_accuracy: 0.6939\n",
      "Epoch 190/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6070 - accuracy: 0.6806 - val_loss: 0.6026 - val_accuracy: 0.6939\n",
      "Epoch 191/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6074 - accuracy: 0.6788 - val_loss: 0.6073 - val_accuracy: 0.6862\n",
      "Epoch 192/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6048 - accuracy: 0.6850 - val_loss: 0.5995 - val_accuracy: 0.6913\n",
      "Epoch 193/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6026 - accuracy: 0.6827 - val_loss: 0.6029 - val_accuracy: 0.6926\n",
      "Epoch 194/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6037 - accuracy: 0.6839 - val_loss: 0.6057 - val_accuracy: 0.6862\n",
      "Epoch 195/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6023 - accuracy: 0.6853 - val_loss: 0.6035 - val_accuracy: 0.6964\n",
      "Epoch 196/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6070 - accuracy: 0.6820 - val_loss: 0.6036 - val_accuracy: 0.6990\n",
      "Epoch 197/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6044 - accuracy: 0.6798 - val_loss: 0.6019 - val_accuracy: 0.6990\n",
      "Epoch 198/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6028 - accuracy: 0.6826 - val_loss: 0.6018 - val_accuracy: 0.6952\n",
      "Epoch 199/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6864 - val_loss: 0.6068 - val_accuracy: 0.6862\n",
      "Epoch 200/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6029 - accuracy: 0.6816 - val_loss: 0.6012 - val_accuracy: 0.6939\n",
      "Epoch 201/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5980 - accuracy: 0.6834 - val_loss: 0.6010 - val_accuracy: 0.6913\n",
      "Epoch 202/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5947 - accuracy: 0.6875 - val_loss: 0.6002 - val_accuracy: 0.6977\n",
      "Epoch 203/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6075 - accuracy: 0.6762 - val_loss: 0.5987 - val_accuracy: 0.7028\n",
      "Epoch 204/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6031 - accuracy: 0.6841 - val_loss: 0.6028 - val_accuracy: 0.6990\n",
      "Epoch 205/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6026 - accuracy: 0.6887 - val_loss: 0.5943 - val_accuracy: 0.7028\n",
      "Epoch 206/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5991 - accuracy: 0.6863 - val_loss: 0.5997 - val_accuracy: 0.7015\n",
      "Epoch 207/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5972 - accuracy: 0.6875 - val_loss: 0.6005 - val_accuracy: 0.6990\n",
      "Epoch 208/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6028 - accuracy: 0.6835 - val_loss: 0.6031 - val_accuracy: 0.6913\n",
      "Epoch 209/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6036 - accuracy: 0.6854 - val_loss: 0.6049 - val_accuracy: 0.6952\n",
      "Epoch 210/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6779 - val_loss: 0.6041 - val_accuracy: 0.6901\n",
      "Epoch 211/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5992 - accuracy: 0.6846 - val_loss: 0.6009 - val_accuracy: 0.6977\n",
      "Epoch 212/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5993 - accuracy: 0.6841 - val_loss: 0.6097 - val_accuracy: 0.6824\n",
      "Epoch 213/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6010 - accuracy: 0.6825 - val_loss: 0.6042 - val_accuracy: 0.6964\n",
      "Epoch 214/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5949 - accuracy: 0.6896 - val_loss: 0.6067 - val_accuracy: 0.6773\n",
      "Epoch 215/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6812 - val_loss: 0.6002 - val_accuracy: 0.6964\n",
      "Epoch 216/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6025 - accuracy: 0.6821 - val_loss: 0.6007 - val_accuracy: 0.7015\n",
      "Epoch 217/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5971 - accuracy: 0.6863 - val_loss: 0.6040 - val_accuracy: 0.6952\n",
      "Epoch 218/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6020 - accuracy: 0.6887 - val_loss: 0.6010 - val_accuracy: 0.7015\n",
      "Epoch 219/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5977 - accuracy: 0.6879 - val_loss: 0.5991 - val_accuracy: 0.6990\n",
      "Epoch 220/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6027 - accuracy: 0.6842 - val_loss: 0.6004 - val_accuracy: 0.6913\n",
      "Epoch 221/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5999 - accuracy: 0.6842 - val_loss: 0.6016 - val_accuracy: 0.6837\n",
      "Epoch 222/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.6020 - accuracy: 0.6821 - val_loss: 0.6024 - val_accuracy: 0.6849\n",
      "Epoch 223/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5984 - accuracy: 0.6930 - val_loss: 0.6011 - val_accuracy: 0.6990\n",
      "Epoch 224/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5968 - accuracy: 0.6921 - val_loss: 0.5956 - val_accuracy: 0.7054\n",
      "Epoch 225/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5945 - accuracy: 0.6896 - val_loss: 0.6058 - val_accuracy: 0.6798\n",
      "Epoch 226/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5949 - accuracy: 0.6875 - val_loss: 0.6065 - val_accuracy: 0.6786\n",
      "Epoch 227/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5975 - accuracy: 0.6877 - val_loss: 0.6048 - val_accuracy: 0.6901\n",
      "Epoch 228/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6023 - accuracy: 0.6823 - val_loss: 0.6008 - val_accuracy: 0.6913\n",
      "Epoch 229/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5966 - accuracy: 0.6912 - val_loss: 0.6042 - val_accuracy: 0.6875\n",
      "Epoch 230/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5966 - accuracy: 0.6851 - val_loss: 0.6042 - val_accuracy: 0.6901\n",
      "Epoch 231/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5907 - accuracy: 0.6933 - val_loss: 0.6011 - val_accuracy: 0.6952\n",
      "Epoch 232/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.6004 - accuracy: 0.6839 - val_loss: 0.5952 - val_accuracy: 0.7028\n",
      "Epoch 233/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5957 - accuracy: 0.6889 - val_loss: 0.6056 - val_accuracy: 0.6760\n",
      "Epoch 234/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5940 - accuracy: 0.6872 - val_loss: 0.5937 - val_accuracy: 0.7028\n",
      "Epoch 235/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5987 - accuracy: 0.6889 - val_loss: 0.6005 - val_accuracy: 0.6913\n",
      "Epoch 236/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5960 - accuracy: 0.6919 - val_loss: 0.6044 - val_accuracy: 0.6760\n",
      "Epoch 237/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5943 - accuracy: 0.6926 - val_loss: 0.5987 - val_accuracy: 0.6888\n",
      "Epoch 238/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5918 - accuracy: 0.6959 - val_loss: 0.5988 - val_accuracy: 0.6901\n",
      "Epoch 239/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5970 - accuracy: 0.6855 - val_loss: 0.5975 - val_accuracy: 0.7015\n",
      "Epoch 240/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5910 - accuracy: 0.6949 - val_loss: 0.6081 - val_accuracy: 0.6760\n",
      "Epoch 241/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5950 - accuracy: 0.6872 - val_loss: 0.6001 - val_accuracy: 0.6926\n",
      "Epoch 242/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5952 - accuracy: 0.6938 - val_loss: 0.6046 - val_accuracy: 0.6901\n",
      "Epoch 243/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5909 - accuracy: 0.6905 - val_loss: 0.5985 - val_accuracy: 0.6964\n",
      "Epoch 244/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5916 - accuracy: 0.6906 - val_loss: 0.5953 - val_accuracy: 0.6990\n",
      "Epoch 245/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5956 - accuracy: 0.6882 - val_loss: 0.5983 - val_accuracy: 0.7015\n",
      "Epoch 246/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5880 - accuracy: 0.6958 - val_loss: 0.6052 - val_accuracy: 0.6786\n",
      "Epoch 247/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5919 - accuracy: 0.6936 - val_loss: 0.5993 - val_accuracy: 0.6926\n",
      "Epoch 248/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5945 - accuracy: 0.6868 - val_loss: 0.5957 - val_accuracy: 0.7041\n",
      "Epoch 249/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5961 - accuracy: 0.6933 - val_loss: 0.5984 - val_accuracy: 0.6952\n",
      "Epoch 250/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5887 - accuracy: 0.6980 - val_loss: 0.5978 - val_accuracy: 0.6952\n",
      "Epoch 251/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5882 - accuracy: 0.6961 - val_loss: 0.5972 - val_accuracy: 0.6913\n",
      "Epoch 252/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5881 - accuracy: 0.6975 - val_loss: 0.6000 - val_accuracy: 0.6862\n",
      "Epoch 253/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5907 - accuracy: 0.6929 - val_loss: 0.5961 - val_accuracy: 0.6926\n",
      "Epoch 254/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5925 - accuracy: 0.6915 - val_loss: 0.5986 - val_accuracy: 0.6888\n",
      "Epoch 255/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5894 - accuracy: 0.6963 - val_loss: 0.6034 - val_accuracy: 0.6798\n",
      "Epoch 256/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5906 - accuracy: 0.6934 - val_loss: 0.5996 - val_accuracy: 0.6913\n",
      "Epoch 257/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5885 - accuracy: 0.6959 - val_loss: 0.6021 - val_accuracy: 0.6952\n",
      "Epoch 258/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5894 - accuracy: 0.6989 - val_loss: 0.5988 - val_accuracy: 0.6990\n",
      "Epoch 259/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5913 - accuracy: 0.6936 - val_loss: 0.5980 - val_accuracy: 0.6977\n",
      "Epoch 260/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5906 - accuracy: 0.6942 - val_loss: 0.5988 - val_accuracy: 0.6990\n",
      "Epoch 261/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5948 - accuracy: 0.6901 - val_loss: 0.6001 - val_accuracy: 0.6977\n",
      "Epoch 262/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5871 - accuracy: 0.6964 - val_loss: 0.5991 - val_accuracy: 0.6952\n",
      "Epoch 263/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5949 - accuracy: 0.6909 - val_loss: 0.6011 - val_accuracy: 0.6875\n",
      "Epoch 264/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5951 - accuracy: 0.6879 - val_loss: 0.6033 - val_accuracy: 0.6849\n",
      "Epoch 265/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5904 - accuracy: 0.6931 - val_loss: 0.6029 - val_accuracy: 0.6862\n",
      "Epoch 266/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5896 - accuracy: 0.6994 - val_loss: 0.6030 - val_accuracy: 0.6837\n",
      "Epoch 267/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5847 - accuracy: 0.6995 - val_loss: 0.6053 - val_accuracy: 0.6735\n",
      "Epoch 268/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5888 - accuracy: 0.7030 - val_loss: 0.6047 - val_accuracy: 0.6747\n",
      "Epoch 269/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5877 - accuracy: 0.6989 - val_loss: 0.6047 - val_accuracy: 0.6824\n",
      "Epoch 270/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5866 - accuracy: 0.6931 - val_loss: 0.6056 - val_accuracy: 0.6824\n",
      "Epoch 271/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5863 - accuracy: 0.6963 - val_loss: 0.5976 - val_accuracy: 0.6901\n",
      "Epoch 272/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5892 - accuracy: 0.6925 - val_loss: 0.6034 - val_accuracy: 0.6773\n",
      "Epoch 273/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5889 - accuracy: 0.6972 - val_loss: 0.6035 - val_accuracy: 0.6849\n",
      "Epoch 274/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5903 - accuracy: 0.6968 - val_loss: 0.6008 - val_accuracy: 0.6862\n",
      "Epoch 275/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5872 - accuracy: 0.6936 - val_loss: 0.6044 - val_accuracy: 0.6824\n",
      "Epoch 276/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5916 - accuracy: 0.6933 - val_loss: 0.6028 - val_accuracy: 0.6798\n",
      "Epoch 277/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5837 - accuracy: 0.6986 - val_loss: 0.6025 - val_accuracy: 0.6837\n",
      "Epoch 278/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5903 - accuracy: 0.6956 - val_loss: 0.6010 - val_accuracy: 0.6811\n",
      "Epoch 279/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5795 - accuracy: 0.7064 - val_loss: 0.6034 - val_accuracy: 0.6875\n",
      "Epoch 280/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5901 - accuracy: 0.6949 - val_loss: 0.6006 - val_accuracy: 0.6837\n",
      "Epoch 281/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5850 - accuracy: 0.6963 - val_loss: 0.5944 - val_accuracy: 0.6977\n",
      "Epoch 282/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5855 - accuracy: 0.7034 - val_loss: 0.6000 - val_accuracy: 0.6837\n",
      "Epoch 283/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5854 - accuracy: 0.6970 - val_loss: 0.5967 - val_accuracy: 0.6939\n",
      "Epoch 284/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5846 - accuracy: 0.6962 - val_loss: 0.6050 - val_accuracy: 0.6811\n",
      "Epoch 285/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5797 - accuracy: 0.7056 - val_loss: 0.6064 - val_accuracy: 0.6684\n",
      "Epoch 286/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5860 - accuracy: 0.6943 - val_loss: 0.6000 - val_accuracy: 0.6786\n",
      "Epoch 287/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5821 - accuracy: 0.7025 - val_loss: 0.5988 - val_accuracy: 0.6849\n",
      "Epoch 288/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5815 - accuracy: 0.7017 - val_loss: 0.5989 - val_accuracy: 0.6888\n",
      "Epoch 289/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5792 - accuracy: 0.6995 - val_loss: 0.6005 - val_accuracy: 0.6849\n",
      "Epoch 290/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5824 - accuracy: 0.6994 - val_loss: 0.5989 - val_accuracy: 0.6837\n",
      "Epoch 291/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5834 - accuracy: 0.6943 - val_loss: 0.6056 - val_accuracy: 0.6798\n",
      "Epoch 292/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5830 - accuracy: 0.6978 - val_loss: 0.6066 - val_accuracy: 0.6709\n",
      "Epoch 293/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5861 - accuracy: 0.6944 - val_loss: 0.5989 - val_accuracy: 0.6837\n",
      "Epoch 294/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5822 - accuracy: 0.7032 - val_loss: 0.6023 - val_accuracy: 0.6837\n",
      "Epoch 295/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5817 - accuracy: 0.7000 - val_loss: 0.6035 - val_accuracy: 0.6888\n",
      "Epoch 296/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5813 - accuracy: 0.6968 - val_loss: 0.6027 - val_accuracy: 0.6747\n",
      "Epoch 297/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5812 - accuracy: 0.7028 - val_loss: 0.5988 - val_accuracy: 0.6824\n",
      "Epoch 298/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5850 - accuracy: 0.6964 - val_loss: 0.6016 - val_accuracy: 0.6811\n",
      "Epoch 299/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5834 - accuracy: 0.6996 - val_loss: 0.6014 - val_accuracy: 0.6901\n",
      "Epoch 300/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5814 - accuracy: 0.7014 - val_loss: 0.6041 - val_accuracy: 0.6824\n",
      "Epoch 301/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5846 - accuracy: 0.7036 - val_loss: 0.6032 - val_accuracy: 0.6760\n",
      "Epoch 302/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5825 - accuracy: 0.7030 - val_loss: 0.5998 - val_accuracy: 0.6849\n",
      "Epoch 303/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5800 - accuracy: 0.6968 - val_loss: 0.6035 - val_accuracy: 0.6760\n",
      "Epoch 304/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5782 - accuracy: 0.7008 - val_loss: 0.6002 - val_accuracy: 0.6849\n",
      "Epoch 305/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5838 - accuracy: 0.6990 - val_loss: 0.6075 - val_accuracy: 0.6760\n",
      "Epoch 306/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5812 - accuracy: 0.7042 - val_loss: 0.6024 - val_accuracy: 0.6901\n",
      "Epoch 307/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5852 - accuracy: 0.6976 - val_loss: 0.6010 - val_accuracy: 0.6837\n",
      "Epoch 308/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5806 - accuracy: 0.7009 - val_loss: 0.5948 - val_accuracy: 0.7003\n",
      "Epoch 309/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5762 - accuracy: 0.7029 - val_loss: 0.5972 - val_accuracy: 0.6875\n",
      "Epoch 310/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5803 - accuracy: 0.7029 - val_loss: 0.5955 - val_accuracy: 0.6901\n",
      "Epoch 311/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5817 - accuracy: 0.7009 - val_loss: 0.5970 - val_accuracy: 0.6913\n",
      "Epoch 312/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5809 - accuracy: 0.7042 - val_loss: 0.5961 - val_accuracy: 0.6837\n",
      "Epoch 313/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5795 - accuracy: 0.6991 - val_loss: 0.5949 - val_accuracy: 0.6862\n",
      "Epoch 314/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5810 - accuracy: 0.6987 - val_loss: 0.5946 - val_accuracy: 0.6913\n",
      "Epoch 315/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5738 - accuracy: 0.7061 - val_loss: 0.5955 - val_accuracy: 0.6901\n",
      "Epoch 316/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5775 - accuracy: 0.6996 - val_loss: 0.5978 - val_accuracy: 0.6952\n",
      "Epoch 317/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5784 - accuracy: 0.7066 - val_loss: 0.5944 - val_accuracy: 0.7003\n",
      "Epoch 318/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5806 - accuracy: 0.6975 - val_loss: 0.6006 - val_accuracy: 0.6888\n",
      "Epoch 319/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5765 - accuracy: 0.7053 - val_loss: 0.6012 - val_accuracy: 0.6849\n",
      "Epoch 320/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5750 - accuracy: 0.7093 - val_loss: 0.5981 - val_accuracy: 0.6811\n",
      "Epoch 321/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5814 - accuracy: 0.6994 - val_loss: 0.5928 - val_accuracy: 0.6952\n",
      "Epoch 322/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5791 - accuracy: 0.7058 - val_loss: 0.5978 - val_accuracy: 0.6786\n",
      "Epoch 323/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5769 - accuracy: 0.7093 - val_loss: 0.6009 - val_accuracy: 0.6760\n",
      "Epoch 324/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5786 - accuracy: 0.7117 - val_loss: 0.6029 - val_accuracy: 0.6811\n",
      "Epoch 325/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5735 - accuracy: 0.7086 - val_loss: 0.6043 - val_accuracy: 0.6786\n",
      "Epoch 326/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5829 - accuracy: 0.7032 - val_loss: 0.6016 - val_accuracy: 0.6811\n",
      "Epoch 327/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5812 - accuracy: 0.7039 - val_loss: 0.5953 - val_accuracy: 0.6888\n",
      "Epoch 328/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5720 - accuracy: 0.7014 - val_loss: 0.5987 - val_accuracy: 0.6913\n",
      "Epoch 329/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5789 - accuracy: 0.7038 - val_loss: 0.5999 - val_accuracy: 0.6913\n",
      "Epoch 330/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5789 - accuracy: 0.7033 - val_loss: 0.5990 - val_accuracy: 0.6888\n",
      "Epoch 331/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5746 - accuracy: 0.7034 - val_loss: 0.5988 - val_accuracy: 0.6837\n",
      "Epoch 332/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5780 - accuracy: 0.7025 - val_loss: 0.6018 - val_accuracy: 0.6888\n",
      "Epoch 333/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5748 - accuracy: 0.7080 - val_loss: 0.5981 - val_accuracy: 0.6824\n",
      "Epoch 334/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5784 - accuracy: 0.7005 - val_loss: 0.6031 - val_accuracy: 0.6824\n",
      "Epoch 335/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5787 - accuracy: 0.7033 - val_loss: 0.5985 - val_accuracy: 0.6952\n",
      "Epoch 336/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5782 - accuracy: 0.7044 - val_loss: 0.5956 - val_accuracy: 0.7041\n",
      "Epoch 337/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5708 - accuracy: 0.7075 - val_loss: 0.6049 - val_accuracy: 0.6760\n",
      "Epoch 338/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5720 - accuracy: 0.7046 - val_loss: 0.5964 - val_accuracy: 0.6964\n",
      "Epoch 339/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5808 - accuracy: 0.6995 - val_loss: 0.6031 - val_accuracy: 0.6798\n",
      "Epoch 340/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5779 - accuracy: 0.7056 - val_loss: 0.5994 - val_accuracy: 0.6901\n",
      "Epoch 341/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5712 - accuracy: 0.7144 - val_loss: 0.6024 - val_accuracy: 0.6798\n",
      "Epoch 342/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5757 - accuracy: 0.7060 - val_loss: 0.6003 - val_accuracy: 0.6888\n",
      "Epoch 343/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5750 - accuracy: 0.7099 - val_loss: 0.6061 - val_accuracy: 0.6798\n",
      "Epoch 344/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5672 - accuracy: 0.7132 - val_loss: 0.6027 - val_accuracy: 0.6977\n",
      "Epoch 345/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5737 - accuracy: 0.7086 - val_loss: 0.6009 - val_accuracy: 0.6939\n",
      "Epoch 346/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5734 - accuracy: 0.7060 - val_loss: 0.6006 - val_accuracy: 0.6952\n",
      "Epoch 347/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5767 - accuracy: 0.7070 - val_loss: 0.5990 - val_accuracy: 0.6926\n",
      "Epoch 348/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5759 - accuracy: 0.7086 - val_loss: 0.5973 - val_accuracy: 0.6939\n",
      "Epoch 349/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5740 - accuracy: 0.7072 - val_loss: 0.5961 - val_accuracy: 0.6926\n",
      "Epoch 350/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5701 - accuracy: 0.7051 - val_loss: 0.5945 - val_accuracy: 0.6875\n",
      "Epoch 351/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5686 - accuracy: 0.7081 - val_loss: 0.6008 - val_accuracy: 0.6798\n",
      "Epoch 352/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5741 - accuracy: 0.7111 - val_loss: 0.5895 - val_accuracy: 0.7066\n",
      "Epoch 353/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5686 - accuracy: 0.7133 - val_loss: 0.5956 - val_accuracy: 0.6952\n",
      "Epoch 354/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5788 - accuracy: 0.7043 - val_loss: 0.5980 - val_accuracy: 0.6888\n",
      "Epoch 355/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5727 - accuracy: 0.7067 - val_loss: 0.5978 - val_accuracy: 0.6862\n",
      "Epoch 356/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5765 - accuracy: 0.7005 - val_loss: 0.5977 - val_accuracy: 0.6913\n",
      "Epoch 357/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5667 - accuracy: 0.7093 - val_loss: 0.6022 - val_accuracy: 0.6837\n",
      "Epoch 358/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5733 - accuracy: 0.7051 - val_loss: 0.5977 - val_accuracy: 0.6913\n",
      "Epoch 359/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5725 - accuracy: 0.7085 - val_loss: 0.5954 - val_accuracy: 0.6964\n",
      "Epoch 360/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5715 - accuracy: 0.7139 - val_loss: 0.6000 - val_accuracy: 0.6952\n",
      "Epoch 361/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5749 - accuracy: 0.7033 - val_loss: 0.6006 - val_accuracy: 0.6875\n",
      "Epoch 362/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5705 - accuracy: 0.7104 - val_loss: 0.5983 - val_accuracy: 0.6862\n",
      "Epoch 363/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5677 - accuracy: 0.7152 - val_loss: 0.6001 - val_accuracy: 0.6849\n",
      "Epoch 364/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5668 - accuracy: 0.7119 - val_loss: 0.5992 - val_accuracy: 0.6939\n",
      "Epoch 365/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5698 - accuracy: 0.7126 - val_loss: 0.5976 - val_accuracy: 0.6977\n",
      "Epoch 366/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5712 - accuracy: 0.7104 - val_loss: 0.6005 - val_accuracy: 0.6824\n",
      "Epoch 367/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5647 - accuracy: 0.7111 - val_loss: 0.5934 - val_accuracy: 0.6990\n",
      "Epoch 368/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5730 - accuracy: 0.7044 - val_loss: 0.5992 - val_accuracy: 0.6939\n",
      "Epoch 369/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5699 - accuracy: 0.7103 - val_loss: 0.6007 - val_accuracy: 0.6901\n",
      "Epoch 370/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5738 - accuracy: 0.7056 - val_loss: 0.5963 - val_accuracy: 0.6888\n",
      "Epoch 371/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5605 - accuracy: 0.7155 - val_loss: 0.6008 - val_accuracy: 0.6786\n",
      "Epoch 372/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5717 - accuracy: 0.7114 - val_loss: 0.6002 - val_accuracy: 0.6786\n",
      "Epoch 373/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5710 - accuracy: 0.7069 - val_loss: 0.5962 - val_accuracy: 0.6913\n",
      "Epoch 374/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5724 - accuracy: 0.7078 - val_loss: 0.6011 - val_accuracy: 0.6837\n",
      "Epoch 375/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5687 - accuracy: 0.7086 - val_loss: 0.5993 - val_accuracy: 0.6939\n",
      "Epoch 376/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5668 - accuracy: 0.7126 - val_loss: 0.6044 - val_accuracy: 0.6824\n",
      "Epoch 377/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5704 - accuracy: 0.7112 - val_loss: 0.5979 - val_accuracy: 0.6901\n",
      "Epoch 378/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5649 - accuracy: 0.7133 - val_loss: 0.5978 - val_accuracy: 0.6939\n",
      "Epoch 379/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5649 - accuracy: 0.7184 - val_loss: 0.6023 - val_accuracy: 0.6875\n",
      "Epoch 380/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5667 - accuracy: 0.7132 - val_loss: 0.5986 - val_accuracy: 0.6926\n",
      "Epoch 381/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5689 - accuracy: 0.7091 - val_loss: 0.5997 - val_accuracy: 0.6939\n",
      "Epoch 382/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5609 - accuracy: 0.7186 - val_loss: 0.6016 - val_accuracy: 0.6862\n",
      "Epoch 383/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5657 - accuracy: 0.7139 - val_loss: 0.5992 - val_accuracy: 0.6926\n",
      "Epoch 384/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5642 - accuracy: 0.7166 - val_loss: 0.5982 - val_accuracy: 0.6939\n",
      "Epoch 385/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5616 - accuracy: 0.7201 - val_loss: 0.6017 - val_accuracy: 0.6811\n",
      "Epoch 386/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5706 - accuracy: 0.7116 - val_loss: 0.5979 - val_accuracy: 0.6786\n",
      "Epoch 387/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5736 - accuracy: 0.7055 - val_loss: 0.5936 - val_accuracy: 0.6849\n",
      "Epoch 388/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5719 - accuracy: 0.7118 - val_loss: 0.5944 - val_accuracy: 0.6939\n",
      "Epoch 389/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5579 - accuracy: 0.7192 - val_loss: 0.6004 - val_accuracy: 0.6888\n",
      "Epoch 390/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5641 - accuracy: 0.7168 - val_loss: 0.6017 - val_accuracy: 0.6824\n",
      "Epoch 391/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5664 - accuracy: 0.7122 - val_loss: 0.6034 - val_accuracy: 0.6849\n",
      "Epoch 392/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5685 - accuracy: 0.7177 - val_loss: 0.5987 - val_accuracy: 0.6888\n",
      "Epoch 393/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5662 - accuracy: 0.7132 - val_loss: 0.6020 - val_accuracy: 0.6875\n",
      "Epoch 394/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5655 - accuracy: 0.7141 - val_loss: 0.6022 - val_accuracy: 0.6888\n",
      "Epoch 395/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5660 - accuracy: 0.7116 - val_loss: 0.6013 - val_accuracy: 0.6849\n",
      "Epoch 396/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5651 - accuracy: 0.7172 - val_loss: 0.6023 - val_accuracy: 0.6773\n",
      "Epoch 397/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5754 - accuracy: 0.7050 - val_loss: 0.5998 - val_accuracy: 0.6811\n",
      "Epoch 398/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5681 - accuracy: 0.7141 - val_loss: 0.5986 - val_accuracy: 0.6798\n",
      "Epoch 399/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5650 - accuracy: 0.7158 - val_loss: 0.6020 - val_accuracy: 0.6798\n",
      "Epoch 400/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5643 - accuracy: 0.7116 - val_loss: 0.6001 - val_accuracy: 0.6849\n",
      "Epoch 401/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5627 - accuracy: 0.7147 - val_loss: 0.6023 - val_accuracy: 0.6811\n",
      "Epoch 402/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5642 - accuracy: 0.7154 - val_loss: 0.5998 - val_accuracy: 0.6824\n",
      "Epoch 403/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5594 - accuracy: 0.7227 - val_loss: 0.5986 - val_accuracy: 0.6952\n",
      "Epoch 404/500\n",
      "246/246 [==============================] - 3s 11ms/step - loss: 0.5640 - accuracy: 0.7187 - val_loss: 0.5989 - val_accuracy: 0.6849\n",
      "Epoch 405/500\n",
      "246/246 [==============================] - 2s 8ms/step - loss: 0.5628 - accuracy: 0.7161 - val_loss: 0.6029 - val_accuracy: 0.6901\n",
      "Epoch 406/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5596 - accuracy: 0.7180 - val_loss: 0.6011 - val_accuracy: 0.6901\n",
      "Epoch 407/500\n",
      "246/246 [==============================] - 2s 7ms/step - loss: 0.5643 - accuracy: 0.7179 - val_loss: 0.6010 - val_accuracy: 0.6811\n",
      "Epoch 408/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5613 - accuracy: 0.7210 - val_loss: 0.5994 - val_accuracy: 0.6913\n",
      "Epoch 409/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5580 - accuracy: 0.7215 - val_loss: 0.6036 - val_accuracy: 0.6786\n",
      "Epoch 410/500\n",
      "246/246 [==============================] - 1s 5ms/step - loss: 0.5638 - accuracy: 0.7152 - val_loss: 0.6003 - val_accuracy: 0.6875\n",
      "Epoch 411/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5571 - accuracy: 0.7184 - val_loss: 0.6016 - val_accuracy: 0.6824\n",
      "Epoch 412/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5595 - accuracy: 0.7210 - val_loss: 0.6071 - val_accuracy: 0.6760\n",
      "Epoch 413/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5635 - accuracy: 0.7149 - val_loss: 0.6021 - val_accuracy: 0.6760\n",
      "Epoch 414/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5641 - accuracy: 0.7119 - val_loss: 0.6025 - val_accuracy: 0.6786\n",
      "Epoch 415/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5605 - accuracy: 0.7193 - val_loss: 0.6016 - val_accuracy: 0.6837\n",
      "Epoch 416/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5574 - accuracy: 0.7150 - val_loss: 0.6065 - val_accuracy: 0.6760\n",
      "Epoch 417/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5593 - accuracy: 0.7152 - val_loss: 0.6038 - val_accuracy: 0.6747\n",
      "Epoch 418/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5662 - accuracy: 0.7151 - val_loss: 0.5987 - val_accuracy: 0.6837\n",
      "Epoch 419/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5530 - accuracy: 0.7235 - val_loss: 0.6017 - val_accuracy: 0.6824\n",
      "Epoch 420/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5533 - accuracy: 0.7230 - val_loss: 0.6001 - val_accuracy: 0.6888\n",
      "Epoch 421/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5609 - accuracy: 0.7164 - val_loss: 0.6035 - val_accuracy: 0.6901\n",
      "Epoch 422/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5598 - accuracy: 0.7169 - val_loss: 0.6031 - val_accuracy: 0.6849\n",
      "Epoch 423/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5577 - accuracy: 0.7219 - val_loss: 0.6021 - val_accuracy: 0.6888\n",
      "Epoch 424/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5608 - accuracy: 0.7199 - val_loss: 0.6016 - val_accuracy: 0.6939\n",
      "Epoch 425/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5575 - accuracy: 0.7199 - val_loss: 0.6001 - val_accuracy: 0.6875\n",
      "Epoch 426/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5633 - accuracy: 0.7116 - val_loss: 0.5997 - val_accuracy: 0.6837\n",
      "Epoch 427/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5644 - accuracy: 0.7116 - val_loss: 0.5997 - val_accuracy: 0.6798\n",
      "Epoch 428/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5657 - accuracy: 0.7177 - val_loss: 0.6032 - val_accuracy: 0.6773\n",
      "Epoch 429/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5591 - accuracy: 0.7147 - val_loss: 0.6015 - val_accuracy: 0.6760\n",
      "Epoch 430/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5589 - accuracy: 0.7166 - val_loss: 0.6042 - val_accuracy: 0.6722\n",
      "Epoch 431/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5567 - accuracy: 0.7272 - val_loss: 0.6029 - val_accuracy: 0.6709\n",
      "Epoch 432/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5592 - accuracy: 0.7169 - val_loss: 0.6010 - val_accuracy: 0.6786\n",
      "Epoch 433/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5582 - accuracy: 0.7202 - val_loss: 0.5990 - val_accuracy: 0.6913\n",
      "Epoch 434/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5605 - accuracy: 0.7159 - val_loss: 0.6020 - val_accuracy: 0.6901\n",
      "Epoch 435/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5572 - accuracy: 0.7201 - val_loss: 0.5966 - val_accuracy: 0.6952\n",
      "Epoch 436/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5567 - accuracy: 0.7173 - val_loss: 0.6027 - val_accuracy: 0.6926\n",
      "Epoch 437/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5620 - accuracy: 0.7168 - val_loss: 0.6012 - val_accuracy: 0.6913\n",
      "Epoch 438/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5584 - accuracy: 0.7222 - val_loss: 0.6040 - val_accuracy: 0.6875\n",
      "Epoch 439/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5631 - accuracy: 0.7182 - val_loss: 0.6032 - val_accuracy: 0.6824\n",
      "Epoch 440/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5566 - accuracy: 0.7217 - val_loss: 0.6038 - val_accuracy: 0.6824\n",
      "Epoch 441/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5555 - accuracy: 0.7244 - val_loss: 0.6043 - val_accuracy: 0.6926\n",
      "Epoch 442/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5681 - accuracy: 0.7113 - val_loss: 0.6002 - val_accuracy: 0.6811\n",
      "Epoch 443/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5537 - accuracy: 0.7202 - val_loss: 0.6046 - val_accuracy: 0.6735\n",
      "Epoch 444/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5545 - accuracy: 0.7177 - val_loss: 0.6037 - val_accuracy: 0.6888\n",
      "Epoch 445/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5574 - accuracy: 0.7164 - val_loss: 0.5963 - val_accuracy: 0.6926\n",
      "Epoch 446/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5553 - accuracy: 0.7169 - val_loss: 0.5993 - val_accuracy: 0.6773\n",
      "Epoch 447/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5572 - accuracy: 0.7215 - val_loss: 0.6021 - val_accuracy: 0.6786\n",
      "Epoch 448/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5547 - accuracy: 0.7208 - val_loss: 0.5967 - val_accuracy: 0.6926\n",
      "Epoch 449/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5519 - accuracy: 0.7269 - val_loss: 0.5989 - val_accuracy: 0.6837\n",
      "Epoch 450/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5558 - accuracy: 0.7216 - val_loss: 0.6039 - val_accuracy: 0.6735\n",
      "Epoch 451/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5561 - accuracy: 0.7240 - val_loss: 0.5990 - val_accuracy: 0.6888\n",
      "Epoch 452/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5560 - accuracy: 0.7221 - val_loss: 0.6009 - val_accuracy: 0.6786\n",
      "Epoch 453/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5544 - accuracy: 0.7252 - val_loss: 0.6015 - val_accuracy: 0.6747\n",
      "Epoch 454/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5590 - accuracy: 0.7198 - val_loss: 0.5985 - val_accuracy: 0.6824\n",
      "Epoch 455/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5586 - accuracy: 0.7206 - val_loss: 0.6052 - val_accuracy: 0.6824\n",
      "Epoch 456/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5558 - accuracy: 0.7178 - val_loss: 0.6065 - val_accuracy: 0.6722\n",
      "Epoch 457/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5551 - accuracy: 0.7233 - val_loss: 0.5979 - val_accuracy: 0.6939\n",
      "Epoch 458/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5570 - accuracy: 0.7174 - val_loss: 0.5999 - val_accuracy: 0.6849\n",
      "Epoch 459/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5568 - accuracy: 0.7207 - val_loss: 0.6021 - val_accuracy: 0.6773\n",
      "Epoch 460/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5545 - accuracy: 0.7215 - val_loss: 0.6007 - val_accuracy: 0.6952\n",
      "Epoch 461/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5556 - accuracy: 0.7224 - val_loss: 0.5987 - val_accuracy: 0.7054\n",
      "Epoch 462/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5475 - accuracy: 0.7271 - val_loss: 0.5989 - val_accuracy: 0.7028\n",
      "Epoch 463/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5562 - accuracy: 0.7239 - val_loss: 0.6007 - val_accuracy: 0.6875\n",
      "Epoch 464/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5528 - accuracy: 0.7224 - val_loss: 0.5973 - val_accuracy: 0.6875\n",
      "Epoch 465/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5537 - accuracy: 0.7255 - val_loss: 0.5981 - val_accuracy: 0.6862\n",
      "Epoch 466/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5543 - accuracy: 0.7240 - val_loss: 0.5935 - val_accuracy: 0.7015\n",
      "Epoch 467/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5549 - accuracy: 0.7210 - val_loss: 0.6024 - val_accuracy: 0.6824\n",
      "Epoch 468/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5537 - accuracy: 0.7182 - val_loss: 0.6038 - val_accuracy: 0.6786\n",
      "Epoch 469/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5571 - accuracy: 0.7230 - val_loss: 0.6032 - val_accuracy: 0.6773\n",
      "Epoch 470/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5569 - accuracy: 0.7194 - val_loss: 0.6011 - val_accuracy: 0.6952\n",
      "Epoch 471/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5476 - accuracy: 0.7266 - val_loss: 0.6090 - val_accuracy: 0.6735\n",
      "Epoch 472/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5529 - accuracy: 0.7247 - val_loss: 0.6013 - val_accuracy: 0.6849\n",
      "Epoch 473/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5486 - accuracy: 0.7296 - val_loss: 0.5988 - val_accuracy: 0.6926\n",
      "Epoch 474/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5524 - accuracy: 0.7248 - val_loss: 0.6024 - val_accuracy: 0.6798\n",
      "Epoch 475/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5489 - accuracy: 0.7264 - val_loss: 0.5969 - val_accuracy: 0.6952\n",
      "Epoch 476/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5475 - accuracy: 0.7215 - val_loss: 0.6028 - val_accuracy: 0.6875\n",
      "Epoch 477/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5500 - accuracy: 0.7272 - val_loss: 0.5989 - val_accuracy: 0.6939\n",
      "Epoch 478/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5526 - accuracy: 0.7196 - val_loss: 0.6029 - val_accuracy: 0.6862\n",
      "Epoch 479/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5554 - accuracy: 0.7177 - val_loss: 0.6018 - val_accuracy: 0.6862\n",
      "Epoch 480/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5535 - accuracy: 0.7233 - val_loss: 0.5997 - val_accuracy: 0.6888\n",
      "Epoch 481/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5509 - accuracy: 0.7224 - val_loss: 0.5999 - val_accuracy: 0.6786\n",
      "Epoch 482/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5551 - accuracy: 0.7183 - val_loss: 0.6006 - val_accuracy: 0.6824\n",
      "Epoch 483/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5520 - accuracy: 0.7193 - val_loss: 0.5953 - val_accuracy: 0.6862\n",
      "Epoch 484/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5544 - accuracy: 0.7250 - val_loss: 0.5998 - val_accuracy: 0.6901\n",
      "Epoch 485/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5545 - accuracy: 0.7252 - val_loss: 0.5987 - val_accuracy: 0.6913\n",
      "Epoch 486/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5541 - accuracy: 0.7235 - val_loss: 0.6023 - val_accuracy: 0.6888\n",
      "Epoch 487/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5491 - accuracy: 0.7266 - val_loss: 0.5971 - val_accuracy: 0.6926\n",
      "Epoch 488/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5500 - accuracy: 0.7236 - val_loss: 0.5990 - val_accuracy: 0.6913\n",
      "Epoch 489/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5537 - accuracy: 0.7208 - val_loss: 0.6069 - val_accuracy: 0.6760\n",
      "Epoch 490/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5498 - accuracy: 0.7235 - val_loss: 0.6041 - val_accuracy: 0.6709\n",
      "Epoch 491/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5465 - accuracy: 0.7296 - val_loss: 0.5969 - val_accuracy: 0.6875\n",
      "Epoch 492/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5479 - accuracy: 0.7254 - val_loss: 0.6030 - val_accuracy: 0.6798\n",
      "Epoch 493/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5465 - accuracy: 0.7271 - val_loss: 0.6065 - val_accuracy: 0.6786\n",
      "Epoch 494/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5479 - accuracy: 0.7239 - val_loss: 0.5997 - val_accuracy: 0.6747\n",
      "Epoch 495/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5543 - accuracy: 0.7227 - val_loss: 0.6002 - val_accuracy: 0.6888\n",
      "Epoch 496/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5479 - accuracy: 0.7259 - val_loss: 0.5995 - val_accuracy: 0.6798\n",
      "Epoch 497/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5495 - accuracy: 0.7215 - val_loss: 0.6033 - val_accuracy: 0.6798\n",
      "Epoch 498/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5448 - accuracy: 0.7323 - val_loss: 0.6004 - val_accuracy: 0.6773\n",
      "Epoch 499/500\n",
      "246/246 [==============================] - 1s 4ms/step - loss: 0.5473 - accuracy: 0.7269 - val_loss: 0.6015 - val_accuracy: 0.6837\n",
      "Epoch 500/500\n",
      "246/246 [==============================] - 1s 3ms/step - loss: 0.5497 - accuracy: 0.7249 - val_loss: 0.6013 - val_accuracy: 0.6811\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "\n",
    "model = init_keras_model(dense1=750, dense2=150, dropout=0.8, optimizer='sgd')\n",
    "\n",
    "history = model.fit(X_train_tensor,\n",
    "          y_train_tensor,\n",
    "          epochs=n_epochs,\n",
    "          validation_data=(X_val_tensor, y_val_tensor),\n",
    "          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(history):\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax1.plot(history.history['loss'], label='Training Loss', c='red')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy', c='blue')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Training Loss/Accuracy')\n",
    "\n",
    "\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='lower right')\n",
    "\n",
    "\n",
    "\n",
    "    ax3 = plt.subplot(1, 2, 2)\n",
    "    ax3.plot(history.history['val_accuracy'], label='Validation Accuracy', c='purple')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "\n",
    "    ax3.plot(history.history['accuracy'], label='Training Accuracy', c='orange')\n",
    "    ax3.set_title('Training Accuracy/Validation Accuracy')\n",
    "    ax3.legend(loc='lower right')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqwAAAHWCAYAAACyk9sKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAADHA0lEQVR4nOzdd1wT5x8H8E8S9lZkKCJuBfcW3HvXUXfdWrXuapfVarX2R6ut4qjaKkptnS11tE7ce68qdQ9AQQRlyYb7/XEkuUsuIZMMvu/XK6/cPffc3ZMQLt889wwRwzAMCCGEEEIIMVNiUxeAEEIIIYQQdShgJYQQQgghZo0CVkIIIYQQYtYoYCWEEEIIIWaNAlZCCCGEEGLWKGAlhBBCCCFmjQJWQgghhBBi1ihgJYQQQgghZo0CVkIIIYQQYtYoYNWDSCTS6HHy5Em9zvP1119DJBLptO/JkycNUgZ9zv3nn3+W+LmLc/v2bYhEIty4cYOXnpSUBHt7e4hEIly9etVEpSOlHV1bNLdv3z6IRCJ4enoiJyfHpGWxBI0bN8bUqVPh4+ODli1bqsxXWFiISpUqoX79+hofW+gzoc1nrHLlyhgzZozG55PKzMzE119/LfhZjIiIgEgkwrNnz7Q+riE1btwYIpEIP/zwg0nLYclsTF0AS3bhwgXe+jfffIMTJ07g+PHjvPSgoCC9zjNhwgR0795dp30bN26MCxcu6F0GaxMZGYkqVaqgUaNGvPTffvsNubm5AIDw8HA0bdrUFMUjpRxdWzQXHh4OAHjz5g327NmDIUOGmLQ85uzp06e4ceMGwsLC4OjoiB9//BHR0dGCf8OjR48iNjYWc+bM0euc+nzGNJWZmYlFixYBANq3b8/b1qtXL1y4cAHly5c3ahnUuXnzpqxyJDw8HJ988onJymLJKGDVg+KvUy8vL4jFYrW/WgH2n8vJyUnj81SsWBEVK1bUqYxubm7Flqc0+vPPP/H+++8rpW/atAne3t4ICAjA9u3bsXz5cjg6OpqghOrl5eVBJBLBxob+ha0RXVs0k5CQgAMHDqBjx444f/48wsPDzTZg1fZvYwx//vknvL290bp1a3h5eeHHH3/Epk2bBGv9Nm3aBDs7O4wYMUKvc+rzGTMELy8veHl5mez8ALBx40YAbPC8f/9+nD9/HiEhISYtkxCGYZCdnW2W33kANQkwuvbt26Nu3bo4ffo0QkJC4OTkhHHjxgEAdu7cia5du6J8+fJwdHREYGAgvvjiC7x79453DKFbKpUrV0bv3r1x6NAhNG7cGI6OjqhduzY2bdrEyyd0i2bMmDFwcXHBo0eP0LNnT7i4uMDf3x9z5sxRuqUWFxeHgQMHwtXVFR4eHvjggw9w5coViEQiREREGOQ9unPnDvr27YsyZcrAwcEBDRs2xK+//srLU1hYiCVLlqBWrVpwdHSEh4cH6tevj5UrV8ryvH79GhMnToS/vz/s7e3h5eWFVq1a4ejRo7xj3bt3D9HR0UoB66VLl3Dnzh2MHDkSH374IVJTUxEZGalU3sLCQqxevRoNGzaUlaVly5bYt28fL9+2bdsQHBwMFxcXuLi4oGHDhrLaIED17a/27dvzagmkf8PffvsNc+bMgZ+fH+zt7fHo0SO8fv0aU6ZMQVBQEFxcXODt7Y2OHTvizJkzSsfNycnB4sWLERgYCAcHB3h6eqJDhw44f/48AKBTp06oXbs2GIbh7ccwDKpXr45evXopHZOYDl1bgF9//RX5+fn4+OOPMWDAABw7dgzPnz9XypeSkoI5c+agatWqsLe3h7e3N3r27Il79+7J8hT3//Hs2TOVZROJRPj666+V3tfr169j4MCBKFOmDKpVqwYAuHr1KoYOHYrKlSvD0dERlStXxrBhwwTL/eLFC9k1zc7ODhUqVMDAgQPx6tUrZGRkwMPDA5MmTVLa79mzZ5BIJFi2bBkvPTIyEv3794dYLEZgYCCCg4Px22+/IT8/X+n92rt3L/r27QtPT0+tyqxI6DOWl5eHzz77DL6+vnByckLr1q1x+fJlpX01ub49e/ZMFpAuWrRI1lxGem1V1SRg06ZNaNCgARwcHFC2bFn0798f//33Hy+PNp9nVbKzs7Ft2zY0adIEK1askJ1byKFDh9CpUye4u7vDyckJgYGBCA0N5eW5dOkS+vTpA09PTzg4OKBatWqYNWsWr8yVK1dWOrbQ30EkEmHatGlYv349AgMDYW9vL/vuXbRoEVq0aIGyZcvCzc0NjRs3Rnh4uNL3A6D+u+6bb76BjY0NYmNjlfYbN24cPD09kZ2drfoN5KDqmRIQHx+PESNG4LPPPsP//vc/iMXs74SHDx+iZ8+emDVrFpydnXHv3j18//33uHz5stKtPyG3bt3CnDlz8MUXX8DHxwcbN27E+PHjUb16dbRt21btvnl5eXjvvfcwfvx4zJkzB6dPn8Y333wDd3d3LFiwAADw7t07dOjQAW/evMH333+P6tWr49ChQwatwbh//z5CQkLg7e2NVatWwdPTE7///jvGjBmDV69e4bPPPgMALF26FF9//TXmz5+Ptm3bIi8vD/fu3UNKSorsWCNHjsT169fx7bffombNmkhJScH169eRnJzMO2dkZCT8/PzQokULXrr0H2zcuHHw9/fHrFmzEB4erlTDMGbMGPz+++8YP348Fi9eDDs7O1y/fp13QVywYAG++eYbDBgwAHPmzIG7uzvu3Lmj0QVelblz5yI4OBjr16+HWCyGt7c3Xr9+DQBYuHAhfH19kZGRgd27d6N9+/Y4duyYLPDNz89Hjx49cObMGcyaNQsdO3ZEfn4+Ll68iJiYGISEhGDmzJno27cvjh07hs6dO8vOe/DgQTx+/BirVq3SuezEOEr7tWXTpk0oX748evToAUdHR2zbtg0RERFYuHChLE96ejpat26NZ8+e4fPPP0eLFi2QkZGB06dPIz4+HrVr19bo/0MXAwYMwNChQzF58mTZj4Vnz56hVq1aGDp0KMqWLYv4+HisW7cOzZo1Q3R0NMqVKweADVabNWuGvLw8fPnll6hfvz6Sk5Nx+PBhvH37Fj4+Phg3bhx++eUXLF26FO7u7rLzrl27FnZ2drIfMAD7A+Hy5cv45ptvZGnjx4/HhAkTsH//fvTt21eWvm3bNmRnZ2P8+PFalVlTH374IbZs2YJPPvkEXbp0wZ07dzBgwACkp6fz8r158waA+utb+fLlcejQIXTv3l32egCorVUNDQ3Fl19+iWHDhiE0NBTJycn4+uuvERwcjCtXrqBGjRqyvJp8ntX566+/8PbtW4wbNw41atRA69atsXPnToSFhcHFxUWWLzw8HB9++CHatWuH9evXw9vbGw8ePMCdO3dkeQ4fPow+ffogMDAQy5cvR6VKlfDs2TMcOXJEszdewJ49e3DmzBksWLAAvr6+8Pb2BsD+zSdNmoRKlSoBAC5evIjp06fjxYsXvNdd3HfdpEmT8O233+Lnn3/GkiVLZPu9efMGO3bswLRp0+Dg4KBZYRliMKNHj2acnZ15ae3atWMAMMeOHVO7b2FhIZOXl8ecOnWKAcDcunVLtm3hwoWM4p8qICCAcXBwYJ4/fy5Ly8rKYsqWLctMmjRJlnbixAkGAHPixAleOQEwu3bt4h2zZ8+eTK1atWTrP/30EwOAOXjwIC/fpEmTGADM5s2b1b4m6bn/+OMPlXmGDh3K2NvbMzExMbz0Hj16ME5OTkxKSgrDMAzTu3dvpmHDhmrP5+LiwsyaNUttHoZhmIYNGzLTp0/npb17945xc3NjWrZsKUsbPXo0IxKJmEePHsnSTp8+zQBg5s2bp/L4T548YSQSCfPBBx+oLUdAQAAzevRopfR27dox7dq1k61L38e2bdsW88oYJj8/n8nLy2M6derE9O/fX5a+ZcsWBgCzYcMGlfsWFBQwVatWZfr27ctL79GjB1OtWjWmsLCw2PMT46BrizLp/+IXX3whe51VqlRhAgICeJ/VxYsXMwCYqKgolcfS5P/j6dOnKssGgFm4cKFsXfq+LliwoNjXkZ+fz2RkZDDOzs7MypUrZenjxo1jbG1tmejoaJX7Pn78mBGLxcyKFStkaVlZWYynpyczduxYXt6wsDCmTJkyTF5eniwtPT2dcXFxYd577z1e3iZNmjD+/v5MQUGBVmUW+kwofsb+++8/BgDz8ccf8465detWBoDgNZF7XqHr2+vXr5X+BlKbN29mADBPnz5lGIZh3r59yzg6OjI9e/bk5YuJiWHs7e2Z4cOHy9I0/Tyr07FjR8bBwYF5+/Ytrzzh4eGyPOnp6YybmxvTunVrtdfZatWqMdWqVWOysrJU5hk9ejQTEBCglC70vw6AcXd3Z968eaP2NRQUFDB5eXnM4sWLGU9PT1kZNf2uGz16NOPt7c3k5OTI0r7//ntGLBbL/i6aoCYBJaBMmTLo2LGjUvqTJ08wfPhw+Pr6QiKRwNbWFu3atQMApVsTQho2bCj79QMADg4OqFmzpka1eCKRCH369OGl1a9fn7fvqVOn4OrqqtRgftiwYcUeX1PHjx9Hp06d4O/vz0sfM2YMMjMzZZ1Pmjdvjlu3bmHKlCk4fPgw0tLSlI7VvHlzREREYMmSJbh48SLy8vKU8jx58gQ3b95Uag6wa9cupKWl8Wokxo0bB4ZhsHnzZlnawYMHAQBTp05V+ZqioqJQUFCgNo8uhNrcAsD69evRuHFjODg4wMbGBra2tjh27BjvM3Tw4EE4ODjwXp8isViMadOm4Z9//kFMTAwA4PHjxzh06BCmTJmic29yYjyl+drCvSMiPe+YMWPw/PlzHDt2TJbv4MGDqFmzJu+ugSJN/j90IfQ/m5GRgc8//xzVq1eHjY0NbGxs4OLignfv3in9z3bo0AGBgYEqj1+1alX07t0ba9euld2q3bZtG5KTkzFt2jRe3sjISPTt25fX7t3FxQWDBw/GgQMH8OrVKwBsE61r165hzJgxshp7TcusiRMnTgAAPvjgA1764MGDBdvka3J908aFCxeQlZWl1BzL398fHTt25H12AM0+z6o8ffoUJ06cwIABA+Dh4QEAGDRoEFxdXXnNAs6fP4+0tDS119kHDx7g8ePHGD9+vOY1khro2LEjypQpo5R+/PhxdO7cGe7u7rJryIIFC5CcnIzExEQAmn/XzZw5E4mJifjjjz8AsM3q1q1bh169egk2X1CFAtYSINQ7MSMjA23atMGlS5ewZMkSnDx5EleuXMFff/0FAMjKyir2uJ6enkpp9vb2Gu3r5OSk9KG3t7fntSVJTk6Gj4+P0r5CabpKTk4WfH8qVKgg2w6wt8N/+OEHXLx4ET169ICnpyc6derEG3pq586dGD16NDZu3Ijg4GCULVsWo0aNQkJCgiwPt9MBV3h4OBwcHNC9e3ekpKQgJSUF9evXR+XKlREREYGCggIAbJsqiUQCX19fla9Jepve0B0NhN6n5cuX46OPPkKLFi0QGRmJixcv4sqVK+jevTvvc/D69WtUqFBB9gWkyrhx4+Do6Ij169cDAH766Sc4Ojoa/IucGEZpvbakp6fjjz/+QPPmzeHl5SX7n+3fvz9EIhGvrfjr16+L/V/U9P9DW0J/n+HDh2PNmjWYMGECDh8+jMuXL+PKlSvw8vJS+p/V5Boyc+ZMPHz4EFFRUQDY/9ng4GA0btxYlichIQHnzp0TDKDHjx+P/Px8/PbbbwDYZhYikQhjx47VusyakF7TFa+hNjY2Sp87Ta9vupxf1feOYhMyTT7PqmzatAkMw2DgwIGyz6i0icG5c+dkbag1+c4oye+Vy5cvo2vXrgCADRs24Ny5c7hy5QrmzZsHQH4N0bRMjRo1Qps2bfDTTz8BAP755x88e/ZM6UdVcagNawkQ+sV0/PhxvHz5EidPnpTVfADgtck0NU9PT8GG8NwA0BDniI+PV0p/+fIlAMjaRtnY2GD27NmYPXs2UlJScPToUXz55Zfo1q0bYmNj4eTkhHLlyiEsLAxhYWGIiYnBvn378MUXXyAxMRGHDh0CwNYy9OvXDxKJRHauBw8e4OzZswDAq1XiOnz4MHr27AkvLy8UFBQgISFB5TAp0rZTcXFxSjXHXA4ODoIN95OSkgTbhAl9jn7//Xe0b98e69at46UrtgXz8vLC2bNnUVhYqPZL2d3dXRb0f/LJJ9i8eTOGDx8uqx0g5qW0Xlu2b9+OzMxMXL58WbB2aPfu3Xj79i3KlCkDLy8vxMXFqT2eJv8f0qBF8X9WMcDhUvz7pKam4p9//sHChQvxxRdfyNJzcnJk7TW5ZSqu3ABbQ1a3bl2sWbMGLi4uuH79On7//Xdent27d8PZ2RldunRR2j8kJASBgYHYvHkzZs6cid9//x0dO3ZElSpVtC6zJqRBaUJCAvz8/GTp+fn5Su+lptc3Xc6v6ntH2/a4qhQWFso66A0YMEAwz6ZNm7B06VLed4YqmuQB1H+vCBG6huzYsQO2trb4559/eMH6nj17VJZJ3XcdAMyYMQODBg3C9evXsWbNGtSsWVPw86gO1bCaiPRDYm9vz0v/+eefTVEcQe3atUN6errsNrjUjh07DHaOTp06yb5gubZs2QInJyfBYXM8PDwwcOBATJ06FW/evBEcELpSpUqYNm0aunTpguvXrwMAYmNjceXKFaVaBmltzIYNG3DixAne48CBA7C1tZXdvunRowcAKF1Aubp27QqJRKI2D8D2xr59+zYv7cGDB7h//77a/bhEIpHSZ+j27dtK43j26NED2dnZGvW+njFjBpKSkmS1Atr+CiamVRquLeHh4XB1dcWxY8eU/meXLVuGnJwcbN26FQD72X/w4IHazmaa/H/4+PjAwcFB6X927969GpUZYP82DMMo/W02btwou4vDLdOJEyc0uh7MmDED+/fvx9y5c+Hj44NBgwbxtkdGRqJ3795K55UaN24coqOjMX/+fLx+/Zp3R0WbMmtC2hFU+veR2rVrl9JoBZpe36R5NKl1DQ4OhqOjo1JQHxcXJ2uiZgiHDx9GXFwcpk6dqvQZPXHiBOrUqYMtW7YgPz8fISEhcHd3x/r16wV74QNAzZo1Ua1aNWzatEntCAWVK1dGYmKirIkHAOTm5uLw4cMal106ZCK3YicrK0tWCy+l6XcdAPTv3x+VKlXCnDlzcPToUZ2amVENq4mEhISgTJkymDx5MhYuXAhbW1ts3boVt27dMnXRZEaPHo0VK1ZgxIgRWLJkCapXr46DBw/KPvia3j67ePGiYHq7du2wcOFC/PPPP+jQoQMWLFiAsmXLYuvWrdi/fz+v52ufPn1Qt25dNG3aFF5eXnj+/DnCwsIQEBCAGjVqIDU1FR06dMDw4cNRu3ZtuLq64sqVKzh06JDs121kZCQ8PDzQoUMHWRny8/OxZcsWBAYGynqXKurTpw/27duH169fo02bNhg5ciSWLFmCV69eyb4Ebty4AScnJ0yfPh2VK1fGl19+iW+++QZZWVkYNmwY3N3dER0djaSkJNkA1yNHjsSIESMwZcoUvP/++3j+/Dnv17YmevfujW+++QYLFy5Eu3btcP/+fSxevBhVqlThXfyHDRuGzZs3Y/Lkybh//z46dOiAwsJCXLp0CYGBgRg6dKgsb82aNdG9e3ccPHgQrVu3RoMGDTQuDzE9a7+23LlzB5cvX8ZHH30k2H63VatW+PHHHxEeHo5p06Zh1qxZ2LlzJ/r27YsvvvgCzZs3R1ZWFk6dOoXevXujQ4cOGv1/iEQijBgxAps2bUK1atXQoEEDXL58Gdu2bdP4dbu5uaFt27ZYtmwZypUrh8qVK+PUqVMIDw9XuouxePFiHDx4EG3btsWXX36JevXqISUlBYcOHcLs2bNRu3ZtWd4RI0Zg7ty5OH36NObPnw87OzvZtuTkZJw6dUrtj4FRo0bhyy+/xLJly+Dh4cGrEdSmzJoIDAzEiBEjEBYWBltbW3Tu3Bl37tzBDz/8ADc3N15eTa9vrq6uCAgIwN69e9GpUyeULVtWVlZFHh4e+Oqrr/Dll19i1KhRGDZsGJKTk7Fo0SI4ODjwRpjQR3h4OGxsbPDll1/KmrhxTZo0SfZDo2/fvvjxxx8xYcIEdO7cGR9++CF8fHzw6NEj3Lp1C2vWrAHANvfo06cPWrZsiY8//hiVKlVCTEwMDh8+LPsBMGTIECxYsABDhw7Fp59+iuzsbKxatUqrHxe9evXC8uXLMXz4cEycOBHJycn44YcflH48aPpdBwASiQRTp07F559/DmdnZ51mNKNRAgxIVU/eOnXqCOY/f/48ExwczDg5OTFeXl7MhAkTmOvXryv1RFXVk7dXr15Kx1TVw1yxJ69iOVWdJyYmhhkwYADj4uLCuLq6Mu+//z5z4MABBgCzd+9eVW8F79yqHtIy/fvvv0yfPn0Yd3d3xs7OjmnQoIFST9wff/yRCQkJYcqVK8fY2dkxlSpVYsaPH888e/aMYRiGyc7OZiZPnszUr1+fcXNzYxwdHZlatWoxCxcuZN69e8cwDMO0bt1aqQfqnj17GABMWFiYytdx6NAhBgDz448/MgzD9phcsWIFU7duXcbOzo5xd3dngoODmb///pu335YtW5hmzZoxDg4OjIuLC9OoUSPe6yosLGSWLl3KVK1alXFwcGCaNm3KHD9+XOXfUGi0hZycHOaTTz5h/Pz8GAcHB6Zx48bMnj17BHuKZmVlMQsWLGBq1KjB2NnZMZ6enkzHjh2Z8+fPKx03IiKCAcDs2LFD5ftCSg5dW+RmzZrFAGBu3rypMs8XX3zBAGCuXbvGMAzbM3zmzJlMpUqVGFtbW8bb25vp1asXc+/ePdk+mvx/pKamMhMmTGB8fHwYZ2dnpk+fPsyzZ89UjhLw+vVrpbLFxcUx77//PlOmTBnG1dWV6d69O3Pnzh3BUUNiY2OZcePGMb6+voytrS1ToUIFZvDgwcyrV6+UjjtmzBjGxsaGiYuL46Vv3LiRcXJykl0HVenfvz8DgJkyZYrOZdZklACGYa9bc+bMYby9vRkHBwemZcuWzIULF5SOp8317ejRo0yjRo0Ye3t73mgDiqMEcN+X+vXry67hffv2Ze7evcvLo83nmev169eMnZ0d069fP5V5pKMV9OnTR5Z24MABpl27doyzszPj5OTEBAUFMd9//z1vvwsXLjA9evRg3N3dGXt7e6ZatWpKIy4cOHCAadiwIePo6MhUrVqVWbNmjcpRAqZOnSpYvk2bNjG1atVi7O3tmapVqzKhoaFMeHi44HtZ3HedlPR/ZfLkySrfF3VERYUmRGP/+9//MH/+fMTExJh0BhNtSNtL7dmzR6nHJ1H2/vvv4+LFi3j27BlsbW1NXRxSSljitcUc5ObmonLlymjdujV27drF29azZ084OjoKToJCSElavXo1ZsyYgTt37qBOnTpa709NAoha0lsRtWvXRl5eHo4fP45Vq1ZhxIgRFvWF4uvrq1N7q9IkJycH169fx+XLl7F7924sX76cglViNNZybTGl169f4/79+9i8eTNevXrF6xQldeDAAROUjBC5Gzdu4OnTp1i8eDH69u2rU7AKUMBKiuHk5IQVK1bg2bNnyMnJQaVKlfD5559j/vz5pi4aMbD4+HiEhITAzc0NkyZNwvTp001dJGLF6Nqiv/3792Ps2LEoX7481q5dyxvKihBz0b9/fyQkJKBNmzayIRN1QU0CCCGEEEKIWaNhrQghhBBCiFmjgJUQQgghhJg1ClgJIYQQQohZo05XAvLz83Hjxg34+PgYfG5pQqxZYWEhXr16hUaNGsHGhi4vhK+wsBAvX76Eq6ur1rPcEFKaMQyD9PR0VKhQodTGJfSNIuDGjRto3ry5qYtBiMW6fPkymjVrZupiEDPz8uXLYuccJ4SoFhsbW2qHfaOAVYCPjw8A9ku3fPnyJi4NIZYjPj4ezZs3l/0PEcLl6uoKgP3SVZyGkxCiWlpaGvz9/WX/Q6URBawCpNXt5cuXL7W/ZAjRR2m9ZUXUkzYDcHNzo4CVEB2U5qY09K1CCCGEEELMGgWshBBCCCHErFHASgghhBBCzBoFrIQQQgghxKxRwEoIIYQQQswaBayEEEIIIcSsUcBKCCGEEELMGgWshBBCCCHErFHASgghhBBCzBoFrIQQQgghxKxRwEoIIYQQQswaBayEEEIIIcSsUcBKCCGEEELMGgWsukpPBx48AOLiTF0SQoyusBDIyDB1KQghxMrkZQBMoalLYREoYNXVvn1ArVrA2LGmLgkhRterF+DqCsTEmLokhBBiJbJeAX+4Akfbm7okFoECVn0xjKlLQIjRHTrEPv/2m2nLQQghViPuL/b59RnTlsNCUMCqK5GIfaaAlZQiubmmLgEhhFgJsZ2pS2BRKGDVFQWspBTKyzN1CQghxEpwA1aKJYpFAauuKGAlViA7G/jwQ2DvXuVtT58CI0cCt27J06iGlRBCNJB0GTg/Csh8qbzt+U7g8kcARPK0wpwSK5qlsjF1ASyWSFR8HkLM3Nq1wMaN7EPxt9fQocDly8CuXfI0ClgJIUQDR1qwzzmvgQ4H+dvODWWfMzmjDOW/AyQOJVM2C0U1rPqiGlZiwdT1+r99m33mBqnUJIAQQrSQekf1tkzOBTj/nfHLYuEoYNUVNQkgVs5G4P4L1bASQoiB5GdylilgLQ4FrLqigJWYAYYBPvsM+Okn4W1Xr7JzXEi9ewesWiVcszp2LLtdSihgpRpWQkipkPEUOD8CeHtTeVv+O7aNKvf7P+UO8HCd8iQAmXFsuuA5HvGPSdSigFVX1IaVmIEbN4Bly4Bp05S37d0LNGsGBAfL0+bNA2bOBFq2ZNdXrpRvi4gAli5ll9evB1JSlI+ZnW2okhNCiBk7MxB4thU41ER524nubBvVJxHytAP1gCtTgCe/Ajlv+PmvTGGf89KAy5OFz5dPUwkWhwJWfVENKzGh1FTV27ZuZZ/v3pWnHSxq+x8fD2RmKu8TF8fWon70kfAx37wRTieEEKuS+i/7LDRt6uuz7POjX4S3/btQ+Jj/LgIe/Sy8LStB+zKWMhSw6oqaBBAzU6hwXeXeBDh1in224wz798knyseQSPjDWCl69Ur38hFCiOXQ4C5q8kUg4xk/7ckmIOZP5byF+cDD9aqPlRmrVelKIwpYdUUBKzEz0val0ppTMee/u3174PRpwN5enrZOoFlVZibbjECVFy/0LiYhhFimfIHbUvuqKKdlC9SWPt4AFAjsL0UBa7EoYNUVBazEzOTmAteuAc7ObEcsxWbWJ08Ctrbqj/FSYIxrrrdv+R2zCCGkVLg+B9jlDLy5prxNkzgg+Yr67RSwFosCVl1RpytiAv/9B8ydK9whKi8P+PRTdnnZMuWPaFISv9ZVCLe9q5A3bwAnJ42LSwghluHZNuE2qVL3lrPPtxcob0u7V/zxM56q395qe/HHKOVMHrCuXbsWVapUgYODA5o0aYIzZ86ozb9161Y0aNAATk5OKF++PMaOHYvk5GRensjISAQFBcHe3h5BQUHYvXu38V4A1bASI3j7lu0gVVDAT588GfjuO6BjR3ad2241NxdIS5OvKwasq1cD58+rP29iovrtZcrQbzVCiAV7ewt4e5ufVpgPnP8AuDwJeHVS/f62bspp+4OKP2+imuM6VqBZrjRg0oB1586dmDVrFubNm4cbN26gTZs26NGjB2JUTL9z9uxZjBo1CuPHj8fdu3fxxx9/4MqVK5gwYYIsz4ULFzBkyBCMHDkSt27dwsiRIzF48GBcunTJsIWnJgHEiDp2BHr2ZINMrtOn2ecbN9hn7kD+r16xTQKkKLAkhBCO/EzgYEPgYAOgIEeeXsAZry/xtPJ+SZfly0IBq74KaLxATZg0YF2+fDnGjx+PCRMmIDAwEGFhYfD398c6od4gAC5evIjKlStjxowZqFKlClq3bo1Jkybh6tWrsjxhYWHo0qUL5s6di9q1a2Pu3Lno1KkTwsLCDFt4CliJEd28yT5HRADR0UBO0bXV1VWeZ9Ys/kD+48bxjxEfb5iyeHgY5jiEEGJSeZxZVLJeAmn32eVCTvD670Ig/gh/vyMt5MvGCFi55ycqmSxgzc3NxbVr19C1a1deeteuXXFexX3LkJAQxMXF4cCBA2AYBq9evcKff/6JXr16yfJcuHBB6ZjdunVTeUydUfUVKQG3bgF16gD16gFPn/LboK5cCWRlydevX+fve/y46uNWrap5GaSTCRBCiEVjOL/w/6nNPm7N5weyAHCim+pjvDykept3W83LIuaMMehcWfP9SjGTBaxJSUkoKCiAj48PL93HxwcJCcID6IaEhGDr1q0YMmQI7Ozs4OvrCw8PD6zm3DdNSEjQ6pgAkJOTg7S0NNkjPT1dZV4ZqmElJejhQ6BGDeXfSSpaz6jVqxfw/vua5x89Ghg2DNi4UftzEUKI2eDeei8sak9191vVA/0LSb2jelt1hRlX/PqozutaHeh6ic3TWmDcVqLE5J2uRArfwAzDKKVJRUdHY8aMGViwYAGuXbuGQ4cO4enTp5g8mT/VmTbHBIDQ0FC4u7vLHkFBGjSglh9c87yEcJw/DwQEAH/9pVn+ggLlXv6aBJFNm/LXmzUDPD35acuX89f79AF69ADWrGEnG9i2DRg/XrNyEkKISd38EvgnCMhN4aeraiv6XKCHfmGuclpxPBUGsXauAqUJCAI/A7zbAa12AeWaA+32Ae61tT9XKWSygLVcuXKQSCRKNZ+JiYlKNaRSoaGhaNWqFT799FPUr18f3bp1w9q1a7Fp0ybEFzXY8/X11eqYADB37lykpqbKHtHR0cW/AKphJXrq2ZOtIX3/fWDvXs32UZwa9cED9fmrVQPee4+f5unJjtXK5aDQQdXeHjhwAJg6VbNyEUKI2YgOBdL+A6LaAGkP5emqAtbCPOF0dep9rZxmX46/XpAFQCFGqNAT6HwS8Kij/TlLOZMFrHZ2dmjSpAmioqJ46VFRUQgJCRHcJzMzE2KFKiaJRAKArUUFgODgYKVjHjlyROUxAcDe3h5ubm6yhyu3Z4sqFLASHeXmAleuAKmp8rR+/YDHjw1/LldXtnMWl7MzP0D97DOgnMJ1trjxWgkhxOxkJQAJR+XrqXeAf2rK1w3ZG9/GBWi8gp9mqxA71JoBiO35aXYehitDKWNjypPPnj0bI0eORNOmTREcHIxffvkFMTExslv8c+fOxYsXL7BlyxYAQJ8+ffDhhx9i3bp16NatG+Lj4zFr1iw0b94cFSpUAADMnDkTbdu2xffff4++ffti7969OHr0KM6ePWvYwlOnK6KjXr2Ao0eV0+/fZ2tEDcnVlT+yAMAGq9zxXb/7Drh8mZ8nMNCw5SCEEKPKSwN2l1efp9CAAautK+DZkp8mUvil71GXrXXN4sxp7a5Fk0PCY9J6lCFDhiAsLAyLFy9Gw4YNcfr0aRw4cAABAQEAgPj4eN6YrGPGjMHy5cuxZs0a1K1bF4MGDUKtWrXwF6cRYEhICHbs2IHNmzejfv36iIiIwM6dO9GiRQul8xsE1bASLQkFqwAbyMbEGPYj5eKinObgALRvzy5LJwKoVEm+XSwGZswwXBkIIcToUtXMNnV1JhvQGrSG1RWwc1dOr7eIfa4zn3128JJvqz4JEBczPzZRyaQ1rAAwZcoUTJkyRXBbRESEUtr06dMxffp0tcccOHAgBg4caIjiqUZNAogRBAQAY8ca7njStqo7dgBDh7LLNWsCFSuywbF70fXW1xdo1IgdJuv2bcCWrqmEEEui7q7ng1Xso4YBG+XbugJOlZTT684HKg0C3Gqx6803AFGtgTpzgXpajEZAlJg8YLVYFLASI9m82XDHkk44MGQI4OfHTiZQp6itv7+/PJ9IxLarLSykYJUQYoE06dX/8CfDnc/Bl71wDs0HnmwCvFqz6SIx4M5pU+XZFBiUCkjshY9DNEZdK3RFbViJFuLjgX/+Mf7vm759+evffitfbt0aGDRI9b4SCQWrhBAL8/ockHIXyH9n3PO415Uve7UCyjZhl8USoPqH/CBVEQWrBkEBq66ohpUU49tvgcqVgRcv2FrNPn2ArVu1O8Ynnwin79qlnNawIbBqlXx9yxZ2hixCCLEaBbnAwSbA+VFA5gv2dvuBukB+hv7H9mwO1FBoomjjCjQI5Y+x2iGKKq1MgAJWfVHAWmrt26e6AxUAzJ8PPH8OfPMN8PYtm7Z7t+bHr1UL+P57oH595W01avDXN2xgp2blDlfl66v5uQghxCzkZQD31wCZccLbX58B3l4Hnv0GvHvO2S9N83PUXwJ0vaicXqaR8tBUg1KBOl8ADGdoFRtHzc9FDIYCVl1RDWuplpjI3n7v0oVt96lo5075cgbnh7/iwP/qtGrF9ti/do2f/vChcu//4cPZjyT3ln7REMWEEGI5bnwCXJsOHGmlvK0wD7g+R77ODSKz4jU/h3dboFwLoPMpTlo7oEkYf9xU73ac73qBCz0pURSw6opuB5Rqycny5aVL2efLl4EOHdiaTmmPfADIyxPerzh2duyzDadrpIsLUL06OxOV1EcfAU5Oynlp8H9CiMV5uZ99zowBXp9nl++GAmcGAg9+AlJuyfPmvpUvaxOw2hfNTe1YQZ4WNBeQOPDbmzYIlS9TwGpyNEqArqiGlRSZO5cdQ1U61G+fPvzt3Pam//6r+XFtBP47pbW13IDVkXN3ilvDSgErIcTycCqDoloBwxng1pfseuodftbE0/LlB6s1P4VdWfZZcSpVgF/DauPE2UABq6nRV5quKGAtNbIFxppW/LO/fi1ffvnSMOdV12NfWvsK8ANbqmElhFgEhtFsIH9unsJ8/rZ7P+p2bmnAassZ+L+gaJQBbsAq4dQGUA2rydFXmr4oYLVq+/axt+E3buSn5yoM+ccNDoVqRnXBPU7roiH+OnVin7k1rNxzc9utUsBKCDFb54cDf/kCWa/U58vm1AaIDXRxlRT94uc27XP0K9qmooaVAlaTo680XVENa6nQty9QUAB8+CE/XTFgzcyUL+sSsLZsCQQH89MmTJAv//UXsGwZsG0bu+7gIB8poBWnbwL3+uvtrX05CCGkRDzfAeSlAs9+V9ig0D8khxOw6jLWavMN/HXpDFRSnc8ALTaynbAAwKNoLEAHb8CeM61qmYban5sYFLVh1RV1uirVpDNISaWny5d1CVidnYH16+VB6PLl7BSqUl5e/DFZRSLg5k0gKQmopDA74O7dbHr16tqXgxBCjK6Q0xNVVMwFk1vDmqfDWKtOfkDVMcCTCHa9xy3+du/W7EOqXEugXxxg68avbQ2cA4ABKvTSvgzEIChg1RXVsFq9/HzV2xQD1tRU+bKuHwluu9Ty5YvP7+SkHKwCQL9+up2fEEJKBHf81OJkccZjzUvR/lwSB0DizFnXYNYpJz/h49Sdr/35icFQkwB9UcBqlRiGnZ1KFcUmAY8fy5ffFXPXys1NOU1xDFVu8EoIIVYj6TLwN2fmE+7QVIDy3cu0B/qdT+II6uFvHShg1RXVsFq1rCzggcB1UvrnVqxhlY7FqgkvL8DTU7lTFDdgVTdCACGEWKybn/LXc4sGp1b1XfqfFhdX387sM3d8VYkDdZiyEhSw6orasFo17i1+qfx8oHlz4L33gDgVswZqompVdn/uORRrWKmHPyHEKuVn8dfz0tgJAv7yBh6Haz7Fqo2LclrlkcD7SUCr7fI0iSMAqliyBtSGVVdUw2rVhALWS5eAq1fZ5b//1v3Y7duzvfwVcTtr0e8hQohVKlAIWPPfAWfeB3KSgEsThPcRIhL4Ve/TgZ3Fijt+KtWwWg2qx9EVBaxmr7CQ7UmvrvOUEIYBZsxQThcKYnXRsqVyGtWwEkIsRl4akHpP+/3ePVeerSr/nXIQqxGBX/XO/uyz0uD/9D1tDehrUV8UsJqtH34AGjUCxo7V7s80aBAQFaWcnqbhnSp1unYF2rYV3sYNWKmGlRBitv6uCewPBN5c0/zimv8O2FtZOb0gE2AKtC+DYg1r05842zgXUKphtRoUsOqKaljN3rffss+//w5UqwZMnqzZfpGRwumGqGE9fFh4nFaRiH+NpRpWQoghvHn8Bkc+PYL0l+nFZ9ZUdtHsVBfHApHlgPgjxe/z8Gfh9Px3AKPlbTBAOWCtOUW+zP1eljgaPWBlGAZnvzuLe3t1qHUmGqOvRV1RFZjZ416znj4FflZxvdTU69fKaZ6ewIEDuh+zQwf2ecoUfjoFrIQQQ9jcejMu/HABkcNV/BLXR8q/QO4b4ES34vOq6kyV/044oKy3CGi8Qs0BOd/BIdv4m1yqcLLZwNhNAl5ceoFjc49hZ7+dRj1PaUdfi7qiGlarkZMDfPQRsH8/sELN9fHJE+W04cOBHj2AHTuKP8+WLcppBw8C9+6xIw8A7PSsZcsqT9NKCDFfN3+9ibhLegwdYkQZCezsUHEXVJcvMzkTl1ZdwrvXOkx9qk7yVeDieCA1GrizSDhP2j3+zFdS5bsDtWbiZXIn4f1sOJMBVB7G32brCvR7AQx4zX5XB81l06treJtNSzlp8nEOC/J0aN5ANEKjBOiKAlazp+5PI922dCmwdy9w4QI7Nao6mzcrpzk5sc9DhrDtU8uWVc7z6afA6dNs21hF9vZALc7U1mfPsp3EaOIAQizD89PPsXfMXgDAQmahiUujmsReonLbX8P/wuMjj3F3512MOzdO95MwDPvdmBkH/PcDcH8lm/5kU3E7KifZOAEiEe7Gfo4rf7qg76S9/O01pwPPfgf83xc+pBNnLFb32sCQLLY9qxHYucgv2O8S38HNT2B2GKI3qmHVFwWsZuPUKTZoFBrwn+uzz9hhpaZPB774gg1WdeXIGT2lTBnA1ZW/3c6ODYovXhQeykqRWEzBKiGW5HW0QFuhEpSXmYdLqy/h5q83waj5PrKxt0FOeg4uhl3E3T/u8vI+PsJO1Rd7PlZ451tfAZc+VP99l5/JzmC1vw5wooc8WNWVDVsbIJKIcPN0I+XtzgFAjxuaT5dqpGAVAArz5U0apDXaxPCohlVX1IbV7LRvzz6PGAFcvix8bf3tN2DZMnb5p5+Ut2tLMQj18ADSi/o2dOoEfPed/ucghJgvdUFiSTg8+zCu/XwNAFCYV4jGExoL5pPYSfDX8L/w4B/2F/3QfUNRq08twbw8hQXA3SXscq2ZgEdd4XxPfwUyHgtv00XRWKpiiUC9mv/7QMV+hjuXnrjNADJfZ5qwJNaNalh1RU0CzNazZ6q3jRpl3HNzO0sdPQo0bWrc81mLtWvXokqVKnBwcECTJk1w5swZlXnHjBkDkUik9KhTpw4vX2RkJIKCgmBvb4+goCDs3r3b2C/DrNB7WjKYQuN/B6Q8S0HK8xTBbdJgFQCu/cIu5+fk49GhR4i/ES/bJrGXyIJVALi74y6S7iUhL0ug/ShXHmd4lMJc1fluzVN/HCHc8VIVSeQ1rDw1pwNt/gTE5lPfVpArD1hfXHmB7NRsE5bGelHAqisKWM2WRHVTLZ2oq0wvVOjcauhzlwY7d+7ErFmzMG/ePNy4cQNt2rRBjx49EBMTI5h/5cqViI+Plz1iY2NRtmxZDOI0Er5w4QKGDBmCkSNH4tatWxg5ciQGDx6MS5culdTLMil6T0uOsQPWvKw8rKyyEisrrxTs0CO2lX+Np8WyPfH/nvA3tvbYil8a/yLbZmPPD/D+3fYvfgr8iZdHuAAp8uWCbNXfeblv1R9HiIOX6m2qalh1GbPVyLgB68kFJ/FTbQPcviNKKGDVFwWsZkday2moP42zM/D++0BIiPI2Clj1t3z5cowfPx4TJkxAYGAgwsLC4O/vj3Xr1gnmd3d3h6+vr+xx9epVvH37FmPHjpXlCQsLQ5cuXTB37lzUrl0bc+fORadOnRAWFlZCr8q06D0tOYYOWBmGQVqcfAiolGcpsuU3j94oNUGQ2MovOrnv2BrQ27/fVjquxE744pR0L0l9gXLl50demm5jpqrSIBRwqgTUmKq8TczOpCISK9QYmOEkANyAFaB2rMZCAauuqIbVbBk6aJRIgD//BM6dA8qX529TDFilFVI1axq2DJYmPT0daWlpskdOTo5gvtzcXFy7dg1du3blpXft2hXnz5/X6Fzh4eHo3LkzAgICZGkXLlxQOma3bt00PqYlM6f3NCcnh/c5SDPEdHFmxtAB6+klp7HCfwUurWJrrn9t/6ts29qgtTj73Vlefm4Na3626mDSxkHHW+jcmtO8NOEhqHTlWh3o9xyov1h5W9F3rNhGIUzxVjFVoAkV5plfEG2NKGDVFXW6MiuJifJlQ9ewcgPgy5eBrVvl64oB64IF7Mxap08b5tyWKigoCO7u7rJHaGioYL6kpCQUFBTAx8eHl+7j44OEhIRizxMfH4+DBw9iwoQJvPSEhASdj2npzOk9DQ0N5X0O/P39tXgllsHQAevJBScBAIdmHgLADpPEdfzL4wDYWr28zDwU5Mhr9wrzClV2AlM3rJWQ/Jyi4Jc7i1V+uvp2rNoSFZXJviz2/zUHN042FMjCfteunDUThcHbgYChymU0McUaVmIcFLDqimpYTe7pU6B5c2DXLnbQfynnovGks7IMc56ZM+XLFSuykwVIB/rn3DEFwI6r+sEHgML3eqkTHR2N1NRU2WPu3Llq84sUfgAyDKOUJiQiIgIeHh7o16+fwY5pLczhPZ07dy7vcxAbq2LYJAtWEp2uFOW+y8XKqiuxzGuZUq1qboZwQKmqSYCQu6GzEf9dFfxY9jMUJl2Rb8hLk0/LagguVWWLL57WxNEdXVBYKAK828vSpW1YU16XQYHv+7Lv3strLuNbh29lQ3KZEgWsJYMCVl1RwGoSeXnAX38BSUnAxInAlSvsoP0ZnCZDubnsmKy6CArir/v4AF9+qZxvzx52+KpKlXQ7j7VzdXWFm5ub7GFvL9wbuFy5cpBIJEq1dImJiUq1eYoYhsGmTZswcuRI2CkMXuvr66vTMa2BOb2n9vb2vM+Bm5v1DahuioA16b8kpL9IR16m8u353HQVAaut+oDVtWwqajW5BzAM6gSsgH+NOLTucwr5adwmAenA6f46lTm7UKF2vft1wN5TtlqYX4jMdGd8N2Eu0OmYLJ07SgD31vvB6QcBwDhTzmqJAtaSQQGrvihgLVGrVrEdoLy8gH//ladncoa+e/cOUDOCj1pz5gAnT8rX27cHbASafolEgIuLbucgcnZ2dmjSpAmioqJ46VFRUQgR6uXGcerUKTx69Ajjx49X2hYcHKx0zCNHjhR7TGtA72nJ4gasDw8+xPKKy/HkqMA8zjpYWVV48P2MV6o79Sz3Wy6YrtR5ScHEJb9g6OwdwMlesjRbu3wUZHFrA94Caf+pPY4q0VnbcXZfa3lCmYa87dIREPJy7JD2MgMrq67E2e/O8sotzcPtKCYUtJc0mo61ZFDAqqtSdGvRnHCHfXzFuTP1jtPMKz4e+Oor3Y7foQPQrp18va35te+3OrNnz8bGjRuxadMm/Pfff/j4448RExODyZPZeb/nzp2LUQID6IaHh6NFixaoW1d5IPOZM2fiyJEj+P7773Hv3j18//33OHr0KGbNmmXsl2MW6D0tOdyAdVvPbUh/kY7fuvym8/G4NYopT1ME86TGpAqmqz+w+s0u7kUX0fiDsrS8HFsgn3NxvR+m/XkBvE30AMM44OqxJpzy8AvEnS3q3PfnkPI0BcfmHuM1OZHWsO4bv0+Wlp9l+nasVMNaMsxn5F1LQ00CSgTDsB2bpB2fypUTzvfNN/qf68EDoEoVdvnhQ7bjlLEnGiDAkCFDkJycjMWLFyM+Ph5169bFgQMHZD3U4+PjlcYPTU1NRWRkJFauFK6BCgkJwY4dOzB//nx89dVXqFatGnbu3IkWLVoY/fWYA3pPtZOdko0dfXcgNyMXts62aPtVWxyZfQR2LnYYum8onL2clfa5uPIi7u+9D7/mflqf79aWW7i+4ToqNK+AV7deYejeobBzZptg2DraqmyHKvX2sfZjnirWsIpEhWAY9XVWLbrrP8bu5cPNceyPjuiygkFqUhls/3EosjKcMG44Px83YH11S14bwf1B8N/u/3Ar4hZeXH6hd7kMiQLWkkEBq64oYC0R/foBd+4Ad+8CP/wA7N1r2ON37Aj06sWep6q8/T+qV2cfpGRMmTIFU6ZMEdwWERGhlObu7o7MTPVTIA4cOBADBw40RPEsEr2nmruw/AKen34uW/+96++y5SdHn6DesHpK+xyedRgAdAqe9ozeAwCIOcv+aLi08hLafNkGADv8VHEBK3ecVk1xA1b3cimY9O16XD/RBKf3tsHgmbu0Pl6x6n8D2Lji0Ig3YBixLPB8cL22YHZuwMr9W3AD1gNTDhi+nAZAAWvJoIBVVxSwGh3DAPuK7vxs26b7bX5VXr8GypblT6dKCDEvCbcScPqb0+i4pCPK1VZxi0VPmcmqA/Wc1Bw8PPAQt3+7jV7resHBw4G3Pe+dchtKpbFDwQ5PdXD6QTSe2FhpW/y1eNz+/TaeRD3RqBNX9lvlqT9tHG3U3h7nzhjVYeBxOLpko1Wfc8jNtkO1eoZpcwsAEImB/vGAgzcYhgHDFI2xqvCyjnx6BF2Xycf1VTWWaeKdRMF0XZ1YcAIiiQjtF7bXKP+5peeQkZCBrj92VTkihlDZI4dHwq+5HxLvJKL3z72VZ+wiWqOAVV8UsBoNt+e/QB8QvalqXkAIMR/hweHIz8pH0r0kTLkjXGNsTNmp2djfix03z9nXGd1XdC92H+5g/lJRn0Xh7q67uLvrrtK2tLg07B65Wyldlay3ymP2FduWkxNr2TnIa3A7DDqh8Xk1whQCDt5Fi/LvR8XxYS/8cAGNJzRGuVrshZhbw8p1I/yGwYqWmZSJ09+wg2QHfxwMezfh0UukGIbB0c+PAgAajG4A3wa+gvmEOl3d2X4Hd7bfAQBU71EdQe8HKeUh2qGAVVfU6cro3uowNbWQdeuAjz4yzLEIISVLGoi9efRGr+O8S3yHEwtPwK+ZHxqNa8TbpipYAoBTi+Rj5GUmqm8yISU05mn6y3SV+bXtZf7yykut8iviBqy6in/qi/vXa6P9+yf5G8p3ky3y3leBuh1uD//CAt1ni3r932ucWnQK+Vn5KMgrQEFuAfxb+SM3Ixftv24Pe1c2MOXeulf1np9beg5uFd1Qb3g93hi3ihM4SMXfiMfF5RfVlu/yqssUsBoABay6oiYBBsUwwOPHQLVqwJMnwNq1QKdO+h930CBg8mT21v+hQ0D9+sDChcAB82wKRQhRwcHdofhMatzYdAPX1l/DtfXX0GB0A94tWqFb7FLcmksbR82+MoXGPJV2qhKiLmDWl71jNmxs85H3Lg9Bze/CwysFdvb6B6x/rX0fSfGecHHPQNPPRwMv9wMF2UCwvP0vU6C6hhXgT0Shz3i2G5puUBre6umxpwDYILXn6p5FJ5Rv55ZN6tXtV7Ia1XrD6/HaEqtqV/xL41+KLR+3TS7RHQWsuqKA1aCWLgW++AJYsYLtXPXiBbBceDhBrXTsyD4PHsw+AKCUj8JDiEWyd1d/+xZgg6Kr66/izcM3qNa1Gqp3Z3tOpr9Mx8mFJ2X5rm+4jsYfNsaVn66gcvvKSrNFqWLjoGHAyqlhTXmWgujIaLUBmTED1nFfh8O74mts37wcg2b+YbDjvktzBhgx9m/ujaabpgG1pinl4b4u7hSyMsUEkJpSNxZrwg35ZBe88gjUsHLHt2UYhtc+OfZcLFKepUBsI0bKsxRU6VgFNXvV1Kqc+dn5uLL2Cqr3qA6vQC+t9iUUsOqPAlaD+OIL9vnjj7Xbz9MTSE6Wr/foARwsGkbwzz/ZEQAIIZZPeltXnfv77st6kl9ccRELmYUAgC2dtvBuB+//aD9eR7/G5dWXAQDVulXTqAwa17ByAtYNzTYgM0l9UwJNA2ZdeFd8DQCo5H/OYMdMrPAXsjJuF5uPe5u/uJEPjDVjGDcQ5gWsAj37uWXIz85H7jt5mS/8eIGX9+Lyi/gqT/OewIX5hbj681UcmXMER+YckX02ieYoYNUVtWE1qGrV2CYB2nJ15QesX30FhIQAvr7sjFiEkNLjdfRr3vrTE09RrlY53sxIUvf23JMtC9b+CbB1tNUoX8qzFBz6+BCqd69ebLAKqJ4gQH/yAMzV8YF+h+oXB9z5Bqg5Ddl33ABoELByAsQLyy8obWcKGfy77V+4+Lro1YZV0zJwe/M/3P8QldtXhnddb3l5OMFtflY+kh9wvlwEaNP2OD8nHxdXyNu6MgyjctQBIowCVl1RkwCDKl9et4A1KAh49ky+7uwMzJ9vsGIRQsyEJjVwNvb8r7QtHbeozMttw6rpOJqa1rACwKWwS7gUpv/A+5pyKuekFByLxPIArX6rfxV3USntjSvcyip0EnPyA5qvBwAU5j/T6DjcADA3XbmG9dGhRzj+5XGV+0vsJHqPccoLWDnLB6ezt+K4NZ35OfKa7rysPOwaoH58Wm3KlnQvCanP5TOUZSZlCk5IQVSjgcF0RQErALZ288ABoEDPcZPLlNEu/+7dwPnzQK1a/HQnJ/3KQQgxH9yOOsXVwCXeScSLS5oP4p/yLEW2zA1U1NG0Dash+FZ+CW//V8VnBNDy45YYe3asUrrERvsLc8SS0djw1USl9Hev3yH+ejyA4tvcvrj8AllvspCTnqM2X/I99TWYxQ07pQnu56a4GtFnJ5/JljWZ8lWbgFU6SYTs+EZsBmKtqIZVVxSwAgCaNQOePgVWrwamKbe5V+vAASA9HRgyRPu3MSgIqFkTuHWLn04BKyHWg3sLV12nnMzkTKyrt07n82gaeJTU4O92jtmY9C3b+3zxiAXFTqFao2cN2XimEpt8NOt8BY9uV0f6W1etz/38P3Z+aoYRQySSv//LKyxHYX4hPrz6odqA9cmxJ/it829w9naGUzn1F2R1Q30BbEc7TZpUqKOqhlXRs1PPcHnVZdl6XpbqjlxSWtWw/sdvlmLMjnbWimpY9VXKA9an7Mgh+EPDzqd5RdcAhmE7RA0dyo4IkFf8tYHHuehOyoQJbMcrKQpYCbEe3Bqxt0/fqmwWoO9sSKrasCrWqBqrnaUiZzf5mJ9iDWpJbRxsgMI81BteDyG9z6HbyMOYuuwnnWpYpS4/+Zm3Lg2wnh5/qhRspTxLAVPIoCC3AFd+ugKAHbdUsU2xorQX6qeY1XcoMwBIi01D4t1EMIWMytm0AOD6L9d56/lZ+fCs6akid9GxtZgi99Utfm15YX4hUmNTKXDVAgWsuqLG0jyaTG967Rrg7g78739ALqc5U8WKwOHD2p1PGpja2ADr1yunE0IsHzfAyM/Kx9+T/hbM92v7X/U6j6qaMsU2q/oMvaQNkUh+HrG4+HOWSV8G/OGBph+4wL9mrCz90/XLdC5DVmF1wXSxRKwUZK2sshL7p+7H5jabcW/3PcH9hBRXw2rnonrsWk3lZuRiXd11OP7VcbVNAt4+5c9Uk5eVB1tn9Z3swluGa1yOuItxvPUnUU8QVikMW3tu1fgYpR0FrLqiJgE8J08CR46ozzNiBJCVBcybxz7rw5nTVt3RUb5sp//1jRBiJhQDjBsbDTdNJ5eqNqyKowJIa3g1uV2sD259SOchR+HiwdbkiSTKFSViST5c364BCjLh9W4BREJTSulAaPIDaRmEAr9r66/hxWXN2xADQE6q+jauEnvhMuji7P/OCtZmSv+mim1K87PyjVr7efa7swDYwJVohgJWXVHAqqRbN9XbVq4E7nF+eO9S3/myWLac75Hmzdlne/3b5xNCzIi6W7hSQjMoaYs7QDyXYg1rYUEhjn5xFP9z+p/e51RP/pqad7uMIR/vBADYOinX+E0Old9icsy7BBs7w3TmEduKcXp/HwBAhseH8nSJGJFDIw1yjuJoOoyYpoQ+T8fns6MUKLZPzsvM0+jzp6u0WM2bExAWBay6ooBVY9nZyrNLaTtBAAD07y9f5tZAeHkBcXFAQoLyPoQQy6XJOJfFDUiviZw04Zo+xTasTAGDc98bbgB+IWWrl0Wzjxrz0ipWZ2suFQM417Kp8PLjd+Zx90yFIUhsJbh7uzt++nQqXkk+laWLJCKjDfKvVAY7w9WwelTxEKwxPRvK1nSKxPza69yMXK3GWSXGRwErMbphw5TTtB3GCgA2bmSfGzdW3ubnB3h4aH9MQoj5Kq6GK/FOIr5z+85o51e8LX7iqxNq8zt56deI3tbZFtMfTkeZKsLjc3IDOJGoELNXr1DKY2evXQB/7p8Q7P3lPQDA/s09ZeliWzEcyjgi6aUXfu++TZYunUmsJAg1gdD5WGLhpgzyDPzV7JRsWYCrGMwS06CAVVdUwyro2DH++qlTwJ49yvleaNfUCQBQtiyQkQFcvlx8XkKI5Suuhuufyf8Y9fzaBipiGz2/Uou+TsRi5dft4JaPgbsGokKzCrCxy0WLPsKdm5zdtRsGKu5hRdw81RjfTfgC/15uI0uX2EqK7eVvbLrU5Aa0DRBMz0nNUdsmVXGkiOzUbNkPJocymo1WUKFZBQ1LSXRBAauuKGAV1Lkz+xwfz75F7dvrd7xPi+5ELV7MPjs7AxLD3SUihJgphmGws/9OtXkUe/d/lfcVvsr7Cl51vAxSBm0DVlUdlTQlDdDEEuWA9dMzdvAP9seE7baYt/l/6DZEv44Aeblsc4e4R/4AgJwsB0y6OUm2XWyrPBpASdNlVIbRJ0cLpmcmZeKPgarHX1TsSMcNcDUdXivk0xDBHy3c8WhLcvIJa0MBq64oYFVr1SrDHOf774H792m6VUJKm3eJ75B8X/1MSIoBpdhGDLGNWO0c7apq4ASPr+UtabGt5l+pLWa1AABUDK4oS5N2IJNIlANFcS7bVlV0SXlGK1388NGnWD5tNjJS5JMLcIMpia0EvX/ubZBz8Wjxluoy7q26v706ijNbZadky2r4NZ1xS2wjFhzZgDs8l6Ono9J2ohmTB6xr165FlSpV4ODggCZNmuDMmTMq844ZMwYikUjpUadOHVmeiIgIwTzZ2dnGeQEUsCrJzga2bzfMsUQidkYrGvaWkNJFVfvVHX134NVtdhB2bsA6cNdA2bK6mtG2X7VV6v2vijGaBPTZ0AfTH01H9xXd8VnyZxh7mhOASpsESIR6+hcC6Y+0Ko86udn2SH/rxkuzsee8LyKg7pC6cPF1Mdg5AcDOWfOxB1XV8M6KmWWg0rA2NN+A1Bi2s1qdwWw8kZOaI/sMFjceq5RYIhb8zORlymtvHcsqB6x/Dv1T4+mBSzOTBqw7d+7ErFmzMG/ePNy4cQNt2rRBjx49EBMTI5h/5cqViI+Plz1iY2NRtmxZDBo0iJfPzc2Nly8+Ph4ODvrPmMFDEZRKS5YAz5/rfxzFkQUIIaWHqsH87++7j9+6/gaAH1DyBplXc3m2cbSBs7dypyahWjRjNAkQSUQoW60sADZ4EduI0Wh8IwBA2wVtAQg3CQBTCPxdQ6vyCMnOtMev3wrfNud26pI2T/AKMkzzCgCo3L6yxsEfINwkoFq3anDyNOwMMS+vvJQtlwtip7jNTpV3uhIaTkyISCKC0DC43ICV96OgyN2dd3Fl7RVtilwqmbQxxfLlyzF+/HhMmDABABAWFobDhw9j3bp1CA0NVcrv7u4Od3d32fqePXvw9u1bjB3Lv0UiEong6+tr3MJTkwCVIg0wRF+XLsAK5Q6whJBSQl2N07tX73Do40OIPSef1Yk75JO6QFMoWAWEb+drXcPKOUat92qheo/q2P/Rfl6et4/fKu6GXut6odnUZvBtwH5vicUCNYuMYdqT/jjlE+TnCQdgvNvZjECajob9PQxu/m7wrOmJdfXW4d2rd7ztfs39BCcd4NawVutaDZ2/74xygeX0Lo86Lj5sjXJOao6sSYCmAauqGnZu+1hVeVKepmhRytLJZDWsubm5uHbtGrp27cpL79q1K86fP6/RMcLDw9G5c2cEBPDbJGVkZCAgIAAVK1ZE7969ceOGEWZHoYBVpYcP9T9Gx476H4MQYrkUe20ruhR2ibfODSrUBZrSgESRUCDh19xPbRkUcWtY7d3sUbFlRaU8QmkSWwnKNyovK7fQKAF4tF45TQcF+aoDUO57IK1hVTcWqqajIjiUcYBvA1/YOtoKNgkIGhwkuB+3DWtBXgF8G/rCxt5G785t6rgHsJVi2SnZshpenwY+Gu2rqkkAt6ZY1XumaixgImeyGtakpCQUFBTAx4f/QfDx8UGCBiPAx8fH4+DBg9i2bRsvvXbt2oiIiEC9evWQlpaGlStXolWrVrh16xZq1BC+nZKTk4OcHPmHJT1d/fzGAChgVaNAh7GWJ00CYmKAn34Cjh8HRo40fLkIIZZDVZMAVbjtUtV1vLFzFW5DyQ2CZjyegdTY1GLnulfEDUbKBZXjBXtjTo9B3rs8VOtardjjSMQGas8Y+ClyH+3H4XVVkZrsDu/6/mAY1UEm932TdQBTN3i/hhXQ3PdFqElAs4+awdnbGXtG7eGlcwM9bptmkViEybcn48nRJzgym50TvFbfWuixqofSscedH4dtvbYh+23x/Vh8GvjAtQLbCS0zWT48WLMpzZBwIwEP/n6gdn+RRAQ7Fzu1wScFrLozeacrxQsLwzAa9fKLiIiAh4cH+vXrx0tv2bIlRowYgQYNGqBNmzbYtWsXatasidWrV6s8VmhoqKy5gbu7O4KChH/tET5dAlMhM2YA69cDBw4AVaoA48cDdpq3yyeEWCFtO6Fo2iRA1fcL93Z+maplULldZa1r8rgdaio0qcC7ne7u747q3atr1MxAJDLQxbXGR3hV8RCun2iKx7dr4HVyXY13ldWwqnkPNB32inuMGr2UK45snWxRd6hy2bjHVxyT16eeD/yD/WXr1bpWg3sldyjyD/bXKFgFAP9W/rIhrLKSs2Tpds52qD+ifrH7iyXiYtvoqhp5ggLW4pksYC1XrhwkEolSbWpiYqJSrasihmGwadMmjBw5EnbFRDZisRjNmjXDQzX3qefOnYvU1FTZIzo6uvgXUIprWCMigA8/ZINMfe3dC/zwg/7HIYRYF24N69R7U1G2elm1+XmBIGexSqcqsuUPr37IbhYIWoVqvrQZpgpgp/98f/v76LW+F6p2qcqrnSz29nnOG+DWV8ClibDPv63VeRXdONkQG776EHCpwntftArApW1YVdSw9lzbU7CDkRDua2/1aSvBPEKBPK9JgECNO+/91fBvJbGX4NOkT4W32Ulg767c+U7VcFVCeJ3/BKj6waRJQFzamSxgtbOzQ5MmTRAVFcVLj4qKQkhIiNp9T506hUePHmH8+PHFnodhGNy8eRPly5dXmcfe3h5ubm6yh6urq8q8MqUwYH3xgu25P3YsO03qtGn6H/O99wBbzTuNEkJKCWkb1vJNyqNcrXKo90E9tfm5gQI3+KnUppJsuUIT1TMReQR4KKVpPXOVCKg7tC6aTmoKkUjEn0pVXc1qwlEgshxwdwnweANcUn/V7rwKXr/wxssnbPtb7mvQJgCX1laK7YT3qdqpqsbH4pZBYidBveHKf0uh96d8E/n3doWmyn87rX4QFGk5q6XKUQZs7G3Y0SIUiiK2FWs04H9hQSHsXTUbs1VR3WGa136XViYdJWD27NkYOXIkmjZtiuDgYPzyyy+IiYnB5MmTAbA1ny9evMCWLVt4+4WHh6NFixaoW1f5D7xo0SK0bNkSNWrUQFpaGlatWoWbN2/ip59+MmzhS+GwVh06FN+hqlIlti2qVGAg0Lo1UL068Pnnxi0fIcR6pMay42JKhwFSN4i8xE7C6/3PrcVq9VkrgAFq9qmp9nwVgyuiRu8aKFO1jPy4WjYJUKw943ViUle5cbyL2uMyEEGkWJ0Zsg14fQZ4uE45PyerdAgtAMh7l6eUV9EHhz7Aq1uvULULG5AKvQc91vSAZ03PYo8lpRhMCt0WV3zv6gyug86hndF8anP8u/1fhHyiXJHFDcA1/Vu1mNFC5TaJnQRiiRiOZR3lTQJE7K1+oeGoFBXmFyrVsI45PQYRbSMAFE3MoCJ0UNtWmAAwccA6ZMgQJCcnY/HixYiPj0fdunVx4MABWa//+Ph4pTFZU1NTERkZiZUrVwoeMyUlBRMnTkRCQgLc3d3RqFEjnD59Gs2bNzds4bn/XAxTKgLY4oLVYcPYNqj/+588bcoUtib2xAnjlo0QYl32T2aHg3p5lR0jkzu9paK2X7XlrXNr62wdbdH+6/a87V5BXkh5lsLfRyJCy5kteWnaNglQDEa4QY7KgKSg+LaLr73C4f3uayCT833o3x+oPAx4vBEoVAhEGXlBHDzkY5CrDZqLVO9WHdW7VVdb7uZTtfs+VQxYi2v7+sGhD2Rl8AryQsdvhIeN4Y4Mwa0B9W/lj9hzsUq38cvWKCvrVCVEmt/Fx0UWsErLrlENq0DAGtAmgP1cMECFZqpr+HWdoas0MfmktlOmTMGUKVMEt0VERCilubu7IzMzUzlzkRUrVmAFDeBpcIcOFZ/HxgaYNw+oXx+oVg1ITgY6dWK3FXKuT15ewOvXxiknIcS6SNsuNvmwCRL/TcSNcPkwhcMPDMd/kf+h5cf8QLO43ut9NvbBsbnHUKFZBRycdhCA8C3l4tojKlIMOuxc7NBlWRcU5BbA2Ut4/FecVO7ZrsirTgXA9RiQfBVwrQaI7QBJUSDKKLftzMni35Ye9OcgPD7yGBWaVsDjw4+V8jee2Bg1ewvXQBui5k/xveXWhk68NlEpv6bBm7u/O1p90QoZLzN4oy+8v/19nJh/Ai1m8mtTuT8ghv0zDNt786dklL5WZx9nvI5+zdtHk4CVKWBkw2JxTbw6EZdWXkLHbzti77i9Gr02oszkAavFKmU1rEuXFp/HxgZwcgKGDFHexg1Yo6LYkQG+/dZw5SOEWA+h2/+2TrZ4b+N7eJf4Tja8UI0eNVCjh3Kv8+J64ruWd0W/iH5IuCXv9CuWKAes0h7jGhM4rdCtbJ5Xxd9+EoltAdfq7EMRd0KBxiuQcOh33D7H78AT9H4Qgt4Pws1fbwoev8/PfVSeW+taZgGKTQBsXeQ1o+Ubq+5foonOoZ2V0tz93dHv135K6R6VPWTLNXvVhHc9byT+myhLkwan3LF6pYGqJp2uJHYSwba25RuXFywP0Q4FrLpSDFitWEoK8ORJ8fls1HyauAFrgwbAqVN6F4sQYqW4Qwop6v1zb+zL36dcq8qhaQ0dt9ZMqIaVeztdE1rf1k1Xru0UJNbwq7r2LLiUmYDKB/eg8YeNlbfr8FXFrWG1c7XDsL+HaX8QhfMKTR7AY+D6n6H7huLyqsvota4XLz03PZe3Ln2tZapz2jEXBarF1bD6t/JH1c5VZcOBqSL0GekW1k3tPoRFAauuSlHAOmAA8Px58fnUBaxqBmkghBCejFcZKre5lnfFBwc+ULu/plOqcoMQoY5AQkMcqT+xFnnfPQf+Ud8RTH5cNRdX3y5AQhTgVgsAWzs44vAIway63N7n7jPu3Dj41NNs1icuxbazTl6q2yMDOozOUIxafWqhVp9aSumKU65Kg1Of+vLXKP2McD8rHpU9lNpAD/pjEERiEURiEQLaBuD5aQ2+NAE0HNtQqe00EWbyiQOI+WEY4NUr9gFo3mFKouZaWL8+8MsvwMGD+pePEGLduDVfg/4cpP0BNAwcuW0ahZoEaNIznHfa4mpY8zPZ8VZz3gApd/i389UeWE05QrYC9b8BOh4t9jCB7wfKpoZ18nKCZy1PjD4xWu0+3IBV2/cDYKeidavoxktrOqkpfBv5ou0Cfme5ph81ReX2lRHQlj/durG0XdCWF4jmZbKd17hDnMkCVs5rF/pBxP38qKtlVWxikZuRqyInUUQ1rLqy4hrW2bOBsDB2WZO2q1LF1cJ++KHORSKElCLSWa68grwQ9L72Mw/qUsOqSQ96pfNIRLzpQ9Wel2GASC+goKjTcDPl4ahUUtckwMELqDtfo8PY2Ntg/IXixy/n4naQ0qTjkdRCZqHKbfZu9ph0fZJSeq+1vQRyG0/dIXVRd0hdLBItAgDZVLzcmnWhTldCASn3b68uYFV8D6XjDZPiUQ2rrqw0YH3wQB6sAsBnn6nPz+1gFRdnlCIRQkqZ/Gw2YNUmQOLqvrI7HDwc0PF/wsMhSXE70mg6zSiXSCxCrffYW832bvZoOUvNrd3/lsqDVQCIjVR77KvnenNOZLq6pay38vbEjp6OanLK9Vhd/MgH5qTV563gUMYBjcez7X65bZelNczcz4rQjxtukxJ1YwZzP9OafEaJHNWw6spKA9Zx4zTPu2kTMHo0sHMnu/7ypXHKRAgpXaS1TppOh6nIK9ALnyV/VmxNK/c2r64B65A9Q2SditSe7+YX/HWB4ahkhuajSf83wG7vorz5WpfNUN4+fitbLrazFIAPDn6A6t0FRjMwY52/64yO33aU3dbnjg5RkMf+nYq75a9pkwBuwKrJZ5TIUQ2rrqwkYL16FejaFbh5k11/rGGnVQAYMwYQi4EKRaN4NG1q6NIRQkojfWtYAc2aBXDzaBKwetXxUtpfJBLJOtvw/PcDcG4YUKgiMFXVfjXoc0AsgcihnDxNrN14sIbUbGozAEDDMQ1NVoaSwA04uZ876WcRgKwtbv/f+gPgD8nF/ft3XMLWmjaZ3ETpPLyOfhSsaoVqWEu51q2BnBygc2cgKYkdwkpT0pj93Dng55/ZsVUJIURfJxawPT1LcrpKTQLWvpv7YmPzjbJ1tQHHjU/Z58ojAK/WAidUMU2qS9Wig4uANruBd08BD9PNM+/XzA+fJX8GhzKaDfFV3LBOloYbsM54PAMFuQWwc7HDZ28+Q3ZKNlZVXcVu5HwUqnWths/efCY4LJo+P8JKO3rndGUlNaw5RbMCJiezz9nZ2h+jcmUgNNRgRSKElHJvHr4BACTcTCgmp+FoErAqjgKgUQ1Z/jsgS6C9VO5b5TSAH8j69yv++CXAsaxmbVcB6wtYM5Pk7Y4ldhLZjyjHMo6wdeRMDaswgoJjGeH3jAJW3dE7pysrCVgJIcSccDu05GeVXNtNXduwapBJODjNfaOiICpqXi2EtQWsOak5KrfZONhg9svZEIlFGo8dSwGr7qgNq64oYCWEEIMryDXNMD+FecIBa7NpbBvOoIFBgvPECx+MG2iLgNwU5TxWGrB61/U2dREMovmM5gDYgf3VcS3vypvKtThVOlXRp1ilGoX6utJ2Cj4LQHE3IcTUuG0GS5KqGtauP3RFjZ41ENA2AHbOdhh1bBS2dNoCQE1tYiGnVk4kBvJSBPKoCExNOCKAPmY8noF3r9+hTNUyxWe2AF2XdUX1btUR0M6wkxj4B/tjzOkxKFPFOt6nkkQ1rIZgoZHeu3f89aSk4vfZt4+dgnXDBuOUiRBSunEHUi/J28t+LfwE023sbVCjRw3ZkE4VmlWQbVNZvrR7nBURkKVBW9wqowBbd6CqFmMLmpEyVcugYouKpi6GwUjsJKjRs4ZGQ3lpK6BNgNLsX6R4VMOqKytoEtC3L3/dW4M7OX36sIGunelGWSGEWDFuDat0xitjmnpvKl5cfoG6QzTric+bwlXo0p+fBRzijPH33zIg6bz6g7rWAFpGsLWuErq4EiKEAlZdWUHAeuyYdvkPHWKfKVglhBgLN2Ct2qmq0c9XrlY5lKtVrviMRbgzGglO5/ruKX+9uGC1TEOg3d/sd4qFBateQV54Hf0aldpUMnVRSClATQJ0ZcEBa0qK+iLv3g04KAwf17gx0K2bUYtFCCG8WtV+v/YzXUFUUDmjEcOwowFkxaveueVm5bT63wJOlnkr/YNDH6DdwnYYtGuQqYtCSgEKWEuZ69eBMmWAAQOEt3t4AP36AZmZ/KlWs7KE8xNCiCFJa1jdK7nD2dvZxKVRxq1h5TUJuDYL+LMs8Hij4i6sRj8CVccAwwqB4N/l6XYajjxghtz93dH+6/Zw8dW8lzwhuqImAbqyoBrWwkLgzRu2w9TevWzanj3Ced97j30WiYDy8lnndJpQgBBCtGWIaVmNiT95QAGQfAXIfgU8KJrx6PkO4R096kkPAHi3kafbWm7ASkhJMs8rgiUw84CVYdgi5uYCjRoB0dGa7eesokKjEjVRIoSUAOkoARL7kpuWVWPSCyuAgMCnGP7JNuDw15rta8OphXTwkS+L7Q1XPkKsGDUJ0JUZB6zTpwM1agBpacC1a5oHqwDgonBn5+RJoEcPIDzcoEUkhBBBZlvDmpsC7KsGXPsYADBk1k7YOWgxyL8t5+IqsQfqfgVUnwy4VjdsOQmxUmZ2RbAgZjhxwPbtgK8vsGYNu751K1Bdy2uhYsDarh37IISQkmB2AWvuW+BJBPAuhh0B4H4YgK9h76R6yk5BNgoX1/qLDVRAQkoHM7kiWDgzqGGNjgaGD1dOc9eyeZSqJgGEEFISpKME2NibydfTxbFA3F5e0rhDLSBO1vK6b0MXV0L0QU0CdGVmTQIePlROW7MG+OAD7Y7j62uY8hBCiC7MroZVIVgFAP/kHtofx9ZD/7IQUopRwKorMwtYMzN137dZM+FlQggpadJOV2YTsOrKxpW/bmGTAhBibiz8imBCZhaw6jJOqrs7O9yVWAxMmsSm1ahh2HIRQog2pDWsZjlKgKZqzwEaLQPePQMujgFqzTJxgQixfBSwWgldAlZPTzZYBYCffzZseQghRBdm1yRAF07+bKWGSxWg8ylTl4YQq0BNAgzBQmtYFadfJYQQU5N2urLoGla7MqYuASFWhwJWfUibBVhowGoGxSaEEB6rqGGlgJUQg6OAVR8WFLAGByunvXplnLIQQoiuLCpgda4inG5H060SYmgUsOrDjCYPyM1Vva15c+Dvv4EyCj/6AwONWyZCCNGWbJQAcxmHVZ3aHwMtf1VOlziWfFkIsXIUsBqCGdSwZmer3ubhwXawio/np2/ebNQiEUKI1iyqhtXOA6g6Cmi1Q57m0xEo29RkRSLEWlHAqg8LaRLg4cE+29vL0/r2pSGsCCHmx6LGYZVOBmDrJk9rssqs7r4RYi0oYNWHmQSsT58Cmzap3i4NWLkKCoxWHEII0ZlZjcMqMMsVj50H+yzmTAogtoBAmxALRAGrPiRFF1QTRn/37wNVq6rPIxSwFhYapTiEEKIXs2gSUJgPJF0ETvdTn08oYBVRwEqIMdB/lj5MHLDGxgK1axefjwJWQoilqNmnJjyqeMAr0Mt0hfj3a+Dut8XnkzYJoBpWQoyO/rP0IZ0mykQB6+nTmuVzFxhhhQJWQog5ajalmamLoFmwClANKyEliJoE6ENaw1oC0V9BAXDsGJCWJk8bPVqzfV1dldMoYCWEEADZr4FEzq//nDea72vjwj5TwEqI0VHAqo8SbBKwejXQuTPQq5c8TdPTOjsrp1HASggp9RgGOBIMHG0n72D14m/N95d2vBVxOohRkwBCjIICVn2UUMC6aRPw8cfs8tmz7LO6iQIAoFEj+TIFrIQQIuBYRyDjMbv84Cf2mdHhei7ifJVSDSshRkEBqz5KKGAdP56/np8PvFFz1+q994CrV+XrTk7KeShgJYSUeokn5ctp99jnnGTV+QelApUGK6dzA1aqYSXEKChg1YeJRglISwMaN1a9XSKR9wcDgGrVlPOYwVwHhMisXbsWVapUgYODA5o0aYIzZ86ozZ+Tk4N58+YhICAA9vb2qFatGjZxBiOOiIiASCRSemSrmxLOytB7WgzFi2BuCpDyL3DzM9X7iO0B387ssoRTE8ANWOlrlRCjoJ+C+jBRwPrkifI0q1zSYl2+DGRkABUqyLd98w37CAszahEJ0djOnTsxa9YsrF27Fq1atcLPP/+MHj16IDo6GpUqVRLcZ/DgwXj16hXCw8NRvXp1JCYmIj8/n5fHzc0N9+/f56U5ODgY7XWYE3pPNVCgEGgXZAKXPlS/j9gOqDoOsHUHvELk6U4BgFsgYOMESCz0/SDEzFHAqg8TBayKwWqlSkBMjHw9OJh9biYwOsz8+cDnnwO2tsYrHyHaWL58OcaPH48JEyYAAMLCwnD48GGsW7cOoaGhSvkPHTqEU6dO4cmTJyhbtiwAoHLlykr5RCIRfH19jVp2c0XvqQYKMvnrTAGQnyFf9+kIvDrOzyMSsR2sAhSaBYglQM9/i7bTtKyEGAPdu9CHEQPW1FTgjz+AzEzlbbGx8uVKlYCJE+Xrc+YA06erPzYFq8Rc5Obm4tq1a+jatSsvvWvXrjh//rzgPvv27UPTpk2xdOlS+Pn5oWbNmvjkk0+QlZXFy5eRkYGAgABUrFgRvXv3xo0bN4z2OswJvafFSLkDvDoB5L9T3laYJ1+uMoq/rcMR9ccVSxSaBhBCDIlqWPVhxIB12DDg4EGgqIKEJy5OviwSATacv+Knn1JASkwvPT0daZxBg+3t7WFvb6+ULykpCQUFBfDx8eGl+/j4ICEhQfDYT548wdmzZ+Hg4IDdu3cjKSkJU6ZMwZs3b2RtLmvXro2IiAjUq1cPaWlpWLlyJVq1aoVbt26hRo0aBnyl5sec3tOcnBzk5OTI1rmfCZM5UI99brdfeRvDaQJhyxnAWiQGyncxbrkIIWrRz0F9GHGmq4MH2eeNG5W3cWtYFe9A2dkp5yekpAUFBcHd3V32ELoNzSVSuI3KMIxSmlRhYSFEIhG2bt2K5s2bo2fPnli+fDkiIiJkNYItW7bEiBEj0KBBA7Rp0wa7du1CzZo1sXr1asO8QAtgDu9paGgo73Pg7+9vuBeor7cCtcPcIa1suDOu0G1+QkyNAlZ9mKgN6/Pn8mWxmD9EFQWsxBxER0cjNTVV9pg7d65gvnLlykEikSjV/CUmJirVEEqVL18efn5+cOfMORwYGAiGYRDHvf3AIRaL0axZMzx8+FDHV2Q5zOk9nTt3Lu9zEMv9tW0KhZwa1McCtQH5nDZY0lmsALrVT4gZoP9CfRgpYE1MVL+dOzoNBazEHLm6usLNzU32EGoOAAB2dnZo0qQJoqKieOlRUVEICQkR3KdVq1Z4+fIlMjLkHWQePHgAsViMihUrCu7DMAxu3ryJ8uXL6/iKLIc5vaf29va8z4Gbm5sOr8iAnm2TL797prw957V8mdskgL4qCTE5+i/Uh5EC1q+/1jyvSMQPWG2oVTKxMLNnz8bGjRuxadMm/Pfff/j4448RExODyZMnA2Br6UaNkneAGT58ODw9PTF27FhER0fj9OnT+PTTTzFu3Dg4OjoCABYtWoTDhw/jyZMnuHnzJsaPH4+bN2/Kjmnt6D1V4eJozfPaqBpnlRBiChTe6MNIAevbt5rnbdiQH7DSiCrE0gwZMgTJyclYvHgx4uPjUbduXRw4cAABAQEAgPj4eMRwxm1zcXFBVFQUpk+fjqZNm8LT0xODBw/GkiVLZHlSUlIwceJEJCQkwN3dHY0aNcLp06fRvHnzEn99pkDvqQFIuHNa04WVEFMTMQzNeaQoLi4O/v7+iI2NVXk7DAAQEgJcuAD89RfQv7/Bzj96NLBlS/H5WrQA/v4bWLtWXitLf01iShr/75BSKS0tDe7u7khNTTVN84BtGgaejX4AAufI89u4AIPTjVcuQoph8v8dM0D3OfQhrWHlVnEagKbtUH/7DfDyMvjpCSGkdAuco5BANayEmBoFrPowUpMATQNW56I7VhSwEkKIEVEbVkJMzuT/hWvXrkWVKlXg4OCAJk2a4Ay3C7yCMWPGQCQSKT3q1KnDyxcZGYmgoCDY29sjKCgIu3fvNk7hDRSw5uUBJ08C2UVTW1PASgghBpD1Cki+YoADUQ0rIaZm0oB1586dmDVrFubNm4cbN26gTZs26NGjB68zANfKlSsRHx8ve8TGxqJs2bIYNGiQLM+FCxcwZMgQjBw5Erdu3cLIkSMxePBgXLp0yfAvwEAB64IFQIcO8lmtNO3pTwErIYSosccPONwceHNNv+NQDSshJmfS/8Lly5dj/PjxmDBhAgIDAxEWFgZ/f3+sW7dOML+7uzt8fX1lj6tXr+Lt27cYO3asLE9YWBi6dOmCuXPnonbt2pg7dy46deqEsLAww78AA8109d137PPWrexzXp7qvFzSwJY6WhFiWJUrV8bixYtV/ni2lnNaPenMVYmn9TsOBayEmJzJ/gtzc3Nx7do1dO3alZfetWtXnD9/XqNjhIeHo3PnzrKhWgC2hlXxmN26ddP4mFoxUhtWadMATbVvb9DTE1LqzZkzB3v37kXVqlXRpUsX7NixAzk5OVZ3TqvG/SUvcSh6dhLOyyVxlC+XK5pooeoYgxWLEKIbkwWsSUlJKCgoUJoq0MfHR2lKQSHx8fE4ePAgJkjvoxdJSEjQ+pg5OTlIS0uTPdLTNRy+xAgB6+HDmgWs0lpZAOjWDThwgD9lKyFEd9OnT8e1a9dw7do1BAUFYcaMGShfvjymTZuG69evW805rVpBlnz5yhQgJxko1OAHQOdT8uX2+4E2u4H6S1TnJ4SUCJPf5xApjHTPMIxSmpCIiAh4eHigX79+eh8zNDQU7u7uskdQUJBmhTdCwNq9O/D0afH5Pv9cviwSAT16AJUqGawYhBAADRo0wMqVK/HixQssXLgQGzduRLNmzdCgQQNs2rQJxhjG2hTntEq5CjOw/OUjbyKgimtNwLOZfN3OA/DvB0iEpxYmhJQckwWs5cqVg0QiUar5TExMVKohVcQwDDZt2oSRI0fCTqFLva+vr9bHnDt3LlJTU2WP6OhozV6EkZoEnC5qbvXjj8CaNcDmzQY9PCFEQ3l5edi1axfee+89zJkzB02bNsXGjRsxePBgzJs3Dx988IFVnNMqKQas3GC160Wg+QbA00pn6SLECplsalY7Ozs0adIEUVFR6M+ZJSoqKgp9+/ZVu++pU6fw6NEjjB8/XmlbcHAwoqKi8PHHH8vSjhw5gpCQEJXHs7e3h729/Bd0WlqaZi/CSAGrlK8vMHw4cPy4UQ5PCFHh+vXr2Lx5M7Zv3w6JRIKRI0dixYoVqF27tixP165d0bZtW4s+p1XLS1WxQQSUbQqUawE83qCwiYavIsRcmSxgBYDZs2dj5MiRaNq0KYKDg/HLL78gJiYGkydPBsDWfL548QJbFOYpDQ8PR4sWLVC3bl2lY86cORNt27bF999/j759+2Lv3r04evQozp49a/gXYOSAtU8f9lnTYa4IIYbRrFkzdOnSBevWrUO/fv1ga2urlCcoKAhDhw616HNatbwM4XTPFoC46Nqd/67kykMI0YtJQ6EhQ4YgOTkZixcvRnx8POrWrYsDBw7Iev3Hx8crDfGSmpqKyMhIrFy5UvCYISEh2LFjB+bPn4+vvvoK1apVw86dO9GiRQvDvwAjBqwzZwKurvzTEEJKxpMnT3ijjwhxdnbGZgO21zHFOa1agYpgtNlP8uWc5JIpCyFEbyavu5syZQqmTJkiuC0iIkIpzd3dHZmZmWqPOXDgQAwcONAQxVPPAAGrqkH/ly2TL9PINoSUrMTERCQkJCj90L106RIkEgmaNm1qFee0akK1pzWnA2Uby9dzKWAlxFKYfJQAi2aAgFUo9ra3B7h3A5PpmkpIiZo6dSpiY2OV0l+8eIGpU6dazTmtmlDA6q4wAkyh4iwt1IaVEHNFAas+9AxYjx0D6tcvPp+a/mKEECOIjo5G48aNldIbNWqk+SgiFnBOq1SQC0QvBa58pLxNzB9VhsZXJcRyUMCqD4ei2VO0nZoKQFIS0Lmz8Jirih1V/fyAuDhg3z52vWZNrU9HCNGCvb09Xr16pZQeHx8PGyP1gjTFOa3SvR+Am58LbxMrdGSr8yXQ+wFQoSe7XmumcctGCNEZBaz6cHZmn4tpUytE29v8fn7sqAF37gA06Q0hxtWlSxfZ+MxSKSkp+PLLL9GlSxerOadVeqdmyj+RQsAqEgFuNdjZrHrcAKpPMm7ZCCE6o5/t+nAqmpdah4A1N1e3U9apo9t+hBDN/fjjj2jbti0CAgLQqFEjAMDNmzfh4+OD3377zWrOaZ0UblH5vw/ERrLLijWsUhI7oExDo5aKEKIfClj1oUfASj3/CTFffn5+uH37NrZu3Ypbt27B0dERY8eOxbBhwwTHR7XUc1onhYDVzkO+rCpgJYSYPQpY9WGCGlZCSMlwdnbGxIkTrf6cVs/WQ75MASshFosCVn1QwEqIVYuOjkZMTAxyFf5h33vvPas6p1VhFIaqohpWQqwCBaz6MGDAKpEYbYZXQoiWnjx5gv79++Pff/+FSCQCwzAAAFHREB4FRvhnNcU5rVKBwsWVW8Oq2OmKEGIxdBolIDY2FnFxcbL1y5cvY9asWfjll18MVjCLYMA2rGIar4EQszFz5kxUqVIFr169gpOTE+7evYvTp0+jadOmOHnypNWc0yoVKlxcuTWsimMGEkIshk5h0vDhw3HixAkAQEJCArp06YLLly/jyy+/xOLFiw1aQLNm4BpWKbqmEmJaFy5cwOLFi+Hl5QWxWAyxWIzWrVsjNDQUM2bMsJpzWqVCxRpWd84KXVwJsVQ6Bax37txB8+bNAQC7du1C3bp1cf78eWzbtg0RERGGLJ95o4CVEKtUUFAAFxcXAEC5cuXw8uVLAEBAQADu379vNee0SkoBq4tpykEIMSidAta8vDzY29sDAI4ePSrrDFC7dm3Ex8cbrnTmTouANS0N6NQJWL+eXacmAYSYr7p16+L27dsAgBYtWmDp0qU4d+4cFi9ejKpVq1rNOa3Cy0NAVGsgrSioL1C8uNqXfJkIIQanU6erOnXqYP369ejVqxeioqLwzTffAABevnwJT09PgxbQrGkRsK5aBRw/zj5CQoD58/nbaZhFQszH/Pnz8e7dOwDAkiVL0Lt3b7Rp0waenp7YuXOn1ZzTKpzswT7/Uxto8xfw6hh/O40MQIhV0Clg/f7779G/f38sW7YMo0ePRoMGDQAA+/btkzUVKBW4ASvDqL2Xn5EhXy56u3giI4FevdhDNWli4HISQrTSrVs32XLVqlURHR2NN2/eoEyZMrJe+9ZwTqtzZoByGncGK24HLEKIRdEpYG3fvj2SkpKQlpaGMmXKyNInTpwIJ2kQVxpIX2thIdso1V71rSc7O9WHEYmA9u2BK1eANWuAefMMW0xCiOby8/Ph4OCAmzdvom7durL0smXLWtU5S4WG37M1rC03A5kvAI96pi4RIURHOgWsWVlZYBhGFqw+f/4cu3fvRmBgIK+WwOpxg/PMTJ0D1ocP2eegIGDtWgOVjRCiExsbGwQEBJTouKemOKfVqzQIqP0xu1x1jEmLQgjRn05dffr27YstW7YAAFJSUtCiRQv8+OOP6NevH9atW2fQApo1W1vApijmL6Ydq6qAdcIEoFo1A5eLEKKX+fPnY+7cuXjz5o1Vn9Oq1ZpF7VcJsSI61bBev34dK1asAAD8+eef8PHxwY0bNxAZGYkFCxbgo48+MmghzZqTEzsEQFaWyiwvXgCffy68zYbmGiPE7KxatQqPHj1ChQoVEBAQAGdnZ97269evW8U5LV7sX6q32XuVXDkIIUanU7iUmZkJV1dXAMCRI0cwYMAAiMVitGzZEs+fPzdoAc2eNGBVU8MaGqp6dz8/I5SJEKKXfv36lYpzWrwz76ve5kJDgRFiTXQKWKtXr449e/agf//+OHz4MD7+mG0nlJiYCDc3N4MW0OxJ27EWDUcjRFUs6+ICjB9vhDIRQvSycOHCUnFOq1WxHyCWFJuNEGI5dGrDumDBAnzyySeoXLkymjdvjuDgYABsbWujRo0MWkCzJx13NiFBZRahyb9GjABevgTKlzdOsQghxKplvhBOb/MX0CayZMtCCDE6nWpYBw4ciNatWyM+Pl42BisAdOrUCf379zdY4SxC1arseFRPnwpuvnKFHaJVkZMTUNSqghBiZsRisdqxT43Rm98U57RoF0YKp7vWBEQ0dSAh1kbnLj++vr7w9fVFXFwcRCIR/Pz8StekAVJVqrDP//0nuDk9XXi3vDwjlYcQorfdu3fz1vPy8nDjxg38+uuvWLRokdWc06K9OiGc7uhbsuUghJQInQLWwsJCLFmyBD/++CMyiqZwcnV1xZw5czBv3jyIxaXo123btsB33wF//gn8/DOg8NpVDc3q718CZSOE6KRv375KaQMHDkSdOnWwc+dOjDdC43NTnNOiiW2BQoFf/nZllNMIIRZPp4B13rx5CA8Px3fffYdWrVqBYRicO3cOX3/9NbKzs/Htt98aupzmq3NnNkhNSWHbsVaowNuck8PP7u0NdOgAfPJJyRWREGIYLVq0wIcffmj157QIYgd+wBowHCjbhJoDEGKldApYf/31V2zcuBHvvfeeLK1Bgwbw8/PDlClTSlfAamvLjk0VGwscOwaM5Ler4gas48YBP/4IeHiUbBEJIfrLysrC6tWrUbFiRas+p8Xw7QTE7WGXW2wCqo01aXEIIcalU8D65s0b1K5dWym9du3apXOWFmnA+vnnSgFrdjb7HBwMhIeboGyEEK2VKVOG1wGKYRikp6fDyckJv//+u9Wc07IVvVfN1lKwSkgpoFPA2qBBA6xZswarVq3ipa9Zswb169c3SMEsytSpwMWLwKtXQEYGO8Aq2KGrIotGV3FwMGH5CCFaWbFiBS94FIvF8PLyQosWLVCmjHHaSJrinBYp7SFwfjjw5iq7LlbRUYAQYlV0CliXLl2KXr164ejRowgODoZIJML58+cRGxuLAwcOGLqM5m/ECLZR6qtX7GgBzZohKQnYulWehQJWQizHmDFjSsU5LdLVafJgFQAkdHElpDTQqXV6u3bt8ODBA/Tv3x8pKSl48+YNBgwYgLt372Lz5s2GLqNlqFmTfX74EACgOJyiqtECCCHmZ/Pmzfjjjz+U0v/44w/8+uuvVnNOi1SgMHUg1bASUiro3J2yQoUK+PbbbxEZGYm//voLS5Yswdu3b0vvhbV6dfb58WMAyuOsUg0rIZbju+++Q7ly5ZTSvb298b///c9qzmmRbFz461TDSkipQON/GIqPD/v8/DkAID+fv5lqWAmxHM+fP0cV6aQgHAEBAYiJibGac1okpYCVLq6ElAYUsBqKdKyq8HAgJoYCVkIsmLe3N27fvq2UfuvWLXh6elrNOS2SrcKc1tQkgJBSgQJWQ+H24v35Z6WAlZoEEGI5hg4dihkzZuDEiRMoKChAQUEBjh8/jpkzZ2Lo0KFWc06LRE0CCCmVtBolYMCAAWq3p6Sk6FMWy+bK+dUvFuPKFf5mClgJsRxLlizB8+fP0alTJ9jYsJfJwsJCjBo1ymjtSU1xTosU+yd/nQJWQkoFrQJWd3f3YrePGjVKrwJZLO6UVtu3Y/jjb3iby5Yt4fIQQnRmZ2eHnTt3YsmSJbh58yYcHR1Rr149BAQEWNU5LU7SZSArnp9mRxdXQkoDrQLWUjtklSbq1JEvF40UwEUBKyGWp0aNGqhRo4bVn9NipN9XTqOAlZBSgdqwGkqzZsCKFSo3U58JQizHwIED8d133ymlL1u2DIMGDbKac1qcwjzlNBvHki8HIaTEUcBqSLNmAX5+gpuohpUQy3Hq1Cn06tVLKb179+44ffq01ZzT4hTmmroEhBAToYDV0BgGjECywHjghBAzlZGRATs7O6V0W1tbpKWlWc05LY5iDavY1jTlIISUOApYDW3lSmSBf4tq5kx+E1dCiHmrW7cudu7cqZS+Y8cOBAUFWc05LY5iDWtr5alsCSHWSatOV0QDAwcio88h4G95UliYyUpDCNHBV199hffffx+PHz9Gx44dAQDHjh3Dtm3b8Oeffxazt+Wc0+Jwa1jrzAcq9jVdWQghJYoCViPICO7CC1gJIZblvffew549e/C///0Pf/75JxwdHdGgQQMcP34cbm5uVnNOi1OYw10xWTEIISWPAlYjSLYvb+oiEEL01KtXL1knqJSUFGzduhWzZs3CrVu3UFBQYDXntCg5b+TLDAWshJQm1IbVwF69AprPactPzMgwTWEIIXo5fvw4RowYgQoVKmDNmjXo2bMnrl69anXntAhxe4EHqzgJQt1bCSHWimpYDWzYMIHEChWA2FigmJnCCCGmFxcXh4iICGzatAnv3r3D4MGDkZeXh8jISKN1fjLFOS0KwwCn+ymkUY0zIaUJ1bAa2NmzAonp6cCpUyVeFkKIdnr27ImgoCBER0dj9erVePnyJVavXm1157Q4eQLDelGTAEJKFaphNTCRSMWGxYuB994r0bIQQrRz5MgRzJgxAx999FGJTY9qinNaHKEZrihgJaRUoRpWAxOrekevXQNyaZYWQszZmTNnkJ6ejqZNm6JFixZYs2YNXr9+bXXntDiMQMBKbVgJKVUoYDUwlQErANy9W2LlIIRoLzg4GBs2bEB8fDwmTZqEHTt2wM/PD4WFhYiKikJ6erpVnNPiFOYrp1ENKyGlCgWsBqY2YL13r8TKQQjRnZOTE8aNG4ezZ8/i33//xZw5c/Ddd9/B29sb7xmpaY8pzmkxGApYCSntKGA1MMWAdQVmyVeePGE7YN2+XaJlIoTorlatWli6dCni4uKwfft2qz2nWVNswypxBAI/MU1ZCCEmQQGrgXED1q1bgVlfOMoTHj0C2rQBGjQAjh8v+cIRQnQmkUjQr18/7Nu3z6rPaZYUa1gHpQIulU1SFEKIaVDAamDcgNXZGcA33wBff80mHD4M3LrFLlOtCSGEaEaxDavY1jTlIISYjMkD1rVr16JKlSpwcHBAkyZNcObMGbX5c3JyMG/ePAQEBMDe3h7VqlXDpk2bZNsjIiIgEomUHtnZ2cZ+KQD4AautLQAbG2DuXPY5Pl6+0cGhRMpDCCEWT6gNKyGkVDHpOKw7d+7ErFmzsHbtWrRq1Qo///wzevTogejoaFSqVElwn8GDB+PVq1cIDw9H9erVkZiYiPx8/sXMzc0N9+/f56U5lFCAKJHIl2VjstrZAf7+wNOn3AKVSHkIIcTiCY3DSggpVUwasC5fvhzjx4/HhAkTAABhYWE4fPgw1q1bh9DQUKX8hw4dwqlTp/DkyROULVsWAFC5cmWlfCKRCL6+vkYtuyoqRwmoXJkfsP7wA1vjunEjBa+EEKIO1bASUuqZrElAbm4url27hq5du/LSu3btivPnzwvus2/fPjRt2hRLly6Fn58fatasiU8++QRZWVm8fBkZGQgICEDFihXRu3dv3LhxQ21ZcnJykJaWJnvoM+6hypmuAgKU07ZuBTZs0PlchBBSKgiNw0oIKVVMFrAmJSWhoKAAPj4+vHQfHx8kJCQI7vPkyROcPXsWd+7cwe7duxEWFoY///wTU6dOleWpXbs2IiIisG/fPmzfvh0ODg5o1aoVHj58qLIsoaGhcHd3lz2CgoJ0fl2FnKEBGe5ELEIBKwBcuKDzuQixFoZuyw4AkZGRCAoKgr29PYKCgrB7925jvgSzY1XvqeBMV4SQ0sTkna5EClWSDMMopUkVFhZCJBJh69ataN68OXr27Inly5cjIiJCVsvasmVLjBgxAg0aNECbNm2wa9cu1KxZE6tXr1ZZhrlz5yI1NVX2iI6O1vn15Km6rjIqphGkyQRIKSdtyz5v3jzcuHEDbdq0QY8ePRATE6Nyn8GDB+PYsWMIDw/H/fv3sX37dtSuXVu2/cKFCxgyZAhGjhyJW7duYeTIkRg8eDAuXbpUEi/J5KzuPaUaVkIIYyI5OTmMRCJh/vrrL176jBkzmLZt2wruM2rUKKZatWq8tOjoaAYA8+DBA5XnmjBhAtO9e3eNyxYbG8sAYGJjYzXeR8rdnWHY6JRh0tM5Gx48YBg7O4bp21eeAWAYiYRhUlK0Pg8h5kiX/53mzZszkydP5qXVrl2b+eKLLwTzHzx4kHF3d2eSk5NVHnPw4MFK//PdunVjhg4dqnG5LJm5vqepqakMACY1NVXjfRiGYZi4vxlmK9jHhXHa7UuIFdD5f8eKmKyG1c7ODk2aNEFUVBQvPSoqCiEhIYL7tGrVCi9fvkRGRoYs7cGDBxCLxahYsaLgPgzD4ObNmyhfvrzhCq+GdMCC6GjAxYWzoUYNIC4O2LEDmDNHnl5QAHh4sGO0ElLKGKst+4ULF5SO2a1bN5XHtCZW+Z5KRwko0whosdH45yOEmB2TNgmYPXs2Nm7ciE2bNuG///7Dxx9/jJiYGEyePBkAe6t+1KhRsvzDhw+Hp6cnxo4di+joaJw+fRqffvopxo0bB0dHdkapRYsW4fDhw3jy5Alu3ryJ8ePH4+bNm7JjGtu7d+wzL1iV8vJiRwRYvBg4dw4YP16+rXt3dlKB0aMBNbftCLEE6enpvI6MOTk5gvmM1ZY9ISFBq2NaE3N6TxU7tKalpen2ogqKPj+2rmp6thJCrJlJh7UaMmQIkpOTsXjxYsTHx6Nu3bo4cOAAAoo6KMXHx/PaXLm4uCAqKgrTp09H06ZN4enpicGDB2PJkiWyPCkpKZg4cSISEhLg7u6ORo0a4fTp02jevLnRX8/8+fJlG3XvrJMTEBLC9tAKD5en9+/PDn119y5w9arRykmIsSl2XFy4cCG+ls74JkDXtuzu7u4A2CHyBg4ciJ9++kn241WbY1ojc3hPQ0NDsWjRIn1eBhusnh/GLqfdV5+XEGK1TBqwAsCUKVMwZcoUwW0RERFKabVr11ZqRsC1YsUKrFixwlDF09jdu8C338rX1QasUk2a8Nel47Reu2awchFiCtHR0fDz85Ot29vbC+YrV64cJBKJUi1dYmKiUm2eVPny5eHn5ycLrAAgMDAQDMMgLi4ONWrUgK+vr1bHtCbm9J7OnTsXs2fPlq2npaXB399fuxd0hXN3LPuVdvsSQqyGyUcJsBaKzbg0ClgdHYH9+41SHkJMydXVFW5ubrKHqoDVWG3Zg4ODlY555MgRlce0Jub0ntrb2/M+B25ubtq/oCcR2u9DCLE+JuzwZbZ06em8YQO/8z9vhAB1Cgv5O0ofGRm6FZ4QE9Llf2fHjh2Mra0tEx4ezkRHRzOzZs1inJ2dmWfPnjEMwzBffPEFM3LkSFn+9PR0pmLFiszAgQOZu3fvMqdOnWJq1KjBTJgwQZbn3LlzjEQiYb777jvmv//+Y7777jvGxsaGuXjxouFerBkz1/dUp57O0tEBpA9CSiEaJYBhTN4kwFooTsmqUQ0roLoDwahRQGSkXmUixBIYoy17SEgIduzYgfnz5+Orr75CtWrVsHPnTrRo0aLEX58p0HtKCLE2IoZRNaJ96RUXFwd/f3/ExsaqHC5L0a+/AmPGyNfz8wGJRMMTfvYZsGyZcjr9aYiF0eV/h5QeaWlpcHd3R2pqqubNA7Yp/KgfTtdFUvro9L9jZagNq4Fwa1iDg7UIVgFg6VIgMRFITuanP3pkkLIRQohV6EpTWRNSWlHAaiDcgLVLFx0O4OUFlC3LT6tRg4a3IoQQKZdqpi4BIcREKGA1dz17AhkZbG3rrVumLg0hhJiORHi0CUKI9aNOVwaSlydfNmjT09evAVdX+frdu0DNmsCbN4C3twFPRAghZk7sYOoSEEJMhGpYDSQ310AHKm5c1vv32RmxfHyAe/cMdFJCCLEAYltTl4AQYiIUsBoIN2DVq4a1Z09g4EDV2wcMAP75h13eulWPExFCiIUpRVPrEkL4KGA1EG6TAL1pGvGWL2/AkxJCCCGEmCcKWA2EW8Pq6FhCJ83MLKETEUIIIYSYDgWsBsINWKdNM+CBV69Wve32beDyZQOejBBCzIwzOzsXvNqYthyEEJOigNVApE0Cpk0D9J6EgtskQF30+9tvQIsWwIMHbL6//9bzxIQQYm6K2q02EpgNkBBSalDAaiDSGlY7OyMcvFEj9ds//hj46SfgvfeMcHJCCDGhwqLaALExLq6EEEtBAauBSANWW0OMuqLYmervv4F584Dq1YFOndiaVa4DB/jrBQXAtWsG7glGCCEmUFh0caUhrQgp1ShgNZBXr9hne0NMxLJoEdCnDxAZya77+QFLlgAPHwJHjwINGqjfPzQUaNoUmD7dAIUhhBATyc8C8lLZZTHNckVIaUYBq4EcOsQ+t25tgIN5egL79rFjrgqpXZud7UpIdjbw1Vfs8s8/G6AwhBBiIq9OsDWsjhUAlyqmLg0hxIQoYDUAhgHevmWX69UrgRPa2gI3b7JtVxUpjql17lwJFIgQQowgt+jC6h4EiGkmcUJKMwpYDSAnR96x38mphE7q6AjUrVt8vtatgbg445eHEEIMraBorGlJSV1YCSHmigJWA8jKki+X2KQBADB4sGb5rl0zbjkIIcQYCoourpKSvLASQswRBawGIJ1wysbGQKMEaMrFBdi7t/h8jx8bvyyEEGJo+dKLK9WwElLaUcBqANIa1hJrDsD13nvA8+fA55+rzkMBKyHEEslqWClgJaS0o4DVAKQ1rCXaHICrUiXAx0f19rVrgfR0+Tp3Ji1CCDFX0jasNtQkgJDSjrpdGsCuXeyzSWpYpcqUUb89NJSddKBzZ3Y9IoJtUmBjA/Tta/TiEUKIVvLfAfeWs8tUw0pIqUcBq54SE4Fvv2WXS7T9qiJnZ/XbQ0PZh9SYMfLl3FwTF54QQhTc+gpgCtll6nRFSKlHTQL0lJQkX05JMVkxADuBebavXQOOHCl+X2mbBkIIMRcv/pYvU6crQko9qmHVU1qafDk52XTl4NWwPnwI+Pqyt/zv3y9+38xMQCJhl11cjFM+QgjRhq2bfNmmmDtIhBCrRwGrnqQzXAFAnz6mKwfatQOCg4HAQKB6dXl61arF7xsQAOTlsY1w//0XqFIFEImMV1ZCCCmOnYd82autyYpBCDEP1CRAT9xmAJs3m6wYbBvU8+eB8HDldDc34X2k8vLY58xMoFo14H//M04ZCSFEUxIH9tmjAeBWw7RlIYSYHAWsepIGrAMGAB4epiyJGsePs00EpMqWVZ9//nzjlocQQoqTm8I+11to0mIQQswDBax6Sk1ln4urxDSpJk2Aly/l6xMnAgMHmq48hBBSnLyii6utOV9cCSElhQJWPWVns88ODqYtR7FEIuCDD9h2qtOmAX/8oT7/J59oNsIAIYQYQ0HRxVVi7hdXQkhJoIBVT7m57LO9vWnLoZHffmOHMvDzKz7vjz8C3boZv0yEECKkMId9FlvCxZUQYmwUsOopp+iaahEBq0ikW1Vwfr7hy0IIIeoUFtUGSCzh4koIMTYKWPVkUQGrKo5qZpH5+We2ge7JkyVWHEIIQQHVsBJC5Chg1ZM0YBWaaMrsff45+7x3L7Bli3CeyZOBrCxg+PCSKxchhEibBFANKyEEFLDqzaJrWENDgTdvgC5dgIYN1eeVjtVKCCHGxjBUw0oI4aGAVU8W1elKkUgElCnDLtety85wpUpSEjvYrHQcL0IIMRYmHwDDLlMNKyEEFLDqzaJrWLlEIuDWLfV5du8GNm2Sv2hCCDGGAs41hmpYCSGggFVvVhOwAoCra/F5Zs9mRxq4dMn45SGElE6F3IDVEjsIEEIMjQJWPVlVwAqws2BpomVLebvWzExqKkAIMRxpDatIDIhtTFsWQohZoIBVTxY9SoCQ9ev56wcPqs67fDn73KEDULMm8Pat8cpFCCk9aNIAQogCClj1ZNGdroSIRPLl4GCge3fVeb/4Ati5E7h8GUhMBA4fNn75CCHWTzppAAWshJAiFLDqyeqaBHC9e1d8nqFD5cuPHhmvLISQ0qOAxmAlhPBRwKonq2sSwCWtPtZUcrJxykEIKV1kTQKs8cJKCNEFBax6kvY7sqqAdfVqwMkJ2LCBXf/2W832S0sDCguBu3fZZ0II0UVh0YWVAlZCSBEKWPUkrYS0tTVtOQxq2jS213/r1uz6F18AN28CFy4AQ4YA9+4BPj7K+6WmAt98w05C8OmnwO+/A/n58u2bNwNHj5bISyCEWDBZG1ZrurASQvRB44XoySprWAHAhvPREIuBBg3Y5R072OfoaMDTk79PWhrw9dfssnQEgWXL2KB1zRpg3Dg2jWGMVmxCiBWgGlZCiAKqYdWTNGC1qhpWTZQtC+zZw08Tqj29fZsNbqdPl6dxa10JIUSRLGAtbRdWQogqFLDqqdQGrADQty9/XV3NaVqafDklxSjFIYRYCabowioqjRdWQogQClj1VKoDVm1wA9a3b9k2sdqOQkAIKR2ohpUQooACVj1JYy6ra8Oqr0uX+OvcqVsnTwYaNQJWrizZMhFCLAN1uiKEKKCAVQ+FhfLRm0p9DWu7dvLlkBCgcmXVeY8fZ59DQ9k3MDcXePjQqMUjhFgQ6nRFCFFg8oB17dq1qFKlChwcHNCkSROcOXNGbf6cnBzMmzcPAQEBsLe3R7Vq1bBp0yZensjISAQFBcHe3h5BQUHYvXu3UcoubQ4AUMCKKlXkywUFgJdX8fu8fcuORmBvD9SsCcyfD+zeDXTpAsTHG6+shBDzRk0CCCEKTBqw7ty5E7NmzcK8efNw48YNtGnTBj169EBMTIzKfQYPHoxjx44hPDwc9+/fx/bt21G7dm3Z9gsXLmDIkCEYOXIkbt26hZEjR2Lw4MG4pHiL2gAoYAUQFcWOzbp0KdCqFZs2bhwgEgEfflj8/tyOWt9+CwwYwI428OmnxikvIcT8MRSwEkL4RAxjukExW7RogcaNG2PdunWytMDAQPTr1w+hoaFK+Q8dOoShQ4fiyZMnKFu2rOAxhwwZgrS0NBw8eFCW1r17d5QpUwbbt2/XqFxxcXHw9/dHbGwsKlasqDLfmzfyoUjz8vhDl5ZKqanA9ets8wBx0W+hp0+Bly/lkxBoys2NHQ7Lz8/w5SRGo+n/Dimd0tLS4O7ujtTUVLi5uanOeG8lcH0WUGkI0HpHiZWPEHOl8f+OFTNZDWtubi6uXbuGrl278tK7du2K8+fPC+6zb98+NG3aFEuXLoWfnx9q1qyJTz75BFlZWbI8Fy5cUDpmt27dVB4TYJsZpKWlyR7p6ekavQZuDatEotEu1s3dHejQQR6sAmxTAW5zAU2lpQEU8BBSOjHUhpUQwmeyOsGkpCQUFBTAR2GKTx8fHyQkJAju8+TJE5w9exYODg7YvXs3kpKSMGXKFLx580bWjjUhIUGrYwJAaGgoFi1apPVr4A5pJRJpvXvp4e6u+74MQ28uIaUNtWElhCgweacrkUIwwjCMUppUYWEhRCIRtm7diubNm6Nnz55Yvnw5IiIieLWs2hwTAObOnYvU1FTZIzo6WqOy0xisGnJy0n3f1q1pKldCShsKWAkhCkwWsJYrVw4SiUSp5jMxMVGphlSqfPny8PPzgzunxi4wMBAMwyAuLg4A4Ovrq9UxAcDe3h5ubm6yh6urq0avgQJWDelTQ3r+PJCcbLiyEELMHwWshBAFJgtY7ezs0KRJE0RFRfHSo6KiEBISIrhPq1at8PLlS2RkZMjSHjx4ALFYLOvgERwcrHTMI0eOqDymPmjSAD14e2ueNyAAuHbNeGUhhJgX6cQBNDUrIaSISZsEzJ49Gxs3bsSmTZvw33//4eOPP0ZMTAwmT54MgL1VP2rUKFn+4cOHw9PTE2PHjkV0dDROnz6NTz/9FOPGjYOjoyMAYObMmThy5Ai+//573Lt3D99//z2OHj2KWbNmGbz8VMOqo6AgQGHsXKV1rsxMoHt3ICcHOHgQmDoV4DQBIYRYGWkNq4RqAwghLJMOxDRkyBAkJydj8eLFiI+PR926dXHgwAEEBAQAAOLj43ljsrq4uCAqKgrTp09H06ZN4enpicGDB2PJkiWyPCEhIdixYwfmz5+Pr776CtWqVcPOnTvRokULg5efAlYd3b0LpKTI12/eLL7GNSkJcHCQrzdoAEycyC4zDHDuHODjA9SoYejSEkJKmnSUAKphJYQUMfnIoVOmTMGUKVMEt0VERCil1a5dW+mWv6KBAwdi4MCBhiieWhSw6sHFRb5sa8sGm9p49459TkqSz6pVsSIQG2uY8hFCTIfasBJCFJh8lABLRm1YtbBhA/u8fj37bGPDzoT13ntAYCA7dmujRpofT9rxbvlyeVpcHDstLCHEsknbsFLASggpYvIaVktGNaxamDABGDgQ8PCQp/3yCz/P1q1sc4FBg4o/XlYW2xQgLY2fnpYGlCmjd3EJISZENayEEAVUw6oHCli1xA1WhQQGskGtJqZNY9urKg55xW0bSwixTIU00xUhhI8CVj1QwGpkxc13+/gxsENhnnFpwHr+PNCjB3D/vlGKRggxIoZqWAkhfNQkQA/UhtXICguBsmWBN28030casLZqxT536wY8e2bokhFCjKmAxmElhPBRDaseqIbVSIYNY5/nzAFiYoAqVTTfNyEBuHxZvv78ORu0EkIsB9WwEkIUUA2rHihgNZJNm9gxVlu1Yt/cx4+B9HT5yADqrF4NXLjATztyBMjPZ0cmIISYP+p0RQhRQDWseqCA1UgcHID27eVvrEgEuLlptq9isCqlOJoAIcR8UacrQogCClj1QAGrBaGAlRDLQTWshBAFFLDqgTpdlbAFC4DevYEVK4DOnYF//wWePNFs39RU45aNEGI4hdTpihDCR4369EA1rCVs0SL58qxZ8uVhw4Dt29Xve/EiOynBsGFAaChQrx7Qp49RikkI0RN1uiKEKKAaVj1QwGomtm5lRwPgql2bvz55MvDBB8CMGcC8eeyUsC9eyKvJb98G/vmnZMpLlKxduxZVqlSBg4MDmjRpgjNnzqjMe/LkSYhEIqXHvXv3ZHkiIiIE82RnZ5fEyzELFv2eUhtWQogCqmHVAwWsZkIkAjw95esXLrCP2bOV80ZEyJcrVmSfN25kp44FgJs3gQYNjFVSImDnzp2YNWsW1q5di1atWuHnn39Gjx49EB0djUqVKqnc7/79+3DjdMbz8vLibXdzc8N9hYkjHBwcDFt4M2Xx7ym1YSWEKKAaVj1QG1Yz4uQE+Piwf4wGDdhJB4RkZCinSYNVgG02QErU8uXLMX78eEyYMAGBgYEICwuDv78/1q1bp3Y/b29v+Pr6yh4ShZnRRCIRb7uvr68xX4ZZsfj3VNqGlQJWQkgRClj1QDWsZkQkAp4+BZKSAEdHoHFj3Y4jFviXePQIWL+eHcuVaCQ9PR1paWmyR05OjmC+3NxcXLt2DV27duWld+3aFefPn1d7jkaNGqF8+fLo1KkTTpw4obQ9IyMDAQEBqFixInr37o0bN27o/oIsiDm9pzk5ObzPQZqmo3VQDSshRAEFrHqggNXMODoCrq7scocOuh1DKLAKDgY++ghYtkz3spUyQUFBcHd3lz1CQ0MF8yUlJaGgoAA+Pj68dB8fHyQkJAjuU758efzyyy+IjIzEX3/9hVq1aqFTp044ffq0LE/t2rURERGBffv2Yfv27XBwcECrVq3w8OFDw71IM2VO72loaCjvc+Dv76/Zi5B2uqJRAgghRagNqx4oYDVzbm7aj786Zgw7I9YHH7Cdsv76i621BYDffgPmzjV4Ma1RdHQ0/Pz8ZOv29vZq84tEIt46wzBKaVK1atVCrVq1ZOvBwcGIjY3FDz/8gLZt2wIAWrZsiZYtW8rytGrVCo0bN8bq1auxatUqrV+PJTKH93Tu3LmYzWlLnpaWplnQSp2uCCEKqIZVDxSwWqkRI9hHxYrsqAJST5/KlzdsAPr1A7KySrx4lsDV1RVubm6yh6qAtVy5cpBIJEo1f4mJiUo1hOq0bNlSbU2fWCxGs2bNSkUNqzm9p/b29rzPgZumM9ZRkwBCiAIKWPVAna4sVLdu7O39opojQVu3Kqdxh++ZOBHYuxdYvdrw5StF7Ozs0KRJE0RFRfHSo6KiEBISovFxbty4gfLly6vczjAMbt68qTaPtbCK95Q6XRFCFFCTAD1QDauZs1Hx8c7KAj75hH3s2MFOJqCpt2/ZpgZS3FpXAEhPZ9vSqjo3UTJ79myMHDkSTZs2RXBwMH755RfExMRg8uTJANjbyi9evMCWLVsAAGFhYahcuTLq1KmD3Nxc/P7774iMjERkZKTsmIsWLULLli1Ro0YNpKWlYdWqVbh58yZ++uknk7zGkmbR7ynDAEwBu0wBKyGkCH2r6oECVjO3ezd72z4sDLh2DZC2sxsyRJ6nWjXtjlm2LODsLF9PSZEvJycD5coBjRoB16/rWOjSZ8iQIUhOTsbixYsRHx+PunXr4sCBAwgICAAAxMfHIyYmRpY/NzcXn3zyCV68eAFHR0fUqVMH+/fvR8+ePWV5UlJSMHHiRCQkJMDd3R2NGjXC6dOn0bx58xJ/faZg0e+ptDkAQAErIURGxDAMY+pCmJu4uDj4+/sjNjYWFaWDywsYMICNidatYydSImaIYdghrwDg9Wvg3Dmgd295Dejr14C3t+7H79oVOHyYXd6+HRg+XH7eUkjT/x1SOqWlpcHd3R2pqamq27PmvwN2ubDLgzMAG2fhfISUIhr971g5asOqB2rDagG4vaK9vNgaV+7tei8vYMUKoH173Y6fnQ28egVcucL/IJTSgJUQvUnbrwI0rBUhRIYCVj1QkwArMWsW+9BFUhJbq9q8OXDokDx94kRDlIyQ0oeaBBBCBFDAqgcKWK1IcDD7rO2tluho4PhxdnnjRnn6xo3yKnht/PYb8L//ab8fIdZCGrCKJPw7JISQUo0CVj1QwGpFvL2BuDggNtZwx3R1BXbu1G6fUaOAefOAmzcNVw5CLAlNGkAIEUABqx6oDauV8fNTrmHV54+bmwsMHcour1nDjtuq6O1b4MQJ5Tavr1/rfl5CLBmNwUoIEUABqx6ohtXKXbkCnDnDT/vqK0CL2YIAsJMUTJ/OdvhS1L8/0LEjO1FBfr48nbtMSGlCs1wRQgRQwKoHClit1L17bAeqpk2BZs2AgQPl23r3BhwctDveZ5/Jlzt0AO7fl6+fOsU+r1jBn0mroED7chNiDRhpG1a6sBJC5GjiAD0YOmAtKChAXl5e8RmJcQUEsA9pALllC1vbCrCzWPn66n7sp0/x//buPCyqsu8D+PewMzAggjAQ+4OCuKCypJJbmohimprkBQoieqFCUvmgZLiUppYIT5n45svik75uYT6W5poWIT6aieKaGSoqiJqxiAgy9/vHNEcODDBsMwPz+1zXXM6cOcs9xzM/fnOfe8Hq1cCmTS+OBciS4LKyF685TpjAahADAwPo6NBvXdJOGqlhpRhJOit9fX3o6uqquxgajRLWVmirhJUxhqKiIvxVe9Ykolm2b5fVeurpAcuXt25fHAf88YfsuTxx1deXzZQlf21lVX/aVw2ho6MDFxcXGFDjbdIeFHS6ohhJtEGXLl0gkUjA0egYClHC2gpt1elKHoitra0hEonoYtV0JiZAcXHr9mFsLEtKKypevHZweFGr+tJLgIVF647RDqRSKe7du4fCwkI4OjrStUranoJOVxQjSWfGGENFRQWK//67Ymtrq+YSaSZKWFuhLWpYa2pq+EBsaWnZNgUj7cvBAbC0lP1SOX++Zft49Eg4JayOjvBC0tVtfltZFenWrRvu3buH58+fQ58acJO2VqdJAMVIog2MjY0BAMXFxbC2tqbmAQpQQ7RWaIuEVd4eSyQStUGJiEpwnKyWtbXtOK9cefH8yRPgxo0Xr6XSlu9XKm3XTlvypgA11DGMtAepsNMVxUiiLeTXOLXTVowS1lZoy05XdIurA6qdsNrZtX5/tTtZtSZhvXABOHeu3ZJWulZJu2KKO13RdUc6O7rGG0cJayvI27DSXdG2M3z4cMTGxiq9/s2bN8FxHHLVMTNU7eBiYgJ4eQGenm2z75aOw8rYi22fPm2bshCiSjRxgEDdmOjs7Izk5ORGt+E4Dnv37m31sdtqP4S0BUpYW0GeF2hjwspxXKOP8PDwFu13z549+Oijj5Re38HBAYWFhejdu3eLjqesBhNjc3PA0FA2Dau+vrAHXteuLT9gQ7eECguBe/devGas/ixZcvRrnXRE0r8DawdPWMePH49Ro0YpfC8nJwccx+HXX39t9n7PnDmDOXPmtLZ4AsuXL0e/fv3qLS8sLERgYGCbHqshT58+hYWFBbp27Yqn9GObKECdrlpBfsdVG9tGFxYW8s937tyJpUuX4lqtAfHlDcjlqqurleqg07WZSZ6uri4krRkXtbXc3GT/ypNDPT1ZD39ANiPW48cNJ5SNUZSw1tQAd+/KnnfrJtvvpUuyxFg+fmtrmhIQognY34GV69iBddasWZg0aRJu3boFJ/n3829paWno168fBgwY0Oz9duvWra2K2CRVxtbMzEz07t0bjDHs2bMHISEhKjt2XYwx1NTUQE+PUiRNQjWsLVQ7L9DGhFUikfAPc3NzcBzHv66srESXLl2wa9cuDB8+HEZGRti6dSsePXqEadOmwd7eHiKRCH369MH27dsF+1V0++vjjz9GREQExGIxHB0d8eWXX/Lv1635PHHiBDiOw7Fjx+Dj4wORSITBgwcLkmkAWLlyJaytrSEWixEZGYnFixcrrGFoEscBHIdnz57h7bffhrW1NYxcXPDKm2/izNmz/EgAj0tLEfLBB+g2ejSMX3kF3SdNQvq+fQCAqupqRH/yCWzHjIGRvz+cX38dqzdskCWoVVUvfhnVTUbv3JG99+DBi2UtSY4J0SSdJGENCgqCtbU1MjIyBMsrKiqwc+dOzJo1S6mYWFfdJgHXr1/H0KFDYWRkBE9PTxw5cqTeNosWLUKPHj0gEong6uqKhIQEvmNPRkYGVqxYgfPnz/N3yORlrtskIC8vD6+++iqMjY1haWmJOXPmoLy8nH8/PDwcEydOxLp162BrawtLS0vMnz9fqU5EqampCA0NRWhoKFJTU+u9f+nSJYwbNw5mZmYQi8UYMmQIbtTqqJqWloZevXrB0NAQtra2iI6OBqD47thff/0FjuNw4sQJAC/+bhw6dAg+Pj4wNDREVlYWbty4gQkTJsDGxgampqbw9fXF0aNHBeV69uwZ4uLi4ODgAENDQ3Tv3h2pqalgjMHNzQ3r1q0TrH/x4kXo6OgIyk6UQz8fWqh2E8M2/xHG2IvxOVVNJGqzW8mLFi1CYmIi0tPTYWhoiMrKSnh7e2PRokUwMzPD/v37MX36dLi6uuLll19ucD+JiYn46KOP8P777+Prr7/G3LlzMXToUHh4eDS4zZIlS5CYmIhu3bohKioKERERyM7OBgBs27YNq1atwsaNG+Hv748dO3YgMTERLi4uLf6scXFxyMzMxJYtW+Dk5IRPPvkEAQEB+P38eXQFkLBpEy7n5+P71FRYmZjg9+vX8fTZMwDAZzt2YN9PP2HXhg1wlEhQ8PvvKLh/X9ZxSs7CQlZbK9fQNUIJK+no2N/BlWs4sDLGUF2hnp7U+iJ9pTrH6OnpYcaMGcjIyMDSpUv5bXbv3o2qqiqEhISgoqKiRTFRTiqVYtKkSbCyssKpU6dQWlqqsA+AWCxGRkYG7OzskJeXh9mzZ0MsFiMuLg7BwcG4ePEiDh48yCdj5ubm9fZRUVGBMWPGYODAgThz5gyKi4sRGRmJ6OhoQVJ+/Phx2Nra4vjx4/j9998RHByMfv36Yfbs2Q1+jhs3biAnJwd79uwBYwyxsbH4448/4OrqCgC4e/cuhg4diuHDh+OHH36AmZkZsrOz8fzvP8QpKSl49913sWbNGgQGBqKkpISP980RFxeHdevWwdXVFV26dMGdO3cwduxYrFy5EkZGRtiyZQvGjx+Pa9euwdHREQAwY8YM5OTk4LPPPoOXlxfy8/Px8OFDcByHiIgIpKenY+HChfwx0tLSMGTIEPzjH/9odvm0HSWsLVS7A3ab17BWVACmpm28UyWVl8s6ELWB2NhYTJo0SbCs9hc3JiYGBw8exO7duxsNzmPHjsW8efMAyJLgpKQknDhxotGEddWqVRg2bBgAYPHixRg3bhwqKythZGSEzz//HLNmzcLMmTMBAEuXLsXhw4cFNQXN8eTJE6SkpCAjI4Nv77V582YcOXIEqTt24J8xMbhdVIT+7u7w6dsXcHWFc62mD7fv30d3Bwe8MnQoODs7OJmZ1a9NrZ2sArLEVNHUrbWaalDySjokJWpYqyuqsdp0tYoKJBRfHg8DE+Vmi4mIiMCnn36KEydOYMSIEQBkCcukSZNgYWEBCwuLFsVEuaNHj+LKlSu4efMm7O3tAQAff/xxvXanH3zwAf/c2dkZ7733Hnbu3Im4uDgYGxvD1NQUenp6jTYB2LZtG54+fYp///vfMPn7b8SGDRswfvx4rF27FjY2NgAACwsLbNiwAbq6uvDw8MC4ceNw7NixRhPWtLQ0BAYGwuLvyVLGjBmDtLQ0rFy5EgDwxRdfwNzcHDt27OCblvXo0YPffuXKlXjvvfewYMECfpmvr2+T56+uDz/8EK+99hr/2tLSEl5eXoLjfPPNN9i3bx+io6Px22+/YdeuXThy5AjfXlmeZAPAzJkzsXTpUpw+fRp+fn6orq7G1q1b8emnnza7bISaBLRYuyasnYSPj4/gdU1NDVatWoW+ffvC0tISpqamOHz4MG7fvt3ofvr27cs/lzc9KG5ipqna28hnDZFvc+3aNfj5+QnWr/u6OW7cuIHq6mr4+/vzy/T19eHn54crV64ARkaYO3kydhw+jH7jxyNu0SKcrDXhQHhQEHKvX4f7kCF4++23cViZjhiKhqwqKxPOwEUJK+mIOkmTAADw8PDA4MGDkZaWBkAWK7KyshAREQGg5TFR7sqVK3B0dOSTVQAYNGhQvfW+/vprvPLKK5BIJDA1NUVCQoLSx6h9LC8vLz5ZBQB/f39IpVJBk6tevXoJBr23tbVtNF7X1NRgy5YtCA0N5ZeFhoZiy5Yt/FjPubm5GDJkiMJ+EMXFxbh37x5GjhzZrM+jSN2/WU+ePEFcXBw8PT3RpUsXmJqa4urVq/y5y83Nha6uLl85UpetrS3GjRvH//9/9913qKysxJtvvtnqsmojqmFtoXZtEiASyWo61aENB+c2qVNTm5iYiKSkJCQnJ6NPnz4wMTFBbGwsquTjgzWgbpDiOA7SJjoX1d5Gfiuu9jZ1b+mxViR38m0V7VO+LNDfH7e+/Rb7z53D0QsXMPKzzzB/yhSsi43FAA8P5O/di++vX8fR06cxdeFCjPL2xtdr1zZ80LrDXkmlQJ12upSwkg6JHyWg4cCqL9JHfHm8igpU/9jNMWvWLERHR+OLL75Aeno6nJyc+OSqpTFRTlHcqhuHTp06hbfeegsrVqxAQEAAX1OZmJjYrM9RO541dszmxutDhw7h7t27CA4OFiyvqanB4cOHERgYWK8Tb22NvQcAOn+Pl137XDXUprbu36x//vOfOHToENatWwc3NzcYGxtjypQp/P9PU8cGgMjISEyfPh1JSUlIT09HcHAwTYLRQlTD2kLtWsMqn0lJHY92HAopKysLEyZMQGhoKLy8vODq6orr16+32/Ea4u7ujtOnTwuW/fLLLy3en5ubGwwMDPDzzz/zy6qrq/HLL7+gZ8+e/LJuFhYInzIFW7duRfK77+LLWp0ZzExNETxpEjZv3oydO3ci84cf8GdJScMH/e034eu/28MKUMJKOiIlalg5joOBiYFaHs0d3H3q1KnQ1dXF//3f/2HLli2YOXMmv4/WxkRPT0/cvn0b92oNdZeTkyNYJzs7G05OTliyZAl8fHzQvXt33Lp1S7COgYFBkzPXeXp6Ijc3F0+ePBHsW0dHR3B7vrlSU1Px1ltvITc3V/AICQnhO1/17dsXWVlZChNNsVgMZ2dnHDt2TOH+5aMq1B7ZRtlxu7OyshAeHo433ngDffr0gUQiwc2bN/n3+/TpA6lUih9//LHBfYwdOxYmJiZISUnB999/z9euk+ajGtYWqv3dbu0MndrCzc0NmZmZOHnyJCwsLLB+/XoUFRUJkjpViImJwezZs+Hj44PBgwdj586duHDhgqDtUUPqjjYAyAL53Llz8c9//hNdu3aFo6MjPvnkE1RUVGDWrFkAgKWbNsG7Z0/08vPDs6dP8d3p0+jp7Ax07YqkDRtga2WFfiNHQufRI+zevRsSS0t0EYuV/1CXLtVfRgkr6Yg6UZMAADA1NUVwcDDef/99lJSUCMaobm1MHDVqFNzd3TFjxgwkJiaitLQUS5YsEazj5uaG27dvY8eOHfD19cX+/fvxzTffCNZxdnZGfn4+cnNzYW9vD7FYDENDQ8E6ISEhWLZsGcLCwrB8+XI8ePAAMTExmD59Ot9+tbkePHiAb7/9Fvv27as3lnZYWBjGjRuHBw8eIDo6Gp9//jneeustxMfHw9zcHKdOnYKfnx/c3d2xfPlyREVFwdraGoGBgSgrK0N2djZiYmJgbGyMgQMHYs2aNXB2dsbDhw8FbXob4+bmhj179mD8+PHgOA4JCQmC2mJnZ2eEhYUhIiKC73R169YtFBcXY+rUqQBkQy+Gh4cjPj4ebm5uCptsEOVQqtVCtcdgpfHZlZOQkIABAwYgICAAw4cPh0QiwcSJE1VejpCQEMTHx2PhwoUYMGAA8vPzER4eDiMjoya3feutt9C/f3/B4969e1izZg0mT56M6dOnY8CAAfj9999x6NAhvhOBgY0N4lNS0DcgAEOHDoWuqSl2fP014OwMU2NjrN2yBT4jR8LX1xc3b97EgX/9i7+V1WLyhFUqlXXaaqepWglpU3zC2nnqU2bNmoXHjx9j1KhRfO9yoPUxUUdHB9988w2ePXsGPz8/REZGYtWqVYJ1JkyYgHfeeQfR0dHo168fTp48iYSEBME6kydPxpgxYzBixAh069ZN4dBaIpEIhw4dwp9//glfX19MmTIFI0eOxIYNG5p3MmqRd+BS1P50xIgREIvF+Oqrr2BpaYkffvgB5eXlGDZsGLy9vbF582a++UFYWBiSk5OxceNG9OrVC0FBQYKa6rS0NFRXV8PHxwcLFizgO3M1JSkpCRYWFhg8eDDGjx+PgICAemPnpqSkYMqUKZg3bx48PDwwe/ZsQS00IPv/r6qqotrVVuJYaxrvdVJ37tyBg4MDCgoKBI3ZaysoABwdZRMbKbobq6zKykrk5+fDxcVFqYSJtI/XXnsNEokEX331lWoPzBhw9qzseY8egJmZ7HlBAXD/fuv27eoqm1Tg4UPg5k3ZxVqrM1pLNXbNKvPdIdqrtLQU5ubmKCkpgZn8Wq/r0sfA+SWAawQwMJViJOnwsrOzMXz4cNy5c6fR2ujGrnWlvjudXOf5Cati2jzLVUdXUVGBTZs2ISAgALq6uti+fTuOHj2qcMDtdle7er72c3t7wNZW1pGqpdMUyn9JyTvwySchoIuWaDJp52oSQLTXs2fPUFBQgISEBEydOrXFTSeIDDUJaCF5wkozt3U8HMfhwIEDGDJkCLy9vfHtt98iMzOzwXm/251IJEsia/cc5TjZxdWKyQxw9y5w8aKwkfWNGy+mfS0uBq5eVd+IFIQoIm8S0MgoAYR0BNu3b4e7uztKSkrwySefqLs4HR5FhBaSjypElVUdj7Gxcb3p9dSqZ09Z0wBFbVZFIsDODrh3T3Z7/88/X7xnaiqbAaugoOF9V1YKJxgoLQXOn5e1Z7l7V/bL69YtoFevtvs8hLQGP9MVBVfSsYWHhws62ZHWoYS1hahJAGkzHNd4zz07O8DGRnaxlZfLbu0DgIeHLNF9/hzQ15ctLypS7pi1Bw2X//p68EBW+2phASgxviAh7aKTjRJACGkblLC2ECWsRKXkF1rdPpIcB7z00ovXJiay2/7N3be8phWQ1eZaW8va0eroyEYZYIwudqIalLASQhSghLWF5JVS1IaVqFRTg3r8PYxWs+jqvmjXKldcDPz1F+DmJpuk4PlzoH9/SlpJ+5PPdNWJhrUihLQedbpqIaphJWqhzCh0iiYcsLQE6owfyHvyRPHYbFVVwOXLL36d1W4LS0h7oRpWQogCak9YN27cyI855u3tjaysrAbXPXHiBDiOq/e4evUqv05GRobCdSrb+I8tJaxELeQ1qI3NRe3kJLulX7sdqouL7Pa+gYHibdQwRS4hClHCSghRQK33XHbu3InY2Fhs3LgR/v7++J//+R8EBgbi8uXLgtlA6rp27Zpg4Fz5XMFyZmZm9abQbOsBp6lJAFELOztZstq1a8PrGBnJRgH444/Gx3B1dZWtoyyaY4SognyUABrWihBSi1prWNevX49Zs2YhMjISPXv2RHJyMhwcHJCSktLodtbW1pBIJPxDt041J8dxgvclEkmbl51qWNvH8OHDERsbq/T6N2/eBMdxyM3NbbcyaRQDA1ntqTK/lF56CTA0lHWeUqTOXOFNqjWHNiHthmpYG0UxkmgrtSWsVVVVOHv2LEaPHi1YPnr0aJw8ebLRbfv37w9bW1uMHDkSx48fr/d+eXk5nJycYG9vj6CgIJw7d67R/T179gylpaX8o6ysrMnya3vCqqjZRe1HS8ee27NnDz766COl13dwcEBhYSF69+7douO1xOjRo6Grq4tTp06p7JgtYmgI9OkDNPSDTdG4r0DDzQYeP5bV2FJNK2lPnSRhpRjZAWIk6VDUlrA+fPgQNTU19aYqs7GxQVEDY0na2triyy+/RGZmJvbs2QN3d3eMHDkSP/30E7+Oh4cHMjIysG/fPmzfvh1GRkbw9/fH9Uba6K1evRrm5ub8w9PTs8nya/tMV4WFhfwjOTkZZmZmgmX/+te/BOtX1+2F3oCuXbtCrKjTUAN0dXUhkUigp6L/iNu3byMnJwfR0dFITU1VyTEbo+x5VUhRwurkJJvIQJEHD4BLl6imlbQvPmHt2MGVYmQniJFEo6i90xVXZ8B0xli9ZXLu7u6YPXs2BgwYgEGDBmHjxo0YN24c1q1bx68zcOBAhIaGwsvLC0OGDMGuXbvQo0cPfP755w2WIT4+HiUlJfzj8uXLTZZb22e6qt3cwtzcXNAMo7KyEl26dMGuXbswfPhwGBkZYevWrXj06BGmTZsGe3t7iEQi9OnTB9u3bxfst+7tLmdnZ3z88ceIiIiAWCyGo6MjvvzyS/79ure75B3zjh07Bh8fH4hEIgwePLhem+aVK1fC2toaYrEYkZGRWLx4Mfr169fk505PT0dQUBDmzp2LnTt34smTJ4L3//rrL8yZMwc2NjYwMjJC79698d133/HvZ2dnY9iwYRCJRLCwsEBAQAAeP37Mf9bk5GTB/vr164fly5fzrzmOw6ZNmzBhwgSYmJhg5cqVqKmpwaxZs+Di4gJjY2O4u7vX+2MIAGlpaej1xhswHDwYtmPGIPrv8xzx4YcIeucd2UqmpoCeHp4/fw5JQADS9u0T7kRPT3sveqIa0s4x0xXFyA4aI3v1gqGhIWxtbREdHQ0AiIiIQFBQkGDd58+fQyKRIC0trclzQtqG2hJWKysr6Orq1qtNLS4urlfr2piBAwc2Wnuqo6MDX1/fRtcxNDSEmZkZ/1Dm12t7NglgTDbSkDoebXm3d9GiRXj77bdx5coVBAQEoLKyEt7e3vjuu+9w8eJFzJkzB9OnT8d///vfRveTmJgIHx8fnDt3DvPmzcPcuXMFI0MosmTJEiQmJuKXX36Bnp4eIiIi+Pe2bduGVatWYe3atTh79iwcHR2bbDcNyH5MpaenIzQ0FB4eHujRowd27drFvy+VShEYGIiTJ09i69atuHz5MtasWcO3sc7NzcXIkSPRq1cv5OTk4Oeff8b48eNRI7+YlLRs2TJMmDABeXl5iIiIgFQqhb29PXbt2oXLly9j6dKleP/99wVlS0lJwfz58zFnyhTkbd+OfYmJcHNzAwBETpiAgzk5KHz4UDZjFsfhwB9/oPzpU0wdNUp48IaaCxDSVpRpEsAY8PyJeh5tGCQpRmpgjJwzB3l5edi3b9+LGBkZiYMHD6KwsJBf/8CBAygvL8fUqVObVTbSCkyN/Pz82Ny5cwXLevbsyRYvXqz0PiZPnsxGjBjR4PtSqZT5+PiwmTNnKr3PgoICBoAVFBQ0uM5//sMYwNjAgUrvVqGnT5+yy5cvs6dPn/LLystl+1bHo7y8+Z8hPT2dmZub86/z8/MZAJacnNzktmPHjmXvvfce/3rYsGFswYIF/GsnJycWGhrKv5ZKpcza2pqlpKQIjnXu3DnGGGPHjx9nANjRo0f5bfbv388A8Of45ZdfZvPnzxeUw9/fn3l5eTVa1sOHD7Nu3bqx6upqxhhjSUlJzN/fn3//0KFDTEdHh127dk3h9tOmTROsX5eTkxNLSkoSLPPy8mLLli3jXwNgsbGxjZaTMcbmzZvHJk+ezL+2s7NjS5YsYez6dcbOnJE9nj/nn3u6uLC1MTGMSaWMMcYmTpzIwidMeLGu/HH9usJrVk6Z7w7RXiUlJQwAKykpaXiln6Ywtg2MXdvAGFMcI1l1uWwddTyqmx8kKUbKdIgY2QBPT0+2du1a/vXEiRNZeHh4k8dpjsZiq1LfnU5OrU0C3n33Xfzv//4v0tLScOXKFbzzzju4ffs2oqKiAMhu1c+YMYNfPzk5GXv37sX169dx6dIlxMfHIzMzk6+2B4AVK1bg0KFD+OOPP5Cbm4tZs2YhNzeX32db0fYmAcrw8fERvK6pqcGqVavQt29fWFpawtTUFIcPH8bt2vPaK9C3b1/+ufy2WnFxsdLb2NraAgC/zbVr1+Dn5ydYv+5rRVJTUxEcHMy3BZs2bRr++9//8rfScnNzYW9vjx49eijcXl570Fp1zysAbNq0CT4+PujWrRtMTU2xefNm/rwWFxfj3r17smM7OQHdusnaqerqAs7OgLMzIsPCkH7wIMBxKC4uxv79+xExaVL9g9MFT9ob6xxNApRBMVJII2JkAyIjI5Gens6vv3//fkGtNGl/am3VHhwcjEePHuHDDz/kezEeOHAATk5OAGSN1mt/UauqqrBw4ULcvXsXxsbG6NWrF/bv34+xY8fy68jbxxQVFcHc3Bz9+/fHTz/9pNSXrTnas0mASASUl7f9fpU9dlsxMTERvE5MTERSUhKSk5PRp08fmJiYIDY2FlVVVY3uR19fX/Ca4zhIm+j4U3sbeZvo2tsoajvdmD///BN79+5FdXW14NZYTU0N0tLSsHbtWhjXHqhfgabe19HRqVcORR0G6p7XXbt24Z133kFiYiIGDRoEsViMTz/9lL+NKDiuvr4saZWzsgIAzIiOxuKPP0ZOTg5ycnLg7OyMIW+8AdRp10YJK2l3yjQJ0BUBU9UUJHXbLkhSjBTSiBjZgBkzZmDx4sXCGDlkSJPbkbaj9m6Y8+bNw7x58xS+l5GRIXgdFxeHuLi4RveXlJSEpKSktipeg9ozYeU4oM73rVPIysrChAkTEBoaCkAWHK9fv46eDfVKbyfu7u44ffo0pk+fzi/75ZdfGt1m27ZtsLe3x969ewXLjx07htWrV/O1Infu3MFvv/2msAahb9++OHbsGFasWKHwGN26dRO0kSotLUV+fn6TnycrKwuDBw8WfI9u3LjBPxeLxXB2dsaxY8cwYsQIhfuwtLTExIkTkZ6ejpycHMycOVM2xau8pkJ+fihhJe1NmYSV4wC9zhckKUZ2sBhJVErtCWtHRTNdNZ+bmxsyMzNx8uRJWFhYYP369SgqKlJ5MI6JicHs2bPh4+ODwYMHY+fOnbhw4QJcXV0b3CY1NRVTpkypN5ahk5MTFi1ahP3792PChAkYOnQoJk+ejPXr18PNzQ1Xr14Fx3EYM2YM4uPj0adPH8ybNw9RUVEwMDDA8ePH8eabb8LKygqvvvoqMjIyMH78eFhYWCAhIaHepBiKuLm54d///jcOHToEFxcXfPXVVzhz5gxcXFz4dZYvX46oqChYW1sjMDAQZWVlyM7ORkxMDL9OZGQkgoKCUFNTg7CwMOFBxGKgrAywtFTyLBPSQvwoAdoXXClGduAYSdqd2oe16qjGjpVVOjUyWhapIyEhAQMGDEBAQACGDx8OiUSCiRMnqrwcISEhiI+Px8KFCzFgwADk5+cjPDy8wel7z549i/Pnz2Py5Mn13hOLxRg9ejQ/3mBmZiZ8fX0xbdo0eHp6Ii4uju/h2qNHDxw+fBjnz5+Hn58fBg0ahP/85z98e6/4+HgMHToUQUFBGDt2LCZOnIh//OMfTX6eqKgoTJo0CcHBwXj55Zfx6NGjenctwsLCkJycjI0bN6JXr14ICgqqN3LGqFGjYGtri4CAANjZ2QkP0r074OUlm/aVkPbk+wUQcAawG9v0up0MxcgOHCNJu+NYUw1TtNCdO3fg4OCAgoIC2Dc0rWUbqaysRH5+PlxcXBoMBqT9vfbaa5BIJPjqq6/UXRS1qaiogJ2dHdLS0jBJUYervzV2zaryu0M6ntLSUpibm6OkpARmZmZKbUMxUjNQjFQ+RrZUY9d6S747nY323XMhWq+iogKbNm1CQEAAdHV1sX37dhw9ehRHjhxRd9HUQiqVoqioCImJiTA3N8frr7+u7iIRQtSIYqQQxUjNQAkr0Tocx+HAgQNYuXIlnj17Bnd3d2RmZmJU3UHytcTt27fh4uICe3t7ZGRkqGwKR0KIZqIYKUQxUjPQWSdax9jYGEePHlV3MTSGs7Nzk0PWEEK0B8VIIYqRmoE6XRFCCCGEEI1GCSshhBBCCNFolLBqCLrdQDoKulaJOtB1Rzo7usYbRwmrmsmnx6uoqFBzSQhRjnyaSGUG7CaktShGEm0hv8brTrVLZKjTlZrp6uqiS5cuKC4uBgCIRKJ6czgToimkUikePHgAkUhEPWWJSlCMJJ0dYwwVFRUoLi5Gly5dqDKgAfQXRwNIJBIA4AMyIZpMR0cHjo6OlDQQlaEYSbRBly5d+Gud1EcJqwbgOA62trawtrZGdXW1uotDSKMMDAygo0OtiYjqUIwknZ2+vj7VrDaBElYNoqurSxcsIYQ0gGIkIdqLqkkIIYQQQohGo4SVEEIIIYRoNEpYCSGEEEKIRqM2rApIpVIAQGFhoZpLQkjHIv/OyL9DhNQmHxi9tLRUzSUhpGORf2e0eXIBSlgVuH//PgDAz89PzSUhpGO6f/8+HB0d1V0MomHKysoAAA4ODmouCSEdU1lZGczNzdVdDLXgmDan6w14/vw5zp07Bxsbm0aH7ykrK4OnpycuX74MsViswhJ2PHSumqejni+pVIr79++jf//+NLEAqUcqleLevXsQi8UNjuNbWloKBwcHFBQUwMzMTMUl7HjofCmvI58rxhjKyspgZ2entcMKUsLaCqWlpTA3N0dJSUmHu/hVjc5V89D5ItqKrv3mofOlPDpXHZt2pumEEEIIIaTDoISVEEIIIYRoNEpYW8HQ0BDLli2DoaGhuoui8ehcNQ+dL6Kt6NpvHjpfyqNz1bFRG1ZCCCGEEKLRqIaVEEIIIYRoNEpYCSGEEEKIRqOElRBCCCGEaDRKWFto48aNcHFxgZGREby9vZGVlaXuIqnc6tWr4evrC7FYDGtra0ycOBHXrl0TrMMYw/Lly2FnZwdjY2MMHz4cly5dEqzz7NkzxMTEwMrKCiYmJnj99ddx584dVX4UlVu9ejU4jkNsbCy/jM4VIRRbKa62DsXWToyRZtuxYwfT19dnmzdvZpcvX2YLFixgJiYm7NatW+oumkoFBASw9PR0dvHiRZabm8vGjRvHHB0dWXl5Ob/OmjVrmFgsZpmZmSwvL48FBwczW1tbVlpayq8TFRXFXnrpJXbkyBH266+/shEjRjAvLy/2/PlzdXysdnf69Gnm7OzM+vbtyxYsWMAvp3NFtB3FVoqrrUGxtXOjhLUF/Pz8WFRUlGCZh4cHW7x4sZpKpBmKi4sZAPbjjz8yxhiTSqVMIpGwNWvW8OtUVlYyc3NztmnTJsYYY3/99RfT19dnO3bs4Ne5e/cu09HRYQcPHlTtB1CBsrIy1r17d3bkyBE2bNgwPqjSuSKEYqsiFFeVQ7G186MmAc1UVVWFs2fPYvTo0YLlo0ePxsmTJ9VUKs1QUlICAOjatSsAID8/H0VFRYJzZWhoiGHDhvHn6uzZs6iurhasY2dnh969e3fK8zl//nyMGzcOo0aNEiync0W0HcVWxSiuKodia+enp+4CdDQPHz5ETU0NbGxsBMttbGxQVFSkplKpH2MM7777Ll555RX07t0bAPjzoehc3bp1i1/HwMAAFhYW9dbpbOdzx44d+PXXX3HmzJl679G5ItqOYmt9FFeVQ7FVO1DC2kIcxwleM8bqLdMm0dHRuHDhAn7++ed677XkXHW281lQUIAFCxbg8OHDMDIyanA9OldE21FsfYHiatMotmoPahLQTFZWVtDV1a33q6u4uLjeLzhtERMTg3379uH48eOwt7fnl0skEgBo9FxJJBJUVVXh8ePHDa7TGZw9exbFxcXw9vaGnp4e9PT08OOPP+Kzzz6Dnp4e/1npXBFtRbFViOKqcii2ag9KWJvJwMAA3t7eOHLkiGD5kSNHMHjwYDWVSj0YY4iOjsaePXvwww8/wMXFRfC+i4sLJBKJ4FxVVVXhxx9/5M+Vt7c39PX1BesUFhbi4sWLnep8jhw5Enl5ecjNzeUfPj4+CAkJQW5uLlxdXelcEa1GsVWG4mrzUGzVIuro6dXRyYdeSU1NZZcvX2axsbHMxMSE3bx5U91FU6m5c+cyc3NzduLECVZYWMg/Kioq+HXWrFnDzM3N2Z49e1heXh6bNm2awuFE7O3t2dGjR9mvv/7KXn31Va0YTqR2T1bG6FwRQrGV4mpboNjaOVHC2kJffPEFc3JyYgYGBmzAgAH8kCPaBIDCR3p6Or+OVCply5YtYxKJhBkaGrKhQ4eyvLw8wX6ePn3KoqOjWdeuXZmxsTELCgpit2/fVvGnUb26QZXOFSEUWymuth7F1s6JY4wx9dTtEkIIIYQQ0jRqw0oIIYQQQjQaJayEEEIIIUSjUcJKCCGEEEI0GiWshBBCCCFEo1HCSgghhBBCNBolrIQQQgghRKNRwkoIIYQQQjQaJayEEEIIIUSjUcJKtAbHcdi7d6+6i0EIIZ0KxVaiCpSwEpUIDw8Hx3H1HmPGjFF30QghpMOi2Eq0hZ66C0C0x5gxY5Ceni5YZmhoqKbSEEJI50CxlWgDqmElKmNoaAiJRCJ4WFhYAJDdUkpJSUFgYCCMjY3h4uKC3bt3C7bPy8vDq6++CmNjY1haWmLOnDkoLy8XrJOWloZevXrB0NAQtra2iI6OFrz/8OFDvPHGGxCJROjevTv27dvXvh+aEELaGcVWog0oYSUaIyEhAZMnT8b58+cRGhqKadOm4cqVKwCAiooKjBkzBhYWFjhz5gx2796No0ePCoJmSkoK5s+fjzlz5iAvLw/79u2Dm5ub4BgrVqzA1KlTceHCBYwdOxYhISH4888/Vfo5CSFElSi2kk6BEaICYWFhTFdXl5mYmAgeH374IWOMMQAsKipKsM3LL7/M5s6dyxhj7Msvv2QWFhasvLycf3///v1MR0eHFRUVMcYYs7OzY0uWLGmwDADYBx98wL8uLy9nHMex77//vs0+JyGEqBLFVqItqA0rUZkRI0YgJSVFsKxr167880GDBgneGzRoEHJzcwEAV65cgZeXF0xMTPj3/f39IZVKce3aNXAch3v37mHkyJGNlqFv3778cxMTE4jFYhQXF7f0IxFCiNpRbCXagBJWojImJib1biM1heM4AABjjH+uaB1jY2Ol9qevr19vW6lU2qwyEUKIJqHYSrQBtWElGuPUqVP1Xnt4eAAAPD09kZubiydPnvDvZ2dnQ0dHBz169IBYLIazszOOHTum0jITQoimo9hKOgOqYSUq8+zZMxQVFQmW6enpwcrKCgCwe/du+Pj44JVXXsG2bdtw+vRppKamAgBCQkKwbNkyhIWFYfny5Xjw4AFiYmIwffp02NjYAACWL1+OqKgoWFtbIzAwEGVlZcjOzkZMTIxqPyghhKgQxVaiDShhJSpz8OBB2NraCpa5u7vj6tWrAGS9THfs2IF58+ZBIpFg27Zt8PT0BACIRCIcOnQICxYsgK+vL0QiESZPnoz169fz+woLC0NlZSWSkpKwcOFCWFlZYcqUKar7gIQQogYUW4k24BhjTN2FIITjOHzzzTeYOHGiuotCCCGdBsVW0llQG1ZCCCGEEKLRKGElhBBCCCEajZoEEEIIIYQQjUY1rIQQQgghRKNRwkoIIYQQQjQaJayEEEIIIUSjUcJKCCGEEEI0GiWshBBCCCFEo1HCSgghhBBCNBolrIQQQgghRKNRwkoIIYQQQjQaJayEEEIIIUSj/T/Ca5v9ZvrjqAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 2ms/step\n",
      "F1 score: 0.6641123882503193 \n",
      "F2 score: 0.6641123882503193\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "y_pred = model.predict(X_test_tensor)\n",
    "\n",
    "\n",
    "f1score = f1_score(y_test_tensor, y_pred.round(), average='micro')\n",
    "f2score = fbeta_score(y_test_tensor, y_pred.round(), beta=2, average='micro')\n",
    "\n",
    "\n",
    "print(f'F1 score: {f1score} \\nF2 score: {f2score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x2393946db88>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test_tensor, y_pred.round())\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2393953e788>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score\n",
    "\n",
    "roc_score = roc_auc_score(y_test_tensor, y_pred)\n",
    "RocCurveDisplay.from_predictions(y_test_tensor, y_pred)\n",
    "plt.plot([0, 1], [0, 1], 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x23939523288>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test_tensor, y_pred)\n",
    "\n",
    "threshold = 0.4\n",
    "\n",
    "idx = (thresholds >= threshold).argmax()\n",
    "\n",
    "plt.plot(thresholds, precision[:-1], label='precision')\n",
    "plt.plot(thresholds, recall[:-1], label='recall')\n",
    "plt.vlines(threshold, 0, 1.0, colors='r', linestyles='--', label='threshold')\n",
    "\n",
    "plt.plot(thresholds[idx], precision[idx], 'bo')\n",
    "plt.plot(thresholds[idx], recall[idx], 'o', c='orange')\n",
    "\n",
    "plt.axis([0.2, 0.9, 0, 1])\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 1ms/step\n",
      "F2 score: 0.5266955266955267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x23939770688>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Since we want to minimise false negatives, we should favour recall over precision\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "y_pred = model.predict(X_test_tensor)\n",
    "\n",
    "y_pred = np.where(y_pred >= threshold, 1, 0)\n",
    "\n",
    "score = fbeta_score(y_test_tensor, y_pred, beta=2)\n",
    "print(f'F2 score: {score}')\n",
    "\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test_tensor, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 4ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#Determine the ideal threshold for f2\n",
    "\n",
    "threshold_dict = {}\n",
    "best_score = float('-inf')\n",
    "best_threshold = 0\n",
    "\n",
    "for i in range(0,100, 1):\n",
    "    \n",
    "    threshold = i/100\n",
    "\n",
    "    y_pred = model.predict(X_test_tensor)\n",
    "\n",
    "    y_pred = np.where(y_pred >= threshold, 1, 0)\n",
    "\n",
    "    score = fbeta_score(y_test_tensor, y_pred, beta=2)\n",
    "\n",
    "    threshold_dict[threshold] = score\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_threshold = threshold\n",
    "\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_dict.items(), columns=['threshold', 'f2score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.04 \n",
      "Best F2 score: 0.7328042328042328\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(threshold_df['threshold'], threshold_df['f2score'], label='f2score')\n",
    "\n",
    "plt.vlines(best_threshold, 0, 0.8, colors='r', linestyles='--', label='threshold')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F2 score')\n",
    "plt.legend()\n",
    "print(f'Best threshold: {best_threshold} \\nBest F2 score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 1ms/step\n",
      "F1 score: 0.5231350330500472 \n",
      "F2 score: 0.7328042328042328 \n",
      " Accuracy: 0.3550446998722861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x23939829308>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This looks like a really high f2 but I can already presume that the confusion matrix will look horrible (super high false positives)\n",
    "#Lets take a look\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "y_pred = np.where(model.predict(X_test_tensor) >= best_threshold, 1, 0)\n",
    "\n",
    "f1 = f1_score(y_test_tensor, y_pred)\n",
    "f2 = fbeta_score(y_test_tensor, y_pred, beta=2)\n",
    "accuracy = accuracy_score(y_test_tensor, y_pred)\n",
    "\n",
    "print(f'F1 score: {f1} \\nF2 score: {f2} \\n Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test_tensor, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 4ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 4ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 658us/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 915us/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 924us/step\n",
      "25/25 [==============================] - 0s 990us/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 808us/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "25/25 [==============================] - 0s 959us/step\n",
      "25/25 [==============================] - 0s 3ms/step\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "25/25 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.522642</td>\n",
       "      <td>0.732417</td>\n",
       "      <td>0.353768</td>\n",
       "      <td>0.627529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.02</th>\n",
       "      <td>0.522642</td>\n",
       "      <td>0.732417</td>\n",
       "      <td>0.353768</td>\n",
       "      <td>0.627529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.03</th>\n",
       "      <td>0.522642</td>\n",
       "      <td>0.732417</td>\n",
       "      <td>0.353768</td>\n",
       "      <td>0.627529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.04</th>\n",
       "      <td>0.523135</td>\n",
       "      <td>0.732804</td>\n",
       "      <td>0.355045</td>\n",
       "      <td>0.627970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>0.523135</td>\n",
       "      <td>0.732804</td>\n",
       "      <td>0.355045</td>\n",
       "      <td>0.627970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>0.014337</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.648787</td>\n",
       "      <td>0.011673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.96</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.646232</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.646232</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.98</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.646232</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.646232</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1        f2  accuracy  combined\n",
       "0.01  0.522642  0.732417  0.353768  0.627529\n",
       "0.02  0.522642  0.732417  0.353768  0.627529\n",
       "0.03  0.522642  0.732417  0.353768  0.627529\n",
       "0.04  0.523135  0.732804  0.355045  0.627970\n",
       "0.05  0.523135  0.732804  0.355045  0.627970\n",
       "...        ...       ...       ...       ...\n",
       "0.95  0.014337  0.009009  0.648787  0.011673\n",
       "0.96  0.000000  0.000000  0.646232  0.000000\n",
       "0.97  0.000000  0.000000  0.646232  0.000000\n",
       "0.98  0.000000  0.000000  0.646232  0.000000\n",
       "0.99  0.000000  0.000000  0.646232  0.000000\n",
       "\n",
       "[99 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets try the same, taking into account both f1 and f2\n",
    "\n",
    "best_threshold = {}\n",
    "best_f1_score = 0\n",
    "best_f2_score = 0\n",
    "best_combined_score = 0\n",
    "best_accuracy_score = 0\n",
    "\n",
    "threshold_scores = {}\n",
    "\n",
    "\n",
    "for i in range(1,100, 1):\n",
    "    \n",
    "    threshold = i/100\n",
    "\n",
    "    y_pred = np.where(model.predict(X_test_tensor) >= threshold, 1, 0)\n",
    "\n",
    "    f1 = f1_score(y_test_tensor, y_pred)\n",
    "    f2 = fbeta_score(y_test_tensor, y_pred, beta=2)\n",
    "    accuracy = accuracy_score(y_test_tensor, y_pred)\n",
    "\n",
    "    combined_score = (f1 + f2) / 2\n",
    "\n",
    "    threshold_scores[threshold] = (f1, f2, accuracy, combined_score)\n",
    "\n",
    "    if combined_score > best_combined_score:\n",
    "        best_combined_score = combined_score\n",
    "        best_threshold['combined'] = threshold\n",
    "\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_threshold['f1'] = threshold\n",
    "\n",
    "    if f2 > best_f2_score:\n",
    "        best_f2_score = f2\n",
    "        best_threshold['f2'] = threshold\n",
    "\n",
    "    if accuracy > best_accuracy_score:\n",
    "        best_accuracy_score = accuracy\n",
    "        best_threshold['accuracy'] = threshold\n",
    "\n",
    "threshold_scores_df = pd.DataFrame.from_dict(threshold_scores, orient='index', columns=['f1', 'f2', 'accuracy', 'combined'])\n",
    "\n",
    "threshold_scores_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Score')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold_scores_df.plot()\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.5518018018018018 at threshold: 0.22\n",
      "      Best F2 score: 0.7328042328042328 at threshold: 0.04\n",
      "      Best Accuracy score: 0.6998722860791826 at threshold: 0.69\n",
      "      Best Combined score: 0.6322127100922912 at threshold: 0.22\n"
     ]
    }
   ],
   "source": [
    "print(f'''Best F1 score: {best_f1_score} at threshold: {best_threshold[\"f1\"]}\n",
    "      Best F2 score: {best_f2_score} at threshold: {best_threshold[\"f2\"]}\n",
    "      Best Accuracy score: {best_accuracy_score} at threshold: {best_threshold[\"accuracy\"]}\n",
    "      Best Combined score: {best_combined_score} at threshold: {best_threshold[\"combined\"]}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.22, 0.22, 0.04, 0.69]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2925"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_list = list(best_threshold.values())\n",
    "print(threshold_list)\n",
    "\n",
    "combined_threshold = np.average(threshold_list)\n",
    "combined_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x23939a3c188>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = np.where(model.predict(X_test_tensor) >= combined_threshold, 1, 0)\n",
    "\n",
    "cm = confusion_matrix(y_test_tensor, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_for_chemists_tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
