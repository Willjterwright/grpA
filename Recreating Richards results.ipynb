{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import tensorflow as tf\n",
    "from rdkit.Chem import Descriptors as ds\n",
    "from rdkit.Chem import Fragments as fr\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is not important, it just gets the number of threads on your CPU and returns that number.\n",
    "import threading\n",
    "\n",
    "total_threads =threading.active_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TensorFlow to use multiple CPU cores - This will speed up the training (hopefully)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(total_threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(total_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import the dataset - the featurizer and splitter dont matter as we will be combining the datasets and\n",
    "#designing our own featurizer\n",
    "tasks, datasets, transformers = dc.molnet.load_tox21(\n",
    "    featurizer='GraphConv', \n",
    "    splitter='random')\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11',\n",
       "       'y12', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6', 'w7', 'w8', 'w9', 'w10',\n",
       "       'w11', 'w12', 'ids'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets convert them to dataframes because deepchem is a piece of shit and it cant handle numpy arrays <-codeium generated this sentence\n",
    "\n",
    "train_df = train_dataset.to_dataframe()\n",
    "test_dataset = test_dataset.to_dataframe()\n",
    "valid_dataset = valid_dataset.to_dataframe()\n",
    "\n",
    "#This line concatenates all the dataframes meaning all the data is in one dataframe, we will resplit them later with SKLEARN\n",
    "dataset = pd.concat([train_df, test_dataset, valid_dataset])\n",
    "\n",
    "\n",
    "#print the columns\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>...</th>\n",
       "      <th>w4</th>\n",
       "      <th>w5</th>\n",
       "      <th>w6</th>\n",
       "      <th>w7</th>\n",
       "      <th>w8</th>\n",
       "      <th>w9</th>\n",
       "      <th>w10</th>\n",
       "      <th>w11</th>\n",
       "      <th>w12</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.186419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>1.186419</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>1.186419</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>1.182927</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.182927</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>6.364256</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>6.466667</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.364256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.182927</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>6.364256</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>1.182927</td>\n",
       "      <td>16.767081</td>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>1.186419</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.767081</td>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>6.364256</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>1.182927</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     X   y1   y2   y3   y4  \\\n",
       "0    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "1    <deepchem.feat.mol_graphs.ConvMol object at 0x...  1.0  0.0  0.0  0.0   \n",
       "2    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "3    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "4    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  1.0  0.0   \n",
       "..                                                 ...  ...  ...  ...  ...   \n",
       "778  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "779  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "780  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  1.0  0.0  0.0   \n",
       "781  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "782  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      y5   y6   y7   y8   y9  ...        w4        w5        w6        w7  \\\n",
       "0    0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1    0.0  0.0  0.0  0.0  0.0  ...  0.000000  1.145708  1.050492  1.029118   \n",
       "2    0.0  0.0  0.0  0.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "3    0.0  0.0  0.0  0.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "4    0.0  0.0  0.0  1.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "..   ...  ...  ...  ...  ...  ...       ...       ...       ...       ...   \n",
       "778  0.0  0.0  0.0  1.0  0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "779  0.0  0.0  0.0  0.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "780  0.0  0.0  0.0  1.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "781  0.0  0.0  0.0  0.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "782  0.0  0.0  0.0  1.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "\n",
       "           w8        w9       w10       w11        w12  \\\n",
       "0    1.186419  0.000000  1.058113  0.000000   0.000000   \n",
       "1    1.186419  1.038037  1.058113  0.000000   1.063423   \n",
       "2    1.186419  1.038037  1.058113  1.182927   1.063423   \n",
       "3    0.000000  1.038037  0.000000  1.182927   1.063423   \n",
       "4    6.364256  1.038037  1.058113  6.466667   1.063423   \n",
       "..        ...       ...       ...       ...        ...   \n",
       "778  6.364256  0.000000  0.000000  0.000000   0.000000   \n",
       "779  0.000000  1.038037  0.000000  1.182927   1.063423   \n",
       "780  6.364256  1.038037  1.058113  1.182927  16.767081   \n",
       "781  1.186419  1.038037  1.058113  0.000000  16.767081   \n",
       "782  6.364256  1.038037  1.058113  1.182927   1.063423   \n",
       "\n",
       "                                                   ids  \n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  \n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  \n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  \n",
       "3                                   O=c1oc2cc(O)ccc2s1  \n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  \n",
       "..                                                 ...  \n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  \n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  \n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  \n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  \n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  \n",
       "\n",
       "[7831 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>y10</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids   y1   y2   y3   y4  \\\n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  0.0  0.0  0.0  0.0   \n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  1.0  0.0  0.0  0.0   \n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  0.0  0.0  0.0  0.0   \n",
       "3                                   O=c1oc2cc(O)ccc2s1  0.0  0.0  0.0  0.0   \n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  0.0  0.0  1.0  0.0   \n",
       "..                                                 ...  ...  ...  ...  ...   \n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  0.0  0.0  0.0  0.0   \n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  0.0  0.0  0.0  0.0   \n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  0.0  1.0  0.0  0.0   \n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  0.0  0.0  0.0  0.0   \n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      y5   y6   y7   y8   y9  y10  y11  y12  tox  \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  3.0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "778  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  \n",
       "779  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "780  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  3.0  \n",
       "781  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  \n",
       "782  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  \n",
       "\n",
       "[7831 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets first select only the useful information, ie the y values (toxicity)\n",
    "#As well as the ids - ie the smiles strings\n",
    "data = dataset[['ids', 'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12']]\n",
    "\n",
    "# Lets add up all the y values into one 'toxicity measure\n",
    "col_to_sum = ['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12']\n",
    "\n",
    "#This adds a new column called 'tox' which is the sum of each row \n",
    "data['tox'] = data[col_to_sum].sum(axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>y10</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids   y1   y2   y3   y4  \\\n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  0.0  0.0  0.0  0.0   \n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  1.0  0.0  0.0  0.0   \n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  0.0  0.0  0.0  0.0   \n",
       "3                                   O=c1oc2cc(O)ccc2s1  0.0  0.0  0.0  0.0   \n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  0.0  0.0  1.0  0.0   \n",
       "..                                                 ...  ...  ...  ...  ...   \n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  0.0  0.0  0.0  0.0   \n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  0.0  0.0  0.0  0.0   \n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  0.0  1.0  0.0  0.0   \n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  0.0  0.0  0.0  0.0   \n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      y5   y6   y7   y8   y9  y10  y11  y12  tox  tox_bin  \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0        1  \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "4    0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  3.0        1  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...      ...  \n",
       "778  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0        1  \n",
       "779  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "780  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  3.0        1  \n",
       "781  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0        1  \n",
       "782  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0        1  \n",
       "\n",
       "[7831 rows x 15 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets add another row with a binary (1 or 0) that tells us if the compound is toxic against one or more of the assays or not\n",
    "data['tox_bin'] = data['tox'].apply(lambda x: 1 if x>0 else 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of the y columns - we dont need them anymore\n",
    "data.drop(columns=['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids  tox  tox_bin\n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  0.0        0\n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  1.0        1\n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  0.0        0\n",
       "3                                   O=c1oc2cc(O)ccc2s1  0.0        0\n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  3.0        1\n",
       "..                                                 ...  ...      ...\n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  1.0        1\n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  0.0        0\n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  3.0        1\n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  1.0        1\n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  1.0        1\n",
       "\n",
       "[7831 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurisation of the dataset\n",
    "\n",
    "Lets get all the features we think we need from the dataset, I have copied richard here but feel free to have a play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fr_Al_COO',\n",
       " 'fr_Al_OH',\n",
       " 'fr_Al_OH_noTert',\n",
       " 'fr_ArN',\n",
       " 'fr_Ar_COO',\n",
       " 'fr_Ar_N',\n",
       " 'fr_Ar_NH',\n",
       " 'fr_Ar_OH',\n",
       " 'fr_COO',\n",
       " 'fr_COO2',\n",
       " 'fr_C_O',\n",
       " 'fr_C_O_noCOO',\n",
       " 'fr_C_S',\n",
       " 'fr_HOCCN',\n",
       " 'fr_Imine',\n",
       " 'fr_NH0',\n",
       " 'fr_NH1',\n",
       " 'fr_NH2',\n",
       " 'fr_N_O',\n",
       " 'fr_Ndealkylation1',\n",
       " 'fr_Ndealkylation2',\n",
       " 'fr_Nhpyrrole',\n",
       " 'fr_SH',\n",
       " 'fr_aldehyde',\n",
       " 'fr_alkyl_carbamate',\n",
       " 'fr_alkyl_halide',\n",
       " 'fr_allylic_oxid',\n",
       " 'fr_amide',\n",
       " 'fr_amidine',\n",
       " 'fr_aniline',\n",
       " 'fr_aryl_methyl',\n",
       " 'fr_azide',\n",
       " 'fr_azo',\n",
       " 'fr_barbitur',\n",
       " 'fr_benzene',\n",
       " 'fr_benzodiazepine',\n",
       " 'fr_bicyclic',\n",
       " 'fr_diazo',\n",
       " 'fr_dihydropyridine',\n",
       " 'fr_epoxide',\n",
       " 'fr_ester',\n",
       " 'fr_ether',\n",
       " 'fr_furan',\n",
       " 'fr_guanido',\n",
       " 'fr_halogen',\n",
       " 'fr_hdrzine',\n",
       " 'fr_hdrzone',\n",
       " 'fr_imidazole',\n",
       " 'fr_imide',\n",
       " 'fr_isocyan',\n",
       " 'fr_isothiocyan',\n",
       " 'fr_ketone',\n",
       " 'fr_ketone_Topliss',\n",
       " 'fr_lactam',\n",
       " 'fr_lactone',\n",
       " 'fr_methoxy',\n",
       " 'fr_morpholine',\n",
       " 'fr_nitrile',\n",
       " 'fr_nitro',\n",
       " 'fr_nitro_arom',\n",
       " 'fr_nitro_arom_nonortho',\n",
       " 'fr_nitroso',\n",
       " 'fr_oxazole',\n",
       " 'fr_oxime',\n",
       " 'fr_para_hydroxylation',\n",
       " 'fr_phenol',\n",
       " 'fr_phenol_noOrthoHbond',\n",
       " 'fr_phos_acid',\n",
       " 'fr_phos_ester',\n",
       " 'fr_piperdine',\n",
       " 'fr_piperzine',\n",
       " 'fr_priamide',\n",
       " 'fr_prisulfonamd',\n",
       " 'fr_pyridine',\n",
       " 'fr_quatN',\n",
       " 'fr_sulfide',\n",
       " 'fr_sulfonamd',\n",
       " 'fr_sulfone',\n",
       " 'fr_term_acetylene',\n",
       " 'fr_tetrazole',\n",
       " 'fr_thiazole',\n",
       " 'fr_thiocyan',\n",
       " 'fr_thiophene',\n",
       " 'fr_unbrch_alkane',\n",
       " 'fr_urea']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#Get all the possible functions within the fr(fragments) module and put them into a list\n",
    "#This is called a list comprehension it says 'for each element in this list, do this'\n",
    "#E.g for each name in the directory of the fr module, if the function is callable (i.e. it has a value) then append it to the list\n",
    "function_names = [name for name in dir(fr) if callable(getattr(fr, name))]\n",
    "\n",
    "#Remove the first 2 functions (they are not important for us)\n",
    "function_names.pop(0)\n",
    "function_names.pop(0)\n",
    "\n",
    "function_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:22:57] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_sulfide</th>\n",
       "      <th>fr_sulfonamd</th>\n",
       "      <th>fr_sulfone</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids  tox  tox_bin  \\\n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  0.0        0   \n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  1.0        1   \n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  0.0        0   \n",
       "3                                   O=c1oc2cc(O)ccc2s1  0.0        0   \n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  3.0        1   \n",
       "..                                                 ...  ...      ...   \n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  1.0        1   \n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  0.0        0   \n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  3.0        1   \n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  1.0        1   \n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  1.0        1   \n",
       "\n",
       "     fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  fr_ArN  fr_Ar_COO  fr_Ar_N  \\\n",
       "0          0.0       0.0              0.0     0.0        1.0      0.0   \n",
       "1          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "2          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "3          0.0       4.0              4.0     0.0        0.0      0.0   \n",
       "4          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "..         ...       ...              ...     ...        ...      ...   \n",
       "778        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "779        0.0       4.0              4.0     0.0        2.0      0.0   \n",
       "780        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "781        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "782        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "\n",
       "     fr_Ar_NH  ...  fr_sulfide  fr_sulfonamd  fr_sulfone  fr_term_acetylene  \\\n",
       "0         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "1         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "2         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "3         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "4         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "..        ...  ...         ...           ...         ...                ...   \n",
       "778       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "779       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "780       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "781       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "782       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "\n",
       "     fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  fr_unbrch_alkane  \\\n",
       "0             0.0          0.0          0.0           0.0               0.0   \n",
       "1             0.0          0.0          0.0           0.0               3.0   \n",
       "2             0.0          0.0          0.0           0.0               3.0   \n",
       "3             0.0          0.0          0.0           0.0               5.0   \n",
       "4             0.0          0.0          0.0           0.0               0.0   \n",
       "..            ...          ...          ...           ...               ...   \n",
       "778           0.0          0.0          0.0           0.0               0.0   \n",
       "779           0.0          0.0          0.0           0.0               0.0   \n",
       "780           0.0          0.0          0.0           0.0               0.0   \n",
       "781           0.0          0.0          0.0           0.0               0.0   \n",
       "782           0.0          0.0          0.0           0.0               0.0   \n",
       "\n",
       "     fr_urea  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "..       ...  \n",
       "778      0.0  \n",
       "779      0.0  \n",
       "780      0.0  \n",
       "781      0.0  \n",
       "782      1.0  \n",
       "\n",
       "[7831 rows x 88 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Iterate through the rows in the dataframe\n",
    "# For each row, create a molecule object from the smiles strings\n",
    "# For each function in the list of functions, get the value for that function (in this case the total number of each functional group)\n",
    "#Then add that value to the dataframe for that row with the column heading as the name of the function\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    mol = Chem.MolFromSmiles(row['ids']) # Create molecule object\n",
    "    for function in function_names: # For each function in functions\n",
    "        data.at[index, function] = getattr(fr, function)(mol) # Set the value in the dataframe at the molecule index and column name to the value of the function\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:24:30] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "#We have a list of functional groups, lets also get a binary value for if the molecule matches any of the tests below:\n",
    "#PAINS, BRENK, NIH\n",
    "\n",
    "from rdkit.Chem.FilterCatalog import FilterCatalog, FilterCatalogParams\n",
    "from rdkit.Chem import Descriptors as ds\n",
    "\n",
    "params_pains = FilterCatalogParams()\n",
    "params_pains.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS)\n",
    "pains = FilterCatalog(params_pains)\n",
    "\n",
    "params_brenk = FilterCatalogParams()\n",
    "params_brenk.AddCatalog(FilterCatalogParams.FilterCatalogs.BRENK)\n",
    "brenk = FilterCatalog(params_brenk)\n",
    "\n",
    "params_nih = FilterCatalogParams()\n",
    "params_nih.AddCatalog(FilterCatalogParams.FilterCatalogs.NIH)\n",
    "nih = FilterCatalog(params_nih)\n",
    "\n",
    "\n",
    "SMILES_strings = data['ids'] # Get smiles strings\n",
    "mol = [Chem.MolFromSmiles(formula) for formula in SMILES_strings] # Get a list of molecule objects\n",
    "\n",
    "for row in data.index: # For each row\n",
    "    molecule = mol[row] # Get the molecule\n",
    "    data.loc[row, 'pain'] = 1 if pains.HasMatch(molecule) else 0 # Does it match the PAINS filter?\n",
    "    data.loc[row, 'brenk'] = 1 if pains.HasMatch(molecule) else 0 # Does it match the BRENK filter?\n",
    "    data.loc[row, 'nih'] = 1 if pains.HasMatch(molecule) else 0 # Does it match the NIH filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "      <th>pain</th>\n",
       "      <th>brenk</th>\n",
       "      <th>nih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids  tox  tox_bin  \\\n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  0.0        0   \n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  1.0        1   \n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  0.0        0   \n",
       "3                                   O=c1oc2cc(O)ccc2s1  0.0        0   \n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  3.0        1   \n",
       "..                                                 ...  ...      ...   \n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  1.0        1   \n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  0.0        0   \n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  3.0        1   \n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  1.0        1   \n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  1.0        1   \n",
       "\n",
       "     fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  fr_ArN  fr_Ar_COO  fr_Ar_N  \\\n",
       "0          0.0       0.0              0.0     0.0        1.0      0.0   \n",
       "1          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "2          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "3          0.0       4.0              4.0     0.0        0.0      0.0   \n",
       "4          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "..         ...       ...              ...     ...        ...      ...   \n",
       "778        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "779        0.0       4.0              4.0     0.0        2.0      0.0   \n",
       "780        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "781        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "782        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "\n",
       "     fr_Ar_NH  ...  fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  \\\n",
       "0         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "1         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "2         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "3         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "4         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "..        ...  ...                ...           ...          ...          ...   \n",
       "778       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "779       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "780       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "781       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "782       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "\n",
       "     fr_thiophene  fr_unbrch_alkane  fr_urea  pain  brenk  nih  \n",
       "0             0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "1             0.0               3.0      0.0   0.0    0.0  0.0  \n",
       "2             0.0               3.0      0.0   0.0    0.0  0.0  \n",
       "3             0.0               5.0      0.0   0.0    0.0  0.0  \n",
       "4             0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "..            ...               ...      ...   ...    ...  ...  \n",
       "778           0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "779           0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "780           0.0               0.0      0.0   1.0    1.0  1.0  \n",
       "781           0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "782           0.0               0.0      1.0   0.0    0.0  0.0  \n",
       "\n",
       "[7831 rows x 91 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e5594515f8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets have a look at the distribution of the data:\n",
    "\n",
    "percent_toxic = len(data[data['tox_bin'] == 1]) / len(data) # Divides the length of the dataframe where tox_bin == 1 (ie toxic) by the length of the dataframe (all mols)\n",
    "percent_non_toxic = 1 - percent_toxic\n",
    "\n",
    "\n",
    "#Lets plot the train_df as a stacked bar graph:\n",
    "ax=plt.bar('Toxicity',percent_toxic, label='Toxic')\n",
    "plt.bar('Toxicity',percent_non_toxic, bottom= percent_toxic, label='Non-Toxic')\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.ylabel(\"Molecules %\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this dataset is unbalanced so we will need to deal with this before training- but first we need to split the data so we can balance ONLY the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>fr_Ar_OH</th>\n",
       "      <th>fr_COO</th>\n",
       "      <th>fr_COO2</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "      <th>pain</th>\n",
       "      <th>brenk</th>\n",
       "      <th>nih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3706</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4229</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2341</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4646</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6264 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  fr_ArN  fr_Ar_COO  fr_Ar_N  \\\n",
       "8           0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "2697        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "26          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "2144        0.0       0.0              0.0     0.0        0.0      1.0   \n",
       "4073        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "...         ...       ...              ...     ...        ...      ...   \n",
       "3706        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "409         0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "4229        0.0       0.0              0.0     0.0        0.0      1.0   \n",
       "2341        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "4646        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "\n",
       "      fr_Ar_NH  fr_Ar_OH  fr_COO  fr_COO2  ...  fr_term_acetylene  \\\n",
       "8          0.0       0.0     0.0      0.0  ...                0.0   \n",
       "2697       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "26         0.0       0.0     0.0      0.0  ...                0.0   \n",
       "2144       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "4073       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "...        ...       ...     ...      ...  ...                ...   \n",
       "3706       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "409        0.0       0.0     0.0      0.0  ...                0.0   \n",
       "4229       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "2341       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "4646       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "\n",
       "      fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  fr_unbrch_alkane  \\\n",
       "8              0.0          0.0          0.0           0.0               0.0   \n",
       "2697           0.0          0.0          0.0           0.0               0.0   \n",
       "26             0.0          0.0          0.0           0.0              13.0   \n",
       "2144           0.0          0.0          0.0           0.0               0.0   \n",
       "4073           0.0          0.0          0.0           0.0               0.0   \n",
       "...            ...          ...          ...           ...               ...   \n",
       "3706           0.0          0.0          0.0           0.0               0.0   \n",
       "409            0.0          0.0          0.0           0.0               0.0   \n",
       "4229           0.0          0.0          0.0           0.0               0.0   \n",
       "2341           0.0          0.0          0.0           0.0               0.0   \n",
       "4646           0.0          0.0          0.0           0.0               0.0   \n",
       "\n",
       "      fr_urea  pain  brenk  nih  \n",
       "8         0.0   0.0    0.0  0.0  \n",
       "2697      0.0   0.0    0.0  0.0  \n",
       "26        0.0   0.0    0.0  0.0  \n",
       "2144      0.0   0.0    0.0  0.0  \n",
       "4073      0.0   0.0    0.0  0.0  \n",
       "...       ...   ...    ...  ...  \n",
       "3706      0.0   0.0    0.0  0.0  \n",
       "409       0.0   0.0    0.0  0.0  \n",
       "4229      1.0   0.0    0.0  0.0  \n",
       "2341      0.0   0.0    0.0  0.0  \n",
       "4646      0.0   0.0    0.0  0.0  \n",
       "\n",
       "[6264 rows x 88 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data['tox_bin'], test_size=0.2) # Split the entire dataset into train and test sets\n",
    "\n",
    "X_train = X_train.drop(['ids', 'tox', 'tox_bin'], axis=1) # Drop the targets (toxicity indicator) from the X values of the training set\n",
    "X_test = X_test.drop(['ids', 'tox', 'tox_bin'], axis=1) # Drop the targets (toxicity indicator) from the X values of the test set\n",
    "\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set:  (6264, 88)\n",
      "Shape of test set:  (1567, 88)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of train set: ', X_train.shape)\n",
    "print('Shape of test set: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e5594fdb70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets resample our data - there are a number of ways of doing this but the simplest is to jsut upsample the minority class\n",
    "#Ie duplicate the samples\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#First concatenate the training X and y into one dataframe\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "#Select the data in the majority and minority class\n",
    "df_majority = train_df[train_df['tox_bin']==0]\n",
    "df_minority = train_df[train_df['tox_bin']==1]\n",
    "\n",
    "#Then resample the minority class - this will duplicate randomly the training data in the minority class to match the number of samples in the majority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "#Then combine the majority and upsampled minority class\n",
    "train_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#Split them back into X and y values\n",
    "y_train = train_df['tox_bin']\n",
    "X_train = train_df.drop(['tox_bin'], axis=1)\n",
    "\n",
    "#Then lets do what we did above to check it has worked!\n",
    "percent_toxic = len(y_train[y_train == 1]) / len(y_train)\n",
    "percent_non_toxic = 1 - percent_toxic\n",
    "\n",
    "\n",
    "#Lets plot the train_df as a stacked bar graph:\n",
    "ax=plt.bar('Toxicity',percent_toxic, label='Toxic')\n",
    "plt.bar('Toxicity',percent_non_toxic, bottom= percent_toxic, label='Non-Toxic')\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.ylabel(\"Molecules %\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, this looks perfectly balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test set:  (783, 88)\n",
      "Shape of validation set:  (784, 88)\n"
     ]
    }
   ],
   "source": [
    "#Lets now split our test set into vaL and test sets\n",
    "#Test will be used to test our model at the END of training\n",
    "#Whereas validation will be used to test the model DURING training\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "print('Shape of test set: ', X_test.shape)\n",
    "print('Shape of validation set: ', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order for the model to work, we first have to convert our dataframes into tensors:\n",
    "\n",
    "X_train_array = X_train.values\n",
    "X_test_array = X_test.values\n",
    "X_val_array = X_val.values\n",
    "\n",
    "y_train_array = y_train.values\n",
    "y_test_array = y_test.values\n",
    "y_val_array = y_val.values\n",
    "\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_array)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_array)\n",
    "X_val_tensor = tf.convert_to_tensor(X_val_array)\n",
    "\n",
    "y_train_tensor = tf.convert_to_tensor(y_train_array)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test_array)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is simply creating a tensorflow keras model for our machine learning\n",
    "#I have initialised it inside a function as it will allow me to search through all different combinations of parameters\n",
    "# e.g number of dense neurones, dropout rate, etc.\n",
    "\n",
    "def init_keras_model(dense1=700, dense2=100, dropout=0.7, optimizer='adam', callbacks=[]):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dense1, activation='relu'), # Create a fully connected dense layer with dense1 number of neurons\n",
    "        tf.keras.layers.BatchNormalization(), # Batch normalisation reduces variability of our model\n",
    "        tf.keras.layers.Dropout(rate=dropout), # Dropout will reduce overtraining\n",
    "        tf.keras.layers.Dense(dense2, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(rate=dropout),\n",
    "        tf.keras.layers.Dense(50, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(rate=0.3),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid') # output layer\n",
    "    ])\n",
    "\n",
    "    # Call backs are a way of monitoring our model during training, for example to reduce the learning rate or save the best model as we go\n",
    "    #If callbacks is empty, compile the model without them\n",
    "    if callbacks == []: \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'],\n",
    "        )\n",
    "    else: # If we specify callbacks the model will be compiled with them\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7803 - accuracy: 0.5458 - val_loss: 0.6621 - val_accuracy: 0.6314\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7147 - accuracy: 0.5570 - val_loss: 0.6428 - val_accuracy: 0.6454\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.5867 - val_loss: 0.6389 - val_accuracy: 0.6429\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.6005 - val_loss: 0.6401 - val_accuracy: 0.6327\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6091 - val_loss: 0.6356 - val_accuracy: 0.6531\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6202 - val_loss: 0.6343 - val_accuracy: 0.6518\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6320 - val_loss: 0.6242 - val_accuracy: 0.6696\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6451 - val_loss: 0.6164 - val_accuracy: 0.6798\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6493 - val_loss: 0.6218 - val_accuracy: 0.6773\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6467 - val_loss: 0.6127 - val_accuracy: 0.6824\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6467 - val_loss: 0.6163 - val_accuracy: 0.6824\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6521 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6493 - val_loss: 0.6210 - val_accuracy: 0.6722\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6211 - accuracy: 0.6672 - val_loss: 0.6151 - val_accuracy: 0.6811\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6152 - accuracy: 0.6653 - val_loss: 0.6134 - val_accuracy: 0.6837\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6593 - val_loss: 0.6176 - val_accuracy: 0.6735\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6159 - accuracy: 0.6675 - val_loss: 0.6164 - val_accuracy: 0.6760\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6085 - accuracy: 0.6740 - val_loss: 0.6229 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6106 - accuracy: 0.6694 - val_loss: 0.6233 - val_accuracy: 0.6760\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6038 - accuracy: 0.6756 - val_loss: 0.6258 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6742 - val_loss: 0.6237 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6848 - val_loss: 0.6304 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5989 - accuracy: 0.6834 - val_loss: 0.6251 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5961 - accuracy: 0.6889 - val_loss: 0.6246 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5984 - accuracy: 0.6816 - val_loss: 0.6276 - val_accuracy: 0.6671\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5934 - accuracy: 0.6863 - val_loss: 0.6344 - val_accuracy: 0.6671\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5954 - accuracy: 0.6853 - val_loss: 0.6311 - val_accuracy: 0.6607\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5867 - accuracy: 0.6917 - val_loss: 0.6307 - val_accuracy: 0.6735\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5861 - accuracy: 0.6909 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5796 - accuracy: 0.6945 - val_loss: 0.6334 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5844 - accuracy: 0.6937 - val_loss: 0.6338 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5781 - accuracy: 0.7001 - val_loss: 0.6367 - val_accuracy: 0.6696\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5781 - accuracy: 0.7038 - val_loss: 0.6317 - val_accuracy: 0.6735\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5741 - accuracy: 0.6989 - val_loss: 0.6296 - val_accuracy: 0.6798\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5688 - accuracy: 0.7069 - val_loss: 0.6358 - val_accuracy: 0.6798\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5727 - accuracy: 0.7085 - val_loss: 0.6384 - val_accuracy: 0.6645\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5719 - accuracy: 0.7105 - val_loss: 0.6442 - val_accuracy: 0.6531\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5672 - accuracy: 0.7100 - val_loss: 0.6372 - val_accuracy: 0.6709\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5662 - accuracy: 0.7163 - val_loss: 0.6321 - val_accuracy: 0.6773\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5652 - accuracy: 0.7114 - val_loss: 0.6380 - val_accuracy: 0.6556\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5611 - accuracy: 0.7154 - val_loss: 0.6366 - val_accuracy: 0.6607\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.5580 - accuracy: 0.7209 - val_loss: 0.6410 - val_accuracy: 0.6773\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.5597 - accuracy: 0.7100 - val_loss: 0.6384 - val_accuracy: 0.6811\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5597 - accuracy: 0.7170 - val_loss: 0.6413 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5508 - accuracy: 0.7254 - val_loss: 0.6396 - val_accuracy: 0.6888\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5571 - accuracy: 0.7160 - val_loss: 0.6348 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5527 - accuracy: 0.7199 - val_loss: 0.6444 - val_accuracy: 0.6684\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5431 - accuracy: 0.7302 - val_loss: 0.6600 - val_accuracy: 0.6594\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5496 - accuracy: 0.7189 - val_loss: 0.6437 - val_accuracy: 0.6786\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5477 - accuracy: 0.7209 - val_loss: 0.6526 - val_accuracy: 0.6671\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5387 - accuracy: 0.7287 - val_loss: 0.6493 - val_accuracy: 0.6696\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5449 - accuracy: 0.7222 - val_loss: 0.6490 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5387 - accuracy: 0.7286 - val_loss: 0.6645 - val_accuracy: 0.6696\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5411 - accuracy: 0.7268 - val_loss: 0.6523 - val_accuracy: 0.6696\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5379 - accuracy: 0.7265 - val_loss: 0.6479 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5355 - accuracy: 0.7352 - val_loss: 0.6568 - val_accuracy: 0.6684\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5292 - accuracy: 0.7347 - val_loss: 0.6636 - val_accuracy: 0.6633\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5392 - accuracy: 0.7277 - val_loss: 0.6638 - val_accuracy: 0.6773\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5344 - accuracy: 0.7311 - val_loss: 0.6546 - val_accuracy: 0.6773\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5300 - accuracy: 0.7352 - val_loss: 0.6631 - val_accuracy: 0.6722\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5317 - accuracy: 0.7350 - val_loss: 0.6756 - val_accuracy: 0.6709\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5316 - accuracy: 0.7379 - val_loss: 0.6531 - val_accuracy: 0.6773\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5296 - accuracy: 0.7391 - val_loss: 0.6617 - val_accuracy: 0.6684\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5252 - accuracy: 0.7381 - val_loss: 0.6653 - val_accuracy: 0.6811\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5276 - accuracy: 0.7404 - val_loss: 0.6536 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7427 - val_loss: 0.6588 - val_accuracy: 0.6862\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5260 - accuracy: 0.7386 - val_loss: 0.6617 - val_accuracy: 0.6811\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5235 - accuracy: 0.7464 - val_loss: 0.6492 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5219 - accuracy: 0.7415 - val_loss: 0.6626 - val_accuracy: 0.6786\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5225 - accuracy: 0.7443 - val_loss: 0.6551 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5237 - accuracy: 0.7375 - val_loss: 0.6554 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7386 - val_loss: 0.6648 - val_accuracy: 0.6811\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5147 - accuracy: 0.7482 - val_loss: 0.6570 - val_accuracy: 0.6849\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7469 - val_loss: 0.6504 - val_accuracy: 0.6786\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7481 - val_loss: 0.6633 - val_accuracy: 0.6862\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7477 - val_loss: 0.6601 - val_accuracy: 0.6824\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5102 - accuracy: 0.7513 - val_loss: 0.6641 - val_accuracy: 0.6786\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7498 - val_loss: 0.6624 - val_accuracy: 0.6798\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5081 - accuracy: 0.7504 - val_loss: 0.6797 - val_accuracy: 0.6901\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7496 - val_loss: 0.6747 - val_accuracy: 0.6773\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7496 - val_loss: 0.6727 - val_accuracy: 0.6786\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5079 - accuracy: 0.7521 - val_loss: 0.6673 - val_accuracy: 0.6824\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7491 - val_loss: 0.6774 - val_accuracy: 0.6709\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7509 - val_loss: 0.6662 - val_accuracy: 0.6849\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5026 - accuracy: 0.7566 - val_loss: 0.6740 - val_accuracy: 0.6798\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5077 - accuracy: 0.7492 - val_loss: 0.6824 - val_accuracy: 0.6658\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5050 - accuracy: 0.7516 - val_loss: 0.6870 - val_accuracy: 0.6811\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5002 - accuracy: 0.7575 - val_loss: 0.6815 - val_accuracy: 0.6645\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4974 - accuracy: 0.7538 - val_loss: 0.6879 - val_accuracy: 0.6760\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4993 - accuracy: 0.7573 - val_loss: 0.6770 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5039 - accuracy: 0.7557 - val_loss: 0.6658 - val_accuracy: 0.6786\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7455 - val_loss: 0.6724 - val_accuracy: 0.6747\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5022 - accuracy: 0.7504 - val_loss: 0.6885 - val_accuracy: 0.6824\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5008 - accuracy: 0.7567 - val_loss: 0.6848 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4892 - accuracy: 0.7589 - val_loss: 0.6902 - val_accuracy: 0.6747\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4907 - accuracy: 0.7615 - val_loss: 0.6891 - val_accuracy: 0.6798\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.4939 - accuracy: 0.7601 - val_loss: 0.6898 - val_accuracy: 0.6811\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4999 - accuracy: 0.7514 - val_loss: 0.6801 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4976 - accuracy: 0.7576 - val_loss: 0.6888 - val_accuracy: 0.6747\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4988 - accuracy: 0.7538 - val_loss: 0.6827 - val_accuracy: 0.6722\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4930 - accuracy: 0.7591 - val_loss: 0.6919 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5003 - accuracy: 0.7561 - val_loss: 0.6926 - val_accuracy: 0.6798\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4944 - accuracy: 0.7581 - val_loss: 0.6865 - val_accuracy: 0.6786\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4962 - accuracy: 0.7625 - val_loss: 0.6811 - val_accuracy: 0.6786\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4908 - accuracy: 0.7631 - val_loss: 0.6873 - val_accuracy: 0.6735\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4877 - accuracy: 0.7638 - val_loss: 0.6944 - val_accuracy: 0.6747\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4933 - accuracy: 0.7601 - val_loss: 0.6824 - val_accuracy: 0.6952\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4854 - accuracy: 0.7681 - val_loss: 0.6907 - val_accuracy: 0.6849\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4933 - accuracy: 0.7606 - val_loss: 0.7027 - val_accuracy: 0.6773\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4818 - accuracy: 0.7689 - val_loss: 0.6933 - val_accuracy: 0.6849\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4871 - accuracy: 0.7679 - val_loss: 0.6870 - val_accuracy: 0.6773\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4826 - accuracy: 0.7633 - val_loss: 0.6995 - val_accuracy: 0.6747\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4874 - accuracy: 0.7619 - val_loss: 0.6759 - val_accuracy: 0.6913\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4903 - accuracy: 0.7621 - val_loss: 0.6899 - val_accuracy: 0.6849\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4818 - accuracy: 0.7654 - val_loss: 0.6986 - val_accuracy: 0.6658\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4870 - accuracy: 0.7646 - val_loss: 0.6984 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4802 - accuracy: 0.7679 - val_loss: 0.6919 - val_accuracy: 0.6607\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4861 - accuracy: 0.7639 - val_loss: 0.6975 - val_accuracy: 0.6798\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4849 - accuracy: 0.7625 - val_loss: 0.7122 - val_accuracy: 0.6837\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4871 - accuracy: 0.7705 - val_loss: 0.6863 - val_accuracy: 0.6798\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4861 - accuracy: 0.7665 - val_loss: 0.6945 - val_accuracy: 0.6735\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4765 - accuracy: 0.7695 - val_loss: 0.7005 - val_accuracy: 0.6824\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4761 - accuracy: 0.7670 - val_loss: 0.6981 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4773 - accuracy: 0.7719 - val_loss: 0.7054 - val_accuracy: 0.6747\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4748 - accuracy: 0.7764 - val_loss: 0.7081 - val_accuracy: 0.6747\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.4710 - accuracy: 0.7747 - val_loss: 0.7034 - val_accuracy: 0.6633\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4795 - accuracy: 0.7664 - val_loss: 0.7019 - val_accuracy: 0.6696\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4784 - accuracy: 0.7615 - val_loss: 0.7027 - val_accuracy: 0.6607\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4799 - accuracy: 0.7615 - val_loss: 0.6953 - val_accuracy: 0.6875\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4801 - accuracy: 0.7665 - val_loss: 0.6966 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4757 - accuracy: 0.7720 - val_loss: 0.6982 - val_accuracy: 0.6786\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4776 - accuracy: 0.7695 - val_loss: 0.7109 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4799 - accuracy: 0.7699 - val_loss: 0.7027 - val_accuracy: 0.6747\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4780 - accuracy: 0.7714 - val_loss: 0.6958 - val_accuracy: 0.6811\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4729 - accuracy: 0.7743 - val_loss: 0.7001 - val_accuracy: 0.6709\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4685 - accuracy: 0.7807 - val_loss: 0.7035 - val_accuracy: 0.6760\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4684 - accuracy: 0.7738 - val_loss: 0.7007 - val_accuracy: 0.6735\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4742 - accuracy: 0.7742 - val_loss: 0.6828 - val_accuracy: 0.6811\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4728 - accuracy: 0.7729 - val_loss: 0.6899 - val_accuracy: 0.6722\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4701 - accuracy: 0.7702 - val_loss: 0.6956 - val_accuracy: 0.6735\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4729 - accuracy: 0.7710 - val_loss: 0.6898 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4738 - accuracy: 0.7733 - val_loss: 0.7072 - val_accuracy: 0.6735\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4625 - accuracy: 0.7779 - val_loss: 0.7234 - val_accuracy: 0.6582\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4722 - accuracy: 0.7754 - val_loss: 0.7059 - val_accuracy: 0.6773\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4621 - accuracy: 0.7784 - val_loss: 0.7184 - val_accuracy: 0.6798\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4698 - accuracy: 0.7753 - val_loss: 0.7035 - val_accuracy: 0.6696\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4700 - accuracy: 0.7719 - val_loss: 0.7051 - val_accuracy: 0.6735\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.4732 - accuracy: 0.7754 - val_loss: 0.6977 - val_accuracy: 0.6722\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4550 - accuracy: 0.7817 - val_loss: 0.7193 - val_accuracy: 0.6786\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4677 - accuracy: 0.7781 - val_loss: 0.7012 - val_accuracy: 0.6722\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4619 - accuracy: 0.7795 - val_loss: 0.7149 - val_accuracy: 0.6760\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4626 - accuracy: 0.7787 - val_loss: 0.7196 - val_accuracy: 0.6735\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4732 - accuracy: 0.7783 - val_loss: 0.6933 - val_accuracy: 0.6862\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4586 - accuracy: 0.7779 - val_loss: 0.7088 - val_accuracy: 0.6824\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4568 - accuracy: 0.7837 - val_loss: 0.7249 - val_accuracy: 0.6786\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4707 - accuracy: 0.7670 - val_loss: 0.7104 - val_accuracy: 0.6747\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4628 - accuracy: 0.7772 - val_loss: 0.6942 - val_accuracy: 0.6837\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4632 - accuracy: 0.7798 - val_loss: 0.7152 - val_accuracy: 0.6722\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4542 - accuracy: 0.7846 - val_loss: 0.7328 - val_accuracy: 0.6786\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.4543 - accuracy: 0.7782 - val_loss: 0.7311 - val_accuracy: 0.6798\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4674 - accuracy: 0.7744 - val_loss: 0.7126 - val_accuracy: 0.6811\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4627 - accuracy: 0.7796 - val_loss: 0.7154 - val_accuracy: 0.6760\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4644 - accuracy: 0.7724 - val_loss: 0.7265 - val_accuracy: 0.6837\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4617 - accuracy: 0.7796 - val_loss: 0.7146 - val_accuracy: 0.6798\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4562 - accuracy: 0.7865 - val_loss: 0.7211 - val_accuracy: 0.6747\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4578 - accuracy: 0.7817 - val_loss: 0.7149 - val_accuracy: 0.6722\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4651 - accuracy: 0.7806 - val_loss: 0.7036 - val_accuracy: 0.6798\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4579 - accuracy: 0.7821 - val_loss: 0.7130 - val_accuracy: 0.6875\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.4563 - accuracy: 0.7805 - val_loss: 0.7136 - val_accuracy: 0.6760\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4545 - accuracy: 0.7802 - val_loss: 0.7282 - val_accuracy: 0.6760\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4651 - accuracy: 0.7803 - val_loss: 0.7095 - val_accuracy: 0.6824\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4636 - accuracy: 0.7748 - val_loss: 0.7023 - val_accuracy: 0.6798\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4643 - accuracy: 0.7778 - val_loss: 0.7175 - val_accuracy: 0.6837\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4489 - accuracy: 0.7870 - val_loss: 0.7332 - val_accuracy: 0.6735\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4598 - accuracy: 0.7807 - val_loss: 0.6960 - val_accuracy: 0.6786\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4521 - accuracy: 0.7826 - val_loss: 0.7245 - val_accuracy: 0.6620\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4591 - accuracy: 0.7812 - val_loss: 0.7073 - val_accuracy: 0.6709\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4585 - accuracy: 0.7815 - val_loss: 0.7242 - val_accuracy: 0.6735\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4507 - accuracy: 0.7842 - val_loss: 0.7112 - val_accuracy: 0.6786\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4576 - accuracy: 0.7802 - val_loss: 0.7235 - val_accuracy: 0.6709\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4541 - accuracy: 0.7859 - val_loss: 0.7069 - val_accuracy: 0.6569\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4490 - accuracy: 0.7827 - val_loss: 0.7435 - val_accuracy: 0.6633\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4569 - accuracy: 0.7840 - val_loss: 0.7163 - val_accuracy: 0.6773\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4527 - accuracy: 0.7881 - val_loss: 0.7241 - val_accuracy: 0.6722\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4603 - accuracy: 0.7812 - val_loss: 0.7226 - val_accuracy: 0.6645\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4547 - accuracy: 0.7838 - val_loss: 0.7368 - val_accuracy: 0.6747\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4469 - accuracy: 0.7919 - val_loss: 0.7261 - val_accuracy: 0.6696\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4607 - accuracy: 0.7820 - val_loss: 0.7299 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4493 - accuracy: 0.7860 - val_loss: 0.6996 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4573 - accuracy: 0.7793 - val_loss: 0.7262 - val_accuracy: 0.6811\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4500 - accuracy: 0.7865 - val_loss: 0.7145 - val_accuracy: 0.6645\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4548 - accuracy: 0.7838 - val_loss: 0.7198 - val_accuracy: 0.6786\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4553 - accuracy: 0.7820 - val_loss: 0.7138 - val_accuracy: 0.6709\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4495 - accuracy: 0.7841 - val_loss: 0.7345 - val_accuracy: 0.6709\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4511 - accuracy: 0.7828 - val_loss: 0.7189 - val_accuracy: 0.6709\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4518 - accuracy: 0.7859 - val_loss: 0.7160 - val_accuracy: 0.6684\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4541 - accuracy: 0.7894 - val_loss: 0.7244 - val_accuracy: 0.6786\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4503 - accuracy: 0.7895 - val_loss: 0.7086 - val_accuracy: 0.6735\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4410 - accuracy: 0.7940 - val_loss: 0.7284 - val_accuracy: 0.6709\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4463 - accuracy: 0.7869 - val_loss: 0.7193 - val_accuracy: 0.6811\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001 # This is how fast the model should learn\n",
    "n_epochs = 200 # This is how many times to train the model \n",
    "# (more epochs may lead to better accuracy but will also lead to overfitting)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE) # The optimizer is Adam and is the algorithm used to train the model\n",
    "lrr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=15, verbose=1, factor=0.5, min_lr=0.00001) # Reduce learning rate on plateau tells the model to reduce the speed at which it learns when it gets close to converging.\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath='weights_chk.h5',\n",
    "                                        monitor='val_accuracy',\n",
    "                                        verbose=0,\n",
    "                                        save_best_only=True,\n",
    "                                        save_weights_only=True,\n",
    "                                        mode='max') # Model Checkpoint is used to save the best model during training\n",
    "\n",
    "\n",
    "\n",
    "#precision = tf.keras.metrics.Precision()\n",
    "#recall = tf.keras.metrics.Recall()\n",
    "\n",
    "keras_model = init_keras_model() # Initialise the model\n",
    "\n",
    "\n",
    "history = keras_model.fit(X_train_tensor, \n",
    "                          y_train_tensor, \n",
    "                          epochs=n_epochs, \n",
    "                          validation_data=(X_val_tensor, y_val_tensor), \n",
    "                          ) # Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss & accuracy\n",
    "\n",
    "This will plot the loss (how far each prediction is from the truth) - lower is better\n",
    "And the accuracy measures the % of correct predictions - higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "# Plot the training loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss', c='red')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "\n",
    "ax2=ax1.twinx()\n",
    "\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', c='blue')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "plt.title('Training Loss/Accuracy')\n",
    "\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='center right')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training accuracy and validation accuracy\n",
    "\n",
    "This will tell us if the model is overfitting (training accuracy increases and val accuracy decreases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e55952ba58>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy', c='orange')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', c='purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS BLOCK WILL TAKE A LONG TIME, DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for: 700 100 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_132 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8133 - accuracy: 0.5534 - val_loss: 0.6403 - val_accuracy: 0.6543\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7064 - accuracy: 0.5697 - val_loss: 0.6339 - val_accuracy: 0.6684\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6776 - accuracy: 0.5836 - val_loss: 0.6391 - val_accuracy: 0.6633\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5942 - val_loss: 0.6358 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6112 - val_loss: 0.6298 - val_accuracy: 0.6786\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6150 - val_loss: 0.6304 - val_accuracy: 0.6760\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6011 - val_loss: 0.6292 - val_accuracy: 0.6824\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6169 - val_loss: 0.6274 - val_accuracy: 0.6747\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6215 - val_loss: 0.6301 - val_accuracy: 0.6786\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6226 - val_loss: 0.6292 - val_accuracy: 0.6824\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6189 - val_loss: 0.6249 - val_accuracy: 0.6786\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6255 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6315 - val_loss: 0.6278 - val_accuracy: 0.6849\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6304 - val_loss: 0.6239 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6299 - val_loss: 0.6263 - val_accuracy: 0.6798\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6339 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6301 - val_loss: 0.6270 - val_accuracy: 0.6824\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6394 - val_loss: 0.6206 - val_accuracy: 0.6875\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6392 - val_loss: 0.6259 - val_accuracy: 0.6875\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6353 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6400 - val_loss: 0.6205 - val_accuracy: 0.6952\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6385 - val_loss: 0.6239 - val_accuracy: 0.6837\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6403 - val_loss: 0.6216 - val_accuracy: 0.6875\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6497 - val_loss: 0.6212 - val_accuracy: 0.6862\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6451 - val_loss: 0.6190 - val_accuracy: 0.6964\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6559 - val_loss: 0.6184 - val_accuracy: 0.6901\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6500 - val_loss: 0.6198 - val_accuracy: 0.6952\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6482 - val_loss: 0.6178 - val_accuracy: 0.6952\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6491 - val_loss: 0.6213 - val_accuracy: 0.6939\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6583 - val_loss: 0.6170 - val_accuracy: 0.6952\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6574 - val_loss: 0.6163 - val_accuracy: 0.6990\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6565 - val_loss: 0.6172 - val_accuracy: 0.7028\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6505 - val_loss: 0.6158 - val_accuracy: 0.6952\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6507 - val_loss: 0.6168 - val_accuracy: 0.7003\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6555 - val_loss: 0.6171 - val_accuracy: 0.6977\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6629 - val_loss: 0.6175 - val_accuracy: 0.6913\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6583 - val_loss: 0.6211 - val_accuracy: 0.6926\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6198 - accuracy: 0.6576 - val_loss: 0.6176 - val_accuracy: 0.6939\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6567 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6613 - val_loss: 0.6138 - val_accuracy: 0.6952\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6662 - val_loss: 0.6171 - val_accuracy: 0.6926\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6650 - val_loss: 0.6169 - val_accuracy: 0.7003\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6738 - val_loss: 0.6160 - val_accuracy: 0.6901\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6073 - accuracy: 0.6750 - val_loss: 0.6144 - val_accuracy: 0.6901\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6137 - accuracy: 0.6620 - val_loss: 0.6212 - val_accuracy: 0.6926\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.6670 - val_loss: 0.6150 - val_accuracy: 0.6939\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6067 - accuracy: 0.6713 - val_loss: 0.6174 - val_accuracy: 0.6939\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6652 - val_loss: 0.6139 - val_accuracy: 0.6964\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6098 - accuracy: 0.6713 - val_loss: 0.6128 - val_accuracy: 0.6977\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6060 - accuracy: 0.6728 - val_loss: 0.6158 - val_accuracy: 0.6977\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6073 - accuracy: 0.6722 - val_loss: 0.6221 - val_accuracy: 0.6888\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6086 - accuracy: 0.6677 - val_loss: 0.6197 - val_accuracy: 0.6952\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6006 - accuracy: 0.6751 - val_loss: 0.6191 - val_accuracy: 0.6901\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6011 - accuracy: 0.6801 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6048 - accuracy: 0.6747 - val_loss: 0.6217 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6033 - accuracy: 0.6786 - val_loss: 0.6172 - val_accuracy: 0.6939\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5990 - accuracy: 0.6814 - val_loss: 0.6210 - val_accuracy: 0.6888\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6020 - accuracy: 0.6778 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6024 - accuracy: 0.6786 - val_loss: 0.6204 - val_accuracy: 0.6964\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5988 - accuracy: 0.6776 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5999 - accuracy: 0.6806 - val_loss: 0.6274 - val_accuracy: 0.6798\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5971 - accuracy: 0.6856 - val_loss: 0.6221 - val_accuracy: 0.6862\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5901 - accuracy: 0.6876 - val_loss: 0.6235 - val_accuracy: 0.6824\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5951 - accuracy: 0.6876 - val_loss: 0.6233 - val_accuracy: 0.6901\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5964 - accuracy: 0.6800 - val_loss: 0.6239 - val_accuracy: 0.6952\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5966 - accuracy: 0.6859 - val_loss: 0.6279 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5920 - accuracy: 0.6925 - val_loss: 0.6262 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5942 - accuracy: 0.6822 - val_loss: 0.6276 - val_accuracy: 0.6888\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5882 - accuracy: 0.6938 - val_loss: 0.6268 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5860 - accuracy: 0.6907 - val_loss: 0.6240 - val_accuracy: 0.6875\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5859 - accuracy: 0.6966 - val_loss: 0.6261 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5838 - accuracy: 0.6959 - val_loss: 0.6260 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5850 - accuracy: 0.6938 - val_loss: 0.6276 - val_accuracy: 0.6837\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5842 - accuracy: 0.6953 - val_loss: 0.6315 - val_accuracy: 0.6837\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5803 - accuracy: 0.6971 - val_loss: 0.6269 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5854 - accuracy: 0.6886 - val_loss: 0.6335 - val_accuracy: 0.6798\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5825 - accuracy: 0.6952 - val_loss: 0.6270 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5776 - accuracy: 0.6991 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5797 - accuracy: 0.6924 - val_loss: 0.6302 - val_accuracy: 0.6849\n",
      "Calculating for: 700 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_136 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8537 - accuracy: 0.5237 - val_loss: 0.6479 - val_accuracy: 0.6390\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.7240 - accuracy: 0.5501 - val_loss: 0.6423 - val_accuracy: 0.6467\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6984 - accuracy: 0.5558 - val_loss: 0.6462 - val_accuracy: 0.6505\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6860 - accuracy: 0.5569 - val_loss: 0.6497 - val_accuracy: 0.6531\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6782 - accuracy: 0.5698 - val_loss: 0.6446 - val_accuracy: 0.6518\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5727 - val_loss: 0.6492 - val_accuracy: 0.6645\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6758 - accuracy: 0.5786 - val_loss: 0.6472 - val_accuracy: 0.6645\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6721 - accuracy: 0.5874 - val_loss: 0.6437 - val_accuracy: 0.6531\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5909 - val_loss: 0.6458 - val_accuracy: 0.6594\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6720 - accuracy: 0.5878 - val_loss: 0.6458 - val_accuracy: 0.6582\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6680 - accuracy: 0.5993 - val_loss: 0.6367 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6684 - accuracy: 0.5961 - val_loss: 0.6463 - val_accuracy: 0.6518\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6040 - val_loss: 0.6413 - val_accuracy: 0.6582\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6031 - val_loss: 0.6377 - val_accuracy: 0.6620\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6004 - val_loss: 0.6376 - val_accuracy: 0.6594\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5975 - val_loss: 0.6435 - val_accuracy: 0.6671\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6045 - val_loss: 0.6346 - val_accuracy: 0.6696\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.6024 - val_loss: 0.6342 - val_accuracy: 0.6709\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6017 - val_loss: 0.6401 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6609 - accuracy: 0.6058 - val_loss: 0.6328 - val_accuracy: 0.6658\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6592 - accuracy: 0.6073 - val_loss: 0.6371 - val_accuracy: 0.6684\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6076 - val_loss: 0.6399 - val_accuracy: 0.6735\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6104 - val_loss: 0.6386 - val_accuracy: 0.6696\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6103 - val_loss: 0.6356 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6569 - accuracy: 0.6154 - val_loss: 0.6402 - val_accuracy: 0.6696\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6560 - accuracy: 0.6216 - val_loss: 0.6389 - val_accuracy: 0.6620\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6566 - accuracy: 0.6199 - val_loss: 0.6298 - val_accuracy: 0.6811\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6587 - accuracy: 0.6122 - val_loss: 0.6402 - val_accuracy: 0.6620\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6559 - accuracy: 0.6154 - val_loss: 0.6330 - val_accuracy: 0.6798\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6171 - val_loss: 0.6278 - val_accuracy: 0.6786\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6548 - accuracy: 0.6129 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6216 - val_loss: 0.6267 - val_accuracy: 0.6849\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6203 - val_loss: 0.6276 - val_accuracy: 0.6747\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6496 - accuracy: 0.6262 - val_loss: 0.6326 - val_accuracy: 0.6735\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6236 - val_loss: 0.6293 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6231 - val_loss: 0.6299 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6489 - accuracy: 0.6289 - val_loss: 0.6312 - val_accuracy: 0.6709\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6498 - accuracy: 0.6241 - val_loss: 0.6317 - val_accuracy: 0.6620\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6281 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6503 - accuracy: 0.6323 - val_loss: 0.6294 - val_accuracy: 0.6747\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6222 - val_loss: 0.6298 - val_accuracy: 0.6684\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6502 - accuracy: 0.6211 - val_loss: 0.6283 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6321 - val_loss: 0.6320 - val_accuracy: 0.6607\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6258 - val_loss: 0.6302 - val_accuracy: 0.6633\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6487 - accuracy: 0.6323 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6445 - accuracy: 0.6329 - val_loss: 0.6214 - val_accuracy: 0.6837\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6345 - val_loss: 0.6292 - val_accuracy: 0.6633\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6472 - accuracy: 0.6311 - val_loss: 0.6257 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6323 - val_loss: 0.6249 - val_accuracy: 0.6811\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6343 - val_loss: 0.6257 - val_accuracy: 0.6760\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6414 - accuracy: 0.6390 - val_loss: 0.6247 - val_accuracy: 0.6786\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6465 - accuracy: 0.6331 - val_loss: 0.6238 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6425 - accuracy: 0.6361 - val_loss: 0.6188 - val_accuracy: 0.6773\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6384 - val_loss: 0.6223 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6341 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6419 - accuracy: 0.6360 - val_loss: 0.6167 - val_accuracy: 0.6811\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6412 - val_loss: 0.6210 - val_accuracy: 0.6811\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6456 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6374 - val_loss: 0.6253 - val_accuracy: 0.6773\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6393 - val_loss: 0.6206 - val_accuracy: 0.6837\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6373 - val_loss: 0.6175 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6410 - val_loss: 0.6201 - val_accuracy: 0.6862\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6400 - accuracy: 0.6442 - val_loss: 0.6168 - val_accuracy: 0.6786\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6385 - accuracy: 0.6427 - val_loss: 0.6227 - val_accuracy: 0.6811\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6436 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6446 - val_loss: 0.6236 - val_accuracy: 0.6786\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6473 - val_loss: 0.6211 - val_accuracy: 0.6811\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6480 - val_loss: 0.6185 - val_accuracy: 0.6849\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6420 - val_loss: 0.6125 - val_accuracy: 0.6888\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6451 - val_loss: 0.6170 - val_accuracy: 0.6875\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6368 - accuracy: 0.6409 - val_loss: 0.6204 - val_accuracy: 0.6849\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6428 - val_loss: 0.6172 - val_accuracy: 0.6862\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6490 - val_loss: 0.6236 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6510 - val_loss: 0.6198 - val_accuracy: 0.6862\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6433 - val_loss: 0.6231 - val_accuracy: 0.6901\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6507 - val_loss: 0.6255 - val_accuracy: 0.6773\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6485 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6512 - val_loss: 0.6153 - val_accuracy: 0.6926\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6500 - val_loss: 0.6202 - val_accuracy: 0.6862\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6532 - val_loss: 0.6185 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6541 - val_loss: 0.6187 - val_accuracy: 0.6875\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6572 - val_loss: 0.6158 - val_accuracy: 0.6926\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6574 - val_loss: 0.6113 - val_accuracy: 0.6952\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6284 - accuracy: 0.6540 - val_loss: 0.6115 - val_accuracy: 0.6964\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6500 - val_loss: 0.6161 - val_accuracy: 0.6926\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6576 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6262 - accuracy: 0.6560 - val_loss: 0.6153 - val_accuracy: 0.6901\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6320 - accuracy: 0.6559 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6259 - accuracy: 0.6570 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6611 - val_loss: 0.6138 - val_accuracy: 0.6939\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6606 - val_loss: 0.6138 - val_accuracy: 0.6862\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6556 - val_loss: 0.6156 - val_accuracy: 0.6837\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6579 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6550 - val_loss: 0.6103 - val_accuracy: 0.6926\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6282 - accuracy: 0.6610 - val_loss: 0.6173 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6614 - val_loss: 0.6130 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6657 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6253 - accuracy: 0.6623 - val_loss: 0.6173 - val_accuracy: 0.6888\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6561 - val_loss: 0.6156 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6628 - val_loss: 0.6122 - val_accuracy: 0.6888\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6692 - val_loss: 0.6126 - val_accuracy: 0.6926\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6611 - val_loss: 0.6141 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6598 - val_loss: 0.6155 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6643 - val_loss: 0.6135 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6616 - val_loss: 0.6190 - val_accuracy: 0.6773\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6691 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6644 - val_loss: 0.6208 - val_accuracy: 0.6786\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6577 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6600 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6625 - val_loss: 0.6173 - val_accuracy: 0.6837\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6670 - val_loss: 0.6188 - val_accuracy: 0.6862\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6171 - accuracy: 0.6670 - val_loss: 0.6207 - val_accuracy: 0.6811\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.6692 - val_loss: 0.6115 - val_accuracy: 0.6913\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6629 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6727 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6663 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6664 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6615 - val_loss: 0.6146 - val_accuracy: 0.6913\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6712 - val_loss: 0.6197 - val_accuracy: 0.6722\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.6712 - val_loss: 0.6206 - val_accuracy: 0.6786\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6728 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.6728 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6134 - accuracy: 0.6707 - val_loss: 0.6168 - val_accuracy: 0.6862\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6142 - accuracy: 0.6682 - val_loss: 0.6184 - val_accuracy: 0.6888\n",
      "Calculating for: 700 100 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_140 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8488 - accuracy: 0.5085 - val_loss: 0.6684 - val_accuracy: 0.6212\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7409 - accuracy: 0.5149 - val_loss: 0.6685 - val_accuracy: 0.6173\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7102 - accuracy: 0.5151 - val_loss: 0.6778 - val_accuracy: 0.6250\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6996 - accuracy: 0.5225 - val_loss: 0.6755 - val_accuracy: 0.6212\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5180 - val_loss: 0.6765 - val_accuracy: 0.6224\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5232 - val_loss: 0.6674 - val_accuracy: 0.6199\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6905 - accuracy: 0.5310 - val_loss: 0.6712 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5178 - val_loss: 0.6727 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5300 - val_loss: 0.6668 - val_accuracy: 0.6199\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5291 - val_loss: 0.6713 - val_accuracy: 0.6212\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5207 - val_loss: 0.6700 - val_accuracy: 0.6212\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5319 - val_loss: 0.6685 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6879 - accuracy: 0.5416 - val_loss: 0.6634 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5322 - val_loss: 0.6685 - val_accuracy: 0.6237\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5391 - val_loss: 0.6682 - val_accuracy: 0.6250\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6900 - accuracy: 0.5353 - val_loss: 0.6689 - val_accuracy: 0.6237\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6889 - accuracy: 0.5433 - val_loss: 0.6648 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5403 - val_loss: 0.6664 - val_accuracy: 0.6224\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.5339 - val_loss: 0.6617 - val_accuracy: 0.6237\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5328 - val_loss: 0.6650 - val_accuracy: 0.6263\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6863 - accuracy: 0.5528 - val_loss: 0.6612 - val_accuracy: 0.6237\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.5515 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6879 - accuracy: 0.5394 - val_loss: 0.6632 - val_accuracy: 0.6263\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6873 - accuracy: 0.5445 - val_loss: 0.6632 - val_accuracy: 0.6263\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5526 - val_loss: 0.6592 - val_accuracy: 0.6263\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6882 - accuracy: 0.5418 - val_loss: 0.6646 - val_accuracy: 0.6263\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6875 - accuracy: 0.5406 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6875 - accuracy: 0.5471 - val_loss: 0.6618 - val_accuracy: 0.6301\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6844 - accuracy: 0.5504 - val_loss: 0.6618 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6846 - accuracy: 0.5539 - val_loss: 0.6608 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5463 - val_loss: 0.6588 - val_accuracy: 0.6390\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5538 - val_loss: 0.6595 - val_accuracy: 0.6403\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6873 - accuracy: 0.5441 - val_loss: 0.6606 - val_accuracy: 0.6327\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6835 - accuracy: 0.5531 - val_loss: 0.6569 - val_accuracy: 0.6378\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5509 - val_loss: 0.6612 - val_accuracy: 0.6416\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5452 - val_loss: 0.6596 - val_accuracy: 0.6416\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6864 - accuracy: 0.5453 - val_loss: 0.6591 - val_accuracy: 0.6416\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6835 - accuracy: 0.5580 - val_loss: 0.6587 - val_accuracy: 0.6429\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5569 - val_loss: 0.6565 - val_accuracy: 0.6429\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6818 - accuracy: 0.5620 - val_loss: 0.6525 - val_accuracy: 0.6403\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6820 - accuracy: 0.5602 - val_loss: 0.6562 - val_accuracy: 0.6403\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6851 - accuracy: 0.5520 - val_loss: 0.6556 - val_accuracy: 0.6429\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6806 - accuracy: 0.5649 - val_loss: 0.6530 - val_accuracy: 0.6441\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.5569 - val_loss: 0.6548 - val_accuracy: 0.6429\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5592 - val_loss: 0.6533 - val_accuracy: 0.6416\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6832 - accuracy: 0.5605 - val_loss: 0.6550 - val_accuracy: 0.6441\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.5622 - val_loss: 0.6520 - val_accuracy: 0.6429\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6819 - accuracy: 0.5659 - val_loss: 0.6511 - val_accuracy: 0.6441\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6805 - accuracy: 0.5662 - val_loss: 0.6521 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.5653 - val_loss: 0.6528 - val_accuracy: 0.6429\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5556 - val_loss: 0.6572 - val_accuracy: 0.6429\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6811 - accuracy: 0.5653 - val_loss: 0.6536 - val_accuracy: 0.6416\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6793 - accuracy: 0.5602 - val_loss: 0.6523 - val_accuracy: 0.6403\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6773 - accuracy: 0.5721 - val_loss: 0.6549 - val_accuracy: 0.6403\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6803 - accuracy: 0.5678 - val_loss: 0.6520 - val_accuracy: 0.6429\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6812 - accuracy: 0.5602 - val_loss: 0.6557 - val_accuracy: 0.6416\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6791 - accuracy: 0.5654 - val_loss: 0.6535 - val_accuracy: 0.6403\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6784 - accuracy: 0.5691 - val_loss: 0.6517 - val_accuracy: 0.6429\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5735 - val_loss: 0.6528 - val_accuracy: 0.6416\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.5679 - val_loss: 0.6493 - val_accuracy: 0.6429\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6780 - accuracy: 0.5706 - val_loss: 0.6509 - val_accuracy: 0.6416\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6762 - accuracy: 0.5776 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6776 - accuracy: 0.5770 - val_loss: 0.6524 - val_accuracy: 0.6429\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6785 - accuracy: 0.5653 - val_loss: 0.6505 - val_accuracy: 0.6390\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.5759 - val_loss: 0.6470 - val_accuracy: 0.6403\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6764 - accuracy: 0.5749 - val_loss: 0.6488 - val_accuracy: 0.6390\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5733 - val_loss: 0.6485 - val_accuracy: 0.6441\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6756 - accuracy: 0.5756 - val_loss: 0.6452 - val_accuracy: 0.6429\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6763 - accuracy: 0.5809 - val_loss: 0.6472 - val_accuracy: 0.6416\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6765 - accuracy: 0.5759 - val_loss: 0.6446 - val_accuracy: 0.6429\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5813 - val_loss: 0.6495 - val_accuracy: 0.6467\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.5808 - val_loss: 0.6465 - val_accuracy: 0.6454\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5808 - val_loss: 0.6451 - val_accuracy: 0.6441\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5823 - val_loss: 0.6476 - val_accuracy: 0.6492\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6751 - accuracy: 0.5774 - val_loss: 0.6477 - val_accuracy: 0.6441\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5801 - val_loss: 0.6454 - val_accuracy: 0.6403\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5740 - val_loss: 0.6445 - val_accuracy: 0.6454\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5847 - val_loss: 0.6430 - val_accuracy: 0.6429\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5826 - val_loss: 0.6421 - val_accuracy: 0.6467\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6742 - accuracy: 0.5785 - val_loss: 0.6473 - val_accuracy: 0.6492\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5819 - val_loss: 0.6432 - val_accuracy: 0.6441\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.5781 - val_loss: 0.6439 - val_accuracy: 0.6429\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6726 - accuracy: 0.5836 - val_loss: 0.6444 - val_accuracy: 0.6518\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6727 - accuracy: 0.5873 - val_loss: 0.6427 - val_accuracy: 0.6441\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.5854 - val_loss: 0.6460 - val_accuracy: 0.6531\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6738 - accuracy: 0.5831 - val_loss: 0.6460 - val_accuracy: 0.6518\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6690 - accuracy: 0.5922 - val_loss: 0.6439 - val_accuracy: 0.6518\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6728 - accuracy: 0.5889 - val_loss: 0.6422 - val_accuracy: 0.6492\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5899 - val_loss: 0.6405 - val_accuracy: 0.6480\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.5761 - val_loss: 0.6440 - val_accuracy: 0.6492\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5884 - val_loss: 0.6411 - val_accuracy: 0.6505\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6738 - accuracy: 0.5843 - val_loss: 0.6433 - val_accuracy: 0.6467\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5835 - val_loss: 0.6442 - val_accuracy: 0.6467\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5838 - val_loss: 0.6434 - val_accuracy: 0.6543\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5934 - val_loss: 0.6409 - val_accuracy: 0.6582\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6718 - accuracy: 0.5854 - val_loss: 0.6431 - val_accuracy: 0.6531\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6727 - accuracy: 0.5872 - val_loss: 0.6422 - val_accuracy: 0.6492\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5978 - val_loss: 0.6414 - val_accuracy: 0.6684\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6021 - val_loss: 0.6382 - val_accuracy: 0.6518\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5885 - val_loss: 0.6398 - val_accuracy: 0.6684\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5926 - val_loss: 0.6392 - val_accuracy: 0.6531\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5859 - val_loss: 0.6426 - val_accuracy: 0.6556\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6671 - accuracy: 0.5908 - val_loss: 0.6386 - val_accuracy: 0.6645\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5968 - val_loss: 0.6381 - val_accuracy: 0.6531\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.5997 - val_loss: 0.6387 - val_accuracy: 0.6505\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6675 - accuracy: 0.5918 - val_loss: 0.6371 - val_accuracy: 0.6633\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5878 - val_loss: 0.6416 - val_accuracy: 0.6684\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5932 - val_loss: 0.6370 - val_accuracy: 0.6594\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6034 - val_loss: 0.6342 - val_accuracy: 0.6696\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5892 - val_loss: 0.6365 - val_accuracy: 0.6722\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5947 - val_loss: 0.6358 - val_accuracy: 0.6696\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.6002 - val_loss: 0.6362 - val_accuracy: 0.6671\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6639 - accuracy: 0.6021 - val_loss: 0.6347 - val_accuracy: 0.6645\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6698 - accuracy: 0.5934 - val_loss: 0.6393 - val_accuracy: 0.6582\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5987 - val_loss: 0.6339 - val_accuracy: 0.6633\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6689 - accuracy: 0.5962 - val_loss: 0.6360 - val_accuracy: 0.6696\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6039 - val_loss: 0.6353 - val_accuracy: 0.6722\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.5968 - val_loss: 0.6331 - val_accuracy: 0.6633\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6671 - accuracy: 0.6060 - val_loss: 0.6360 - val_accuracy: 0.6735\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6654 - accuracy: 0.6035 - val_loss: 0.6361 - val_accuracy: 0.6735\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6019 - val_loss: 0.6319 - val_accuracy: 0.6645\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.6007 - val_loss: 0.6353 - val_accuracy: 0.6684\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6661 - accuracy: 0.6015 - val_loss: 0.6369 - val_accuracy: 0.6722\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6127 - val_loss: 0.6330 - val_accuracy: 0.6658\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5950 - val_loss: 0.6325 - val_accuracy: 0.6620\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6083 - val_loss: 0.6300 - val_accuracy: 0.6684\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5978 - val_loss: 0.6352 - val_accuracy: 0.6709\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6040 - val_loss: 0.6309 - val_accuracy: 0.6722\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6019 - val_loss: 0.6360 - val_accuracy: 0.6773\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6030 - val_loss: 0.6335 - val_accuracy: 0.6773\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6084 - val_loss: 0.6287 - val_accuracy: 0.6684\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6079 - val_loss: 0.6347 - val_accuracy: 0.6760\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6660 - accuracy: 0.5985 - val_loss: 0.6330 - val_accuracy: 0.6696\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6091 - val_loss: 0.6317 - val_accuracy: 0.6633\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.6010 - val_loss: 0.6357 - val_accuracy: 0.6786\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6088 - val_loss: 0.6286 - val_accuracy: 0.6620\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6017 - val_loss: 0.6318 - val_accuracy: 0.6671\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6014 - val_loss: 0.6359 - val_accuracy: 0.6773\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6118 - val_loss: 0.6293 - val_accuracy: 0.6735\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6041 - val_loss: 0.6296 - val_accuracy: 0.6735\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6643 - accuracy: 0.6041 - val_loss: 0.6382 - val_accuracy: 0.6824\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6081 - val_loss: 0.6295 - val_accuracy: 0.6684\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6132 - val_loss: 0.6278 - val_accuracy: 0.6696\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6061 - val_loss: 0.6308 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6099 - val_loss: 0.6299 - val_accuracy: 0.6709\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6080 - val_loss: 0.6302 - val_accuracy: 0.6696\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6107 - val_loss: 0.6332 - val_accuracy: 0.6735\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6604 - accuracy: 0.6075 - val_loss: 0.6307 - val_accuracy: 0.6786\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6100 - val_loss: 0.6292 - val_accuracy: 0.6811\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6079 - val_loss: 0.6342 - val_accuracy: 0.6786\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6604 - accuracy: 0.6075 - val_loss: 0.6312 - val_accuracy: 0.6773\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6078 - val_loss: 0.6296 - val_accuracy: 0.6722\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6079 - val_loss: 0.6369 - val_accuracy: 0.6811\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6139 - val_loss: 0.6263 - val_accuracy: 0.6722\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6103 - val_loss: 0.6273 - val_accuracy: 0.6773\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6144 - val_loss: 0.6295 - val_accuracy: 0.6773\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6134 - val_loss: 0.6243 - val_accuracy: 0.6798\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6093 - val_loss: 0.6260 - val_accuracy: 0.6760\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6149 - val_loss: 0.6283 - val_accuracy: 0.6760\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6181 - val_loss: 0.6320 - val_accuracy: 0.6798\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6155 - val_loss: 0.6285 - val_accuracy: 0.6798\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6122 - val_loss: 0.6322 - val_accuracy: 0.6811\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6099 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6181 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6177 - val_loss: 0.6314 - val_accuracy: 0.6773\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6143 - val_loss: 0.6296 - val_accuracy: 0.6849\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6148 - val_loss: 0.6269 - val_accuracy: 0.6760\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6132 - val_loss: 0.6279 - val_accuracy: 0.6811\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6177 - val_loss: 0.6304 - val_accuracy: 0.6811\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6118 - val_loss: 0.6279 - val_accuracy: 0.6786\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6186 - val_loss: 0.6291 - val_accuracy: 0.6798\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6114 - val_loss: 0.6305 - val_accuracy: 0.6798\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6599 - accuracy: 0.6167 - val_loss: 0.6294 - val_accuracy: 0.6837\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6125 - val_loss: 0.6306 - val_accuracy: 0.6862\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6149 - val_loss: 0.6277 - val_accuracy: 0.6824\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6599 - accuracy: 0.6115 - val_loss: 0.6275 - val_accuracy: 0.6786\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6240 - val_loss: 0.6308 - val_accuracy: 0.6811\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6183 - val_loss: 0.6263 - val_accuracy: 0.6798\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6561 - accuracy: 0.6142 - val_loss: 0.6246 - val_accuracy: 0.6798\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6119 - val_loss: 0.6245 - val_accuracy: 0.6837\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6584 - accuracy: 0.6169 - val_loss: 0.6285 - val_accuracy: 0.6837\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6184 - val_loss: 0.6320 - val_accuracy: 0.6798\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6215 - val_loss: 0.6267 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6215 - val_loss: 0.6312 - val_accuracy: 0.6798\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6174 - val_loss: 0.6267 - val_accuracy: 0.6849\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6158 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6184 - val_loss: 0.6282 - val_accuracy: 0.6798\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6177 - val_loss: 0.6260 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6198 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6280 - val_loss: 0.6251 - val_accuracy: 0.6824\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6264 - val_loss: 0.6283 - val_accuracy: 0.6824\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6218 - val_loss: 0.6263 - val_accuracy: 0.6786\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6193 - val_loss: 0.6253 - val_accuracy: 0.6837\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6194 - val_loss: 0.6277 - val_accuracy: 0.6849\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6153 - val_loss: 0.6273 - val_accuracy: 0.6811\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6206 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6243 - val_loss: 0.6226 - val_accuracy: 0.6849\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6250 - val_loss: 0.6230 - val_accuracy: 0.6798\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6177 - val_loss: 0.6280 - val_accuracy: 0.6862\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6159 - val_loss: 0.6286 - val_accuracy: 0.6811\n",
      "Calculating for: 700 250 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_144 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8208 - accuracy: 0.5554 - val_loss: 0.6750 - val_accuracy: 0.6008\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7154 - accuracy: 0.5799 - val_loss: 0.6592 - val_accuracy: 0.6314\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6837 - accuracy: 0.5878 - val_loss: 0.6449 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6069 - val_loss: 0.6492 - val_accuracy: 0.6467\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6044 - val_loss: 0.6485 - val_accuracy: 0.6352\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6198 - val_loss: 0.6378 - val_accuracy: 0.6505\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6187 - val_loss: 0.6344 - val_accuracy: 0.6518\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6241 - val_loss: 0.6346 - val_accuracy: 0.6505\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6269 - val_loss: 0.6381 - val_accuracy: 0.6543\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6270 - val_loss: 0.6320 - val_accuracy: 0.6620\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6343 - val_loss: 0.6328 - val_accuracy: 0.6556\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6343 - val_loss: 0.6305 - val_accuracy: 0.6620\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6310 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6456 - val_loss: 0.6263 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6387 - val_loss: 0.6289 - val_accuracy: 0.6658\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6384 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6379 - accuracy: 0.6433 - val_loss: 0.6280 - val_accuracy: 0.6594\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6469 - val_loss: 0.6244 - val_accuracy: 0.6709\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6395 - val_loss: 0.6279 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6444 - val_loss: 0.6264 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6529 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6508 - val_loss: 0.6220 - val_accuracy: 0.6773\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6459 - val_loss: 0.6244 - val_accuracy: 0.6760\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6506 - val_loss: 0.6236 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6454 - val_loss: 0.6246 - val_accuracy: 0.6645\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6570 - val_loss: 0.6264 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6567 - val_loss: 0.6234 - val_accuracy: 0.6735\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6591 - val_loss: 0.6258 - val_accuracy: 0.6671\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6539 - val_loss: 0.6299 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6206 - accuracy: 0.6601 - val_loss: 0.6296 - val_accuracy: 0.6696\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6588 - val_loss: 0.6311 - val_accuracy: 0.6645\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6549 - val_loss: 0.6263 - val_accuracy: 0.6696\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6625 - val_loss: 0.6298 - val_accuracy: 0.6709\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6620 - val_loss: 0.6298 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6649 - val_loss: 0.6262 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6688 - val_loss: 0.6264 - val_accuracy: 0.6709\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6642 - val_loss: 0.6286 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6086 - accuracy: 0.6704 - val_loss: 0.6276 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6706 - val_loss: 0.6300 - val_accuracy: 0.6709\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6670 - val_loss: 0.6295 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6056 - accuracy: 0.6770 - val_loss: 0.6291 - val_accuracy: 0.6671\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6688 - val_loss: 0.6269 - val_accuracy: 0.6709\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6036 - accuracy: 0.6703 - val_loss: 0.6313 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6796 - val_loss: 0.6339 - val_accuracy: 0.6671\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6080 - accuracy: 0.6743 - val_loss: 0.6303 - val_accuracy: 0.6671\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6009 - accuracy: 0.6786 - val_loss: 0.6366 - val_accuracy: 0.6645\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6008 - accuracy: 0.6795 - val_loss: 0.6351 - val_accuracy: 0.6645\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6057 - accuracy: 0.6761 - val_loss: 0.6374 - val_accuracy: 0.6696\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6021 - accuracy: 0.6824 - val_loss: 0.6330 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6018 - accuracy: 0.6711 - val_loss: 0.6349 - val_accuracy: 0.6696\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5986 - accuracy: 0.6843 - val_loss: 0.6337 - val_accuracy: 0.6671\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5929 - accuracy: 0.6871 - val_loss: 0.6407 - val_accuracy: 0.6671\n",
      "Calculating for: 700 250 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_148 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8294 - accuracy: 0.5313 - val_loss: 0.6579 - val_accuracy: 0.6658\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7179 - accuracy: 0.5553 - val_loss: 0.6482 - val_accuracy: 0.6582\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5715 - val_loss: 0.6403 - val_accuracy: 0.6671\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6805 - accuracy: 0.5741 - val_loss: 0.6427 - val_accuracy: 0.6671\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6710 - accuracy: 0.5887 - val_loss: 0.6410 - val_accuracy: 0.6696\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6736 - accuracy: 0.5816 - val_loss: 0.6376 - val_accuracy: 0.6709\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6698 - accuracy: 0.5873 - val_loss: 0.6386 - val_accuracy: 0.6684\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5890 - val_loss: 0.6418 - val_accuracy: 0.6658\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6024 - val_loss: 0.6374 - val_accuracy: 0.6722\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.5997 - val_loss: 0.6356 - val_accuracy: 0.6671\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6090 - val_loss: 0.6320 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6007 - val_loss: 0.6374 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6074 - val_loss: 0.6333 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.5978 - val_loss: 0.6334 - val_accuracy: 0.6760\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6098 - val_loss: 0.6321 - val_accuracy: 0.6735\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6059 - val_loss: 0.6307 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6113 - val_loss: 0.6347 - val_accuracy: 0.6811\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6148 - val_loss: 0.6275 - val_accuracy: 0.6773\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6169 - val_loss: 0.6287 - val_accuracy: 0.6760\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6137 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6192 - val_loss: 0.6284 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6122 - val_loss: 0.6283 - val_accuracy: 0.6824\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6203 - val_loss: 0.6272 - val_accuracy: 0.6798\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6232 - val_loss: 0.6299 - val_accuracy: 0.6849\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6154 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6181 - val_loss: 0.6237 - val_accuracy: 0.6913\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6232 - val_loss: 0.6226 - val_accuracy: 0.6939\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6253 - val_loss: 0.6278 - val_accuracy: 0.6939\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6240 - val_loss: 0.6274 - val_accuracy: 0.6913\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6247 - val_loss: 0.6230 - val_accuracy: 0.6913\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6479 - accuracy: 0.6325 - val_loss: 0.6214 - val_accuracy: 0.6862\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6243 - val_loss: 0.6229 - val_accuracy: 0.6888\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6225 - val_loss: 0.6232 - val_accuracy: 0.6901\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6220 - val_loss: 0.6230 - val_accuracy: 0.6913\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6323 - val_loss: 0.6230 - val_accuracy: 0.6888\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6506 - accuracy: 0.6295 - val_loss: 0.6308 - val_accuracy: 0.6862\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6284 - val_loss: 0.6199 - val_accuracy: 0.6913\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6316 - val_loss: 0.6240 - val_accuracy: 0.6849\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6230 - val_loss: 0.6222 - val_accuracy: 0.6888\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6331 - val_loss: 0.6201 - val_accuracy: 0.6849\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6345 - val_loss: 0.6243 - val_accuracy: 0.6837\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6319 - val_loss: 0.6251 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6329 - val_loss: 0.6245 - val_accuracy: 0.6837\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6341 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6348 - val_loss: 0.6210 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6416 - accuracy: 0.6397 - val_loss: 0.6284 - val_accuracy: 0.6671\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6361 - val_loss: 0.6205 - val_accuracy: 0.6888\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6387 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6361 - val_loss: 0.6228 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6372 - accuracy: 0.6378 - val_loss: 0.6179 - val_accuracy: 0.6888\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6373 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6384 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6410 - val_loss: 0.6186 - val_accuracy: 0.6773\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6458 - val_loss: 0.6165 - val_accuracy: 0.6773\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6415 - val_loss: 0.6149 - val_accuracy: 0.6786\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6436 - val_loss: 0.6149 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6431 - val_loss: 0.6170 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6431 - val_loss: 0.6228 - val_accuracy: 0.6722\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6443 - val_loss: 0.6163 - val_accuracy: 0.6837\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6454 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6480 - val_loss: 0.6144 - val_accuracy: 0.6862\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6517 - val_loss: 0.6131 - val_accuracy: 0.6849\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6526 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6495 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6508 - val_loss: 0.6168 - val_accuracy: 0.6875\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6496 - val_loss: 0.6144 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6482 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6473 - val_loss: 0.6126 - val_accuracy: 0.6862\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6526 - val_loss: 0.6132 - val_accuracy: 0.6952\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6541 - val_loss: 0.6158 - val_accuracy: 0.6849\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6493 - val_loss: 0.6152 - val_accuracy: 0.6888\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6557 - val_loss: 0.6159 - val_accuracy: 0.6888\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6572 - val_loss: 0.6107 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6550 - val_loss: 0.6112 - val_accuracy: 0.6901\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6492 - val_loss: 0.6123 - val_accuracy: 0.6939\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6562 - val_loss: 0.6177 - val_accuracy: 0.6888\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6567 - val_loss: 0.6185 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6577 - val_loss: 0.6107 - val_accuracy: 0.6952\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6511 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6583 - val_loss: 0.6095 - val_accuracy: 0.6913\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6647 - val_loss: 0.6138 - val_accuracy: 0.6913\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6536 - val_loss: 0.6169 - val_accuracy: 0.6952\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6559 - val_loss: 0.6131 - val_accuracy: 0.6939\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6581 - val_loss: 0.6110 - val_accuracy: 0.6926\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6600 - val_loss: 0.6112 - val_accuracy: 0.6939\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6199 - accuracy: 0.6625 - val_loss: 0.6093 - val_accuracy: 0.6990\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6184 - accuracy: 0.6652 - val_loss: 0.6123 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6654 - val_loss: 0.6100 - val_accuracy: 0.7003\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6590 - val_loss: 0.6109 - val_accuracy: 0.6977\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6605 - val_loss: 0.6142 - val_accuracy: 0.6952\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6610 - val_loss: 0.6134 - val_accuracy: 0.6901\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6634 - val_loss: 0.6150 - val_accuracy: 0.6939\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6599 - val_loss: 0.6134 - val_accuracy: 0.6926\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6618 - val_loss: 0.6123 - val_accuracy: 0.6964\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6639 - val_loss: 0.6143 - val_accuracy: 0.7003\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6162 - accuracy: 0.6643 - val_loss: 0.6158 - val_accuracy: 0.6939\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6616 - val_loss: 0.6136 - val_accuracy: 0.6990\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6154 - accuracy: 0.6630 - val_loss: 0.6088 - val_accuracy: 0.7015\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6647 - val_loss: 0.6096 - val_accuracy: 0.7015\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6691 - val_loss: 0.6090 - val_accuracy: 0.6952\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6691 - val_loss: 0.6088 - val_accuracy: 0.6990\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.6675 - val_loss: 0.6115 - val_accuracy: 0.6926\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6644 - val_loss: 0.6142 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6634 - val_loss: 0.6131 - val_accuracy: 0.6926\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6128 - accuracy: 0.6729 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6713 - val_loss: 0.6158 - val_accuracy: 0.6862\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6075 - accuracy: 0.6751 - val_loss: 0.6205 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6071 - accuracy: 0.6712 - val_loss: 0.6139 - val_accuracy: 0.6926\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6694 - val_loss: 0.6148 - val_accuracy: 0.6888\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6093 - accuracy: 0.6693 - val_loss: 0.6176 - val_accuracy: 0.6875\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6760 - val_loss: 0.6145 - val_accuracy: 0.6837\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6668 - val_loss: 0.6181 - val_accuracy: 0.6888\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6098 - accuracy: 0.6704 - val_loss: 0.6128 - val_accuracy: 0.6926\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6714 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6082 - accuracy: 0.6785 - val_loss: 0.6187 - val_accuracy: 0.6811\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6073 - accuracy: 0.6729 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6145 - accuracy: 0.6644 - val_loss: 0.6154 - val_accuracy: 0.6811\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6088 - accuracy: 0.6760 - val_loss: 0.6168 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6101 - accuracy: 0.6740 - val_loss: 0.6148 - val_accuracy: 0.6913\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6069 - accuracy: 0.6727 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6084 - accuracy: 0.6689 - val_loss: 0.6154 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6088 - accuracy: 0.6751 - val_loss: 0.6183 - val_accuracy: 0.6786\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6036 - accuracy: 0.6812 - val_loss: 0.6166 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6042 - accuracy: 0.6788 - val_loss: 0.6184 - val_accuracy: 0.6824\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6029 - accuracy: 0.6794 - val_loss: 0.6183 - val_accuracy: 0.6849\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6037 - accuracy: 0.6829 - val_loss: 0.6138 - val_accuracy: 0.6888\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6045 - accuracy: 0.6822 - val_loss: 0.6154 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6822 - val_loss: 0.6182 - val_accuracy: 0.6786\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5996 - accuracy: 0.6871 - val_loss: 0.6192 - val_accuracy: 0.6798\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6009 - accuracy: 0.6805 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6027 - accuracy: 0.6761 - val_loss: 0.6164 - val_accuracy: 0.6901\n",
      "Calculating for: 700 250 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_152 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8582 - accuracy: 0.4956 - val_loss: 0.6875 - val_accuracy: 0.5651\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7378 - accuracy: 0.5108 - val_loss: 0.6657 - val_accuracy: 0.6352\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7056 - accuracy: 0.5235 - val_loss: 0.6614 - val_accuracy: 0.6186\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6971 - accuracy: 0.5220 - val_loss: 0.6694 - val_accuracy: 0.6212\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5274 - val_loss: 0.6691 - val_accuracy: 0.6224\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.5383 - val_loss: 0.6640 - val_accuracy: 0.6199\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6908 - accuracy: 0.5357 - val_loss: 0.6665 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5345 - val_loss: 0.6660 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6895 - accuracy: 0.5365 - val_loss: 0.6656 - val_accuracy: 0.6186\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5301 - val_loss: 0.6701 - val_accuracy: 0.6199\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6882 - accuracy: 0.5435 - val_loss: 0.6647 - val_accuracy: 0.6212\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6893 - accuracy: 0.5345 - val_loss: 0.6636 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5289 - val_loss: 0.6661 - val_accuracy: 0.6199\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6905 - accuracy: 0.5239 - val_loss: 0.6672 - val_accuracy: 0.6199\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5383 - val_loss: 0.6657 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6895 - accuracy: 0.5358 - val_loss: 0.6650 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5379 - val_loss: 0.6646 - val_accuracy: 0.6224\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6887 - accuracy: 0.5505 - val_loss: 0.6598 - val_accuracy: 0.6224\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6858 - accuracy: 0.5491 - val_loss: 0.6602 - val_accuracy: 0.6212\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.5484 - val_loss: 0.6618 - val_accuracy: 0.6276\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6847 - accuracy: 0.5502 - val_loss: 0.6555 - val_accuracy: 0.6263\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5463 - val_loss: 0.6561 - val_accuracy: 0.6301\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5416 - val_loss: 0.6624 - val_accuracy: 0.6301\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6870 - accuracy: 0.5504 - val_loss: 0.6582 - val_accuracy: 0.6288\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5473 - val_loss: 0.6578 - val_accuracy: 0.6288\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5558 - val_loss: 0.6590 - val_accuracy: 0.6314\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5521 - val_loss: 0.6567 - val_accuracy: 0.6250\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6830 - accuracy: 0.5574 - val_loss: 0.6542 - val_accuracy: 0.6314\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5543 - val_loss: 0.6542 - val_accuracy: 0.6339\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6871 - accuracy: 0.5534 - val_loss: 0.6591 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5526 - val_loss: 0.6555 - val_accuracy: 0.6339\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6847 - accuracy: 0.5490 - val_loss: 0.6527 - val_accuracy: 0.6352\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5569 - val_loss: 0.6531 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5533 - val_loss: 0.6514 - val_accuracy: 0.6339\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6798 - accuracy: 0.5663 - val_loss: 0.6486 - val_accuracy: 0.6390\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6830 - accuracy: 0.5599 - val_loss: 0.6509 - val_accuracy: 0.6365\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6843 - accuracy: 0.5568 - val_loss: 0.6542 - val_accuracy: 0.6390\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.5730 - val_loss: 0.6473 - val_accuracy: 0.6352\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5744 - val_loss: 0.6454 - val_accuracy: 0.6416\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5664 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6815 - accuracy: 0.5627 - val_loss: 0.6494 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6805 - accuracy: 0.5697 - val_loss: 0.6460 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.5639 - val_loss: 0.6470 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6791 - accuracy: 0.5683 - val_loss: 0.6448 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6834 - accuracy: 0.5590 - val_loss: 0.6475 - val_accuracy: 0.6403\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6814 - accuracy: 0.5698 - val_loss: 0.6465 - val_accuracy: 0.6416\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6797 - accuracy: 0.5691 - val_loss: 0.6456 - val_accuracy: 0.6416\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5760 - val_loss: 0.6431 - val_accuracy: 0.6403\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5732 - val_loss: 0.6430 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6777 - accuracy: 0.5663 - val_loss: 0.6426 - val_accuracy: 0.6441\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6809 - accuracy: 0.5664 - val_loss: 0.6473 - val_accuracy: 0.6416\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5687 - val_loss: 0.6474 - val_accuracy: 0.6429\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6784 - accuracy: 0.5746 - val_loss: 0.6448 - val_accuracy: 0.6429\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5703 - val_loss: 0.6435 - val_accuracy: 0.6441\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5717 - val_loss: 0.6425 - val_accuracy: 0.6429\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.5710 - val_loss: 0.6413 - val_accuracy: 0.6429\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5731 - val_loss: 0.6419 - val_accuracy: 0.6429\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6736 - accuracy: 0.5830 - val_loss: 0.6402 - val_accuracy: 0.6441\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6744 - accuracy: 0.5828 - val_loss: 0.6428 - val_accuracy: 0.6531\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5820 - val_loss: 0.6423 - val_accuracy: 0.6480\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6766 - accuracy: 0.5735 - val_loss: 0.6391 - val_accuracy: 0.6429\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6771 - accuracy: 0.5717 - val_loss: 0.6417 - val_accuracy: 0.6467\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6719 - accuracy: 0.5828 - val_loss: 0.6377 - val_accuracy: 0.6441\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5795 - val_loss: 0.6393 - val_accuracy: 0.6454\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6742 - accuracy: 0.5823 - val_loss: 0.6391 - val_accuracy: 0.6429\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6719 - accuracy: 0.5882 - val_loss: 0.6364 - val_accuracy: 0.6518\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6717 - accuracy: 0.5840 - val_loss: 0.6396 - val_accuracy: 0.6582\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5785 - val_loss: 0.6406 - val_accuracy: 0.6492\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6753 - accuracy: 0.5849 - val_loss: 0.6392 - val_accuracy: 0.6429\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6702 - accuracy: 0.5918 - val_loss: 0.6363 - val_accuracy: 0.6467\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6710 - accuracy: 0.5887 - val_loss: 0.6359 - val_accuracy: 0.6467\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5936 - val_loss: 0.6383 - val_accuracy: 0.6480\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5865 - val_loss: 0.6367 - val_accuracy: 0.6556\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5921 - val_loss: 0.6344 - val_accuracy: 0.6531\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5888 - val_loss: 0.6330 - val_accuracy: 0.6556\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5922 - val_loss: 0.6340 - val_accuracy: 0.6492\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5848 - val_loss: 0.6341 - val_accuracy: 0.6492\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5872 - val_loss: 0.6347 - val_accuracy: 0.6492\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5878 - val_loss: 0.6371 - val_accuracy: 0.6582\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5927 - val_loss: 0.6365 - val_accuracy: 0.6658\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6722 - accuracy: 0.5813 - val_loss: 0.6350 - val_accuracy: 0.6607\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5950 - val_loss: 0.6310 - val_accuracy: 0.6531\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5909 - val_loss: 0.6336 - val_accuracy: 0.6556\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5970 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5934 - val_loss: 0.6308 - val_accuracy: 0.6543\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6000 - val_loss: 0.6322 - val_accuracy: 0.6569\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6653 - accuracy: 0.6017 - val_loss: 0.6322 - val_accuracy: 0.6556\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6680 - accuracy: 0.5896 - val_loss: 0.6341 - val_accuracy: 0.6582\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5986 - val_loss: 0.6313 - val_accuracy: 0.6607\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.6006 - val_loss: 0.6307 - val_accuracy: 0.6531\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5962 - val_loss: 0.6329 - val_accuracy: 0.6633\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5978 - val_loss: 0.6313 - val_accuracy: 0.6786\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5981 - val_loss: 0.6299 - val_accuracy: 0.6747\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5955 - val_loss: 0.6308 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6037 - val_loss: 0.6319 - val_accuracy: 0.6709\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6674 - accuracy: 0.5960 - val_loss: 0.6306 - val_accuracy: 0.6735\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.5988 - val_loss: 0.6296 - val_accuracy: 0.6747\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5971 - val_loss: 0.6313 - val_accuracy: 0.6671\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6016 - val_loss: 0.6306 - val_accuracy: 0.6671\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6044 - val_loss: 0.6295 - val_accuracy: 0.6620\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6172 - val_loss: 0.6275 - val_accuracy: 0.6594\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6088 - val_loss: 0.6312 - val_accuracy: 0.6747\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.6014 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6006 - val_loss: 0.6281 - val_accuracy: 0.6735\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6046 - val_loss: 0.6293 - val_accuracy: 0.6709\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6004 - val_loss: 0.6287 - val_accuracy: 0.6735\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6042 - val_loss: 0.6303 - val_accuracy: 0.6722\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.6024 - val_loss: 0.6266 - val_accuracy: 0.6735\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5955 - val_loss: 0.6279 - val_accuracy: 0.6747\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.6039 - val_loss: 0.6280 - val_accuracy: 0.6684\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6117 - val_loss: 0.6244 - val_accuracy: 0.6747\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6101 - val_loss: 0.6248 - val_accuracy: 0.6696\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6052 - val_loss: 0.6284 - val_accuracy: 0.6696\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6019 - val_loss: 0.6270 - val_accuracy: 0.6747\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6114 - val_loss: 0.6249 - val_accuracy: 0.6722\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6095 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6166 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5993 - val_loss: 0.6285 - val_accuracy: 0.6684\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6027 - val_loss: 0.6260 - val_accuracy: 0.6645\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6117 - val_loss: 0.6251 - val_accuracy: 0.6645\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6076 - val_loss: 0.6252 - val_accuracy: 0.6709\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6095 - val_loss: 0.6278 - val_accuracy: 0.6709\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6085 - val_loss: 0.6258 - val_accuracy: 0.6658\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5992 - val_loss: 0.6288 - val_accuracy: 0.6671\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6044 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6088 - val_loss: 0.6261 - val_accuracy: 0.6747\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6127 - val_loss: 0.6293 - val_accuracy: 0.6722\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6101 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6145 - val_loss: 0.6219 - val_accuracy: 0.6696\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6061 - val_loss: 0.6283 - val_accuracy: 0.6760\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6089 - val_loss: 0.6244 - val_accuracy: 0.6735\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6173 - val_loss: 0.6217 - val_accuracy: 0.6645\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6154 - val_loss: 0.6214 - val_accuracy: 0.6671\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6052 - val_loss: 0.6220 - val_accuracy: 0.6684\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6071 - val_loss: 0.6254 - val_accuracy: 0.6760\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6133 - val_loss: 0.6217 - val_accuracy: 0.6696\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6152 - val_loss: 0.6241 - val_accuracy: 0.6722\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6215 - val_loss: 0.6222 - val_accuracy: 0.6773\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6154 - val_loss: 0.6230 - val_accuracy: 0.6811\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6114 - val_loss: 0.6245 - val_accuracy: 0.6786\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6154 - val_loss: 0.6198 - val_accuracy: 0.6747\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6127 - val_loss: 0.6250 - val_accuracy: 0.6747\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6192 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6089 - val_loss: 0.6252 - val_accuracy: 0.6798\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6144 - val_loss: 0.6209 - val_accuracy: 0.6747\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6101 - val_loss: 0.6264 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6182 - val_loss: 0.6238 - val_accuracy: 0.6735\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6198 - val_loss: 0.6219 - val_accuracy: 0.6671\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6117 - val_loss: 0.6222 - val_accuracy: 0.6696\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6197 - val_loss: 0.6233 - val_accuracy: 0.6786\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6220 - val_loss: 0.6218 - val_accuracy: 0.6773\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6109 - val_loss: 0.6254 - val_accuracy: 0.6837\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6587 - accuracy: 0.6157 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6227 - val_loss: 0.6179 - val_accuracy: 0.6760\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6172 - val_loss: 0.6215 - val_accuracy: 0.6824\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6167 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6119 - val_loss: 0.6224 - val_accuracy: 0.6709\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6149 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6163 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6161 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6181 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6209 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6188 - val_loss: 0.6176 - val_accuracy: 0.6811\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6148 - val_loss: 0.6180 - val_accuracy: 0.6722\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6208 - val_loss: 0.6178 - val_accuracy: 0.6786\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6198 - val_loss: 0.6236 - val_accuracy: 0.6709\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6285 - val_loss: 0.6213 - val_accuracy: 0.6811\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6192 - val_loss: 0.6185 - val_accuracy: 0.6798\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6172 - val_loss: 0.6233 - val_accuracy: 0.6849\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6240 - val_loss: 0.6185 - val_accuracy: 0.6837\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6230 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6193 - val_loss: 0.6179 - val_accuracy: 0.6773\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6223 - val_loss: 0.6179 - val_accuracy: 0.6786\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6248 - val_loss: 0.6184 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6208 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6223 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6177 - val_loss: 0.6182 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6206 - val_loss: 0.6181 - val_accuracy: 0.6837\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6197 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6191 - val_loss: 0.6179 - val_accuracy: 0.6760\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6225 - val_loss: 0.6155 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6243 - val_loss: 0.6223 - val_accuracy: 0.6798\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6261 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6264 - val_loss: 0.6180 - val_accuracy: 0.6773\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6251 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6326 - val_loss: 0.6146 - val_accuracy: 0.6811\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6262 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6256 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6233 - val_loss: 0.6196 - val_accuracy: 0.6773\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6286 - val_loss: 0.6156 - val_accuracy: 0.6798\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6314 - val_loss: 0.6166 - val_accuracy: 0.6824\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6218 - val_loss: 0.6196 - val_accuracy: 0.6824\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6217 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6274 - val_loss: 0.6169 - val_accuracy: 0.6798\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6321 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6269 - val_loss: 0.6165 - val_accuracy: 0.6773\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6310 - val_loss: 0.6166 - val_accuracy: 0.6798\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6290 - val_loss: 0.6170 - val_accuracy: 0.6824\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6228 - val_loss: 0.6191 - val_accuracy: 0.6837\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6496 - accuracy: 0.6299 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Calculating for: 700 400 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_156 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7884 - accuracy: 0.5536 - val_loss: 0.6536 - val_accuracy: 0.6798\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6941 - accuracy: 0.5882 - val_loss: 0.6481 - val_accuracy: 0.6620\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6720 - accuracy: 0.6004 - val_loss: 0.6423 - val_accuracy: 0.6569\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6061 - val_loss: 0.6378 - val_accuracy: 0.6645\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6603 - accuracy: 0.6086 - val_loss: 0.6355 - val_accuracy: 0.6786\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6590 - accuracy: 0.6113 - val_loss: 0.6313 - val_accuracy: 0.6709\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6525 - accuracy: 0.6256 - val_loss: 0.6330 - val_accuracy: 0.6760\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6529 - accuracy: 0.6155 - val_loss: 0.6335 - val_accuracy: 0.6735\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6501 - accuracy: 0.6228 - val_loss: 0.6281 - val_accuracy: 0.6798\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6261 - val_loss: 0.6275 - val_accuracy: 0.6747\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6436 - accuracy: 0.6361 - val_loss: 0.6242 - val_accuracy: 0.6773\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6424 - accuracy: 0.6329 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6446 - accuracy: 0.6286 - val_loss: 0.6257 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6434 - accuracy: 0.6345 - val_loss: 0.6228 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6403 - accuracy: 0.6380 - val_loss: 0.6242 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6412 - val_loss: 0.6229 - val_accuracy: 0.6837\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6336 - val_loss: 0.6256 - val_accuracy: 0.6671\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6352 - accuracy: 0.6410 - val_loss: 0.6197 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6439 - val_loss: 0.6224 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6318 - accuracy: 0.6466 - val_loss: 0.6218 - val_accuracy: 0.6658\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6315 - accuracy: 0.6462 - val_loss: 0.6201 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6329 - accuracy: 0.6476 - val_loss: 0.6200 - val_accuracy: 0.6747\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6302 - accuracy: 0.6518 - val_loss: 0.6208 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6284 - accuracy: 0.6510 - val_loss: 0.6196 - val_accuracy: 0.6773\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6278 - accuracy: 0.6535 - val_loss: 0.6213 - val_accuracy: 0.6811\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6240 - accuracy: 0.6555 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6203 - accuracy: 0.6588 - val_loss: 0.6172 - val_accuracy: 0.6926\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6259 - accuracy: 0.6495 - val_loss: 0.6211 - val_accuracy: 0.6811\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6216 - accuracy: 0.6618 - val_loss: 0.6148 - val_accuracy: 0.6990\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6166 - accuracy: 0.6629 - val_loss: 0.6149 - val_accuracy: 0.6888\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6206 - accuracy: 0.6629 - val_loss: 0.6167 - val_accuracy: 0.6901\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6183 - accuracy: 0.6663 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6167 - accuracy: 0.6616 - val_loss: 0.6166 - val_accuracy: 0.6913\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6159 - accuracy: 0.6608 - val_loss: 0.6194 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6113 - accuracy: 0.6701 - val_loss: 0.6247 - val_accuracy: 0.6798\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6146 - accuracy: 0.6631 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6065 - accuracy: 0.6721 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6104 - accuracy: 0.6752 - val_loss: 0.6188 - val_accuracy: 0.6875\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6090 - accuracy: 0.6662 - val_loss: 0.6211 - val_accuracy: 0.6862\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6085 - accuracy: 0.6727 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6040 - accuracy: 0.6765 - val_loss: 0.6230 - val_accuracy: 0.6888\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6057 - accuracy: 0.6723 - val_loss: 0.6187 - val_accuracy: 0.6837\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6088 - accuracy: 0.6750 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6028 - accuracy: 0.6765 - val_loss: 0.6236 - val_accuracy: 0.6939\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6032 - accuracy: 0.6760 - val_loss: 0.6222 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5970 - accuracy: 0.6830 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5954 - accuracy: 0.6830 - val_loss: 0.6243 - val_accuracy: 0.6786\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5966 - accuracy: 0.6802 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5999 - accuracy: 0.6807 - val_loss: 0.6278 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6017 - accuracy: 0.6768 - val_loss: 0.6228 - val_accuracy: 0.6913\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5956 - accuracy: 0.6797 - val_loss: 0.6236 - val_accuracy: 0.6913\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5958 - accuracy: 0.6829 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5944 - accuracy: 0.6858 - val_loss: 0.6289 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5927 - accuracy: 0.6894 - val_loss: 0.6343 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5962 - accuracy: 0.6832 - val_loss: 0.6283 - val_accuracy: 0.6862\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5949 - accuracy: 0.6831 - val_loss: 0.6237 - val_accuracy: 0.6824\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5908 - accuracy: 0.6858 - val_loss: 0.6330 - val_accuracy: 0.6786\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5903 - accuracy: 0.6874 - val_loss: 0.6305 - val_accuracy: 0.6837\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5887 - accuracy: 0.6937 - val_loss: 0.6299 - val_accuracy: 0.6824\n",
      "Calculating for: 700 400 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_160 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8120 - accuracy: 0.5345 - val_loss: 0.6454 - val_accuracy: 0.6352\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7088 - accuracy: 0.5634 - val_loss: 0.6418 - val_accuracy: 0.6633\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6824 - accuracy: 0.5836 - val_loss: 0.6400 - val_accuracy: 0.6658\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6791 - accuracy: 0.5781 - val_loss: 0.6379 - val_accuracy: 0.6582\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6755 - accuracy: 0.5808 - val_loss: 0.6389 - val_accuracy: 0.6620\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6708 - accuracy: 0.5882 - val_loss: 0.6385 - val_accuracy: 0.6671\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.5929 - val_loss: 0.6380 - val_accuracy: 0.6671\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6680 - accuracy: 0.5975 - val_loss: 0.6369 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6660 - accuracy: 0.6007 - val_loss: 0.6316 - val_accuracy: 0.6684\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6685 - accuracy: 0.5941 - val_loss: 0.6346 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6637 - accuracy: 0.5968 - val_loss: 0.6384 - val_accuracy: 0.6747\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6634 - accuracy: 0.6010 - val_loss: 0.6331 - val_accuracy: 0.6709\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6615 - accuracy: 0.6148 - val_loss: 0.6314 - val_accuracy: 0.6709\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6592 - accuracy: 0.6060 - val_loss: 0.6315 - val_accuracy: 0.6709\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6032 - val_loss: 0.6370 - val_accuracy: 0.6773\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6587 - accuracy: 0.6112 - val_loss: 0.6335 - val_accuracy: 0.6824\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6573 - accuracy: 0.6115 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6563 - accuracy: 0.6130 - val_loss: 0.6257 - val_accuracy: 0.6875\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6560 - accuracy: 0.6105 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6528 - accuracy: 0.6152 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6562 - accuracy: 0.6186 - val_loss: 0.6238 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6525 - accuracy: 0.6246 - val_loss: 0.6236 - val_accuracy: 0.6773\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6542 - accuracy: 0.6215 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6554 - accuracy: 0.6148 - val_loss: 0.6255 - val_accuracy: 0.6811\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6522 - accuracy: 0.6265 - val_loss: 0.6216 - val_accuracy: 0.6875\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6497 - accuracy: 0.6272 - val_loss: 0.6226 - val_accuracy: 0.6824\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6159 - val_loss: 0.6191 - val_accuracy: 0.6862\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6517 - accuracy: 0.6238 - val_loss: 0.6238 - val_accuracy: 0.6760\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6499 - accuracy: 0.6247 - val_loss: 0.6219 - val_accuracy: 0.6786\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6498 - accuracy: 0.6222 - val_loss: 0.6250 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6491 - accuracy: 0.6255 - val_loss: 0.6242 - val_accuracy: 0.6798\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6485 - accuracy: 0.6240 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6493 - accuracy: 0.6206 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6476 - accuracy: 0.6217 - val_loss: 0.6198 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6296 - val_loss: 0.6180 - val_accuracy: 0.6811\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6466 - accuracy: 0.6252 - val_loss: 0.6210 - val_accuracy: 0.6837\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6429 - accuracy: 0.6320 - val_loss: 0.6204 - val_accuracy: 0.6849\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6260 - val_loss: 0.6198 - val_accuracy: 0.6837\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6453 - accuracy: 0.6270 - val_loss: 0.6192 - val_accuracy: 0.6849\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6440 - accuracy: 0.6360 - val_loss: 0.6195 - val_accuracy: 0.6849\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6463 - accuracy: 0.6269 - val_loss: 0.6169 - val_accuracy: 0.6875\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6227 - val_loss: 0.6212 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6453 - accuracy: 0.6318 - val_loss: 0.6169 - val_accuracy: 0.6913\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6424 - accuracy: 0.6333 - val_loss: 0.6204 - val_accuracy: 0.6773\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6386 - accuracy: 0.6400 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6387 - accuracy: 0.6404 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6391 - accuracy: 0.6409 - val_loss: 0.6170 - val_accuracy: 0.6786\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6406 - accuracy: 0.6370 - val_loss: 0.6176 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6418 - accuracy: 0.6388 - val_loss: 0.6190 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6393 - accuracy: 0.6374 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6381 - accuracy: 0.6336 - val_loss: 0.6151 - val_accuracy: 0.6824\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6417 - val_loss: 0.6137 - val_accuracy: 0.6811\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6395 - accuracy: 0.6405 - val_loss: 0.6139 - val_accuracy: 0.6875\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6405 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6413 - val_loss: 0.6132 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6323 - accuracy: 0.6513 - val_loss: 0.6152 - val_accuracy: 0.6926\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6365 - accuracy: 0.6414 - val_loss: 0.6191 - val_accuracy: 0.6875\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6463 - val_loss: 0.6126 - val_accuracy: 0.6990\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6365 - accuracy: 0.6446 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6476 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6368 - accuracy: 0.6462 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6462 - val_loss: 0.6125 - val_accuracy: 0.6952\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6327 - accuracy: 0.6469 - val_loss: 0.6147 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6338 - accuracy: 0.6463 - val_loss: 0.6164 - val_accuracy: 0.6862\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6299 - accuracy: 0.6551 - val_loss: 0.6138 - val_accuracy: 0.6862\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6354 - accuracy: 0.6491 - val_loss: 0.6146 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6488 - val_loss: 0.6158 - val_accuracy: 0.6849\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6451 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6540 - val_loss: 0.6095 - val_accuracy: 0.6926\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6594 - val_loss: 0.6179 - val_accuracy: 0.6811\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6276 - accuracy: 0.6506 - val_loss: 0.6127 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6258 - accuracy: 0.6591 - val_loss: 0.6123 - val_accuracy: 0.6888\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6283 - accuracy: 0.6513 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6279 - accuracy: 0.6537 - val_loss: 0.6135 - val_accuracy: 0.6901\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6572 - val_loss: 0.6094 - val_accuracy: 0.6977\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6240 - accuracy: 0.6530 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6269 - accuracy: 0.6541 - val_loss: 0.6154 - val_accuracy: 0.6926\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6207 - accuracy: 0.6564 - val_loss: 0.6135 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6223 - accuracy: 0.6541 - val_loss: 0.6140 - val_accuracy: 0.6888\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6245 - accuracy: 0.6556 - val_loss: 0.6139 - val_accuracy: 0.6875\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6198 - accuracy: 0.6611 - val_loss: 0.6114 - val_accuracy: 0.6926\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6249 - accuracy: 0.6579 - val_loss: 0.6106 - val_accuracy: 0.6913\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6222 - accuracy: 0.6647 - val_loss: 0.6113 - val_accuracy: 0.6939\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6202 - accuracy: 0.6662 - val_loss: 0.6112 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 3s 10ms/step - loss: 0.6173 - accuracy: 0.6668 - val_loss: 0.6125 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6195 - accuracy: 0.6614 - val_loss: 0.6113 - val_accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6191 - accuracy: 0.6650 - val_loss: 0.6097 - val_accuracy: 0.6926\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6203 - accuracy: 0.6589 - val_loss: 0.6129 - val_accuracy: 0.6811\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6193 - accuracy: 0.6609 - val_loss: 0.6109 - val_accuracy: 0.6901\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6194 - accuracy: 0.6635 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6196 - accuracy: 0.6707 - val_loss: 0.6127 - val_accuracy: 0.6939\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6183 - accuracy: 0.6615 - val_loss: 0.6132 - val_accuracy: 0.6913\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6163 - accuracy: 0.6634 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6123 - accuracy: 0.6677 - val_loss: 0.6123 - val_accuracy: 0.6862\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6179 - accuracy: 0.6647 - val_loss: 0.6123 - val_accuracy: 0.6888\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6143 - accuracy: 0.6692 - val_loss: 0.6148 - val_accuracy: 0.6824\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6185 - accuracy: 0.6701 - val_loss: 0.6151 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6146 - accuracy: 0.6648 - val_loss: 0.6138 - val_accuracy: 0.6901\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6100 - accuracy: 0.6756 - val_loss: 0.6129 - val_accuracy: 0.6926\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6119 - accuracy: 0.6708 - val_loss: 0.6174 - val_accuracy: 0.6849\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6113 - accuracy: 0.6687 - val_loss: 0.6126 - val_accuracy: 0.6875\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6128 - accuracy: 0.6625 - val_loss: 0.6161 - val_accuracy: 0.6824\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6107 - accuracy: 0.6753 - val_loss: 0.6133 - val_accuracy: 0.6875\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6123 - accuracy: 0.6716 - val_loss: 0.6139 - val_accuracy: 0.6824\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6100 - accuracy: 0.6718 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Calculating for: 700 400 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_164 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.8457 - accuracy: 0.5013 - val_loss: 0.6613 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7340 - accuracy: 0.5152 - val_loss: 0.6604 - val_accuracy: 0.6212\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7008 - accuracy: 0.5335 - val_loss: 0.6623 - val_accuracy: 0.6224\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6970 - accuracy: 0.5226 - val_loss: 0.6661 - val_accuracy: 0.6224\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6924 - accuracy: 0.5276 - val_loss: 0.6621 - val_accuracy: 0.6199\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6913 - accuracy: 0.5315 - val_loss: 0.6630 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6887 - accuracy: 0.5391 - val_loss: 0.6624 - val_accuracy: 0.6199\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6892 - accuracy: 0.5473 - val_loss: 0.6637 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6906 - accuracy: 0.5308 - val_loss: 0.6645 - val_accuracy: 0.6224\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.6901 - accuracy: 0.5359 - val_loss: 0.6640 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6908 - accuracy: 0.5325 - val_loss: 0.6636 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6899 - accuracy: 0.5413 - val_loss: 0.6647 - val_accuracy: 0.6237\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6891 - accuracy: 0.5387 - val_loss: 0.6616 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6890 - accuracy: 0.5453 - val_loss: 0.6614 - val_accuracy: 0.6224\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6859 - accuracy: 0.5511 - val_loss: 0.6589 - val_accuracy: 0.6237\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6870 - accuracy: 0.5378 - val_loss: 0.6581 - val_accuracy: 0.6237\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6882 - accuracy: 0.5470 - val_loss: 0.6571 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6870 - accuracy: 0.5457 - val_loss: 0.6584 - val_accuracy: 0.6250\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6878 - accuracy: 0.5465 - val_loss: 0.6577 - val_accuracy: 0.6250\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 3s 12ms/step - loss: 0.6862 - accuracy: 0.5528 - val_loss: 0.6566 - val_accuracy: 0.6263\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6862 - accuracy: 0.5529 - val_loss: 0.6544 - val_accuracy: 0.6250\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6868 - accuracy: 0.5489 - val_loss: 0.6554 - val_accuracy: 0.6250\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6854 - accuracy: 0.5558 - val_loss: 0.6571 - val_accuracy: 0.6263\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6831 - accuracy: 0.5608 - val_loss: 0.6535 - val_accuracy: 0.6263\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6846 - accuracy: 0.5551 - val_loss: 0.6539 - val_accuracy: 0.6276\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6855 - accuracy: 0.5502 - val_loss: 0.6548 - val_accuracy: 0.6276\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6824 - accuracy: 0.5566 - val_loss: 0.6517 - val_accuracy: 0.6352\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6844 - accuracy: 0.5539 - val_loss: 0.6509 - val_accuracy: 0.6352\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6823 - accuracy: 0.5599 - val_loss: 0.6546 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6814 - accuracy: 0.5674 - val_loss: 0.6525 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6825 - accuracy: 0.5647 - val_loss: 0.6511 - val_accuracy: 0.6390\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6815 - accuracy: 0.5676 - val_loss: 0.6510 - val_accuracy: 0.6365\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6831 - accuracy: 0.5570 - val_loss: 0.6507 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6794 - accuracy: 0.5703 - val_loss: 0.6485 - val_accuracy: 0.6390\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6801 - accuracy: 0.5727 - val_loss: 0.6485 - val_accuracy: 0.6403\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6814 - accuracy: 0.5672 - val_loss: 0.6492 - val_accuracy: 0.6429\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6805 - accuracy: 0.5604 - val_loss: 0.6488 - val_accuracy: 0.6429\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6804 - accuracy: 0.5661 - val_loss: 0.6482 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6782 - accuracy: 0.5757 - val_loss: 0.6492 - val_accuracy: 0.6441\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6797 - accuracy: 0.5648 - val_loss: 0.6476 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6780 - accuracy: 0.5642 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6753 - accuracy: 0.5791 - val_loss: 0.6453 - val_accuracy: 0.6441\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6777 - accuracy: 0.5750 - val_loss: 0.6455 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6776 - accuracy: 0.5716 - val_loss: 0.6438 - val_accuracy: 0.6454\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6790 - accuracy: 0.5751 - val_loss: 0.6473 - val_accuracy: 0.6441\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6770 - accuracy: 0.5716 - val_loss: 0.6458 - val_accuracy: 0.6429\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6756 - accuracy: 0.5764 - val_loss: 0.6461 - val_accuracy: 0.6403\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6747 - accuracy: 0.5803 - val_loss: 0.6435 - val_accuracy: 0.6416\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6741 - accuracy: 0.5835 - val_loss: 0.6430 - val_accuracy: 0.6403\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6742 - accuracy: 0.5841 - val_loss: 0.6440 - val_accuracy: 0.6403\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6753 - accuracy: 0.5764 - val_loss: 0.6449 - val_accuracy: 0.6416\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6748 - accuracy: 0.5863 - val_loss: 0.6432 - val_accuracy: 0.6403\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6737 - accuracy: 0.5821 - val_loss: 0.6421 - val_accuracy: 0.6403\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6753 - accuracy: 0.5805 - val_loss: 0.6434 - val_accuracy: 0.6403\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.5775 - val_loss: 0.6417 - val_accuracy: 0.6403\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6738 - accuracy: 0.5836 - val_loss: 0.6415 - val_accuracy: 0.6403\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6720 - accuracy: 0.5841 - val_loss: 0.6409 - val_accuracy: 0.6416\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6726 - accuracy: 0.5927 - val_loss: 0.6405 - val_accuracy: 0.6403\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6715 - accuracy: 0.5924 - val_loss: 0.6400 - val_accuracy: 0.6403\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6769 - accuracy: 0.5739 - val_loss: 0.6424 - val_accuracy: 0.6480\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6724 - accuracy: 0.5918 - val_loss: 0.6394 - val_accuracy: 0.6467\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6737 - accuracy: 0.5869 - val_loss: 0.6404 - val_accuracy: 0.6467\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5858 - val_loss: 0.6400 - val_accuracy: 0.6480\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6697 - accuracy: 0.5834 - val_loss: 0.6383 - val_accuracy: 0.6403\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6698 - accuracy: 0.5916 - val_loss: 0.6367 - val_accuracy: 0.6467\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6704 - accuracy: 0.5903 - val_loss: 0.6377 - val_accuracy: 0.6492\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6737 - accuracy: 0.5848 - val_loss: 0.6372 - val_accuracy: 0.6505\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6726 - accuracy: 0.5864 - val_loss: 0.6376 - val_accuracy: 0.6480\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6701 - accuracy: 0.5887 - val_loss: 0.6366 - val_accuracy: 0.6492\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6656 - accuracy: 0.6002 - val_loss: 0.6324 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6665 - accuracy: 0.5973 - val_loss: 0.6330 - val_accuracy: 0.6518\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6678 - accuracy: 0.6001 - val_loss: 0.6387 - val_accuracy: 0.6492\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6687 - accuracy: 0.5921 - val_loss: 0.6358 - val_accuracy: 0.6467\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6689 - accuracy: 0.5975 - val_loss: 0.6352 - val_accuracy: 0.6505\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6664 - accuracy: 0.5953 - val_loss: 0.6337 - val_accuracy: 0.6505\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6673 - accuracy: 0.5960 - val_loss: 0.6347 - val_accuracy: 0.6518\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6671 - accuracy: 0.5965 - val_loss: 0.6326 - val_accuracy: 0.6518\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6671 - accuracy: 0.6054 - val_loss: 0.6314 - val_accuracy: 0.6531\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6672 - accuracy: 0.5934 - val_loss: 0.6329 - val_accuracy: 0.6518\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6687 - accuracy: 0.5908 - val_loss: 0.6334 - val_accuracy: 0.6543\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6694 - accuracy: 0.5982 - val_loss: 0.6333 - val_accuracy: 0.6518\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6006 - val_loss: 0.6319 - val_accuracy: 0.6556\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6668 - accuracy: 0.6040 - val_loss: 0.6307 - val_accuracy: 0.6518\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.5995 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6665 - accuracy: 0.5967 - val_loss: 0.6330 - val_accuracy: 0.6492\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6063 - val_loss: 0.6309 - val_accuracy: 0.6543\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6652 - accuracy: 0.6031 - val_loss: 0.6318 - val_accuracy: 0.6518\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6024 - val_loss: 0.6303 - val_accuracy: 0.6569\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6655 - accuracy: 0.5978 - val_loss: 0.6314 - val_accuracy: 0.6658\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6035 - val_loss: 0.6288 - val_accuracy: 0.6531\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6071 - val_loss: 0.6288 - val_accuracy: 0.6582\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6638 - accuracy: 0.6054 - val_loss: 0.6284 - val_accuracy: 0.6569\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6089 - val_loss: 0.6287 - val_accuracy: 0.6569\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6066 - val_loss: 0.6298 - val_accuracy: 0.6569\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.6015 - val_loss: 0.6300 - val_accuracy: 0.6492\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6665 - accuracy: 0.6022 - val_loss: 0.6282 - val_accuracy: 0.6582\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6619 - accuracy: 0.6075 - val_loss: 0.6272 - val_accuracy: 0.6658\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.6004 - val_loss: 0.6278 - val_accuracy: 0.6658\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6632 - accuracy: 0.6052 - val_loss: 0.6256 - val_accuracy: 0.6645\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6036 - val_loss: 0.6283 - val_accuracy: 0.6607\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6618 - accuracy: 0.6104 - val_loss: 0.6285 - val_accuracy: 0.6582\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6025 - val_loss: 0.6277 - val_accuracy: 0.6594\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6629 - accuracy: 0.6069 - val_loss: 0.6285 - val_accuracy: 0.6531\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6607 - accuracy: 0.6144 - val_loss: 0.6257 - val_accuracy: 0.6735\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6085 - val_loss: 0.6238 - val_accuracy: 0.6684\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6641 - accuracy: 0.5983 - val_loss: 0.6292 - val_accuracy: 0.6735\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6597 - accuracy: 0.6108 - val_loss: 0.6248 - val_accuracy: 0.6696\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6633 - accuracy: 0.6060 - val_loss: 0.6252 - val_accuracy: 0.6722\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6636 - accuracy: 0.6095 - val_loss: 0.6249 - val_accuracy: 0.6709\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6627 - accuracy: 0.6100 - val_loss: 0.6237 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6608 - accuracy: 0.6081 - val_loss: 0.6257 - val_accuracy: 0.6696\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6605 - accuracy: 0.6107 - val_loss: 0.6220 - val_accuracy: 0.6709\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.6122 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6075 - val_loss: 0.6271 - val_accuracy: 0.6709\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6617 - accuracy: 0.6118 - val_loss: 0.6272 - val_accuracy: 0.6633\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6108 - val_loss: 0.6248 - val_accuracy: 0.6645\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6138 - val_loss: 0.6247 - val_accuracy: 0.6684\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6657 - accuracy: 0.6031 - val_loss: 0.6267 - val_accuracy: 0.6722\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6605 - accuracy: 0.6060 - val_loss: 0.6208 - val_accuracy: 0.6760\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6576 - accuracy: 0.6088 - val_loss: 0.6235 - val_accuracy: 0.6747\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6604 - accuracy: 0.6065 - val_loss: 0.6225 - val_accuracy: 0.6735\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6589 - accuracy: 0.6164 - val_loss: 0.6215 - val_accuracy: 0.6696\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6588 - accuracy: 0.6104 - val_loss: 0.6285 - val_accuracy: 0.6773\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6585 - accuracy: 0.6129 - val_loss: 0.6223 - val_accuracy: 0.6747\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6572 - accuracy: 0.6157 - val_loss: 0.6233 - val_accuracy: 0.6722\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6132 - val_loss: 0.6240 - val_accuracy: 0.6747\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6105 - val_loss: 0.6224 - val_accuracy: 0.6747\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6166 - val_loss: 0.6212 - val_accuracy: 0.6735\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6169 - val_loss: 0.6187 - val_accuracy: 0.6722\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6600 - accuracy: 0.6088 - val_loss: 0.6204 - val_accuracy: 0.6747\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6591 - accuracy: 0.6117 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6569 - accuracy: 0.6177 - val_loss: 0.6209 - val_accuracy: 0.6747\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6570 - accuracy: 0.6222 - val_loss: 0.6197 - val_accuracy: 0.6735\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6557 - accuracy: 0.6189 - val_loss: 0.6185 - val_accuracy: 0.6747\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6557 - accuracy: 0.6159 - val_loss: 0.6212 - val_accuracy: 0.6760\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6567 - accuracy: 0.6172 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6564 - accuracy: 0.6206 - val_loss: 0.6219 - val_accuracy: 0.6760\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6528 - accuracy: 0.6236 - val_loss: 0.6183 - val_accuracy: 0.6735\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6554 - accuracy: 0.6275 - val_loss: 0.6201 - val_accuracy: 0.6735\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6562 - accuracy: 0.6181 - val_loss: 0.6212 - val_accuracy: 0.6773\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6549 - accuracy: 0.6267 - val_loss: 0.6197 - val_accuracy: 0.6760\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6568 - accuracy: 0.6203 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6549 - accuracy: 0.6194 - val_loss: 0.6176 - val_accuracy: 0.6747\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6541 - accuracy: 0.6202 - val_loss: 0.6159 - val_accuracy: 0.6760\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6569 - accuracy: 0.6144 - val_loss: 0.6181 - val_accuracy: 0.6760\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6572 - accuracy: 0.6173 - val_loss: 0.6210 - val_accuracy: 0.6747\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6575 - accuracy: 0.6117 - val_loss: 0.6211 - val_accuracy: 0.6824\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6555 - accuracy: 0.6207 - val_loss: 0.6176 - val_accuracy: 0.6798\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6520 - accuracy: 0.6182 - val_loss: 0.6171 - val_accuracy: 0.6786\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6566 - accuracy: 0.6173 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6531 - accuracy: 0.6251 - val_loss: 0.6202 - val_accuracy: 0.6747\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6523 - accuracy: 0.6204 - val_loss: 0.6156 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6521 - accuracy: 0.6281 - val_loss: 0.6162 - val_accuracy: 0.6786\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6548 - accuracy: 0.6217 - val_loss: 0.6170 - val_accuracy: 0.6747\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6484 - accuracy: 0.6301 - val_loss: 0.6149 - val_accuracy: 0.6811\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6528 - accuracy: 0.6260 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6496 - accuracy: 0.6267 - val_loss: 0.6172 - val_accuracy: 0.6811\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 8s 34ms/step - loss: 0.6499 - accuracy: 0.6262 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 5s 20ms/step - loss: 0.6554 - accuracy: 0.6203 - val_loss: 0.6157 - val_accuracy: 0.6811\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6511 - accuracy: 0.6267 - val_loss: 0.6154 - val_accuracy: 0.6786\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6534 - accuracy: 0.6274 - val_loss: 0.6182 - val_accuracy: 0.6760\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6535 - accuracy: 0.6270 - val_loss: 0.6178 - val_accuracy: 0.6760\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6498 - accuracy: 0.6245 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6509 - accuracy: 0.6260 - val_loss: 0.6201 - val_accuracy: 0.6786\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6510 - accuracy: 0.6235 - val_loss: 0.6143 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6496 - accuracy: 0.6248 - val_loss: 0.6163 - val_accuracy: 0.6786\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6481 - accuracy: 0.6276 - val_loss: 0.6138 - val_accuracy: 0.6811\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6518 - accuracy: 0.6209 - val_loss: 0.6141 - val_accuracy: 0.6760\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6540 - accuracy: 0.6212 - val_loss: 0.6206 - val_accuracy: 0.6798\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6519 - accuracy: 0.6235 - val_loss: 0.6161 - val_accuracy: 0.6773\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6542 - accuracy: 0.6245 - val_loss: 0.6187 - val_accuracy: 0.6798\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6513 - accuracy: 0.6242 - val_loss: 0.6152 - val_accuracy: 0.6760\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6518 - accuracy: 0.6213 - val_loss: 0.6147 - val_accuracy: 0.6786\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6524 - accuracy: 0.6220 - val_loss: 0.6154 - val_accuracy: 0.6773\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6470 - accuracy: 0.6277 - val_loss: 0.6155 - val_accuracy: 0.6786\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6507 - accuracy: 0.6238 - val_loss: 0.6142 - val_accuracy: 0.6786\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6475 - accuracy: 0.6307 - val_loss: 0.6149 - val_accuracy: 0.6811\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6479 - accuracy: 0.6262 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6252 - val_loss: 0.6166 - val_accuracy: 0.6798\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6498 - accuracy: 0.6241 - val_loss: 0.6125 - val_accuracy: 0.6798\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6488 - accuracy: 0.6267 - val_loss: 0.6166 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.6255 - val_loss: 0.6140 - val_accuracy: 0.6811\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6262 - val_loss: 0.6171 - val_accuracy: 0.6824\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6532 - accuracy: 0.6267 - val_loss: 0.6157 - val_accuracy: 0.6773\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6497 - accuracy: 0.6256 - val_loss: 0.6162 - val_accuracy: 0.6849\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6295 - val_loss: 0.6156 - val_accuracy: 0.6849\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6167 - val_loss: 0.6143 - val_accuracy: 0.6798\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6493 - accuracy: 0.6326 - val_loss: 0.6177 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6466 - accuracy: 0.6365 - val_loss: 0.6124 - val_accuracy: 0.6824\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6483 - accuracy: 0.6364 - val_loss: 0.6141 - val_accuracy: 0.6811\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6455 - accuracy: 0.6392 - val_loss: 0.6122 - val_accuracy: 0.6837\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6295 - val_loss: 0.6151 - val_accuracy: 0.6798\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6289 - val_loss: 0.6127 - val_accuracy: 0.6773\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6478 - accuracy: 0.6325 - val_loss: 0.6133 - val_accuracy: 0.6786\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6299 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6318 - val_loss: 0.6158 - val_accuracy: 0.6798\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6474 - accuracy: 0.6374 - val_loss: 0.6116 - val_accuracy: 0.6798\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6251 - val_loss: 0.6117 - val_accuracy: 0.6824\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6467 - accuracy: 0.6380 - val_loss: 0.6120 - val_accuracy: 0.6798\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6468 - accuracy: 0.6264 - val_loss: 0.6143 - val_accuracy: 0.6824\n",
      "Calculating for: 850 100 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_168 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7770 - accuracy: 0.5499 - val_loss: 0.6650 - val_accuracy: 0.6148\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5795 - val_loss: 0.6375 - val_accuracy: 0.6607\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6776 - accuracy: 0.5927 - val_loss: 0.6403 - val_accuracy: 0.6543\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6096 - val_loss: 0.6330 - val_accuracy: 0.6735\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6603 - accuracy: 0.6100 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6154 - val_loss: 0.6319 - val_accuracy: 0.6658\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.6202 - val_loss: 0.6323 - val_accuracy: 0.6620\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6208 - val_loss: 0.6280 - val_accuracy: 0.6760\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6209 - val_loss: 0.6340 - val_accuracy: 0.6658\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6225 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6450 - accuracy: 0.6301 - val_loss: 0.6246 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6301 - val_loss: 0.6251 - val_accuracy: 0.6684\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6309 - val_loss: 0.6291 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6402 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6368 - val_loss: 0.6260 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6399 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6462 - val_loss: 0.6252 - val_accuracy: 0.6735\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6437 - val_loss: 0.6202 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6436 - val_loss: 0.6241 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6410 - val_loss: 0.6245 - val_accuracy: 0.6722\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6481 - val_loss: 0.6242 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6483 - val_loss: 0.6288 - val_accuracy: 0.6645\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6491 - val_loss: 0.6232 - val_accuracy: 0.6696\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6496 - val_loss: 0.6280 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6564 - val_loss: 0.6213 - val_accuracy: 0.6696\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6277 - accuracy: 0.6498 - val_loss: 0.6225 - val_accuracy: 0.6735\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6510 - val_loss: 0.6273 - val_accuracy: 0.6735\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6601 - val_loss: 0.6262 - val_accuracy: 0.6645\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6209 - accuracy: 0.6655 - val_loss: 0.6272 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6564 - val_loss: 0.6272 - val_accuracy: 0.6620\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6654 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6601 - val_loss: 0.6282 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6600 - val_loss: 0.6309 - val_accuracy: 0.6709\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6624 - val_loss: 0.6272 - val_accuracy: 0.6709\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6112 - accuracy: 0.6658 - val_loss: 0.6296 - val_accuracy: 0.6747\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6689 - val_loss: 0.6304 - val_accuracy: 0.6709\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6687 - val_loss: 0.6294 - val_accuracy: 0.6722\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.6679 - val_loss: 0.6351 - val_accuracy: 0.6709\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6723 - val_loss: 0.6306 - val_accuracy: 0.6709\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6049 - accuracy: 0.6772 - val_loss: 0.6303 - val_accuracy: 0.6747\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6096 - accuracy: 0.6740 - val_loss: 0.6297 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6061 - accuracy: 0.6680 - val_loss: 0.6321 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6026 - accuracy: 0.6777 - val_loss: 0.6265 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6005 - accuracy: 0.6788 - val_loss: 0.6262 - val_accuracy: 0.6773\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6014 - accuracy: 0.6763 - val_loss: 0.6341 - val_accuracy: 0.6645\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6004 - accuracy: 0.6795 - val_loss: 0.6389 - val_accuracy: 0.6607\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6737 - val_loss: 0.6348 - val_accuracy: 0.6773\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6001 - accuracy: 0.6865 - val_loss: 0.6320 - val_accuracy: 0.6709\n",
      "Calculating for: 850 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_172 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7902 - accuracy: 0.5344 - val_loss: 0.6562 - val_accuracy: 0.6467\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7106 - accuracy: 0.5484 - val_loss: 0.6501 - val_accuracy: 0.6416\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5556 - val_loss: 0.6472 - val_accuracy: 0.6556\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5757 - val_loss: 0.6449 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5770 - val_loss: 0.6465 - val_accuracy: 0.6709\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.5769 - val_loss: 0.6406 - val_accuracy: 0.6671\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5838 - val_loss: 0.6395 - val_accuracy: 0.6735\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5965 - val_loss: 0.6402 - val_accuracy: 0.6760\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5912 - val_loss: 0.6364 - val_accuracy: 0.6786\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6021 - val_loss: 0.6368 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6650 - accuracy: 0.6005 - val_loss: 0.6368 - val_accuracy: 0.6735\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.5995 - val_loss: 0.6392 - val_accuracy: 0.6658\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6084 - val_loss: 0.6359 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6075 - val_loss: 0.6340 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6597 - accuracy: 0.6073 - val_loss: 0.6388 - val_accuracy: 0.6620\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6624 - accuracy: 0.6090 - val_loss: 0.6351 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6565 - accuracy: 0.6163 - val_loss: 0.6282 - val_accuracy: 0.6735\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6152 - val_loss: 0.6343 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6169 - val_loss: 0.6288 - val_accuracy: 0.6709\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6124 - val_loss: 0.6359 - val_accuracy: 0.6684\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6566 - accuracy: 0.6142 - val_loss: 0.6337 - val_accuracy: 0.6735\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6189 - val_loss: 0.6326 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6250 - val_loss: 0.6316 - val_accuracy: 0.6671\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6238 - val_loss: 0.6298 - val_accuracy: 0.6786\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6208 - val_loss: 0.6264 - val_accuracy: 0.6824\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6281 - val_loss: 0.6268 - val_accuracy: 0.6798\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6255 - val_loss: 0.6258 - val_accuracy: 0.6837\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6258 - val_loss: 0.6250 - val_accuracy: 0.6849\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6309 - val_loss: 0.6314 - val_accuracy: 0.6747\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6228 - val_loss: 0.6261 - val_accuracy: 0.6773\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6465 - accuracy: 0.6255 - val_loss: 0.6209 - val_accuracy: 0.6862\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6233 - val_loss: 0.6291 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6277 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6280 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6450 - accuracy: 0.6307 - val_loss: 0.6220 - val_accuracy: 0.6824\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6286 - val_loss: 0.6264 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6335 - val_loss: 0.6225 - val_accuracy: 0.6824\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6274 - val_loss: 0.6248 - val_accuracy: 0.6722\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6319 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6435 - accuracy: 0.6349 - val_loss: 0.6206 - val_accuracy: 0.6811\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6454 - accuracy: 0.6306 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6309 - val_loss: 0.6215 - val_accuracy: 0.6811\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6382 - val_loss: 0.6168 - val_accuracy: 0.6849\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.6346 - val_loss: 0.6208 - val_accuracy: 0.6811\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6406 - accuracy: 0.6354 - val_loss: 0.6214 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6449 - val_loss: 0.6219 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6468 - val_loss: 0.6277 - val_accuracy: 0.6722\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6420 - val_loss: 0.6198 - val_accuracy: 0.6786\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6417 - val_loss: 0.6200 - val_accuracy: 0.6735\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6402 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6380 - accuracy: 0.6408 - val_loss: 0.6185 - val_accuracy: 0.6798\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6317 - accuracy: 0.6500 - val_loss: 0.6120 - val_accuracy: 0.6901\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6436 - val_loss: 0.6135 - val_accuracy: 0.6849\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6335 - accuracy: 0.6442 - val_loss: 0.6168 - val_accuracy: 0.6837\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6495 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6341 - accuracy: 0.6497 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6466 - val_loss: 0.6201 - val_accuracy: 0.6798\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6347 - accuracy: 0.6495 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6467 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6554 - val_loss: 0.6216 - val_accuracy: 0.6811\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6459 - val_loss: 0.6204 - val_accuracy: 0.6875\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6304 - accuracy: 0.6459 - val_loss: 0.6192 - val_accuracy: 0.6888\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6473 - val_loss: 0.6159 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6315 - accuracy: 0.6559 - val_loss: 0.6211 - val_accuracy: 0.6849\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6577 - val_loss: 0.6231 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6603 - val_loss: 0.6144 - val_accuracy: 0.6913\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6493 - val_loss: 0.6186 - val_accuracy: 0.6888\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6557 - val_loss: 0.6116 - val_accuracy: 0.6926\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6511 - val_loss: 0.6119 - val_accuracy: 0.6939\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6288 - accuracy: 0.6540 - val_loss: 0.6207 - val_accuracy: 0.6862\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6505 - val_loss: 0.6181 - val_accuracy: 0.6849\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6608 - val_loss: 0.6165 - val_accuracy: 0.6849\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6276 - accuracy: 0.6555 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6234 - accuracy: 0.6556 - val_loss: 0.6191 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6245 - accuracy: 0.6579 - val_loss: 0.6145 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6626 - val_loss: 0.6158 - val_accuracy: 0.6901\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6223 - accuracy: 0.6591 - val_loss: 0.6149 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6255 - accuracy: 0.6576 - val_loss: 0.6149 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6236 - accuracy: 0.6572 - val_loss: 0.6165 - val_accuracy: 0.6849\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6616 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6246 - accuracy: 0.6601 - val_loss: 0.6215 - val_accuracy: 0.6862\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6222 - accuracy: 0.6608 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6169 - accuracy: 0.6686 - val_loss: 0.6168 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6593 - val_loss: 0.6157 - val_accuracy: 0.6811\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6181 - accuracy: 0.6663 - val_loss: 0.6133 - val_accuracy: 0.6798\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6649 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6213 - accuracy: 0.6687 - val_loss: 0.6165 - val_accuracy: 0.6773\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6673 - val_loss: 0.6196 - val_accuracy: 0.6735\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6154 - accuracy: 0.6687 - val_loss: 0.6188 - val_accuracy: 0.6786\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6161 - accuracy: 0.6643 - val_loss: 0.6180 - val_accuracy: 0.6875\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6170 - accuracy: 0.6713 - val_loss: 0.6218 - val_accuracy: 0.6760\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6643 - val_loss: 0.6220 - val_accuracy: 0.6760\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6615 - val_loss: 0.6210 - val_accuracy: 0.6760\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6572 - val_loss: 0.6163 - val_accuracy: 0.6811\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6155 - accuracy: 0.6683 - val_loss: 0.6172 - val_accuracy: 0.6811\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6628 - val_loss: 0.6208 - val_accuracy: 0.6786\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6140 - accuracy: 0.6645 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6133 - accuracy: 0.6639 - val_loss: 0.6249 - val_accuracy: 0.6747\n",
      "Calculating for: 850 100 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_176 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "227/249 [==========================>...] - ETA: 0s - loss: 0.8466 - accuracy: 0.4985WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8391 - accuracy: 0.4976 - val_loss: 0.6624 - val_accuracy: 0.6250\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7290 - accuracy: 0.5133 - val_loss: 0.6630 - val_accuracy: 0.6327\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.7061 - accuracy: 0.5177 - val_loss: 0.6621 - val_accuracy: 0.6250\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6977 - accuracy: 0.5187 - val_loss: 0.6671 - val_accuracy: 0.6263\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5306 - val_loss: 0.6660 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6937 - accuracy: 0.5256 - val_loss: 0.6760 - val_accuracy: 0.6288\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5251 - val_loss: 0.6706 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6897 - accuracy: 0.5360 - val_loss: 0.6705 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5431 - val_loss: 0.6654 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6893 - accuracy: 0.5407 - val_loss: 0.6686 - val_accuracy: 0.6250\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5285 - val_loss: 0.6636 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6877 - accuracy: 0.5430 - val_loss: 0.6658 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5369 - val_loss: 0.6663 - val_accuracy: 0.6212\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5354 - val_loss: 0.6710 - val_accuracy: 0.6301\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6883 - accuracy: 0.5430 - val_loss: 0.6630 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5340 - val_loss: 0.6679 - val_accuracy: 0.6301\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5360 - val_loss: 0.6665 - val_accuracy: 0.6378\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6887 - accuracy: 0.5357 - val_loss: 0.6642 - val_accuracy: 0.6339\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5443 - val_loss: 0.6660 - val_accuracy: 0.6390\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5421 - val_loss: 0.6608 - val_accuracy: 0.6224\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5421 - val_loss: 0.6637 - val_accuracy: 0.6250\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5472 - val_loss: 0.6644 - val_accuracy: 0.6390\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5510 - val_loss: 0.6614 - val_accuracy: 0.6339\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6856 - accuracy: 0.5517 - val_loss: 0.6615 - val_accuracy: 0.6390\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.5477 - val_loss: 0.6591 - val_accuracy: 0.6365\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6857 - accuracy: 0.5433 - val_loss: 0.6565 - val_accuracy: 0.6352\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6878 - accuracy: 0.5367 - val_loss: 0.6610 - val_accuracy: 0.6390\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6850 - accuracy: 0.5544 - val_loss: 0.6606 - val_accuracy: 0.6390\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6844 - accuracy: 0.5504 - val_loss: 0.6599 - val_accuracy: 0.6365\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6814 - accuracy: 0.5656 - val_loss: 0.6509 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6835 - accuracy: 0.5582 - val_loss: 0.6580 - val_accuracy: 0.6390\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6870 - accuracy: 0.5481 - val_loss: 0.6566 - val_accuracy: 0.6416\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6857 - accuracy: 0.5501 - val_loss: 0.6616 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5501 - val_loss: 0.6634 - val_accuracy: 0.6429\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6833 - accuracy: 0.5576 - val_loss: 0.6543 - val_accuracy: 0.6416\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5637 - val_loss: 0.6559 - val_accuracy: 0.6416\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6823 - accuracy: 0.5620 - val_loss: 0.6549 - val_accuracy: 0.6403\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6819 - accuracy: 0.5587 - val_loss: 0.6531 - val_accuracy: 0.6390\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6816 - accuracy: 0.5600 - val_loss: 0.6541 - val_accuracy: 0.6378\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6830 - accuracy: 0.5574 - val_loss: 0.6538 - val_accuracy: 0.6403\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5657 - val_loss: 0.6524 - val_accuracy: 0.6416\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6801 - accuracy: 0.5736 - val_loss: 0.6584 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5672 - val_loss: 0.6500 - val_accuracy: 0.6429\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5641 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6834 - accuracy: 0.5619 - val_loss: 0.6546 - val_accuracy: 0.6378\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5597 - val_loss: 0.6518 - val_accuracy: 0.6390\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5663 - val_loss: 0.6522 - val_accuracy: 0.6378\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6768 - accuracy: 0.5674 - val_loss: 0.6483 - val_accuracy: 0.6454\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5643 - val_loss: 0.6547 - val_accuracy: 0.6378\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6803 - accuracy: 0.5643 - val_loss: 0.6528 - val_accuracy: 0.6416\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.5705 - val_loss: 0.6467 - val_accuracy: 0.6390\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.5746 - val_loss: 0.6519 - val_accuracy: 0.6416\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6782 - accuracy: 0.5705 - val_loss: 0.6485 - val_accuracy: 0.6416\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6782 - accuracy: 0.5676 - val_loss: 0.6484 - val_accuracy: 0.6416\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6778 - accuracy: 0.5703 - val_loss: 0.6500 - val_accuracy: 0.6403\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5715 - val_loss: 0.6504 - val_accuracy: 0.6416\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5754 - val_loss: 0.6441 - val_accuracy: 0.6416\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5751 - val_loss: 0.6513 - val_accuracy: 0.6429\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6731 - accuracy: 0.5847 - val_loss: 0.6459 - val_accuracy: 0.6416\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6757 - accuracy: 0.5855 - val_loss: 0.6470 - val_accuracy: 0.6416\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6768 - accuracy: 0.5781 - val_loss: 0.6456 - val_accuracy: 0.6403\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5839 - val_loss: 0.6454 - val_accuracy: 0.6505\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6716 - accuracy: 0.5898 - val_loss: 0.6407 - val_accuracy: 0.6454\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.5841 - val_loss: 0.6430 - val_accuracy: 0.6480\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6757 - accuracy: 0.5765 - val_loss: 0.6415 - val_accuracy: 0.6429\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6753 - accuracy: 0.5761 - val_loss: 0.6465 - val_accuracy: 0.6441\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5819 - val_loss: 0.6416 - val_accuracy: 0.6416\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5800 - val_loss: 0.6445 - val_accuracy: 0.6429\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5809 - val_loss: 0.6468 - val_accuracy: 0.6556\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6700 - accuracy: 0.5877 - val_loss: 0.6394 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5793 - val_loss: 0.6460 - val_accuracy: 0.6569\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5813 - val_loss: 0.6432 - val_accuracy: 0.6492\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5885 - val_loss: 0.6418 - val_accuracy: 0.6480\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5889 - val_loss: 0.6431 - val_accuracy: 0.6467\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5830 - val_loss: 0.6428 - val_accuracy: 0.6480\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5847 - val_loss: 0.6422 - val_accuracy: 0.6480\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.5841 - val_loss: 0.6462 - val_accuracy: 0.6594\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5970 - val_loss: 0.6407 - val_accuracy: 0.6518\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5877 - val_loss: 0.6377 - val_accuracy: 0.6518\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6699 - accuracy: 0.5904 - val_loss: 0.6430 - val_accuracy: 0.6671\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.5847 - val_loss: 0.6430 - val_accuracy: 0.6607\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5953 - val_loss: 0.6446 - val_accuracy: 0.6671\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6699 - accuracy: 0.5831 - val_loss: 0.6379 - val_accuracy: 0.6480\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6708 - accuracy: 0.5907 - val_loss: 0.6398 - val_accuracy: 0.6543\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5893 - val_loss: 0.6414 - val_accuracy: 0.6556\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6681 - accuracy: 0.5899 - val_loss: 0.6397 - val_accuracy: 0.6671\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5928 - val_loss: 0.6416 - val_accuracy: 0.6582\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6718 - accuracy: 0.5870 - val_loss: 0.6408 - val_accuracy: 0.6582\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5995 - val_loss: 0.6439 - val_accuracy: 0.6747\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5830 - val_loss: 0.6434 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5903 - val_loss: 0.6457 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5943 - val_loss: 0.6374 - val_accuracy: 0.6671\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5903 - val_loss: 0.6369 - val_accuracy: 0.6594\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.5961 - val_loss: 0.6359 - val_accuracy: 0.6594\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6676 - accuracy: 0.5947 - val_loss: 0.6374 - val_accuracy: 0.6696\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5917 - val_loss: 0.6389 - val_accuracy: 0.6747\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5884 - val_loss: 0.6379 - val_accuracy: 0.6696\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5955 - val_loss: 0.6368 - val_accuracy: 0.6696\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6017 - val_loss: 0.6341 - val_accuracy: 0.6735\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.5957 - val_loss: 0.6362 - val_accuracy: 0.6696\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5997 - val_loss: 0.6356 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6630 - accuracy: 0.6015 - val_loss: 0.6339 - val_accuracy: 0.6658\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5972 - val_loss: 0.6410 - val_accuracy: 0.6722\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.6020 - val_loss: 0.6322 - val_accuracy: 0.6773\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.6041 - val_loss: 0.6386 - val_accuracy: 0.6722\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6634 - accuracy: 0.6000 - val_loss: 0.6331 - val_accuracy: 0.6747\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6652 - accuracy: 0.6068 - val_loss: 0.6313 - val_accuracy: 0.6760\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6660 - accuracy: 0.6030 - val_loss: 0.6327 - val_accuracy: 0.6773\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.5981 - val_loss: 0.6374 - val_accuracy: 0.6760\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6662 - accuracy: 0.6014 - val_loss: 0.6321 - val_accuracy: 0.6747\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6025 - val_loss: 0.6334 - val_accuracy: 0.6722\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5921 - val_loss: 0.6347 - val_accuracy: 0.6709\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6641 - accuracy: 0.6042 - val_loss: 0.6343 - val_accuracy: 0.6709\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6623 - accuracy: 0.6042 - val_loss: 0.6329 - val_accuracy: 0.6760\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6642 - accuracy: 0.5991 - val_loss: 0.6332 - val_accuracy: 0.6747\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5977 - val_loss: 0.6352 - val_accuracy: 0.6747\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6632 - accuracy: 0.6050 - val_loss: 0.6330 - val_accuracy: 0.6722\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6059 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6046 - val_loss: 0.6309 - val_accuracy: 0.6786\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6614 - accuracy: 0.6081 - val_loss: 0.6282 - val_accuracy: 0.6773\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6662 - accuracy: 0.5985 - val_loss: 0.6294 - val_accuracy: 0.6735\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6046 - val_loss: 0.6367 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6644 - accuracy: 0.6044 - val_loss: 0.6321 - val_accuracy: 0.6786\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.6005 - val_loss: 0.6302 - val_accuracy: 0.6760\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6000 - val_loss: 0.6273 - val_accuracy: 0.6786\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6618 - accuracy: 0.6036 - val_loss: 0.6281 - val_accuracy: 0.6786\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6022 - val_loss: 0.6313 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6060 - val_loss: 0.6279 - val_accuracy: 0.6773\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6632 - accuracy: 0.6058 - val_loss: 0.6305 - val_accuracy: 0.6760\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6579 - accuracy: 0.6064 - val_loss: 0.6274 - val_accuracy: 0.6735\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6108 - val_loss: 0.6306 - val_accuracy: 0.6786\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6630 - accuracy: 0.6071 - val_loss: 0.6306 - val_accuracy: 0.6824\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6583 - accuracy: 0.6159 - val_loss: 0.6263 - val_accuracy: 0.6786\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6603 - accuracy: 0.6101 - val_loss: 0.6278 - val_accuracy: 0.6811\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6143 - val_loss: 0.6326 - val_accuracy: 0.6824\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.6103 - val_loss: 0.6312 - val_accuracy: 0.6811\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.6109 - val_loss: 0.6293 - val_accuracy: 0.6837\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6593 - accuracy: 0.6112 - val_loss: 0.6283 - val_accuracy: 0.6849\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6142 - val_loss: 0.6250 - val_accuracy: 0.6837\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6611 - accuracy: 0.6125 - val_loss: 0.6284 - val_accuracy: 0.6824\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.6103 - val_loss: 0.6272 - val_accuracy: 0.6773\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6123 - val_loss: 0.6246 - val_accuracy: 0.6811\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6155 - val_loss: 0.6282 - val_accuracy: 0.6811\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6143 - val_loss: 0.6261 - val_accuracy: 0.6849\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6133 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6132 - val_loss: 0.6274 - val_accuracy: 0.6824\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6084 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6575 - accuracy: 0.6118 - val_loss: 0.6271 - val_accuracy: 0.6837\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6252 - val_loss: 0.6254 - val_accuracy: 0.6798\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6587 - accuracy: 0.6133 - val_loss: 0.6259 - val_accuracy: 0.6862\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6158 - val_loss: 0.6268 - val_accuracy: 0.6798\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6081 - val_loss: 0.6242 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6591 - accuracy: 0.6150 - val_loss: 0.6261 - val_accuracy: 0.6837\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6127 - val_loss: 0.6283 - val_accuracy: 0.6849\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6573 - accuracy: 0.6178 - val_loss: 0.6241 - val_accuracy: 0.6811\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6209 - val_loss: 0.6264 - val_accuracy: 0.6862\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6582 - accuracy: 0.6184 - val_loss: 0.6267 - val_accuracy: 0.6798\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6167 - val_loss: 0.6244 - val_accuracy: 0.6824\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6584 - accuracy: 0.6245 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6208 - val_loss: 0.6225 - val_accuracy: 0.6824\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6191 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6216 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6569 - accuracy: 0.6178 - val_loss: 0.6214 - val_accuracy: 0.6786\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6201 - val_loss: 0.6195 - val_accuracy: 0.6862\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6153 - val_loss: 0.6272 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6246 - val_loss: 0.6232 - val_accuracy: 0.6824\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6122 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6238 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6208 - val_loss: 0.6248 - val_accuracy: 0.6837\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6206 - val_loss: 0.6241 - val_accuracy: 0.6837\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6558 - accuracy: 0.6233 - val_loss: 0.6235 - val_accuracy: 0.6837\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6578 - accuracy: 0.6172 - val_loss: 0.6280 - val_accuracy: 0.6849\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6161 - val_loss: 0.6240 - val_accuracy: 0.6811\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6177 - val_loss: 0.6230 - val_accuracy: 0.6849\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6516 - accuracy: 0.6270 - val_loss: 0.6203 - val_accuracy: 0.6875\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6257 - val_loss: 0.6255 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6207 - val_loss: 0.6243 - val_accuracy: 0.6862\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6260 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6262 - val_loss: 0.6245 - val_accuracy: 0.6786\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6556 - accuracy: 0.6230 - val_loss: 0.6255 - val_accuracy: 0.6811\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6242 - val_loss: 0.6234 - val_accuracy: 0.6837\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6265 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6275 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6237 - val_loss: 0.6244 - val_accuracy: 0.6837\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6286 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6336 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6544 - accuracy: 0.6231 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6204 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6526 - accuracy: 0.6236 - val_loss: 0.6227 - val_accuracy: 0.6837\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6289 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6179 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6211 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6329 - val_loss: 0.6205 - val_accuracy: 0.6849\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6211 - val_loss: 0.6226 - val_accuracy: 0.6849\n",
      "Calculating for: 850 250 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_180 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7977 - accuracy: 0.5529 - val_loss: 0.7656 - val_accuracy: 0.3750\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6968 - accuracy: 0.5901 - val_loss: 0.6800 - val_accuracy: 0.5804\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5987 - val_loss: 0.6595 - val_accuracy: 0.6237\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6035 - val_loss: 0.6506 - val_accuracy: 0.6301\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6140 - val_loss: 0.6521 - val_accuracy: 0.6276\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6297 - val_loss: 0.6568 - val_accuracy: 0.6173\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6167 - val_loss: 0.6469 - val_accuracy: 0.6314\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6266 - val_loss: 0.6445 - val_accuracy: 0.6327\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6247 - val_loss: 0.6429 - val_accuracy: 0.6378\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6316 - val_loss: 0.6456 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6331 - val_loss: 0.6407 - val_accuracy: 0.6390\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6410 - val_loss: 0.6446 - val_accuracy: 0.6365\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6368 - val_loss: 0.6420 - val_accuracy: 0.6403\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6419 - val_loss: 0.6438 - val_accuracy: 0.6339\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6371 - accuracy: 0.6399 - val_loss: 0.6383 - val_accuracy: 0.6454\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6405 - val_loss: 0.6372 - val_accuracy: 0.6467\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6437 - val_loss: 0.6358 - val_accuracy: 0.6467\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6505 - val_loss: 0.6322 - val_accuracy: 0.6543\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6487 - val_loss: 0.6343 - val_accuracy: 0.6582\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6463 - val_loss: 0.6392 - val_accuracy: 0.6480\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6458 - val_loss: 0.6349 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6604 - val_loss: 0.6376 - val_accuracy: 0.6594\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6525 - val_loss: 0.6421 - val_accuracy: 0.6569\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6245 - accuracy: 0.6595 - val_loss: 0.6339 - val_accuracy: 0.6620\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6236 - accuracy: 0.6560 - val_loss: 0.6369 - val_accuracy: 0.6633\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6193 - accuracy: 0.6541 - val_loss: 0.6369 - val_accuracy: 0.6658\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6662 - val_loss: 0.6358 - val_accuracy: 0.6582\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6197 - accuracy: 0.6590 - val_loss: 0.6347 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6611 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6595 - val_loss: 0.6370 - val_accuracy: 0.6582\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6682 - val_loss: 0.6405 - val_accuracy: 0.6607\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6780 - val_loss: 0.6411 - val_accuracy: 0.6543\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6062 - accuracy: 0.6724 - val_loss: 0.6469 - val_accuracy: 0.6403\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6097 - accuracy: 0.6704 - val_loss: 0.6421 - val_accuracy: 0.6569\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6082 - accuracy: 0.6736 - val_loss: 0.6355 - val_accuracy: 0.6645\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6085 - accuracy: 0.6628 - val_loss: 0.6384 - val_accuracy: 0.6556\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6087 - accuracy: 0.6716 - val_loss: 0.6396 - val_accuracy: 0.6633\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6067 - accuracy: 0.6687 - val_loss: 0.6442 - val_accuracy: 0.6454\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6051 - accuracy: 0.6745 - val_loss: 0.6400 - val_accuracy: 0.6582\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6011 - accuracy: 0.6753 - val_loss: 0.6427 - val_accuracy: 0.6607\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5981 - accuracy: 0.6829 - val_loss: 0.6342 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5979 - accuracy: 0.6807 - val_loss: 0.6469 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5963 - accuracy: 0.6848 - val_loss: 0.6422 - val_accuracy: 0.6543\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5989 - accuracy: 0.6797 - val_loss: 0.6467 - val_accuracy: 0.6556\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5984 - accuracy: 0.6848 - val_loss: 0.6489 - val_accuracy: 0.6518\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5930 - accuracy: 0.6850 - val_loss: 0.6454 - val_accuracy: 0.6569\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5920 - accuracy: 0.6897 - val_loss: 0.6549 - val_accuracy: 0.6454\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5954 - accuracy: 0.6849 - val_loss: 0.6482 - val_accuracy: 0.6467\n",
      "Calculating for: 850 250 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_184 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8119 - accuracy: 0.5398 - val_loss: 0.6475 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7113 - accuracy: 0.5630 - val_loss: 0.6427 - val_accuracy: 0.6607\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6849 - accuracy: 0.5669 - val_loss: 0.6406 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5764 - val_loss: 0.6374 - val_accuracy: 0.6556\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5843 - val_loss: 0.6392 - val_accuracy: 0.6645\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.5967 - val_loss: 0.6350 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6675 - accuracy: 0.5941 - val_loss: 0.6352 - val_accuracy: 0.6671\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.6010 - val_loss: 0.6360 - val_accuracy: 0.6671\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.5970 - val_loss: 0.6357 - val_accuracy: 0.6620\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6653 - accuracy: 0.5962 - val_loss: 0.6346 - val_accuracy: 0.6620\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6120 - val_loss: 0.6304 - val_accuracy: 0.6671\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6095 - val_loss: 0.6308 - val_accuracy: 0.6722\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6059 - val_loss: 0.6276 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6059 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6118 - val_loss: 0.6255 - val_accuracy: 0.6722\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6164 - val_loss: 0.6284 - val_accuracy: 0.6735\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6140 - val_loss: 0.6317 - val_accuracy: 0.6735\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6128 - val_loss: 0.6265 - val_accuracy: 0.6747\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6582 - accuracy: 0.6150 - val_loss: 0.6240 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6163 - val_loss: 0.6266 - val_accuracy: 0.6798\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6154 - val_loss: 0.6207 - val_accuracy: 0.6786\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6130 - val_loss: 0.6272 - val_accuracy: 0.6875\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6548 - accuracy: 0.6153 - val_loss: 0.6255 - val_accuracy: 0.6773\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.6202 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6176 - val_loss: 0.6281 - val_accuracy: 0.6824\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6182 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6287 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6223 - val_loss: 0.6274 - val_accuracy: 0.6633\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6281 - val_loss: 0.6243 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6494 - accuracy: 0.6246 - val_loss: 0.6219 - val_accuracy: 0.6824\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6328 - val_loss: 0.6197 - val_accuracy: 0.6798\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6290 - val_loss: 0.6253 - val_accuracy: 0.6645\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6350 - val_loss: 0.6229 - val_accuracy: 0.6747\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6356 - val_loss: 0.6195 - val_accuracy: 0.6798\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6481 - accuracy: 0.6264 - val_loss: 0.6231 - val_accuracy: 0.6786\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6297 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6321 - val_loss: 0.6219 - val_accuracy: 0.6696\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6370 - val_loss: 0.6225 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6365 - val_loss: 0.6168 - val_accuracy: 0.6722\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6326 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6407 - val_loss: 0.6224 - val_accuracy: 0.6620\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6403 - accuracy: 0.6361 - val_loss: 0.6181 - val_accuracy: 0.6735\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6390 - val_loss: 0.6227 - val_accuracy: 0.6620\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6390 - accuracy: 0.6402 - val_loss: 0.6187 - val_accuracy: 0.6760\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6355 - accuracy: 0.6447 - val_loss: 0.6172 - val_accuracy: 0.6671\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6379 - accuracy: 0.6405 - val_loss: 0.6192 - val_accuracy: 0.6709\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6413 - val_loss: 0.6149 - val_accuracy: 0.6849\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6408 - val_loss: 0.6150 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6408 - val_loss: 0.6153 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6367 - accuracy: 0.6395 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6409 - val_loss: 0.6184 - val_accuracy: 0.6811\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6415 - val_loss: 0.6156 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6366 - val_loss: 0.6144 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6492 - val_loss: 0.6171 - val_accuracy: 0.6786\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6512 - val_loss: 0.6136 - val_accuracy: 0.6798\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6488 - val_loss: 0.6139 - val_accuracy: 0.6798\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6555 - val_loss: 0.6135 - val_accuracy: 0.6786\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6483 - val_loss: 0.6169 - val_accuracy: 0.6798\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6451 - val_loss: 0.6158 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6511 - val_loss: 0.6152 - val_accuracy: 0.6798\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6427 - val_loss: 0.6169 - val_accuracy: 0.6824\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6541 - val_loss: 0.6146 - val_accuracy: 0.6811\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6520 - val_loss: 0.6170 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6500 - val_loss: 0.6171 - val_accuracy: 0.6747\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6557 - val_loss: 0.6173 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6544 - val_loss: 0.6194 - val_accuracy: 0.6747\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6487 - val_loss: 0.6154 - val_accuracy: 0.6837\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6505 - val_loss: 0.6170 - val_accuracy: 0.6798\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6464 - val_loss: 0.6122 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6554 - val_loss: 0.6144 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6250 - accuracy: 0.6594 - val_loss: 0.6182 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6560 - val_loss: 0.6135 - val_accuracy: 0.6811\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6605 - val_loss: 0.6167 - val_accuracy: 0.6773\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6562 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6521 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6193 - accuracy: 0.6629 - val_loss: 0.6125 - val_accuracy: 0.6901\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6575 - val_loss: 0.6118 - val_accuracy: 0.6875\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6659 - val_loss: 0.6132 - val_accuracy: 0.6824\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6594 - val_loss: 0.6161 - val_accuracy: 0.6773\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6668 - val_loss: 0.6124 - val_accuracy: 0.6824\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6180 - accuracy: 0.6643 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6199 - accuracy: 0.6574 - val_loss: 0.6116 - val_accuracy: 0.6837\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6638 - val_loss: 0.6109 - val_accuracy: 0.6913\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6652 - val_loss: 0.6144 - val_accuracy: 0.6837\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6644 - val_loss: 0.6118 - val_accuracy: 0.6849\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6654 - val_loss: 0.6176 - val_accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6169 - accuracy: 0.6631 - val_loss: 0.6136 - val_accuracy: 0.6849\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6166 - accuracy: 0.6647 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6153 - accuracy: 0.6659 - val_loss: 0.6125 - val_accuracy: 0.6849\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6677 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6663 - val_loss: 0.6143 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6634 - val_loss: 0.6149 - val_accuracy: 0.6849\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6683 - val_loss: 0.6144 - val_accuracy: 0.6901\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6648 - val_loss: 0.6152 - val_accuracy: 0.6849\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6703 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6158 - accuracy: 0.6654 - val_loss: 0.6187 - val_accuracy: 0.6824\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6125 - accuracy: 0.6628 - val_loss: 0.6165 - val_accuracy: 0.6824\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6135 - accuracy: 0.6682 - val_loss: 0.6185 - val_accuracy: 0.6849\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6680 - val_loss: 0.6168 - val_accuracy: 0.6811\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6688 - val_loss: 0.6179 - val_accuracy: 0.6773\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6691 - val_loss: 0.6161 - val_accuracy: 0.6862\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6130 - accuracy: 0.6718 - val_loss: 0.6173 - val_accuracy: 0.6811\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6071 - accuracy: 0.6763 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6085 - accuracy: 0.6708 - val_loss: 0.6191 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6074 - accuracy: 0.6704 - val_loss: 0.6176 - val_accuracy: 0.6773\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6040 - accuracy: 0.6807 - val_loss: 0.6157 - val_accuracy: 0.6773\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6043 - accuracy: 0.6797 - val_loss: 0.6149 - val_accuracy: 0.6760\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6060 - accuracy: 0.6752 - val_loss: 0.6211 - val_accuracy: 0.6747\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6688 - val_loss: 0.6199 - val_accuracy: 0.6709\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6704 - val_loss: 0.6194 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6064 - accuracy: 0.6756 - val_loss: 0.6162 - val_accuracy: 0.6798\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6051 - accuracy: 0.6771 - val_loss: 0.6161 - val_accuracy: 0.6811\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6031 - accuracy: 0.6737 - val_loss: 0.6149 - val_accuracy: 0.6824\n",
      "Calculating for: 850 250 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_188 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8272 - accuracy: 0.5028 - val_loss: 0.6611 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7332 - accuracy: 0.5162 - val_loss: 0.6675 - val_accuracy: 0.6327\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6992 - accuracy: 0.5324 - val_loss: 0.6636 - val_accuracy: 0.6212\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6964 - accuracy: 0.5231 - val_loss: 0.6668 - val_accuracy: 0.6224\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5250 - val_loss: 0.6718 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5262 - val_loss: 0.6755 - val_accuracy: 0.6199\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5274 - val_loss: 0.6666 - val_accuracy: 0.6199\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5269 - val_loss: 0.6711 - val_accuracy: 0.6186\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5319 - val_loss: 0.6685 - val_accuracy: 0.6199\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.5377 - val_loss: 0.6626 - val_accuracy: 0.6186\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5423 - val_loss: 0.6619 - val_accuracy: 0.6186\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6901 - accuracy: 0.5309 - val_loss: 0.6623 - val_accuracy: 0.6199\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5355 - val_loss: 0.6647 - val_accuracy: 0.6212\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.5339 - val_loss: 0.6620 - val_accuracy: 0.6199\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.5402 - val_loss: 0.6654 - val_accuracy: 0.6212\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5406 - val_loss: 0.6635 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5371 - val_loss: 0.6671 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6878 - accuracy: 0.5484 - val_loss: 0.6627 - val_accuracy: 0.6237\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6869 - accuracy: 0.5465 - val_loss: 0.6619 - val_accuracy: 0.6224\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6884 - accuracy: 0.5481 - val_loss: 0.6620 - val_accuracy: 0.6237\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5394 - val_loss: 0.6600 - val_accuracy: 0.6301\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.5430 - val_loss: 0.6578 - val_accuracy: 0.6250\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.5525 - val_loss: 0.6573 - val_accuracy: 0.6352\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6881 - accuracy: 0.5448 - val_loss: 0.6610 - val_accuracy: 0.6352\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5559 - val_loss: 0.6568 - val_accuracy: 0.6339\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6842 - accuracy: 0.5618 - val_loss: 0.6562 - val_accuracy: 0.6301\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5599 - val_loss: 0.6551 - val_accuracy: 0.6352\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5533 - val_loss: 0.6584 - val_accuracy: 0.6378\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.5536 - val_loss: 0.6541 - val_accuracy: 0.6352\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6812 - accuracy: 0.5657 - val_loss: 0.6533 - val_accuracy: 0.6352\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5602 - val_loss: 0.6521 - val_accuracy: 0.6352\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6843 - accuracy: 0.5584 - val_loss: 0.6537 - val_accuracy: 0.6365\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5584 - val_loss: 0.6553 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.5585 - val_loss: 0.6527 - val_accuracy: 0.6365\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6811 - accuracy: 0.5686 - val_loss: 0.6523 - val_accuracy: 0.6390\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6812 - accuracy: 0.5593 - val_loss: 0.6516 - val_accuracy: 0.6390\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6808 - accuracy: 0.5614 - val_loss: 0.6514 - val_accuracy: 0.6467\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6826 - accuracy: 0.5544 - val_loss: 0.6495 - val_accuracy: 0.6467\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6777 - accuracy: 0.5686 - val_loss: 0.6475 - val_accuracy: 0.6403\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6806 - accuracy: 0.5657 - val_loss: 0.6500 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5687 - val_loss: 0.6492 - val_accuracy: 0.6480\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6768 - accuracy: 0.5737 - val_loss: 0.6474 - val_accuracy: 0.6467\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5721 - val_loss: 0.6478 - val_accuracy: 0.6441\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6795 - accuracy: 0.5708 - val_loss: 0.6497 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6781 - accuracy: 0.5749 - val_loss: 0.6481 - val_accuracy: 0.6429\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6769 - accuracy: 0.5785 - val_loss: 0.6469 - val_accuracy: 0.6441\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6769 - accuracy: 0.5799 - val_loss: 0.6459 - val_accuracy: 0.6441\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5770 - val_loss: 0.6461 - val_accuracy: 0.6429\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5749 - val_loss: 0.6470 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6760 - accuracy: 0.5804 - val_loss: 0.6447 - val_accuracy: 0.6416\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6765 - accuracy: 0.5782 - val_loss: 0.6455 - val_accuracy: 0.6403\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5838 - val_loss: 0.6423 - val_accuracy: 0.6416\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5784 - val_loss: 0.6435 - val_accuracy: 0.6429\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5894 - val_loss: 0.6410 - val_accuracy: 0.6441\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5826 - val_loss: 0.6425 - val_accuracy: 0.6429\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6730 - accuracy: 0.5898 - val_loss: 0.6428 - val_accuracy: 0.6467\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5862 - val_loss: 0.6435 - val_accuracy: 0.6467\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6723 - accuracy: 0.5839 - val_loss: 0.6436 - val_accuracy: 0.6467\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5818 - val_loss: 0.6428 - val_accuracy: 0.6480\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5862 - val_loss: 0.6424 - val_accuracy: 0.6416\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5902 - val_loss: 0.6420 - val_accuracy: 0.6403\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.5823 - val_loss: 0.6441 - val_accuracy: 0.6403\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5874 - val_loss: 0.6421 - val_accuracy: 0.6403\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5869 - val_loss: 0.6405 - val_accuracy: 0.6480\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5916 - val_loss: 0.6392 - val_accuracy: 0.6505\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5931 - val_loss: 0.6389 - val_accuracy: 0.6492\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.5919 - val_loss: 0.6394 - val_accuracy: 0.6505\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5898 - val_loss: 0.6398 - val_accuracy: 0.6480\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5936 - val_loss: 0.6400 - val_accuracy: 0.6505\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5906 - val_loss: 0.6403 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5922 - val_loss: 0.6387 - val_accuracy: 0.6492\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6672 - accuracy: 0.5968 - val_loss: 0.6377 - val_accuracy: 0.6531\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5981 - val_loss: 0.6397 - val_accuracy: 0.6492\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5932 - val_loss: 0.6405 - val_accuracy: 0.6518\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5897 - val_loss: 0.6413 - val_accuracy: 0.6518\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5972 - val_loss: 0.6373 - val_accuracy: 0.6505\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5963 - val_loss: 0.6391 - val_accuracy: 0.6543\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.6002 - val_loss: 0.6353 - val_accuracy: 0.6518\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5951 - val_loss: 0.6383 - val_accuracy: 0.6492\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.6027 - val_loss: 0.6378 - val_accuracy: 0.6505\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5934 - val_loss: 0.6381 - val_accuracy: 0.6543\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.5981 - val_loss: 0.6359 - val_accuracy: 0.6505\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.5986 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6020 - val_loss: 0.6335 - val_accuracy: 0.6594\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5933 - val_loss: 0.6345 - val_accuracy: 0.6594\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.6004 - val_loss: 0.6386 - val_accuracy: 0.6633\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5998 - val_loss: 0.6350 - val_accuracy: 0.6607\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.5980 - val_loss: 0.6338 - val_accuracy: 0.6633\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6656 - accuracy: 0.6004 - val_loss: 0.6350 - val_accuracy: 0.6696\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5982 - val_loss: 0.6344 - val_accuracy: 0.6582\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.5858 - val_loss: 0.6338 - val_accuracy: 0.6645\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6012 - val_loss: 0.6338 - val_accuracy: 0.6645\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.5995 - val_loss: 0.6346 - val_accuracy: 0.6645\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.5991 - val_loss: 0.6317 - val_accuracy: 0.6722\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.5992 - val_loss: 0.6335 - val_accuracy: 0.6607\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.6042 - val_loss: 0.6331 - val_accuracy: 0.6722\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6030 - val_loss: 0.6360 - val_accuracy: 0.6709\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5963 - val_loss: 0.6310 - val_accuracy: 0.6696\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6037 - val_loss: 0.6297 - val_accuracy: 0.6709\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6040 - val_loss: 0.6306 - val_accuracy: 0.6735\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6056 - val_loss: 0.6270 - val_accuracy: 0.6633\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6034 - val_loss: 0.6293 - val_accuracy: 0.6735\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6041 - val_loss: 0.6290 - val_accuracy: 0.6722\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6093 - val_loss: 0.6288 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6127 - val_loss: 0.6280 - val_accuracy: 0.6760\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6046 - val_loss: 0.6313 - val_accuracy: 0.6709\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6075 - val_loss: 0.6292 - val_accuracy: 0.6760\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6010 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6107 - val_loss: 0.6294 - val_accuracy: 0.6671\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6094 - val_loss: 0.6311 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6035 - val_loss: 0.6289 - val_accuracy: 0.6735\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6110 - val_loss: 0.6312 - val_accuracy: 0.6760\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6049 - val_loss: 0.6298 - val_accuracy: 0.6760\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6168 - val_loss: 0.6253 - val_accuracy: 0.6760\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6120 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6119 - val_loss: 0.6289 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6081 - val_loss: 0.6248 - val_accuracy: 0.6747\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6014 - val_loss: 0.6259 - val_accuracy: 0.6747\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6133 - val_loss: 0.6260 - val_accuracy: 0.6735\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6127 - val_loss: 0.6281 - val_accuracy: 0.6798\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6155 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6059 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6083 - val_loss: 0.6259 - val_accuracy: 0.6760\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6089 - val_loss: 0.6253 - val_accuracy: 0.6722\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6088 - val_loss: 0.6310 - val_accuracy: 0.6811\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6058 - val_loss: 0.6258 - val_accuracy: 0.6786\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6085 - val_loss: 0.6273 - val_accuracy: 0.6824\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6078 - val_loss: 0.6299 - val_accuracy: 0.6837\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6158 - val_loss: 0.6221 - val_accuracy: 0.6747\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6197 - val_loss: 0.6233 - val_accuracy: 0.6798\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6173 - val_loss: 0.6257 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6101 - val_loss: 0.6241 - val_accuracy: 0.6798\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6125 - val_loss: 0.6261 - val_accuracy: 0.6824\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6128 - val_loss: 0.6283 - val_accuracy: 0.6798\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6196 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6130 - val_loss: 0.6239 - val_accuracy: 0.6760\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6163 - val_loss: 0.6237 - val_accuracy: 0.6837\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6178 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6193 - val_loss: 0.6225 - val_accuracy: 0.6760\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6167 - val_loss: 0.6262 - val_accuracy: 0.6811\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6171 - val_loss: 0.6237 - val_accuracy: 0.6811\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6154 - val_loss: 0.6223 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6217 - val_loss: 0.6246 - val_accuracy: 0.6798\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6144 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6245 - val_loss: 0.6200 - val_accuracy: 0.6773\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6145 - val_loss: 0.6223 - val_accuracy: 0.6786\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6118 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6162 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6178 - val_loss: 0.6169 - val_accuracy: 0.6824\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6186 - val_loss: 0.6190 - val_accuracy: 0.6837\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6222 - val_loss: 0.6251 - val_accuracy: 0.6773\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6563 - accuracy: 0.6155 - val_loss: 0.6213 - val_accuracy: 0.6798\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6531 - accuracy: 0.6186 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6213 - val_loss: 0.6193 - val_accuracy: 0.6811\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6250 - val_loss: 0.6212 - val_accuracy: 0.6786\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6140 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6174 - val_loss: 0.6218 - val_accuracy: 0.6811\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6198 - val_accuracy: 0.6837\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6187 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6196 - val_loss: 0.6205 - val_accuracy: 0.6824\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6267 - val_loss: 0.6203 - val_accuracy: 0.6786\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6192 - val_loss: 0.6187 - val_accuracy: 0.6824\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6150 - val_loss: 0.6214 - val_accuracy: 0.6786\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6264 - val_loss: 0.6178 - val_accuracy: 0.6811\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.6201 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6236 - val_loss: 0.6194 - val_accuracy: 0.6798\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6521 - accuracy: 0.6212 - val_loss: 0.6191 - val_accuracy: 0.6824\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6233 - val_loss: 0.6198 - val_accuracy: 0.6798\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6198 - val_loss: 0.6199 - val_accuracy: 0.6849\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6243 - val_loss: 0.6182 - val_accuracy: 0.6760\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6230 - val_loss: 0.6217 - val_accuracy: 0.6760\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6208 - val_loss: 0.6175 - val_accuracy: 0.6747\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6204 - val_loss: 0.6191 - val_accuracy: 0.6760\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6140 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6275 - val_loss: 0.6186 - val_accuracy: 0.6849\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6222 - val_loss: 0.6191 - val_accuracy: 0.6773\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6230 - val_loss: 0.6185 - val_accuracy: 0.6773\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6202 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6225 - val_loss: 0.6194 - val_accuracy: 0.6824\n",
      "Calculating for: 850 400 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_192 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7962 - accuracy: 0.5462 - val_loss: 0.6430 - val_accuracy: 0.6556\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6953 - accuracy: 0.5883 - val_loss: 0.6276 - val_accuracy: 0.6773\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6698 - accuracy: 0.5990 - val_loss: 0.6278 - val_accuracy: 0.6837\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6566 - accuracy: 0.6140 - val_loss: 0.6249 - val_accuracy: 0.6875\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6559 - accuracy: 0.6177 - val_loss: 0.6239 - val_accuracy: 0.6824\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6477 - accuracy: 0.6306 - val_loss: 0.6205 - val_accuracy: 0.6786\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6211 - val_loss: 0.6250 - val_accuracy: 0.6875\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6458 - accuracy: 0.6253 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6460 - accuracy: 0.6296 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6432 - accuracy: 0.6314 - val_loss: 0.6159 - val_accuracy: 0.6913\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6434 - accuracy: 0.6334 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6428 - accuracy: 0.6374 - val_loss: 0.6208 - val_accuracy: 0.6888\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6356 - val_loss: 0.6154 - val_accuracy: 0.6862\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6358 - accuracy: 0.6451 - val_loss: 0.6184 - val_accuracy: 0.6862\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6339 - accuracy: 0.6468 - val_loss: 0.6205 - val_accuracy: 0.6926\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.6472 - val_loss: 0.6154 - val_accuracy: 0.6875\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6319 - accuracy: 0.6441 - val_loss: 0.6162 - val_accuracy: 0.6875\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6311 - accuracy: 0.6443 - val_loss: 0.6149 - val_accuracy: 0.6811\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6306 - accuracy: 0.6506 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6269 - accuracy: 0.6565 - val_loss: 0.6167 - val_accuracy: 0.6798\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6291 - accuracy: 0.6475 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6286 - accuracy: 0.6539 - val_loss: 0.6160 - val_accuracy: 0.6849\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6272 - accuracy: 0.6525 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6203 - accuracy: 0.6623 - val_loss: 0.6162 - val_accuracy: 0.6773\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6235 - accuracy: 0.6531 - val_loss: 0.6137 - val_accuracy: 0.6862\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6191 - accuracy: 0.6536 - val_loss: 0.6154 - val_accuracy: 0.6849\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6205 - accuracy: 0.6614 - val_loss: 0.6201 - val_accuracy: 0.6786\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6143 - accuracy: 0.6716 - val_loss: 0.6128 - val_accuracy: 0.6849\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6143 - accuracy: 0.6664 - val_loss: 0.6172 - val_accuracy: 0.6875\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6112 - accuracy: 0.6706 - val_loss: 0.6180 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6137 - accuracy: 0.6637 - val_loss: 0.6134 - val_accuracy: 0.6862\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6047 - accuracy: 0.6746 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6134 - accuracy: 0.6697 - val_loss: 0.6143 - val_accuracy: 0.6837\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6113 - accuracy: 0.6721 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6104 - accuracy: 0.6718 - val_loss: 0.6214 - val_accuracy: 0.6811\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6030 - accuracy: 0.6709 - val_loss: 0.6195 - val_accuracy: 0.6849\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6041 - accuracy: 0.6757 - val_loss: 0.6238 - val_accuracy: 0.6773\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6048 - accuracy: 0.6755 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6043 - accuracy: 0.6737 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6023 - accuracy: 0.6777 - val_loss: 0.6244 - val_accuracy: 0.6773\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6002 - accuracy: 0.6817 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6019 - accuracy: 0.6801 - val_loss: 0.6318 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6004 - accuracy: 0.6799 - val_loss: 0.6283 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5966 - accuracy: 0.6820 - val_loss: 0.6235 - val_accuracy: 0.6811\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6004 - accuracy: 0.6801 - val_loss: 0.6246 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5962 - accuracy: 0.6767 - val_loss: 0.6284 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5919 - accuracy: 0.6873 - val_loss: 0.6281 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5927 - accuracy: 0.6881 - val_loss: 0.6267 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5914 - accuracy: 0.6938 - val_loss: 0.6274 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5949 - accuracy: 0.6849 - val_loss: 0.6320 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5921 - accuracy: 0.6791 - val_loss: 0.6273 - val_accuracy: 0.6735\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5821 - accuracy: 0.6922 - val_loss: 0.6289 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5891 - accuracy: 0.6894 - val_loss: 0.6279 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5834 - accuracy: 0.6976 - val_loss: 0.6359 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5866 - accuracy: 0.6919 - val_loss: 0.6287 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5847 - accuracy: 0.6937 - val_loss: 0.6304 - val_accuracy: 0.6760\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5837 - accuracy: 0.6930 - val_loss: 0.6300 - val_accuracy: 0.6709\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5814 - accuracy: 0.6996 - val_loss: 0.6327 - val_accuracy: 0.6760\n",
      "Calculating for: 850 400 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_196 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.8353 - accuracy: 0.5412 - val_loss: 0.6612 - val_accuracy: 0.6684\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.7284 - accuracy: 0.5497 - val_loss: 0.6513 - val_accuracy: 0.6696\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6854 - accuracy: 0.5764 - val_loss: 0.6548 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6804 - accuracy: 0.5777 - val_loss: 0.6493 - val_accuracy: 0.6722\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6723 - accuracy: 0.5873 - val_loss: 0.6527 - val_accuracy: 0.6543\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6679 - accuracy: 0.5987 - val_loss: 0.6496 - val_accuracy: 0.6569\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6672 - accuracy: 0.5936 - val_loss: 0.6474 - val_accuracy: 0.6671\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6678 - accuracy: 0.5952 - val_loss: 0.6414 - val_accuracy: 0.6671\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6663 - accuracy: 0.5971 - val_loss: 0.6386 - val_accuracy: 0.6696\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6624 - accuracy: 0.6054 - val_loss: 0.6420 - val_accuracy: 0.6722\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6635 - accuracy: 0.6058 - val_loss: 0.6439 - val_accuracy: 0.6454\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6583 - accuracy: 0.6112 - val_loss: 0.6366 - val_accuracy: 0.6620\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6572 - accuracy: 0.6119 - val_loss: 0.6358 - val_accuracy: 0.6735\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6578 - accuracy: 0.6171 - val_loss: 0.6392 - val_accuracy: 0.6518\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6529 - accuracy: 0.6242 - val_loss: 0.6354 - val_accuracy: 0.6607\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6558 - accuracy: 0.6115 - val_loss: 0.6302 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6540 - accuracy: 0.6179 - val_loss: 0.6311 - val_accuracy: 0.6658\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6553 - accuracy: 0.6118 - val_loss: 0.6380 - val_accuracy: 0.6543\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6519 - accuracy: 0.6222 - val_loss: 0.6307 - val_accuracy: 0.6633\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6533 - accuracy: 0.6174 - val_loss: 0.6323 - val_accuracy: 0.6607\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6517 - accuracy: 0.6188 - val_loss: 0.6275 - val_accuracy: 0.6582\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6542 - accuracy: 0.6223 - val_loss: 0.6319 - val_accuracy: 0.6671\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6524 - accuracy: 0.6207 - val_loss: 0.6326 - val_accuracy: 0.6633\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6496 - accuracy: 0.6237 - val_loss: 0.6343 - val_accuracy: 0.6582\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6483 - accuracy: 0.6282 - val_loss: 0.6299 - val_accuracy: 0.6607\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6460 - accuracy: 0.6319 - val_loss: 0.6274 - val_accuracy: 0.6633\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6483 - accuracy: 0.6306 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6493 - accuracy: 0.6279 - val_loss: 0.6293 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6481 - accuracy: 0.6305 - val_loss: 0.6249 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6445 - accuracy: 0.6296 - val_loss: 0.6247 - val_accuracy: 0.6645\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6459 - accuracy: 0.6341 - val_loss: 0.6225 - val_accuracy: 0.6773\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6461 - accuracy: 0.6309 - val_loss: 0.6272 - val_accuracy: 0.6607\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6448 - accuracy: 0.6341 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6434 - accuracy: 0.6343 - val_loss: 0.6233 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6406 - accuracy: 0.6374 - val_loss: 0.6212 - val_accuracy: 0.6722\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6375 - accuracy: 0.6414 - val_loss: 0.6182 - val_accuracy: 0.6798\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6446 - accuracy: 0.6304 - val_loss: 0.6218 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6402 - accuracy: 0.6372 - val_loss: 0.6230 - val_accuracy: 0.6722\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6392 - accuracy: 0.6355 - val_loss: 0.6238 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6402 - accuracy: 0.6358 - val_loss: 0.6237 - val_accuracy: 0.6671\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6375 - accuracy: 0.6370 - val_loss: 0.6261 - val_accuracy: 0.6543\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6406 - accuracy: 0.6385 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6380 - accuracy: 0.6402 - val_loss: 0.6206 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6348 - accuracy: 0.6476 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6342 - accuracy: 0.6417 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6355 - accuracy: 0.6458 - val_loss: 0.6201 - val_accuracy: 0.6760\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6375 - accuracy: 0.6380 - val_loss: 0.6226 - val_accuracy: 0.6773\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6330 - accuracy: 0.6444 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6348 - accuracy: 0.6467 - val_loss: 0.6289 - val_accuracy: 0.6569\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6350 - accuracy: 0.6402 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6314 - accuracy: 0.6516 - val_loss: 0.6197 - val_accuracy: 0.6773\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6307 - accuracy: 0.6415 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6312 - accuracy: 0.6503 - val_loss: 0.6283 - val_accuracy: 0.6620\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6300 - accuracy: 0.6478 - val_loss: 0.6174 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6293 - accuracy: 0.6520 - val_loss: 0.6241 - val_accuracy: 0.6735\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6300 - accuracy: 0.6481 - val_loss: 0.6253 - val_accuracy: 0.6722\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6249 - accuracy: 0.6555 - val_loss: 0.6200 - val_accuracy: 0.6760\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6310 - accuracy: 0.6490 - val_loss: 0.6212 - val_accuracy: 0.6747\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6260 - accuracy: 0.6496 - val_loss: 0.6235 - val_accuracy: 0.6798\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6276 - accuracy: 0.6532 - val_loss: 0.6196 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6278 - accuracy: 0.6518 - val_loss: 0.6187 - val_accuracy: 0.6773\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6249 - accuracy: 0.6487 - val_loss: 0.6194 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6266 - accuracy: 0.6506 - val_loss: 0.6184 - val_accuracy: 0.6773\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6252 - accuracy: 0.6575 - val_loss: 0.6181 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6220 - accuracy: 0.6545 - val_loss: 0.6204 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6253 - accuracy: 0.6583 - val_loss: 0.6154 - val_accuracy: 0.6798\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6262 - accuracy: 0.6539 - val_loss: 0.6155 - val_accuracy: 0.6773\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6211 - accuracy: 0.6640 - val_loss: 0.6166 - val_accuracy: 0.6773\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6213 - accuracy: 0.6603 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6224 - accuracy: 0.6581 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6237 - accuracy: 0.6577 - val_loss: 0.6196 - val_accuracy: 0.6786\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6190 - accuracy: 0.6600 - val_loss: 0.6196 - val_accuracy: 0.6773\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6193 - accuracy: 0.6605 - val_loss: 0.6213 - val_accuracy: 0.6747\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6185 - accuracy: 0.6579 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6178 - accuracy: 0.6654 - val_loss: 0.6225 - val_accuracy: 0.6786\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6176 - accuracy: 0.6603 - val_loss: 0.6250 - val_accuracy: 0.6786\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6157 - accuracy: 0.6626 - val_loss: 0.6203 - val_accuracy: 0.6773\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6629 - val_loss: 0.6244 - val_accuracy: 0.6773\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6190 - accuracy: 0.6652 - val_loss: 0.6207 - val_accuracy: 0.6773\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6118 - accuracy: 0.6683 - val_loss: 0.6206 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6174 - accuracy: 0.6640 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6113 - accuracy: 0.6727 - val_loss: 0.6202 - val_accuracy: 0.6747\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6135 - accuracy: 0.6701 - val_loss: 0.6221 - val_accuracy: 0.6773\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6128 - accuracy: 0.6693 - val_loss: 0.6220 - val_accuracy: 0.6798\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6137 - accuracy: 0.6682 - val_loss: 0.6202 - val_accuracy: 0.6786\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6149 - accuracy: 0.6642 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6123 - accuracy: 0.6663 - val_loss: 0.6242 - val_accuracy: 0.6684\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6141 - accuracy: 0.6659 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6099 - accuracy: 0.6719 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6102 - accuracy: 0.6709 - val_loss: 0.6207 - val_accuracy: 0.6747\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6068 - accuracy: 0.6741 - val_loss: 0.6226 - val_accuracy: 0.6760\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6121 - accuracy: 0.6673 - val_loss: 0.6236 - val_accuracy: 0.6786\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6047 - accuracy: 0.6734 - val_loss: 0.6223 - val_accuracy: 0.6798\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6105 - accuracy: 0.6659 - val_loss: 0.6221 - val_accuracy: 0.6709\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6054 - accuracy: 0.6775 - val_loss: 0.6231 - val_accuracy: 0.6684\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6051 - accuracy: 0.6717 - val_loss: 0.6236 - val_accuracy: 0.6735\n",
      "Calculating for: 850 400 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_200 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.8557 - accuracy: 0.5087 - val_loss: 0.6652 - val_accuracy: 0.6186\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.7391 - accuracy: 0.5270 - val_loss: 0.6619 - val_accuracy: 0.6224\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.7060 - accuracy: 0.5310 - val_loss: 0.6615 - val_accuracy: 0.6224\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6948 - accuracy: 0.5299 - val_loss: 0.6619 - val_accuracy: 0.6224\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6905 - accuracy: 0.5388 - val_loss: 0.6666 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6903 - accuracy: 0.5343 - val_loss: 0.6663 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6904 - accuracy: 0.5329 - val_loss: 0.6645 - val_accuracy: 0.6224\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6905 - accuracy: 0.5285 - val_loss: 0.6667 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6889 - accuracy: 0.5458 - val_loss: 0.6634 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6887 - accuracy: 0.5430 - val_loss: 0.6614 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6899 - accuracy: 0.5430 - val_loss: 0.6642 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6878 - accuracy: 0.5416 - val_loss: 0.6633 - val_accuracy: 0.6224\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6863 - accuracy: 0.5447 - val_loss: 0.6605 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6870 - accuracy: 0.5496 - val_loss: 0.6616 - val_accuracy: 0.6224\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6872 - accuracy: 0.5482 - val_loss: 0.6621 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6869 - accuracy: 0.5521 - val_loss: 0.6586 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6848 - accuracy: 0.5553 - val_loss: 0.6589 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6851 - accuracy: 0.5569 - val_loss: 0.6590 - val_accuracy: 0.6250\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6857 - accuracy: 0.5536 - val_loss: 0.6593 - val_accuracy: 0.6301\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6847 - accuracy: 0.5541 - val_loss: 0.6603 - val_accuracy: 0.6339\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6836 - accuracy: 0.5579 - val_loss: 0.6558 - val_accuracy: 0.6339\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6831 - accuracy: 0.5604 - val_loss: 0.6566 - val_accuracy: 0.6365\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6825 - accuracy: 0.5615 - val_loss: 0.6555 - val_accuracy: 0.6365\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6800 - accuracy: 0.5664 - val_loss: 0.6531 - val_accuracy: 0.6352\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6805 - accuracy: 0.5668 - val_loss: 0.6523 - val_accuracy: 0.6352\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6820 - accuracy: 0.5658 - val_loss: 0.6526 - val_accuracy: 0.6352\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6808 - accuracy: 0.5649 - val_loss: 0.6518 - val_accuracy: 0.6352\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6829 - accuracy: 0.5638 - val_loss: 0.6538 - val_accuracy: 0.6429\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6800 - accuracy: 0.5723 - val_loss: 0.6518 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6820 - accuracy: 0.5674 - val_loss: 0.6532 - val_accuracy: 0.6416\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6805 - accuracy: 0.5728 - val_loss: 0.6511 - val_accuracy: 0.6429\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6804 - accuracy: 0.5646 - val_loss: 0.6522 - val_accuracy: 0.6429\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6786 - accuracy: 0.5749 - val_loss: 0.6495 - val_accuracy: 0.6416\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6793 - accuracy: 0.5744 - val_loss: 0.6504 - val_accuracy: 0.6429\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6813 - accuracy: 0.5668 - val_loss: 0.6516 - val_accuracy: 0.6429\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6787 - accuracy: 0.5685 - val_loss: 0.6501 - val_accuracy: 0.6416\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6762 - accuracy: 0.5767 - val_loss: 0.6501 - val_accuracy: 0.6403\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6775 - accuracy: 0.5745 - val_loss: 0.6487 - val_accuracy: 0.6429\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6774 - accuracy: 0.5757 - val_loss: 0.6474 - val_accuracy: 0.6429\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6765 - accuracy: 0.5762 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6756 - accuracy: 0.5794 - val_loss: 0.6466 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6779 - accuracy: 0.5710 - val_loss: 0.6466 - val_accuracy: 0.6429\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6758 - accuracy: 0.5772 - val_loss: 0.6452 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6769 - accuracy: 0.5775 - val_loss: 0.6467 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6732 - accuracy: 0.5852 - val_loss: 0.6463 - val_accuracy: 0.6416\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6734 - accuracy: 0.5785 - val_loss: 0.6456 - val_accuracy: 0.6429\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6761 - accuracy: 0.5804 - val_loss: 0.6446 - val_accuracy: 0.6429\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6723 - accuracy: 0.5806 - val_loss: 0.6431 - val_accuracy: 0.6441\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6722 - accuracy: 0.5844 - val_loss: 0.6441 - val_accuracy: 0.6441\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6712 - accuracy: 0.5903 - val_loss: 0.6442 - val_accuracy: 0.6429\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6756 - accuracy: 0.5823 - val_loss: 0.6450 - val_accuracy: 0.6467\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6726 - accuracy: 0.5834 - val_loss: 0.6439 - val_accuracy: 0.6454\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6717 - accuracy: 0.5875 - val_loss: 0.6443 - val_accuracy: 0.6429\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6724 - accuracy: 0.5860 - val_loss: 0.6445 - val_accuracy: 0.6492\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6691 - accuracy: 0.5877 - val_loss: 0.6425 - val_accuracy: 0.6518\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6681 - accuracy: 0.5904 - val_loss: 0.6390 - val_accuracy: 0.6467\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6706 - accuracy: 0.5875 - val_loss: 0.6414 - val_accuracy: 0.6492\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6704 - accuracy: 0.5870 - val_loss: 0.6417 - val_accuracy: 0.6531\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6708 - accuracy: 0.5908 - val_loss: 0.6405 - val_accuracy: 0.6492\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6737 - accuracy: 0.5855 - val_loss: 0.6424 - val_accuracy: 0.6480\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6695 - accuracy: 0.5879 - val_loss: 0.6411 - val_accuracy: 0.6505\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6726 - accuracy: 0.5884 - val_loss: 0.6422 - val_accuracy: 0.6480\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6699 - accuracy: 0.5904 - val_loss: 0.6395 - val_accuracy: 0.6480\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6668 - accuracy: 0.5941 - val_loss: 0.6405 - val_accuracy: 0.6518\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6700 - accuracy: 0.5942 - val_loss: 0.6405 - val_accuracy: 0.6505\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6691 - accuracy: 0.5950 - val_loss: 0.6408 - val_accuracy: 0.6531\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6663 - accuracy: 0.5971 - val_loss: 0.6386 - val_accuracy: 0.6543\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6687 - accuracy: 0.5975 - val_loss: 0.6416 - val_accuracy: 0.6492\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6660 - accuracy: 0.6002 - val_loss: 0.6390 - val_accuracy: 0.6492\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5975 - val_loss: 0.6395 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6683 - accuracy: 0.5970 - val_loss: 0.6386 - val_accuracy: 0.6518\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6669 - accuracy: 0.5995 - val_loss: 0.6379 - val_accuracy: 0.6518\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6644 - accuracy: 0.6046 - val_loss: 0.6370 - val_accuracy: 0.6531\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6684 - accuracy: 0.5938 - val_loss: 0.6370 - val_accuracy: 0.6531\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6685 - accuracy: 0.6004 - val_loss: 0.6358 - val_accuracy: 0.6531\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6650 - accuracy: 0.6064 - val_loss: 0.6342 - val_accuracy: 0.6556\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6669 - accuracy: 0.5976 - val_loss: 0.6354 - val_accuracy: 0.6556\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6661 - accuracy: 0.5951 - val_loss: 0.6352 - val_accuracy: 0.6543\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6663 - accuracy: 0.5956 - val_loss: 0.6344 - val_accuracy: 0.6505\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6633 - accuracy: 0.6022 - val_loss: 0.6338 - val_accuracy: 0.6607\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6677 - accuracy: 0.6004 - val_loss: 0.6321 - val_accuracy: 0.6633\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6622 - accuracy: 0.6074 - val_loss: 0.6320 - val_accuracy: 0.6620\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6662 - accuracy: 0.5941 - val_loss: 0.6335 - val_accuracy: 0.6582\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6646 - accuracy: 0.6064 - val_loss: 0.6332 - val_accuracy: 0.6543\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6620 - accuracy: 0.6093 - val_loss: 0.6317 - val_accuracy: 0.6543\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6675 - accuracy: 0.5961 - val_loss: 0.6374 - val_accuracy: 0.6633\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6647 - accuracy: 0.6068 - val_loss: 0.6317 - val_accuracy: 0.6505\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6619 - accuracy: 0.6110 - val_loss: 0.6318 - val_accuracy: 0.6620\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6615 - accuracy: 0.6069 - val_loss: 0.6316 - val_accuracy: 0.6620\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6643 - accuracy: 0.6046 - val_loss: 0.6328 - val_accuracy: 0.6607\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6614 - accuracy: 0.6031 - val_loss: 0.6300 - val_accuracy: 0.6556\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6647 - accuracy: 0.6011 - val_loss: 0.6325 - val_accuracy: 0.6633\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.6118 - val_loss: 0.6294 - val_accuracy: 0.6722\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6636 - accuracy: 0.6045 - val_loss: 0.6299 - val_accuracy: 0.6620\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6641 - accuracy: 0.6084 - val_loss: 0.6291 - val_accuracy: 0.6696\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6629 - accuracy: 0.6009 - val_loss: 0.6304 - val_accuracy: 0.6696\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6627 - accuracy: 0.6109 - val_loss: 0.6299 - val_accuracy: 0.6722\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6596 - accuracy: 0.6133 - val_loss: 0.6290 - val_accuracy: 0.6709\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6610 - accuracy: 0.6103 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6581 - accuracy: 0.6155 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6564 - accuracy: 0.6101 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6612 - accuracy: 0.6049 - val_loss: 0.6295 - val_accuracy: 0.6709\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6631 - accuracy: 0.6081 - val_loss: 0.6303 - val_accuracy: 0.6735\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6554 - accuracy: 0.6178 - val_loss: 0.6254 - val_accuracy: 0.6684\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6577 - accuracy: 0.6178 - val_loss: 0.6258 - val_accuracy: 0.6696\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6582 - accuracy: 0.6133 - val_loss: 0.6271 - val_accuracy: 0.6709\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6624 - accuracy: 0.6109 - val_loss: 0.6268 - val_accuracy: 0.6722\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6592 - accuracy: 0.6142 - val_loss: 0.6288 - val_accuracy: 0.6709\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6581 - accuracy: 0.6150 - val_loss: 0.6257 - val_accuracy: 0.6684\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6598 - accuracy: 0.6169 - val_loss: 0.6264 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6579 - accuracy: 0.6153 - val_loss: 0.6270 - val_accuracy: 0.6722\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6593 - accuracy: 0.6124 - val_loss: 0.6300 - val_accuracy: 0.6709\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6596 - accuracy: 0.6093 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.6119 - val_loss: 0.6233 - val_accuracy: 0.6696\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6582 - accuracy: 0.6084 - val_loss: 0.6235 - val_accuracy: 0.6696\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6589 - accuracy: 0.6081 - val_loss: 0.6263 - val_accuracy: 0.6735\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6570 - accuracy: 0.6164 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6588 - accuracy: 0.6139 - val_loss: 0.6241 - val_accuracy: 0.6709\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6560 - accuracy: 0.6189 - val_loss: 0.6214 - val_accuracy: 0.6735\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6536 - accuracy: 0.6183 - val_loss: 0.6194 - val_accuracy: 0.6722\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6512 - accuracy: 0.6211 - val_loss: 0.6231 - val_accuracy: 0.6722\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6551 - accuracy: 0.6169 - val_loss: 0.6225 - val_accuracy: 0.6709\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6580 - accuracy: 0.6178 - val_loss: 0.6225 - val_accuracy: 0.6722\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6596 - accuracy: 0.6154 - val_loss: 0.6258 - val_accuracy: 0.6760\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6571 - accuracy: 0.6164 - val_loss: 0.6209 - val_accuracy: 0.6722\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6575 - accuracy: 0.6211 - val_loss: 0.6228 - val_accuracy: 0.6760\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.6186 - val_loss: 0.6251 - val_accuracy: 0.6747\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6533 - accuracy: 0.6187 - val_loss: 0.6217 - val_accuracy: 0.6735\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6531 - accuracy: 0.6139 - val_loss: 0.6188 - val_accuracy: 0.6735\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6584 - accuracy: 0.6132 - val_loss: 0.6225 - val_accuracy: 0.6722\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6545 - accuracy: 0.6218 - val_loss: 0.6218 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6558 - accuracy: 0.6179 - val_loss: 0.6219 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6573 - accuracy: 0.6240 - val_loss: 0.6255 - val_accuracy: 0.6798\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6568 - accuracy: 0.6188 - val_loss: 0.6215 - val_accuracy: 0.6735\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6544 - accuracy: 0.6209 - val_loss: 0.6225 - val_accuracy: 0.6824\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6554 - accuracy: 0.6179 - val_loss: 0.6208 - val_accuracy: 0.6760\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6528 - accuracy: 0.6218 - val_loss: 0.6204 - val_accuracy: 0.6773\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6546 - accuracy: 0.6237 - val_loss: 0.6201 - val_accuracy: 0.6773\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6521 - accuracy: 0.6238 - val_loss: 0.6212 - val_accuracy: 0.6773\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.6232 - val_loss: 0.6191 - val_accuracy: 0.6798\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6506 - accuracy: 0.6261 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6546 - accuracy: 0.6216 - val_loss: 0.6210 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6544 - accuracy: 0.6274 - val_loss: 0.6164 - val_accuracy: 0.6824\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6560 - accuracy: 0.6183 - val_loss: 0.6207 - val_accuracy: 0.6798\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6521 - accuracy: 0.6232 - val_loss: 0.6187 - val_accuracy: 0.6760\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6556 - accuracy: 0.6203 - val_loss: 0.6247 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6514 - accuracy: 0.6218 - val_loss: 0.6183 - val_accuracy: 0.6798\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6514 - accuracy: 0.6213 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6534 - accuracy: 0.6216 - val_loss: 0.6235 - val_accuracy: 0.6811\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6528 - accuracy: 0.6236 - val_loss: 0.6175 - val_accuracy: 0.6786\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6483 - accuracy: 0.6275 - val_loss: 0.6163 - val_accuracy: 0.6837\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6508 - accuracy: 0.6216 - val_loss: 0.6184 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6513 - accuracy: 0.6289 - val_loss: 0.6192 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6522 - accuracy: 0.6240 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6511 - accuracy: 0.6295 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6507 - accuracy: 0.6238 - val_loss: 0.6143 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.6275 - val_loss: 0.6145 - val_accuracy: 0.6849\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6521 - accuracy: 0.6247 - val_loss: 0.6186 - val_accuracy: 0.6837\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6517 - accuracy: 0.6236 - val_loss: 0.6163 - val_accuracy: 0.6811\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6504 - accuracy: 0.6250 - val_loss: 0.6164 - val_accuracy: 0.6837\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6477 - accuracy: 0.6295 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6499 - accuracy: 0.6274 - val_loss: 0.6157 - val_accuracy: 0.6837\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6537 - accuracy: 0.6238 - val_loss: 0.6182 - val_accuracy: 0.6798\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6491 - accuracy: 0.6258 - val_loss: 0.6201 - val_accuracy: 0.6824\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6514 - accuracy: 0.6256 - val_loss: 0.6171 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6507 - accuracy: 0.6297 - val_loss: 0.6168 - val_accuracy: 0.6824\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6478 - accuracy: 0.6291 - val_loss: 0.6146 - val_accuracy: 0.6837\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6507 - accuracy: 0.6226 - val_loss: 0.6157 - val_accuracy: 0.6824\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6476 - accuracy: 0.6300 - val_loss: 0.6138 - val_accuracy: 0.6837\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6486 - accuracy: 0.6341 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6458 - accuracy: 0.6271 - val_loss: 0.6138 - val_accuracy: 0.6811\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6480 - accuracy: 0.6358 - val_loss: 0.6150 - val_accuracy: 0.6837\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6488 - accuracy: 0.6321 - val_loss: 0.6135 - val_accuracy: 0.6837\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6508 - accuracy: 0.6257 - val_loss: 0.6176 - val_accuracy: 0.6849\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6463 - accuracy: 0.6330 - val_loss: 0.6154 - val_accuracy: 0.6811\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6484 - accuracy: 0.6304 - val_loss: 0.6127 - val_accuracy: 0.6849\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6458 - accuracy: 0.6318 - val_loss: 0.6123 - val_accuracy: 0.6849\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6445 - accuracy: 0.6341 - val_loss: 0.6137 - val_accuracy: 0.6824\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6444 - accuracy: 0.6311 - val_loss: 0.6105 - val_accuracy: 0.6837\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6456 - accuracy: 0.6351 - val_loss: 0.6124 - val_accuracy: 0.6811\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6440 - accuracy: 0.6345 - val_loss: 0.6117 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6433 - accuracy: 0.6338 - val_loss: 0.6125 - val_accuracy: 0.6837\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6466 - accuracy: 0.6292 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6476 - accuracy: 0.6280 - val_loss: 0.6161 - val_accuracy: 0.6849\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6482 - accuracy: 0.6320 - val_loss: 0.6122 - val_accuracy: 0.6862\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6487 - accuracy: 0.6325 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6470 - accuracy: 0.6296 - val_loss: 0.6133 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6472 - accuracy: 0.6333 - val_loss: 0.6140 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6482 - accuracy: 0.6338 - val_loss: 0.6168 - val_accuracy: 0.6837\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6486 - accuracy: 0.6311 - val_loss: 0.6167 - val_accuracy: 0.6824\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6476 - accuracy: 0.6312 - val_loss: 0.6161 - val_accuracy: 0.6786\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6495 - accuracy: 0.6282 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6497 - accuracy: 0.6248 - val_loss: 0.6182 - val_accuracy: 0.6862\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6467 - accuracy: 0.6372 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6425 - accuracy: 0.6372 - val_loss: 0.6121 - val_accuracy: 0.6849\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6451 - accuracy: 0.6319 - val_loss: 0.6148 - val_accuracy: 0.6798\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6464 - accuracy: 0.6302 - val_loss: 0.6113 - val_accuracy: 0.6811\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6458 - accuracy: 0.6351 - val_loss: 0.6134 - val_accuracy: 0.6798\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6474 - accuracy: 0.6353 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6478 - accuracy: 0.6296 - val_loss: 0.6144 - val_accuracy: 0.6824\n",
      "Calculating for: 1000 100 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_204 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7640 - accuracy: 0.5437 - val_loss: 0.6817 - val_accuracy: 0.5574\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6961 - accuracy: 0.5775 - val_loss: 0.6628 - val_accuracy: 0.6148\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.5878 - val_loss: 0.6581 - val_accuracy: 0.6250\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5986 - val_loss: 0.6518 - val_accuracy: 0.6480\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6014 - val_loss: 0.6514 - val_accuracy: 0.6250\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6073 - val_loss: 0.6476 - val_accuracy: 0.6441\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6197 - val_loss: 0.6399 - val_accuracy: 0.6543\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6220 - val_loss: 0.6447 - val_accuracy: 0.6480\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6168 - val_loss: 0.6450 - val_accuracy: 0.6480\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6281 - val_loss: 0.6447 - val_accuracy: 0.6454\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6295 - val_loss: 0.6413 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6481 - accuracy: 0.6267 - val_loss: 0.6423 - val_accuracy: 0.6480\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6338 - val_loss: 0.6364 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6316 - val_loss: 0.6394 - val_accuracy: 0.6543\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6366 - val_loss: 0.6370 - val_accuracy: 0.6505\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6407 - val_loss: 0.6357 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6333 - val_loss: 0.6382 - val_accuracy: 0.6492\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6384 - val_loss: 0.6338 - val_accuracy: 0.6658\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6355 - val_loss: 0.6331 - val_accuracy: 0.6658\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6457 - val_loss: 0.6301 - val_accuracy: 0.6735\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6476 - val_loss: 0.6343 - val_accuracy: 0.6518\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6527 - val_loss: 0.6267 - val_accuracy: 0.6735\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6436 - val_loss: 0.6313 - val_accuracy: 0.6658\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6521 - val_loss: 0.6317 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6507 - val_loss: 0.6323 - val_accuracy: 0.6607\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6497 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6555 - val_loss: 0.6260 - val_accuracy: 0.6773\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6556 - val_loss: 0.6269 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6629 - val_loss: 0.6279 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6615 - val_loss: 0.6271 - val_accuracy: 0.6747\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6598 - val_loss: 0.6248 - val_accuracy: 0.6849\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6639 - val_loss: 0.6217 - val_accuracy: 0.6862\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6680 - val_loss: 0.6262 - val_accuracy: 0.6798\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6649 - val_loss: 0.6274 - val_accuracy: 0.6786\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6709 - val_loss: 0.6261 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6692 - val_loss: 0.6258 - val_accuracy: 0.6747\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6626 - val_loss: 0.6240 - val_accuracy: 0.6862\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6623 - val_loss: 0.6275 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6703 - val_loss: 0.6246 - val_accuracy: 0.6824\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6069 - accuracy: 0.6760 - val_loss: 0.6246 - val_accuracy: 0.6811\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6126 - accuracy: 0.6630 - val_loss: 0.6233 - val_accuracy: 0.6798\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6747 - val_loss: 0.6260 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6724 - val_loss: 0.6274 - val_accuracy: 0.6811\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6721 - val_loss: 0.6312 - val_accuracy: 0.6684\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6027 - accuracy: 0.6747 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6781 - val_loss: 0.6243 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6056 - accuracy: 0.6763 - val_loss: 0.6198 - val_accuracy: 0.6849\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5993 - accuracy: 0.6825 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5998 - accuracy: 0.6827 - val_loss: 0.6264 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5972 - accuracy: 0.6836 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5984 - accuracy: 0.6792 - val_loss: 0.6227 - val_accuracy: 0.6901\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5959 - accuracy: 0.6879 - val_loss: 0.6288 - val_accuracy: 0.6735\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5959 - accuracy: 0.6840 - val_loss: 0.6297 - val_accuracy: 0.6645\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5972 - accuracy: 0.6797 - val_loss: 0.6316 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5893 - accuracy: 0.6863 - val_loss: 0.6275 - val_accuracy: 0.6735\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5899 - accuracy: 0.6883 - val_loss: 0.6288 - val_accuracy: 0.6735\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5926 - accuracy: 0.6895 - val_loss: 0.6315 - val_accuracy: 0.6760\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5927 - accuracy: 0.6879 - val_loss: 0.6330 - val_accuracy: 0.6735\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5934 - accuracy: 0.6865 - val_loss: 0.6287 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5886 - accuracy: 0.6939 - val_loss: 0.6292 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5865 - accuracy: 0.6937 - val_loss: 0.6332 - val_accuracy: 0.6709\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5828 - accuracy: 0.6902 - val_loss: 0.6307 - val_accuracy: 0.6811\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5836 - accuracy: 0.6940 - val_loss: 0.6316 - val_accuracy: 0.6747\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5819 - accuracy: 0.6943 - val_loss: 0.6372 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5855 - accuracy: 0.6874 - val_loss: 0.6318 - val_accuracy: 0.6760\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5799 - accuracy: 0.6994 - val_loss: 0.6325 - val_accuracy: 0.6696\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5780 - accuracy: 0.6979 - val_loss: 0.6360 - val_accuracy: 0.6696\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5763 - accuracy: 0.6942 - val_loss: 0.6415 - val_accuracy: 0.6735\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5751 - accuracy: 0.7021 - val_loss: 0.6336 - val_accuracy: 0.6709\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5784 - accuracy: 0.6962 - val_loss: 0.6338 - val_accuracy: 0.6747\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5738 - accuracy: 0.6976 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5768 - accuracy: 0.6981 - val_loss: 0.6373 - val_accuracy: 0.6747\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5746 - accuracy: 0.7047 - val_loss: 0.6372 - val_accuracy: 0.6747\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5659 - accuracy: 0.7126 - val_loss: 0.6448 - val_accuracy: 0.6607\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5675 - accuracy: 0.7048 - val_loss: 0.6442 - val_accuracy: 0.6607\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5671 - accuracy: 0.7143 - val_loss: 0.6385 - val_accuracy: 0.6645\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5707 - accuracy: 0.7066 - val_loss: 0.6439 - val_accuracy: 0.6658\n",
      "Calculating for: 1000 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_208 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8118 - accuracy: 0.5320 - val_loss: 0.6590 - val_accuracy: 0.6403\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7199 - accuracy: 0.5573 - val_loss: 0.6486 - val_accuracy: 0.6416\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6927 - accuracy: 0.5610 - val_loss: 0.6466 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6765 - accuracy: 0.5803 - val_loss: 0.6427 - val_accuracy: 0.6645\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5789 - val_loss: 0.6477 - val_accuracy: 0.6492\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5787 - val_loss: 0.6522 - val_accuracy: 0.6531\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5924 - val_loss: 0.6493 - val_accuracy: 0.6531\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.6015 - val_loss: 0.6458 - val_accuracy: 0.6569\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5991 - val_loss: 0.6427 - val_accuracy: 0.6620\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6064 - val_loss: 0.6349 - val_accuracy: 0.6696\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6638 - accuracy: 0.6056 - val_loss: 0.6372 - val_accuracy: 0.6684\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6035 - val_loss: 0.6427 - val_accuracy: 0.6582\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6099 - val_loss: 0.6435 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6044 - val_loss: 0.6386 - val_accuracy: 0.6671\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6045 - val_loss: 0.6412 - val_accuracy: 0.6543\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6075 - val_loss: 0.6334 - val_accuracy: 0.6735\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6596 - accuracy: 0.6101 - val_loss: 0.6426 - val_accuracy: 0.6505\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6149 - val_loss: 0.6355 - val_accuracy: 0.6696\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6091 - val_loss: 0.6334 - val_accuracy: 0.6760\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6218 - val_loss: 0.6391 - val_accuracy: 0.6505\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6204 - val_loss: 0.6364 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6182 - val_loss: 0.6356 - val_accuracy: 0.6556\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6216 - val_loss: 0.6365 - val_accuracy: 0.6569\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6257 - val_loss: 0.6420 - val_accuracy: 0.6441\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6213 - val_loss: 0.6340 - val_accuracy: 0.6645\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6280 - val_loss: 0.6332 - val_accuracy: 0.6671\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6252 - val_loss: 0.6297 - val_accuracy: 0.6798\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6206 - val_loss: 0.6293 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6312 - val_loss: 0.6295 - val_accuracy: 0.6735\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6286 - val_loss: 0.6298 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6228 - val_loss: 0.6300 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6282 - val_loss: 0.6342 - val_accuracy: 0.6607\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6480 - accuracy: 0.6304 - val_loss: 0.6330 - val_accuracy: 0.6696\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6311 - val_loss: 0.6298 - val_accuracy: 0.6735\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6445 - accuracy: 0.6287 - val_loss: 0.6318 - val_accuracy: 0.6722\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6329 - val_loss: 0.6313 - val_accuracy: 0.6594\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6324 - val_loss: 0.6287 - val_accuracy: 0.6735\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6445 - accuracy: 0.6318 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6333 - val_loss: 0.6234 - val_accuracy: 0.6798\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6336 - val_loss: 0.6309 - val_accuracy: 0.6620\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6297 - val_loss: 0.6264 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6349 - val_loss: 0.6205 - val_accuracy: 0.6888\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6389 - val_loss: 0.6270 - val_accuracy: 0.6735\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6274 - val_loss: 0.6303 - val_accuracy: 0.6582\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6453 - val_loss: 0.6247 - val_accuracy: 0.6786\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6385 - val_loss: 0.6261 - val_accuracy: 0.6620\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6348 - val_loss: 0.6307 - val_accuracy: 0.6594\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6436 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6384 - accuracy: 0.6414 - val_loss: 0.6251 - val_accuracy: 0.6633\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6409 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6498 - val_loss: 0.6281 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6433 - val_loss: 0.6262 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6444 - val_loss: 0.6280 - val_accuracy: 0.6786\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6400 - val_loss: 0.6291 - val_accuracy: 0.6645\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6414 - val_loss: 0.6267 - val_accuracy: 0.6747\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6431 - val_loss: 0.6266 - val_accuracy: 0.6747\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6426 - val_loss: 0.6275 - val_accuracy: 0.6645\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6507 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6436 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6438 - val_loss: 0.6184 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6517 - val_loss: 0.6235 - val_accuracy: 0.6760\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6487 - val_loss: 0.6231 - val_accuracy: 0.6798\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6539 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6531 - val_loss: 0.6202 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6593 - val_loss: 0.6253 - val_accuracy: 0.6696\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6457 - val_loss: 0.6172 - val_accuracy: 0.6862\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6600 - val_loss: 0.6253 - val_accuracy: 0.6735\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6270 - accuracy: 0.6596 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6260 - accuracy: 0.6575 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6272 - accuracy: 0.6562 - val_loss: 0.6288 - val_accuracy: 0.6722\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6574 - val_loss: 0.6269 - val_accuracy: 0.6671\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6545 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6529 - val_loss: 0.6237 - val_accuracy: 0.6709\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6583 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6545 - val_loss: 0.6218 - val_accuracy: 0.6747\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6540 - val_loss: 0.6224 - val_accuracy: 0.6760\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6615 - val_loss: 0.6214 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6194 - accuracy: 0.6618 - val_loss: 0.6202 - val_accuracy: 0.6735\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6530 - val_loss: 0.6249 - val_accuracy: 0.6684\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6664 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6647 - val_loss: 0.6230 - val_accuracy: 0.6747\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6606 - val_loss: 0.6221 - val_accuracy: 0.6773\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6684 - val_loss: 0.6245 - val_accuracy: 0.6709\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6642 - val_loss: 0.6242 - val_accuracy: 0.6658\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6644 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6139 - accuracy: 0.6654 - val_loss: 0.6207 - val_accuracy: 0.6786\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6673 - val_loss: 0.6265 - val_accuracy: 0.6696\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6603 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6649 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6194 - accuracy: 0.6672 - val_loss: 0.6277 - val_accuracy: 0.6645\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6595 - val_loss: 0.6223 - val_accuracy: 0.6760\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6642 - val_loss: 0.6255 - val_accuracy: 0.6684\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6637 - val_loss: 0.6213 - val_accuracy: 0.6760\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6655 - val_loss: 0.6234 - val_accuracy: 0.6747\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6638 - val_loss: 0.6224 - val_accuracy: 0.6722\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6117 - accuracy: 0.6706 - val_loss: 0.6225 - val_accuracy: 0.6786\n",
      "Calculating for: 1000 100 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_212 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "236/249 [===========================>..] - ETA: 0s - loss: 0.8126 - accuracy: 0.5097WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8087 - accuracy: 0.5113 - val_loss: 0.6791 - val_accuracy: 0.6237\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7319 - accuracy: 0.5098 - val_loss: 0.6776 - val_accuracy: 0.6224\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7053 - accuracy: 0.5190 - val_loss: 0.6766 - val_accuracy: 0.6212\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6961 - accuracy: 0.5231 - val_loss: 0.6685 - val_accuracy: 0.6199\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5264 - val_loss: 0.6703 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5291 - val_loss: 0.6701 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5285 - val_loss: 0.6743 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5279 - val_loss: 0.6673 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5337 - val_loss: 0.6700 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5322 - val_loss: 0.6685 - val_accuracy: 0.6199\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5480 - val_loss: 0.6736 - val_accuracy: 0.6199\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6904 - accuracy: 0.5383 - val_loss: 0.6664 - val_accuracy: 0.6199\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6888 - accuracy: 0.5409 - val_loss: 0.6657 - val_accuracy: 0.6199\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.5352 - val_loss: 0.6699 - val_accuracy: 0.6224\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5333 - val_loss: 0.6703 - val_accuracy: 0.6237\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6893 - accuracy: 0.5377 - val_loss: 0.6653 - val_accuracy: 0.6212\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.5330 - val_loss: 0.6670 - val_accuracy: 0.6224\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6883 - accuracy: 0.5399 - val_loss: 0.6698 - val_accuracy: 0.6237\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6873 - accuracy: 0.5423 - val_loss: 0.6636 - val_accuracy: 0.6237\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6864 - accuracy: 0.5445 - val_loss: 0.6633 - val_accuracy: 0.6237\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5404 - val_loss: 0.6652 - val_accuracy: 0.6276\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6860 - accuracy: 0.5467 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6854 - accuracy: 0.5455 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6868 - accuracy: 0.5472 - val_loss: 0.6654 - val_accuracy: 0.6352\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6870 - accuracy: 0.5487 - val_loss: 0.6677 - val_accuracy: 0.6378\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5461 - val_loss: 0.6584 - val_accuracy: 0.6288\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5556 - val_loss: 0.6581 - val_accuracy: 0.6339\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5482 - val_loss: 0.6610 - val_accuracy: 0.6352\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6845 - accuracy: 0.5570 - val_loss: 0.6593 - val_accuracy: 0.6390\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6863 - accuracy: 0.5524 - val_loss: 0.6666 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6830 - accuracy: 0.5593 - val_loss: 0.6568 - val_accuracy: 0.6314\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.5622 - val_loss: 0.6595 - val_accuracy: 0.6378\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5555 - val_loss: 0.6612 - val_accuracy: 0.6403\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6801 - accuracy: 0.5588 - val_loss: 0.6572 - val_accuracy: 0.6403\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.5624 - val_loss: 0.6593 - val_accuracy: 0.6416\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.5633 - val_loss: 0.6566 - val_accuracy: 0.6441\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5535 - val_loss: 0.6551 - val_accuracy: 0.6365\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6814 - accuracy: 0.5593 - val_loss: 0.6543 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5608 - val_loss: 0.6593 - val_accuracy: 0.6429\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6802 - accuracy: 0.5662 - val_loss: 0.6578 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6818 - accuracy: 0.5583 - val_loss: 0.6591 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6787 - accuracy: 0.5750 - val_loss: 0.6533 - val_accuracy: 0.6429\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6823 - accuracy: 0.5683 - val_loss: 0.6555 - val_accuracy: 0.6441\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6780 - accuracy: 0.5732 - val_loss: 0.6524 - val_accuracy: 0.6429\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5683 - val_loss: 0.6514 - val_accuracy: 0.6429\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6779 - accuracy: 0.5659 - val_loss: 0.6501 - val_accuracy: 0.6416\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5791 - val_loss: 0.6512 - val_accuracy: 0.6403\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6804 - accuracy: 0.5624 - val_loss: 0.6531 - val_accuracy: 0.6429\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6799 - accuracy: 0.5623 - val_loss: 0.6527 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5756 - val_loss: 0.6500 - val_accuracy: 0.6429\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5647 - val_loss: 0.6507 - val_accuracy: 0.6429\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5742 - val_loss: 0.6496 - val_accuracy: 0.6429\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5774 - val_loss: 0.6493 - val_accuracy: 0.6441\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6771 - accuracy: 0.5745 - val_loss: 0.6500 - val_accuracy: 0.6416\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5662 - val_loss: 0.6529 - val_accuracy: 0.6416\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5735 - val_loss: 0.6531 - val_accuracy: 0.6429\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.5730 - val_loss: 0.6479 - val_accuracy: 0.6416\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5824 - val_loss: 0.6478 - val_accuracy: 0.6390\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6774 - accuracy: 0.5813 - val_loss: 0.6504 - val_accuracy: 0.6429\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.5789 - val_loss: 0.6468 - val_accuracy: 0.6416\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5800 - val_loss: 0.6499 - val_accuracy: 0.6505\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6757 - accuracy: 0.5803 - val_loss: 0.6489 - val_accuracy: 0.6429\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6774 - accuracy: 0.5730 - val_loss: 0.6466 - val_accuracy: 0.6429\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5843 - val_loss: 0.6464 - val_accuracy: 0.6492\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6747 - accuracy: 0.5785 - val_loss: 0.6446 - val_accuracy: 0.6492\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5901 - val_loss: 0.6440 - val_accuracy: 0.6480\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5838 - val_loss: 0.6457 - val_accuracy: 0.6480\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.5870 - val_loss: 0.6469 - val_accuracy: 0.6441\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5867 - val_loss: 0.6442 - val_accuracy: 0.6467\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5926 - val_loss: 0.6448 - val_accuracy: 0.6441\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.5776 - val_loss: 0.6476 - val_accuracy: 0.6492\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.5834 - val_loss: 0.6433 - val_accuracy: 0.6492\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5916 - val_loss: 0.6431 - val_accuracy: 0.6505\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5882 - val_loss: 0.6460 - val_accuracy: 0.6480\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6700 - accuracy: 0.5883 - val_loss: 0.6440 - val_accuracy: 0.6505\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5896 - val_loss: 0.6434 - val_accuracy: 0.6480\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5922 - val_loss: 0.6413 - val_accuracy: 0.6480\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5957 - val_loss: 0.6426 - val_accuracy: 0.6543\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6719 - accuracy: 0.5867 - val_loss: 0.6444 - val_accuracy: 0.6480\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6700 - accuracy: 0.5887 - val_loss: 0.6420 - val_accuracy: 0.6518\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5919 - val_loss: 0.6410 - val_accuracy: 0.6505\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5944 - val_loss: 0.6398 - val_accuracy: 0.6492\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5868 - val_loss: 0.6406 - val_accuracy: 0.6518\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5929 - val_loss: 0.6419 - val_accuracy: 0.6518\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5972 - val_loss: 0.6449 - val_accuracy: 0.6531\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5887 - val_loss: 0.6408 - val_accuracy: 0.6556\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5916 - val_loss: 0.6448 - val_accuracy: 0.6658\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5931 - val_loss: 0.6369 - val_accuracy: 0.6518\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6684 - accuracy: 0.5917 - val_loss: 0.6409 - val_accuracy: 0.6569\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5966 - val_loss: 0.6375 - val_accuracy: 0.6505\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5874 - val_loss: 0.6403 - val_accuracy: 0.6518\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6678 - accuracy: 0.5971 - val_loss: 0.6408 - val_accuracy: 0.6556\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6675 - accuracy: 0.5995 - val_loss: 0.6389 - val_accuracy: 0.6607\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.6021 - val_loss: 0.6356 - val_accuracy: 0.6582\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6007 - val_loss: 0.6403 - val_accuracy: 0.6658\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6678 - accuracy: 0.5919 - val_loss: 0.6385 - val_accuracy: 0.6620\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6004 - val_loss: 0.6351 - val_accuracy: 0.6582\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.6080 - val_loss: 0.6360 - val_accuracy: 0.6620\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.6001 - val_loss: 0.6376 - val_accuracy: 0.6594\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.5966 - val_loss: 0.6352 - val_accuracy: 0.6747\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6071 - val_loss: 0.6349 - val_accuracy: 0.6658\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6673 - accuracy: 0.6014 - val_loss: 0.6412 - val_accuracy: 0.6747\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.5981 - val_loss: 0.6382 - val_accuracy: 0.6684\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6005 - val_loss: 0.6411 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6051 - val_loss: 0.6373 - val_accuracy: 0.6684\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6656 - accuracy: 0.6024 - val_loss: 0.6379 - val_accuracy: 0.6671\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.5988 - val_loss: 0.6355 - val_accuracy: 0.6722\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6639 - accuracy: 0.6065 - val_loss: 0.6343 - val_accuracy: 0.6645\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6096 - val_loss: 0.6339 - val_accuracy: 0.6645\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6660 - accuracy: 0.5996 - val_loss: 0.6348 - val_accuracy: 0.6735\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.5950 - val_loss: 0.6370 - val_accuracy: 0.6747\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6090 - val_loss: 0.6337 - val_accuracy: 0.6696\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6014 - val_loss: 0.6338 - val_accuracy: 0.6709\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6042 - val_loss: 0.6338 - val_accuracy: 0.6735\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6021 - val_loss: 0.6341 - val_accuracy: 0.6722\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.6061 - val_loss: 0.6383 - val_accuracy: 0.6696\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6086 - val_loss: 0.6331 - val_accuracy: 0.6735\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6066 - val_loss: 0.6351 - val_accuracy: 0.6798\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6022 - val_loss: 0.6333 - val_accuracy: 0.6722\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6060 - val_loss: 0.6323 - val_accuracy: 0.6722\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6130 - val_loss: 0.6300 - val_accuracy: 0.6722\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6065 - val_loss: 0.6302 - val_accuracy: 0.6684\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6036 - val_loss: 0.6345 - val_accuracy: 0.6875\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6162 - val_loss: 0.6284 - val_accuracy: 0.6722\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6623 - accuracy: 0.6050 - val_loss: 0.6312 - val_accuracy: 0.6735\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6632 - accuracy: 0.6113 - val_loss: 0.6322 - val_accuracy: 0.6773\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6176 - val_loss: 0.6335 - val_accuracy: 0.6747\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6100 - val_loss: 0.6301 - val_accuracy: 0.6709\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6162 - val_loss: 0.6305 - val_accuracy: 0.6786\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6125 - val_loss: 0.6274 - val_accuracy: 0.6773\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6158 - val_loss: 0.6279 - val_accuracy: 0.6722\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.6120 - val_loss: 0.6291 - val_accuracy: 0.6709\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6578 - accuracy: 0.6187 - val_loss: 0.6331 - val_accuracy: 0.6849\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6130 - val_loss: 0.6330 - val_accuracy: 0.6837\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6566 - accuracy: 0.6196 - val_loss: 0.6272 - val_accuracy: 0.6811\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6109 - val_loss: 0.6312 - val_accuracy: 0.6786\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6114 - val_loss: 0.6252 - val_accuracy: 0.6837\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6118 - val_loss: 0.6270 - val_accuracy: 0.6824\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6584 - accuracy: 0.6094 - val_loss: 0.6277 - val_accuracy: 0.6747\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6117 - val_loss: 0.6284 - val_accuracy: 0.6773\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6063 - val_loss: 0.6277 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6162 - val_loss: 0.6297 - val_accuracy: 0.6760\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6125 - val_loss: 0.6270 - val_accuracy: 0.6760\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6208 - val_loss: 0.6300 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6571 - accuracy: 0.6152 - val_loss: 0.6297 - val_accuracy: 0.6798\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6129 - val_loss: 0.6299 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6145 - val_loss: 0.6284 - val_accuracy: 0.6773\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6101 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6213 - val_loss: 0.6274 - val_accuracy: 0.6773\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6178 - val_loss: 0.6227 - val_accuracy: 0.6811\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6207 - val_loss: 0.6271 - val_accuracy: 0.6798\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6189 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6242 - val_loss: 0.6237 - val_accuracy: 0.6773\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6199 - val_loss: 0.6275 - val_accuracy: 0.6837\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6262 - val_loss: 0.6235 - val_accuracy: 0.6824\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6179 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6196 - val_loss: 0.6228 - val_accuracy: 0.6786\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6237 - val_loss: 0.6224 - val_accuracy: 0.6798\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6227 - val_loss: 0.6278 - val_accuracy: 0.6798\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6241 - val_loss: 0.6217 - val_accuracy: 0.6811\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6562 - accuracy: 0.6271 - val_loss: 0.6225 - val_accuracy: 0.6811\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6215 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6167 - val_loss: 0.6281 - val_accuracy: 0.6824\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6133 - val_loss: 0.6284 - val_accuracy: 0.6811\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6215 - val_loss: 0.6282 - val_accuracy: 0.6837\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6173 - val_loss: 0.6285 - val_accuracy: 0.6849\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6169 - val_loss: 0.6241 - val_accuracy: 0.6849\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6544 - accuracy: 0.6255 - val_loss: 0.6274 - val_accuracy: 0.6849\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6166 - val_loss: 0.6305 - val_accuracy: 0.6811\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6226 - val_loss: 0.6231 - val_accuracy: 0.6811\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6242 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6246 - val_loss: 0.6261 - val_accuracy: 0.6824\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6262 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6245 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6221 - val_loss: 0.6276 - val_accuracy: 0.6824\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.6182 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6521 - accuracy: 0.6243 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6546 - accuracy: 0.6147 - val_loss: 0.6246 - val_accuracy: 0.6811\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6242 - val_loss: 0.6228 - val_accuracy: 0.6824\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6302 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6232 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6206 - val_loss: 0.6195 - val_accuracy: 0.6824\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6211 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6241 - val_loss: 0.6245 - val_accuracy: 0.6824\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6325 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6274 - val_loss: 0.6239 - val_accuracy: 0.6811\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6255 - val_loss: 0.6211 - val_accuracy: 0.6786\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6186 - val_loss: 0.6222 - val_accuracy: 0.6798\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6284 - val_loss: 0.6214 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6282 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6262 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6227 - val_loss: 0.6196 - val_accuracy: 0.6849\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6237 - val_loss: 0.6186 - val_accuracy: 0.6862\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6204 - val_loss: 0.6212 - val_accuracy: 0.6824\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6265 - val_loss: 0.6237 - val_accuracy: 0.6824\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6262 - val_loss: 0.6217 - val_accuracy: 0.6837\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6516 - accuracy: 0.6211 - val_loss: 0.6222 - val_accuracy: 0.6849\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6292 - val_loss: 0.6219 - val_accuracy: 0.6811\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6247 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6188 - val_loss: 0.6247 - val_accuracy: 0.6811\n",
      "Calculating for: 1000 250 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_216 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7943 - accuracy: 0.5633 - val_loss: 0.6410 - val_accuracy: 0.6722\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7021 - accuracy: 0.5840 - val_loss: 0.6257 - val_accuracy: 0.6786\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6703 - accuracy: 0.6049 - val_loss: 0.6279 - val_accuracy: 0.6849\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6610 - accuracy: 0.6090 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6140 - val_loss: 0.6236 - val_accuracy: 0.6862\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6204 - val_loss: 0.6241 - val_accuracy: 0.6811\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6237 - val_loss: 0.6193 - val_accuracy: 0.6888\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6505 - accuracy: 0.6197 - val_loss: 0.6188 - val_accuracy: 0.6849\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6477 - accuracy: 0.6270 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6437 - accuracy: 0.6256 - val_loss: 0.6192 - val_accuracy: 0.6901\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6411 - accuracy: 0.6364 - val_loss: 0.6155 - val_accuracy: 0.6811\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6395 - accuracy: 0.6392 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6382 - val_loss: 0.6182 - val_accuracy: 0.6964\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6320 - accuracy: 0.6481 - val_loss: 0.6145 - val_accuracy: 0.6901\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6434 - val_loss: 0.6175 - val_accuracy: 0.6926\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6353 - accuracy: 0.6483 - val_loss: 0.6190 - val_accuracy: 0.6888\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6426 - val_loss: 0.6183 - val_accuracy: 0.6913\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6446 - val_loss: 0.6151 - val_accuracy: 0.6926\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6551 - val_loss: 0.6163 - val_accuracy: 0.6888\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6461 - val_loss: 0.6138 - val_accuracy: 0.6901\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6498 - val_loss: 0.6164 - val_accuracy: 0.6977\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6525 - val_loss: 0.6158 - val_accuracy: 0.6977\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6550 - val_loss: 0.6210 - val_accuracy: 0.6811\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6569 - val_loss: 0.6186 - val_accuracy: 0.6913\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6611 - val_loss: 0.6199 - val_accuracy: 0.6837\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6265 - accuracy: 0.6518 - val_loss: 0.6150 - val_accuracy: 0.6964\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6655 - val_loss: 0.6186 - val_accuracy: 0.6977\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6198 - accuracy: 0.6615 - val_loss: 0.6191 - val_accuracy: 0.6888\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6133 - accuracy: 0.6680 - val_loss: 0.6220 - val_accuracy: 0.6901\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6196 - accuracy: 0.6613 - val_loss: 0.6195 - val_accuracy: 0.6964\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6142 - accuracy: 0.6694 - val_loss: 0.6193 - val_accuracy: 0.6964\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6098 - accuracy: 0.6733 - val_loss: 0.6199 - val_accuracy: 0.6849\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6110 - accuracy: 0.6714 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6094 - accuracy: 0.6788 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6102 - accuracy: 0.6662 - val_loss: 0.6198 - val_accuracy: 0.6901\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6077 - accuracy: 0.6707 - val_loss: 0.6202 - val_accuracy: 0.6875\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6081 - accuracy: 0.6732 - val_loss: 0.6156 - val_accuracy: 0.6926\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6042 - accuracy: 0.6729 - val_loss: 0.6246 - val_accuracy: 0.6888\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6057 - accuracy: 0.6694 - val_loss: 0.6215 - val_accuracy: 0.6888\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6025 - accuracy: 0.6800 - val_loss: 0.6299 - val_accuracy: 0.6824\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6001 - accuracy: 0.6860 - val_loss: 0.6305 - val_accuracy: 0.6888\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5971 - accuracy: 0.6851 - val_loss: 0.6273 - val_accuracy: 0.6837\n",
      "Calculating for: 1000 250 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_220 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8247 - accuracy: 0.5345 - val_loss: 0.6424 - val_accuracy: 0.6531\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7178 - accuracy: 0.5599 - val_loss: 0.6452 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6887 - accuracy: 0.5745 - val_loss: 0.6401 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6773 - accuracy: 0.5752 - val_loss: 0.6407 - val_accuracy: 0.6671\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6712 - accuracy: 0.5855 - val_loss: 0.6392 - val_accuracy: 0.6696\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6705 - accuracy: 0.5902 - val_loss: 0.6387 - val_accuracy: 0.6658\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6666 - accuracy: 0.5976 - val_loss: 0.6398 - val_accuracy: 0.6582\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6668 - accuracy: 0.5962 - val_loss: 0.6382 - val_accuracy: 0.6645\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6665 - accuracy: 0.5928 - val_loss: 0.6403 - val_accuracy: 0.6556\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6613 - accuracy: 0.6076 - val_loss: 0.6371 - val_accuracy: 0.6505\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6041 - val_loss: 0.6335 - val_accuracy: 0.6671\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6623 - accuracy: 0.6059 - val_loss: 0.6308 - val_accuracy: 0.6658\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6605 - accuracy: 0.6074 - val_loss: 0.6317 - val_accuracy: 0.6671\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6613 - accuracy: 0.6045 - val_loss: 0.6295 - val_accuracy: 0.6658\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6568 - accuracy: 0.6171 - val_loss: 0.6298 - val_accuracy: 0.6722\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6144 - val_loss: 0.6296 - val_accuracy: 0.6722\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6568 - accuracy: 0.6171 - val_loss: 0.6276 - val_accuracy: 0.6633\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6558 - accuracy: 0.6138 - val_loss: 0.6325 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6542 - accuracy: 0.6137 - val_loss: 0.6281 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6540 - accuracy: 0.6191 - val_loss: 0.6305 - val_accuracy: 0.6760\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6290 - val_loss: 0.6254 - val_accuracy: 0.6709\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6183 - val_loss: 0.6299 - val_accuracy: 0.6709\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.6194 - val_loss: 0.6260 - val_accuracy: 0.6696\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6240 - val_loss: 0.6236 - val_accuracy: 0.6747\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6290 - val_loss: 0.6224 - val_accuracy: 0.6735\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6193 - val_loss: 0.6234 - val_accuracy: 0.6747\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6264 - val_loss: 0.6239 - val_accuracy: 0.6773\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6228 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6227 - val_loss: 0.6227 - val_accuracy: 0.6773\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6343 - val_loss: 0.6275 - val_accuracy: 0.6722\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6436 - accuracy: 0.6341 - val_loss: 0.6196 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6261 - val_loss: 0.6199 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6442 - accuracy: 0.6289 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6467 - accuracy: 0.6280 - val_loss: 0.6194 - val_accuracy: 0.6875\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6448 - accuracy: 0.6280 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6306 - val_loss: 0.6197 - val_accuracy: 0.6786\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6368 - val_loss: 0.6166 - val_accuracy: 0.6875\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6433 - accuracy: 0.6329 - val_loss: 0.6181 - val_accuracy: 0.6773\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6454 - accuracy: 0.6309 - val_loss: 0.6231 - val_accuracy: 0.6760\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6387 - val_loss: 0.6179 - val_accuracy: 0.6760\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6355 - val_loss: 0.6241 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6409 - accuracy: 0.6408 - val_loss: 0.6170 - val_accuracy: 0.6849\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6389 - accuracy: 0.6429 - val_loss: 0.6189 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6385 - val_loss: 0.6198 - val_accuracy: 0.6696\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6374 - val_loss: 0.6175 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6436 - val_loss: 0.6156 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6409 - val_loss: 0.6178 - val_accuracy: 0.6811\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6434 - val_loss: 0.6158 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6383 - accuracy: 0.6423 - val_loss: 0.6182 - val_accuracy: 0.6747\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6358 - accuracy: 0.6437 - val_loss: 0.6201 - val_accuracy: 0.6773\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6419 - val_loss: 0.6184 - val_accuracy: 0.6747\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6409 - val_loss: 0.6175 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.6438 - val_loss: 0.6196 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6362 - accuracy: 0.6503 - val_loss: 0.6181 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6434 - val_loss: 0.6168 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6332 - accuracy: 0.6480 - val_loss: 0.6169 - val_accuracy: 0.6798\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6472 - val_loss: 0.6170 - val_accuracy: 0.6837\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6506 - val_loss: 0.6128 - val_accuracy: 0.6849\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6486 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6292 - accuracy: 0.6501 - val_loss: 0.6131 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6462 - val_loss: 0.6157 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6522 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6540 - val_loss: 0.6148 - val_accuracy: 0.6837\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6529 - val_loss: 0.6164 - val_accuracy: 0.6849\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6556 - val_loss: 0.6135 - val_accuracy: 0.6901\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6492 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6572 - val_loss: 0.6125 - val_accuracy: 0.6875\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6659 - val_loss: 0.6131 - val_accuracy: 0.6849\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6551 - val_loss: 0.6156 - val_accuracy: 0.6875\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6546 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6615 - val_loss: 0.6174 - val_accuracy: 0.6964\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6210 - accuracy: 0.6629 - val_loss: 0.6143 - val_accuracy: 0.6964\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6567 - val_loss: 0.6146 - val_accuracy: 0.6990\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6606 - val_loss: 0.6161 - val_accuracy: 0.6939\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6191 - accuracy: 0.6665 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6644 - val_loss: 0.6146 - val_accuracy: 0.6913\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6570 - val_loss: 0.6155 - val_accuracy: 0.6939\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6204 - accuracy: 0.6535 - val_loss: 0.6154 - val_accuracy: 0.6952\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6209 - accuracy: 0.6551 - val_loss: 0.6164 - val_accuracy: 0.6939\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6609 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6605 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6192 - accuracy: 0.6629 - val_loss: 0.6167 - val_accuracy: 0.6926\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6143 - accuracy: 0.6683 - val_loss: 0.6137 - val_accuracy: 0.6939\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6581 - val_loss: 0.6157 - val_accuracy: 0.6939\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6210 - accuracy: 0.6620 - val_loss: 0.6133 - val_accuracy: 0.6977\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6151 - accuracy: 0.6644 - val_loss: 0.6142 - val_accuracy: 0.7003\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6172 - accuracy: 0.6639 - val_loss: 0.6117 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6117 - accuracy: 0.6618 - val_loss: 0.6141 - val_accuracy: 0.6939\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6157 - accuracy: 0.6644 - val_loss: 0.6126 - val_accuracy: 0.6952\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6157 - accuracy: 0.6613 - val_loss: 0.6121 - val_accuracy: 0.6964\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6138 - accuracy: 0.6682 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6140 - accuracy: 0.6619 - val_loss: 0.6173 - val_accuracy: 0.6926\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6124 - accuracy: 0.6667 - val_loss: 0.6141 - val_accuracy: 0.6964\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6090 - accuracy: 0.6747 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6128 - accuracy: 0.6717 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6122 - accuracy: 0.6675 - val_loss: 0.6162 - val_accuracy: 0.6862\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6088 - accuracy: 0.6644 - val_loss: 0.6149 - val_accuracy: 0.6952\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6120 - accuracy: 0.6704 - val_loss: 0.6164 - val_accuracy: 0.6888\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6125 - accuracy: 0.6717 - val_loss: 0.6184 - val_accuracy: 0.6888\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6067 - accuracy: 0.6762 - val_loss: 0.6174 - val_accuracy: 0.6888\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6111 - accuracy: 0.6712 - val_loss: 0.6184 - val_accuracy: 0.6913\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6105 - accuracy: 0.6718 - val_loss: 0.6141 - val_accuracy: 0.6939\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6070 - accuracy: 0.6711 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6077 - accuracy: 0.6713 - val_loss: 0.6160 - val_accuracy: 0.6901\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6043 - accuracy: 0.6777 - val_loss: 0.6158 - val_accuracy: 0.6926\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6074 - accuracy: 0.6765 - val_loss: 0.6138 - val_accuracy: 0.6913\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6035 - accuracy: 0.6760 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6032 - accuracy: 0.6802 - val_loss: 0.6199 - val_accuracy: 0.6811\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6026 - accuracy: 0.6770 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6006 - accuracy: 0.6775 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6046 - accuracy: 0.6791 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6089 - accuracy: 0.6722 - val_loss: 0.6207 - val_accuracy: 0.6760\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5976 - accuracy: 0.6777 - val_loss: 0.6193 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6005 - accuracy: 0.6820 - val_loss: 0.6214 - val_accuracy: 0.6849\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5981 - accuracy: 0.6801 - val_loss: 0.6193 - val_accuracy: 0.6837\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6073 - accuracy: 0.6770 - val_loss: 0.6181 - val_accuracy: 0.6901\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6016 - accuracy: 0.6776 - val_loss: 0.6198 - val_accuracy: 0.6875\n",
      "Calculating for: 1000 250 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_224 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8326 - accuracy: 0.5055 - val_loss: 0.6657 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7330 - accuracy: 0.5206 - val_loss: 0.6654 - val_accuracy: 0.6212\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7068 - accuracy: 0.5092 - val_loss: 0.6653 - val_accuracy: 0.6186\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6916 - accuracy: 0.5319 - val_loss: 0.6661 - val_accuracy: 0.6186\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6936 - accuracy: 0.5229 - val_loss: 0.6691 - val_accuracy: 0.6224\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6902 - accuracy: 0.5323 - val_loss: 0.6713 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6919 - accuracy: 0.5283 - val_loss: 0.6692 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6901 - accuracy: 0.5359 - val_loss: 0.6686 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6907 - accuracy: 0.5381 - val_loss: 0.6664 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.5363 - val_loss: 0.6662 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6909 - accuracy: 0.5335 - val_loss: 0.6703 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6888 - accuracy: 0.5367 - val_loss: 0.6658 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6880 - accuracy: 0.5389 - val_loss: 0.6635 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6900 - accuracy: 0.5322 - val_loss: 0.6662 - val_accuracy: 0.6199\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6892 - accuracy: 0.5382 - val_loss: 0.6639 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6891 - accuracy: 0.5352 - val_loss: 0.6637 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6882 - accuracy: 0.5489 - val_loss: 0.6596 - val_accuracy: 0.6224\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6862 - accuracy: 0.5534 - val_loss: 0.6619 - val_accuracy: 0.6339\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6862 - accuracy: 0.5496 - val_loss: 0.6590 - val_accuracy: 0.6288\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6861 - accuracy: 0.5548 - val_loss: 0.6603 - val_accuracy: 0.6365\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6863 - accuracy: 0.5495 - val_loss: 0.6584 - val_accuracy: 0.6339\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6877 - accuracy: 0.5465 - val_loss: 0.6623 - val_accuracy: 0.6365\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6844 - accuracy: 0.5550 - val_loss: 0.6579 - val_accuracy: 0.6365\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6858 - accuracy: 0.5465 - val_loss: 0.6583 - val_accuracy: 0.6378\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6833 - accuracy: 0.5565 - val_loss: 0.6577 - val_accuracy: 0.6416\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6840 - accuracy: 0.5555 - val_loss: 0.6542 - val_accuracy: 0.6390\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6830 - accuracy: 0.5533 - val_loss: 0.6531 - val_accuracy: 0.6403\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6832 - accuracy: 0.5603 - val_loss: 0.6559 - val_accuracy: 0.6378\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6851 - accuracy: 0.5535 - val_loss: 0.6555 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6824 - accuracy: 0.5662 - val_loss: 0.6550 - val_accuracy: 0.6390\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6791 - accuracy: 0.5721 - val_loss: 0.6502 - val_accuracy: 0.6378\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.5641 - val_loss: 0.6491 - val_accuracy: 0.6416\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6812 - accuracy: 0.5643 - val_loss: 0.6504 - val_accuracy: 0.6429\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6820 - accuracy: 0.5638 - val_loss: 0.6518 - val_accuracy: 0.6441\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6781 - accuracy: 0.5752 - val_loss: 0.6498 - val_accuracy: 0.6441\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6778 - accuracy: 0.5678 - val_loss: 0.6508 - val_accuracy: 0.6390\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6773 - accuracy: 0.5771 - val_loss: 0.6485 - val_accuracy: 0.6429\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.5700 - val_loss: 0.6492 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6773 - accuracy: 0.5723 - val_loss: 0.6466 - val_accuracy: 0.6416\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6790 - accuracy: 0.5701 - val_loss: 0.6447 - val_accuracy: 0.6454\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6784 - accuracy: 0.5737 - val_loss: 0.6488 - val_accuracy: 0.6390\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6784 - accuracy: 0.5681 - val_loss: 0.6481 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6786 - accuracy: 0.5718 - val_loss: 0.6475 - val_accuracy: 0.6403\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6778 - accuracy: 0.5767 - val_loss: 0.6464 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6746 - accuracy: 0.5793 - val_loss: 0.6466 - val_accuracy: 0.6403\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6753 - accuracy: 0.5823 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.5795 - val_loss: 0.6458 - val_accuracy: 0.6429\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6744 - accuracy: 0.5730 - val_loss: 0.6434 - val_accuracy: 0.6403\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6767 - accuracy: 0.5742 - val_loss: 0.6449 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6728 - accuracy: 0.5859 - val_loss: 0.6423 - val_accuracy: 0.6416\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6758 - accuracy: 0.5760 - val_loss: 0.6446 - val_accuracy: 0.6429\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6747 - accuracy: 0.5774 - val_loss: 0.6459 - val_accuracy: 0.6480\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6718 - accuracy: 0.5824 - val_loss: 0.6398 - val_accuracy: 0.6416\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6700 - accuracy: 0.5879 - val_loss: 0.6372 - val_accuracy: 0.6429\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6750 - accuracy: 0.5860 - val_loss: 0.6420 - val_accuracy: 0.6467\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6730 - accuracy: 0.5870 - val_loss: 0.6418 - val_accuracy: 0.6480\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6747 - accuracy: 0.5809 - val_loss: 0.6396 - val_accuracy: 0.6467\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6708 - accuracy: 0.5909 - val_loss: 0.6401 - val_accuracy: 0.6505\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6687 - accuracy: 0.5864 - val_loss: 0.6355 - val_accuracy: 0.6467\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6713 - accuracy: 0.5883 - val_loss: 0.6387 - val_accuracy: 0.6454\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6710 - accuracy: 0.5873 - val_loss: 0.6420 - val_accuracy: 0.6543\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5927 - val_loss: 0.6380 - val_accuracy: 0.6480\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6703 - accuracy: 0.5873 - val_loss: 0.6368 - val_accuracy: 0.6467\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.5938 - val_loss: 0.6388 - val_accuracy: 0.6492\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6727 - accuracy: 0.5898 - val_loss: 0.6378 - val_accuracy: 0.6480\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5937 - val_loss: 0.6376 - val_accuracy: 0.6518\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6706 - accuracy: 0.5916 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.5914 - val_loss: 0.6364 - val_accuracy: 0.6480\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.5950 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6684 - accuracy: 0.5908 - val_loss: 0.6390 - val_accuracy: 0.6684\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6670 - accuracy: 0.5944 - val_loss: 0.6342 - val_accuracy: 0.6684\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.5943 - val_loss: 0.6369 - val_accuracy: 0.6684\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6685 - accuracy: 0.5917 - val_loss: 0.6359 - val_accuracy: 0.6658\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6653 - accuracy: 0.5923 - val_loss: 0.6332 - val_accuracy: 0.6696\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6688 - accuracy: 0.5946 - val_loss: 0.6345 - val_accuracy: 0.6671\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6673 - accuracy: 0.5913 - val_loss: 0.6359 - val_accuracy: 0.6684\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5946 - val_loss: 0.6364 - val_accuracy: 0.6684\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6046 - val_loss: 0.6320 - val_accuracy: 0.6722\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6655 - accuracy: 0.6012 - val_loss: 0.6350 - val_accuracy: 0.6658\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6675 - accuracy: 0.5967 - val_loss: 0.6321 - val_accuracy: 0.6633\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6689 - accuracy: 0.5941 - val_loss: 0.6360 - val_accuracy: 0.6658\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5962 - val_loss: 0.6331 - val_accuracy: 0.6696\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6622 - accuracy: 0.6010 - val_loss: 0.6303 - val_accuracy: 0.6696\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6606 - accuracy: 0.6032 - val_loss: 0.6314 - val_accuracy: 0.6684\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6066 - val_loss: 0.6300 - val_accuracy: 0.6696\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6648 - accuracy: 0.6014 - val_loss: 0.6310 - val_accuracy: 0.6696\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.6095 - val_loss: 0.6314 - val_accuracy: 0.6671\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6627 - accuracy: 0.6083 - val_loss: 0.6286 - val_accuracy: 0.6709\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6623 - accuracy: 0.6002 - val_loss: 0.6287 - val_accuracy: 0.6684\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6059 - val_loss: 0.6285 - val_accuracy: 0.6735\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6664 - accuracy: 0.6006 - val_loss: 0.6305 - val_accuracy: 0.6722\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6617 - accuracy: 0.6096 - val_loss: 0.6284 - val_accuracy: 0.6696\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6609 - accuracy: 0.6137 - val_loss: 0.6275 - val_accuracy: 0.6696\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6608 - accuracy: 0.6049 - val_loss: 0.6262 - val_accuracy: 0.6658\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6656 - accuracy: 0.6024 - val_loss: 0.6286 - val_accuracy: 0.6735\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6089 - val_loss: 0.6294 - val_accuracy: 0.6658\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6034 - val_loss: 0.6278 - val_accuracy: 0.6735\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6084 - val_loss: 0.6280 - val_accuracy: 0.6684\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6615 - accuracy: 0.6128 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6587 - accuracy: 0.6070 - val_loss: 0.6255 - val_accuracy: 0.6684\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6086 - val_loss: 0.6275 - val_accuracy: 0.6786\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6649 - accuracy: 0.6047 - val_loss: 0.6282 - val_accuracy: 0.6684\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6593 - accuracy: 0.6135 - val_loss: 0.6276 - val_accuracy: 0.6773\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6113 - val_loss: 0.6276 - val_accuracy: 0.6684\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6623 - accuracy: 0.6059 - val_loss: 0.6274 - val_accuracy: 0.6696\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6636 - accuracy: 0.6070 - val_loss: 0.6304 - val_accuracy: 0.6671\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.6113 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.6056 - val_loss: 0.6273 - val_accuracy: 0.6747\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6612 - accuracy: 0.6088 - val_loss: 0.6257 - val_accuracy: 0.6684\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6604 - accuracy: 0.6140 - val_loss: 0.6249 - val_accuracy: 0.6684\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6559 - accuracy: 0.6174 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6573 - accuracy: 0.6211 - val_loss: 0.6256 - val_accuracy: 0.6735\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6573 - accuracy: 0.6167 - val_loss: 0.6211 - val_accuracy: 0.6709\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6169 - val_loss: 0.6268 - val_accuracy: 0.6824\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6600 - accuracy: 0.6093 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6157 - val_loss: 0.6252 - val_accuracy: 0.6760\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6576 - accuracy: 0.6139 - val_loss: 0.6258 - val_accuracy: 0.6722\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6578 - accuracy: 0.6164 - val_loss: 0.6251 - val_accuracy: 0.6722\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6148 - val_loss: 0.6250 - val_accuracy: 0.6798\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6610 - accuracy: 0.6083 - val_loss: 0.6265 - val_accuracy: 0.6786\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6563 - accuracy: 0.6221 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6545 - accuracy: 0.6161 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6194 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6581 - accuracy: 0.6140 - val_loss: 0.6287 - val_accuracy: 0.6798\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6565 - accuracy: 0.6173 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6587 - accuracy: 0.6090 - val_loss: 0.6243 - val_accuracy: 0.6837\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6544 - accuracy: 0.6191 - val_loss: 0.6207 - val_accuracy: 0.6798\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6167 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6549 - accuracy: 0.6179 - val_loss: 0.6227 - val_accuracy: 0.6773\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6591 - accuracy: 0.6132 - val_loss: 0.6240 - val_accuracy: 0.6824\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6570 - accuracy: 0.6130 - val_loss: 0.6200 - val_accuracy: 0.6824\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6573 - accuracy: 0.6177 - val_loss: 0.6221 - val_accuracy: 0.6837\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6557 - accuracy: 0.6192 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6535 - accuracy: 0.6163 - val_loss: 0.6233 - val_accuracy: 0.6798\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6540 - accuracy: 0.6218 - val_loss: 0.6189 - val_accuracy: 0.6798\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6230 - val_loss: 0.6215 - val_accuracy: 0.6837\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6560 - accuracy: 0.6168 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6237 - val_loss: 0.6201 - val_accuracy: 0.6849\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6560 - accuracy: 0.6159 - val_loss: 0.6205 - val_accuracy: 0.6760\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6526 - accuracy: 0.6184 - val_loss: 0.6186 - val_accuracy: 0.6862\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6555 - accuracy: 0.6173 - val_loss: 0.6237 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6235 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6564 - accuracy: 0.6163 - val_loss: 0.6230 - val_accuracy: 0.6811\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6247 - val_loss: 0.6218 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6544 - accuracy: 0.6242 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6585 - accuracy: 0.6193 - val_loss: 0.6219 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6237 - val_loss: 0.6205 - val_accuracy: 0.6798\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6271 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6159 - val_loss: 0.6216 - val_accuracy: 0.6798\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6213 - val_loss: 0.6190 - val_accuracy: 0.6811\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6208 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.6291 - val_loss: 0.6190 - val_accuracy: 0.6824\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6539 - accuracy: 0.6269 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6547 - accuracy: 0.6192 - val_loss: 0.6182 - val_accuracy: 0.6824\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6209 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6231 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6270 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6242 - val_loss: 0.6161 - val_accuracy: 0.6849\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6253 - val_loss: 0.6205 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6302 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6539 - accuracy: 0.6215 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6319 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6508 - accuracy: 0.6300 - val_loss: 0.6137 - val_accuracy: 0.6824\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6236 - val_loss: 0.6163 - val_accuracy: 0.6837\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6267 - val_loss: 0.6190 - val_accuracy: 0.6824\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6532 - accuracy: 0.6282 - val_loss: 0.6196 - val_accuracy: 0.6798\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6531 - accuracy: 0.6318 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6284 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6501 - accuracy: 0.6316 - val_loss: 0.6158 - val_accuracy: 0.6786\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6282 - val_loss: 0.6157 - val_accuracy: 0.6862\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6295 - val_loss: 0.6195 - val_accuracy: 0.6798\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6265 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6184 - val_loss: 0.6144 - val_accuracy: 0.6811\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6531 - accuracy: 0.6275 - val_loss: 0.6214 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6541 - accuracy: 0.6196 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6505 - accuracy: 0.6232 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6252 - val_loss: 0.6180 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6282 - val_loss: 0.6150 - val_accuracy: 0.6824\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6449 - accuracy: 0.6340 - val_loss: 0.6126 - val_accuracy: 0.6862\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6483 - accuracy: 0.6326 - val_loss: 0.6157 - val_accuracy: 0.6837\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6350 - val_loss: 0.6165 - val_accuracy: 0.6862\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6267 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6484 - accuracy: 0.6297 - val_loss: 0.6158 - val_accuracy: 0.6862\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6496 - accuracy: 0.6297 - val_loss: 0.6133 - val_accuracy: 0.6824\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6366 - val_loss: 0.6160 - val_accuracy: 0.6862\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6487 - accuracy: 0.6247 - val_loss: 0.6171 - val_accuracy: 0.6849\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6508 - accuracy: 0.6262 - val_loss: 0.6141 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6501 - accuracy: 0.6241 - val_loss: 0.6147 - val_accuracy: 0.6901\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6487 - accuracy: 0.6261 - val_loss: 0.6163 - val_accuracy: 0.6888\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6252 - val_loss: 0.6155 - val_accuracy: 0.6875\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6470 - accuracy: 0.6355 - val_loss: 0.6144 - val_accuracy: 0.6875\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6281 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6478 - accuracy: 0.6279 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6478 - accuracy: 0.6304 - val_loss: 0.6149 - val_accuracy: 0.6875\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6458 - accuracy: 0.6316 - val_loss: 0.6130 - val_accuracy: 0.6862\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6465 - accuracy: 0.6331 - val_loss: 0.6121 - val_accuracy: 0.6875\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6464 - accuracy: 0.6302 - val_loss: 0.6145 - val_accuracy: 0.6875\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6377 - val_loss: 0.6115 - val_accuracy: 0.6901\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6325 - val_loss: 0.6172 - val_accuracy: 0.6888\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6454 - accuracy: 0.6389 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Calculating for: 1000 400 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_228 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.8030 - accuracy: 0.5504 - val_loss: 0.6900 - val_accuracy: 0.5536\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7069 - accuracy: 0.5772 - val_loss: 0.6645 - val_accuracy: 0.6288\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6761 - accuracy: 0.5946 - val_loss: 0.6571 - val_accuracy: 0.6390\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6656 - accuracy: 0.6083 - val_loss: 0.6472 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6576 - accuracy: 0.6204 - val_loss: 0.6441 - val_accuracy: 0.6633\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6509 - accuracy: 0.6238 - val_loss: 0.6493 - val_accuracy: 0.6403\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6527 - accuracy: 0.6233 - val_loss: 0.6443 - val_accuracy: 0.6403\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6493 - accuracy: 0.6286 - val_loss: 0.6378 - val_accuracy: 0.6582\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6494 - accuracy: 0.6292 - val_loss: 0.6433 - val_accuracy: 0.6403\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6420 - accuracy: 0.6417 - val_loss: 0.6347 - val_accuracy: 0.6594\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6408 - accuracy: 0.6368 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6418 - accuracy: 0.6364 - val_loss: 0.6410 - val_accuracy: 0.6480\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6355 - accuracy: 0.6420 - val_loss: 0.6367 - val_accuracy: 0.6569\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6351 - accuracy: 0.6417 - val_loss: 0.6343 - val_accuracy: 0.6543\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6343 - accuracy: 0.6469 - val_loss: 0.6364 - val_accuracy: 0.6429\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6354 - accuracy: 0.6423 - val_loss: 0.6327 - val_accuracy: 0.6594\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6319 - accuracy: 0.6502 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6262 - accuracy: 0.6555 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6309 - accuracy: 0.6472 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6289 - accuracy: 0.6513 - val_loss: 0.6352 - val_accuracy: 0.6518\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6194 - accuracy: 0.6669 - val_loss: 0.6323 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6201 - accuracy: 0.6579 - val_loss: 0.6327 - val_accuracy: 0.6582\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6233 - accuracy: 0.6599 - val_loss: 0.6292 - val_accuracy: 0.6582\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6154 - accuracy: 0.6639 - val_loss: 0.6303 - val_accuracy: 0.6620\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6187 - accuracy: 0.6621 - val_loss: 0.6322 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6202 - accuracy: 0.6594 - val_loss: 0.6331 - val_accuracy: 0.6582\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6132 - accuracy: 0.6696 - val_loss: 0.6327 - val_accuracy: 0.6582\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6146 - accuracy: 0.6655 - val_loss: 0.6365 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6137 - accuracy: 0.6625 - val_loss: 0.6318 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6141 - accuracy: 0.6693 - val_loss: 0.6315 - val_accuracy: 0.6658\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6122 - accuracy: 0.6719 - val_loss: 0.6359 - val_accuracy: 0.6582\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6077 - accuracy: 0.6707 - val_loss: 0.6322 - val_accuracy: 0.6658\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6038 - accuracy: 0.6785 - val_loss: 0.6357 - val_accuracy: 0.6620\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6012 - accuracy: 0.6797 - val_loss: 0.6432 - val_accuracy: 0.6556\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6062 - accuracy: 0.6733 - val_loss: 0.6364 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5997 - accuracy: 0.6796 - val_loss: 0.6398 - val_accuracy: 0.6518\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6013 - accuracy: 0.6801 - val_loss: 0.6423 - val_accuracy: 0.6505\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5986 - accuracy: 0.6821 - val_loss: 0.6360 - val_accuracy: 0.6684\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5955 - accuracy: 0.6855 - val_loss: 0.6446 - val_accuracy: 0.6492\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6026 - accuracy: 0.6752 - val_loss: 0.6380 - val_accuracy: 0.6620\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5943 - accuracy: 0.6844 - val_loss: 0.6412 - val_accuracy: 0.6620\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5975 - accuracy: 0.6815 - val_loss: 0.6345 - val_accuracy: 0.6709\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5939 - accuracy: 0.6850 - val_loss: 0.6397 - val_accuracy: 0.6620\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5966 - accuracy: 0.6810 - val_loss: 0.6435 - val_accuracy: 0.6569\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5901 - accuracy: 0.6870 - val_loss: 0.6451 - val_accuracy: 0.6645\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5961 - accuracy: 0.6854 - val_loss: 0.6442 - val_accuracy: 0.6543\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5904 - accuracy: 0.6866 - val_loss: 0.6353 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5872 - accuracy: 0.6913 - val_loss: 0.6469 - val_accuracy: 0.6480\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5860 - accuracy: 0.6884 - val_loss: 0.6417 - val_accuracy: 0.6594\n",
      "Calculating for: 1000 400 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_232 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.8244 - accuracy: 0.5402 - val_loss: 0.6595 - val_accuracy: 0.6505\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7160 - accuracy: 0.5681 - val_loss: 0.6510 - val_accuracy: 0.6633\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6868 - accuracy: 0.5762 - val_loss: 0.6418 - val_accuracy: 0.6684\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6750 - accuracy: 0.5844 - val_loss: 0.6523 - val_accuracy: 0.6556\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6739 - accuracy: 0.5870 - val_loss: 0.6486 - val_accuracy: 0.6722\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6685 - accuracy: 0.5951 - val_loss: 0.6467 - val_accuracy: 0.6747\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6668 - accuracy: 0.5966 - val_loss: 0.6431 - val_accuracy: 0.6735\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6608 - accuracy: 0.6056 - val_loss: 0.6399 - val_accuracy: 0.6722\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6634 - accuracy: 0.6039 - val_loss: 0.6471 - val_accuracy: 0.6607\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6628 - accuracy: 0.6049 - val_loss: 0.6428 - val_accuracy: 0.6620\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6590 - accuracy: 0.6135 - val_loss: 0.6434 - val_accuracy: 0.6543\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6602 - accuracy: 0.6094 - val_loss: 0.6396 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6551 - accuracy: 0.6173 - val_loss: 0.6415 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6580 - accuracy: 0.6060 - val_loss: 0.6399 - val_accuracy: 0.6607\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6551 - accuracy: 0.6143 - val_loss: 0.6369 - val_accuracy: 0.6607\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6552 - accuracy: 0.6216 - val_loss: 0.6420 - val_accuracy: 0.6492\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6558 - accuracy: 0.6122 - val_loss: 0.6362 - val_accuracy: 0.6620\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6576 - accuracy: 0.6142 - val_loss: 0.6360 - val_accuracy: 0.6582\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6557 - accuracy: 0.6221 - val_loss: 0.6362 - val_accuracy: 0.6582\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.6218 - val_loss: 0.6368 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6489 - accuracy: 0.6242 - val_loss: 0.6377 - val_accuracy: 0.6480\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6503 - accuracy: 0.6267 - val_loss: 0.6409 - val_accuracy: 0.6441\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6521 - accuracy: 0.6253 - val_loss: 0.6355 - val_accuracy: 0.6620\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6506 - accuracy: 0.6242 - val_loss: 0.6365 - val_accuracy: 0.6492\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6484 - accuracy: 0.6247 - val_loss: 0.6330 - val_accuracy: 0.6607\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6466 - accuracy: 0.6368 - val_loss: 0.6369 - val_accuracy: 0.6403\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6468 - accuracy: 0.6240 - val_loss: 0.6359 - val_accuracy: 0.6480\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6518 - accuracy: 0.6240 - val_loss: 0.6403 - val_accuracy: 0.6390\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6454 - accuracy: 0.6300 - val_loss: 0.6287 - val_accuracy: 0.6747\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6447 - accuracy: 0.6323 - val_loss: 0.6321 - val_accuracy: 0.6658\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6449 - accuracy: 0.6312 - val_loss: 0.6263 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6437 - accuracy: 0.6318 - val_loss: 0.6280 - val_accuracy: 0.6658\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6414 - accuracy: 0.6373 - val_loss: 0.6263 - val_accuracy: 0.6671\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6391 - accuracy: 0.6439 - val_loss: 0.6295 - val_accuracy: 0.6569\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6435 - accuracy: 0.6361 - val_loss: 0.6295 - val_accuracy: 0.6633\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6428 - accuracy: 0.6353 - val_loss: 0.6294 - val_accuracy: 0.6594\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6390 - accuracy: 0.6383 - val_loss: 0.6266 - val_accuracy: 0.6633\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6396 - accuracy: 0.6351 - val_loss: 0.6304 - val_accuracy: 0.6531\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6416 - accuracy: 0.6389 - val_loss: 0.6299 - val_accuracy: 0.6620\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6386 - accuracy: 0.6389 - val_loss: 0.6273 - val_accuracy: 0.6747\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6377 - accuracy: 0.6388 - val_loss: 0.6275 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6357 - accuracy: 0.6473 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6384 - accuracy: 0.6364 - val_loss: 0.6282 - val_accuracy: 0.6633\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6347 - accuracy: 0.6419 - val_loss: 0.6273 - val_accuracy: 0.6684\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6327 - accuracy: 0.6443 - val_loss: 0.6301 - val_accuracy: 0.6620\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6362 - accuracy: 0.6395 - val_loss: 0.6278 - val_accuracy: 0.6658\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6358 - accuracy: 0.6448 - val_loss: 0.6283 - val_accuracy: 0.6709\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6354 - accuracy: 0.6429 - val_loss: 0.6343 - val_accuracy: 0.6569\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6315 - accuracy: 0.6438 - val_loss: 0.6253 - val_accuracy: 0.6747\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6328 - accuracy: 0.6478 - val_loss: 0.6308 - val_accuracy: 0.6658\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6319 - accuracy: 0.6492 - val_loss: 0.6292 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6334 - accuracy: 0.6530 - val_loss: 0.6292 - val_accuracy: 0.6696\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6323 - accuracy: 0.6534 - val_loss: 0.6244 - val_accuracy: 0.6696\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6300 - accuracy: 0.6511 - val_loss: 0.6275 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6293 - accuracy: 0.6426 - val_loss: 0.6278 - val_accuracy: 0.6658\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6293 - accuracy: 0.6511 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6285 - accuracy: 0.6549 - val_loss: 0.6260 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6266 - accuracy: 0.6574 - val_loss: 0.6243 - val_accuracy: 0.6709\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6275 - accuracy: 0.6500 - val_loss: 0.6289 - val_accuracy: 0.6684\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6267 - accuracy: 0.6531 - val_loss: 0.6309 - val_accuracy: 0.6671\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6256 - accuracy: 0.6513 - val_loss: 0.6276 - val_accuracy: 0.6735\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6224 - accuracy: 0.6562 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6263 - accuracy: 0.6523 - val_loss: 0.6272 - val_accuracy: 0.6735\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6271 - accuracy: 0.6574 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6260 - accuracy: 0.6495 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6223 - accuracy: 0.6536 - val_loss: 0.6219 - val_accuracy: 0.6773\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6205 - accuracy: 0.6596 - val_loss: 0.6280 - val_accuracy: 0.6684\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6228 - accuracy: 0.6596 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6218 - accuracy: 0.6574 - val_loss: 0.6177 - val_accuracy: 0.6837\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6232 - accuracy: 0.6603 - val_loss: 0.6244 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6180 - accuracy: 0.6638 - val_loss: 0.6277 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6198 - accuracy: 0.6594 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6186 - accuracy: 0.6585 - val_loss: 0.6256 - val_accuracy: 0.6786\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6210 - accuracy: 0.6625 - val_loss: 0.6271 - val_accuracy: 0.6798\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6167 - accuracy: 0.6669 - val_loss: 0.6229 - val_accuracy: 0.6837\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6177 - accuracy: 0.6620 - val_loss: 0.6226 - val_accuracy: 0.6811\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6638 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6633 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6178 - accuracy: 0.6629 - val_loss: 0.6350 - val_accuracy: 0.6696\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6165 - accuracy: 0.6691 - val_loss: 0.6335 - val_accuracy: 0.6684\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6149 - accuracy: 0.6616 - val_loss: 0.6264 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6164 - accuracy: 0.6653 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6144 - accuracy: 0.6708 - val_loss: 0.6309 - val_accuracy: 0.6684\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6147 - accuracy: 0.6694 - val_loss: 0.6314 - val_accuracy: 0.6696\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6102 - accuracy: 0.6734 - val_loss: 0.6304 - val_accuracy: 0.6709\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6114 - accuracy: 0.6743 - val_loss: 0.6285 - val_accuracy: 0.6747\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6093 - accuracy: 0.6728 - val_loss: 0.6302 - val_accuracy: 0.6760\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6125 - accuracy: 0.6689 - val_loss: 0.6274 - val_accuracy: 0.6798\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6105 - accuracy: 0.6673 - val_loss: 0.6303 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6101 - accuracy: 0.6706 - val_loss: 0.6320 - val_accuracy: 0.6735\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6093 - accuracy: 0.6703 - val_loss: 0.6330 - val_accuracy: 0.6607\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6058 - accuracy: 0.6718 - val_loss: 0.6369 - val_accuracy: 0.6556\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6084 - accuracy: 0.6765 - val_loss: 0.6312 - val_accuracy: 0.6633\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6061 - accuracy: 0.6772 - val_loss: 0.6271 - val_accuracy: 0.6747\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6041 - accuracy: 0.6805 - val_loss: 0.6234 - val_accuracy: 0.6811\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6099 - accuracy: 0.6721 - val_loss: 0.6283 - val_accuracy: 0.6722\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6106 - accuracy: 0.6712 - val_loss: 0.6347 - val_accuracy: 0.6671\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6020 - accuracy: 0.6757 - val_loss: 0.6358 - val_accuracy: 0.6658\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6076 - accuracy: 0.6736 - val_loss: 0.6301 - val_accuracy: 0.6709\n",
      "Calculating for: 1000 400 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_236 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.8560 - accuracy: 0.5046 - val_loss: 0.6628 - val_accuracy: 0.6339\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.7370 - accuracy: 0.5106 - val_loss: 0.6602 - val_accuracy: 0.6365\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.7042 - accuracy: 0.5269 - val_loss: 0.6610 - val_accuracy: 0.6339\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6957 - accuracy: 0.5289 - val_loss: 0.6620 - val_accuracy: 0.6212\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6915 - accuracy: 0.5288 - val_loss: 0.6669 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6888 - accuracy: 0.5409 - val_loss: 0.6623 - val_accuracy: 0.6224\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6912 - accuracy: 0.5262 - val_loss: 0.6691 - val_accuracy: 0.6224\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6883 - accuracy: 0.5460 - val_loss: 0.6627 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6899 - accuracy: 0.5392 - val_loss: 0.6641 - val_accuracy: 0.6224\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6883 - accuracy: 0.5397 - val_loss: 0.6606 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6875 - accuracy: 0.5467 - val_loss: 0.6581 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6896 - accuracy: 0.5350 - val_loss: 0.6608 - val_accuracy: 0.6224\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6861 - accuracy: 0.5555 - val_loss: 0.6563 - val_accuracy: 0.6237\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6851 - accuracy: 0.5509 - val_loss: 0.6568 - val_accuracy: 0.6237\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6860 - accuracy: 0.5489 - val_loss: 0.6569 - val_accuracy: 0.6301\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6864 - accuracy: 0.5484 - val_loss: 0.6575 - val_accuracy: 0.6314\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6870 - accuracy: 0.5495 - val_loss: 0.6561 - val_accuracy: 0.6276\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6840 - accuracy: 0.5528 - val_loss: 0.6537 - val_accuracy: 0.6352\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6846 - accuracy: 0.5489 - val_loss: 0.6536 - val_accuracy: 0.6378\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6853 - accuracy: 0.5569 - val_loss: 0.6523 - val_accuracy: 0.6365\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6844 - accuracy: 0.5536 - val_loss: 0.6534 - val_accuracy: 0.6365\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6845 - accuracy: 0.5579 - val_loss: 0.6533 - val_accuracy: 0.6365\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6822 - accuracy: 0.5629 - val_loss: 0.6507 - val_accuracy: 0.6390\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6845 - accuracy: 0.5536 - val_loss: 0.6534 - val_accuracy: 0.6390\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6829 - accuracy: 0.5623 - val_loss: 0.6529 - val_accuracy: 0.6441\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6802 - accuracy: 0.5671 - val_loss: 0.6491 - val_accuracy: 0.6416\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6812 - accuracy: 0.5620 - val_loss: 0.6495 - val_accuracy: 0.6441\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6800 - accuracy: 0.5597 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6807 - accuracy: 0.5630 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6783 - accuracy: 0.5736 - val_loss: 0.6460 - val_accuracy: 0.6429\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6780 - accuracy: 0.5739 - val_loss: 0.6459 - val_accuracy: 0.6429\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6796 - accuracy: 0.5647 - val_loss: 0.6464 - val_accuracy: 0.6416\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6789 - accuracy: 0.5672 - val_loss: 0.6447 - val_accuracy: 0.6416\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6765 - accuracy: 0.5728 - val_loss: 0.6437 - val_accuracy: 0.6390\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6759 - accuracy: 0.5806 - val_loss: 0.6442 - val_accuracy: 0.6441\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6782 - accuracy: 0.5702 - val_loss: 0.6432 - val_accuracy: 0.6429\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6730 - accuracy: 0.5793 - val_loss: 0.6411 - val_accuracy: 0.6429\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6738 - accuracy: 0.5887 - val_loss: 0.6418 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6750 - accuracy: 0.5801 - val_loss: 0.6413 - val_accuracy: 0.6390\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6759 - accuracy: 0.5803 - val_loss: 0.6414 - val_accuracy: 0.6390\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6749 - accuracy: 0.5752 - val_loss: 0.6422 - val_accuracy: 0.6390\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6738 - accuracy: 0.5772 - val_loss: 0.6421 - val_accuracy: 0.6416\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6753 - accuracy: 0.5787 - val_loss: 0.6423 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6757 - accuracy: 0.5825 - val_loss: 0.6416 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6755 - accuracy: 0.5805 - val_loss: 0.6402 - val_accuracy: 0.6480\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6732 - accuracy: 0.5854 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6711 - accuracy: 0.5923 - val_loss: 0.6389 - val_accuracy: 0.6492\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6723 - accuracy: 0.5864 - val_loss: 0.6387 - val_accuracy: 0.6492\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6705 - accuracy: 0.5868 - val_loss: 0.6394 - val_accuracy: 0.6492\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6731 - accuracy: 0.5860 - val_loss: 0.6385 - val_accuracy: 0.6492\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6668 - accuracy: 0.5942 - val_loss: 0.6348 - val_accuracy: 0.6505\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6737 - accuracy: 0.5885 - val_loss: 0.6375 - val_accuracy: 0.6518\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6708 - accuracy: 0.5887 - val_loss: 0.6391 - val_accuracy: 0.6531\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6734 - accuracy: 0.5855 - val_loss: 0.6404 - val_accuracy: 0.6492\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6707 - accuracy: 0.5942 - val_loss: 0.6395 - val_accuracy: 0.6505\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6661 - accuracy: 0.5944 - val_loss: 0.6345 - val_accuracy: 0.6480\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6722 - accuracy: 0.5901 - val_loss: 0.6370 - val_accuracy: 0.6556\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6663 - accuracy: 0.6007 - val_loss: 0.6349 - val_accuracy: 0.6531\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6713 - accuracy: 0.5893 - val_loss: 0.6381 - val_accuracy: 0.6531\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6719 - accuracy: 0.5967 - val_loss: 0.6358 - val_accuracy: 0.6556\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6716 - accuracy: 0.5888 - val_loss: 0.6392 - val_accuracy: 0.6531\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6689 - accuracy: 0.5885 - val_loss: 0.6355 - val_accuracy: 0.6531\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6702 - accuracy: 0.5873 - val_loss: 0.6364 - val_accuracy: 0.6518\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6645 - accuracy: 0.5995 - val_loss: 0.6340 - val_accuracy: 0.6531\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6690 - accuracy: 0.5947 - val_loss: 0.6346 - val_accuracy: 0.6658\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6682 - accuracy: 0.5936 - val_loss: 0.6348 - val_accuracy: 0.6747\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6648 - accuracy: 0.5992 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6626 - accuracy: 0.6093 - val_loss: 0.6319 - val_accuracy: 0.6582\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6653 - accuracy: 0.6001 - val_loss: 0.6345 - val_accuracy: 0.6607\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6644 - accuracy: 0.6035 - val_loss: 0.6322 - val_accuracy: 0.6671\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6665 - accuracy: 0.5963 - val_loss: 0.6332 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6647 - accuracy: 0.5992 - val_loss: 0.6314 - val_accuracy: 0.6709\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6636 - accuracy: 0.6066 - val_loss: 0.6296 - val_accuracy: 0.6658\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6658 - accuracy: 0.6000 - val_loss: 0.6304 - val_accuracy: 0.6671\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6654 - accuracy: 0.6021 - val_loss: 0.6315 - val_accuracy: 0.6645\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6687 - accuracy: 0.6000 - val_loss: 0.6363 - val_accuracy: 0.6658\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6655 - accuracy: 0.6036 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6654 - accuracy: 0.5997 - val_loss: 0.6297 - val_accuracy: 0.6760\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6653 - accuracy: 0.6021 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6623 - accuracy: 0.6064 - val_loss: 0.6285 - val_accuracy: 0.6709\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6602 - accuracy: 0.6083 - val_loss: 0.6275 - val_accuracy: 0.6709\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6629 - accuracy: 0.6063 - val_loss: 0.6303 - val_accuracy: 0.6735\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6649 - accuracy: 0.6019 - val_loss: 0.6291 - val_accuracy: 0.6722\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6604 - accuracy: 0.6083 - val_loss: 0.6257 - val_accuracy: 0.6722\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6620 - accuracy: 0.6040 - val_loss: 0.6273 - val_accuracy: 0.6735\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6614 - accuracy: 0.6084 - val_loss: 0.6274 - val_accuracy: 0.6722\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6624 - accuracy: 0.6084 - val_loss: 0.6281 - val_accuracy: 0.6696\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6628 - accuracy: 0.6055 - val_loss: 0.6265 - val_accuracy: 0.6735\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6617 - accuracy: 0.6114 - val_loss: 0.6256 - val_accuracy: 0.6735\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6628 - accuracy: 0.6005 - val_loss: 0.6276 - val_accuracy: 0.6709\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6572 - accuracy: 0.6177 - val_loss: 0.6258 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6579 - accuracy: 0.6093 - val_loss: 0.6253 - val_accuracy: 0.6760\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6581 - accuracy: 0.6127 - val_loss: 0.6259 - val_accuracy: 0.6709\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6619 - accuracy: 0.6094 - val_loss: 0.6272 - val_accuracy: 0.6722\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6631 - accuracy: 0.6069 - val_loss: 0.6321 - val_accuracy: 0.6722\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6613 - accuracy: 0.6107 - val_loss: 0.6258 - val_accuracy: 0.6722\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6590 - accuracy: 0.6117 - val_loss: 0.6245 - val_accuracy: 0.6773\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6617 - accuracy: 0.6078 - val_loss: 0.6255 - val_accuracy: 0.6735\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6616 - accuracy: 0.6188 - val_loss: 0.6242 - val_accuracy: 0.6747\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6613 - accuracy: 0.6073 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6615 - accuracy: 0.6076 - val_loss: 0.6277 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6571 - accuracy: 0.6191 - val_loss: 0.6198 - val_accuracy: 0.6760\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6592 - accuracy: 0.6228 - val_loss: 0.6223 - val_accuracy: 0.6773\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6605 - accuracy: 0.6119 - val_loss: 0.6277 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6572 - accuracy: 0.6109 - val_loss: 0.6197 - val_accuracy: 0.6760\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6573 - accuracy: 0.6208 - val_loss: 0.6213 - val_accuracy: 0.6747\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6581 - accuracy: 0.6117 - val_loss: 0.6238 - val_accuracy: 0.6786\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6576 - accuracy: 0.6183 - val_loss: 0.6235 - val_accuracy: 0.6798\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6598 - accuracy: 0.6117 - val_loss: 0.6231 - val_accuracy: 0.6760\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6561 - accuracy: 0.6110 - val_loss: 0.6207 - val_accuracy: 0.6773\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6584 - accuracy: 0.6091 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6558 - accuracy: 0.6163 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6584 - accuracy: 0.6132 - val_loss: 0.6187 - val_accuracy: 0.6811\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6600 - accuracy: 0.6094 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6566 - accuracy: 0.6202 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6602 - accuracy: 0.6137 - val_loss: 0.6200 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6587 - accuracy: 0.6128 - val_loss: 0.6263 - val_accuracy: 0.6837\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6534 - accuracy: 0.6217 - val_loss: 0.6229 - val_accuracy: 0.6811\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6568 - accuracy: 0.6217 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6536 - accuracy: 0.6207 - val_loss: 0.6181 - val_accuracy: 0.6798\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6562 - accuracy: 0.6164 - val_loss: 0.6226 - val_accuracy: 0.6786\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6537 - accuracy: 0.6172 - val_loss: 0.6190 - val_accuracy: 0.6811\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6539 - accuracy: 0.6173 - val_loss: 0.6161 - val_accuracy: 0.6798\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6564 - accuracy: 0.6135 - val_loss: 0.6182 - val_accuracy: 0.6811\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6560 - accuracy: 0.6217 - val_loss: 0.6239 - val_accuracy: 0.6837\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6538 - accuracy: 0.6197 - val_loss: 0.6211 - val_accuracy: 0.6798\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6529 - accuracy: 0.6198 - val_loss: 0.6162 - val_accuracy: 0.6773\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6553 - accuracy: 0.6144 - val_loss: 0.6173 - val_accuracy: 0.6811\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6550 - accuracy: 0.6183 - val_loss: 0.6171 - val_accuracy: 0.6773\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6541 - accuracy: 0.6206 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6559 - accuracy: 0.6149 - val_loss: 0.6163 - val_accuracy: 0.6786\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6528 - accuracy: 0.6287 - val_loss: 0.6158 - val_accuracy: 0.6798\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6554 - accuracy: 0.6217 - val_loss: 0.6156 - val_accuracy: 0.6773\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6552 - accuracy: 0.6174 - val_loss: 0.6166 - val_accuracy: 0.6811\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6519 - accuracy: 0.6202 - val_loss: 0.6161 - val_accuracy: 0.6798\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6528 - accuracy: 0.6209 - val_loss: 0.6183 - val_accuracy: 0.6824\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6528 - accuracy: 0.6179 - val_loss: 0.6183 - val_accuracy: 0.6824\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6543 - accuracy: 0.6220 - val_loss: 0.6172 - val_accuracy: 0.6824\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6528 - accuracy: 0.6202 - val_loss: 0.6151 - val_accuracy: 0.6824\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6527 - accuracy: 0.6246 - val_loss: 0.6227 - val_accuracy: 0.6811\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6523 - accuracy: 0.6230 - val_loss: 0.6157 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6543 - accuracy: 0.6191 - val_loss: 0.6149 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6544 - accuracy: 0.6188 - val_loss: 0.6175 - val_accuracy: 0.6824\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6545 - accuracy: 0.6227 - val_loss: 0.6177 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6533 - accuracy: 0.6215 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6484 - accuracy: 0.6206 - val_loss: 0.6147 - val_accuracy: 0.6824\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6500 - accuracy: 0.6281 - val_loss: 0.6144 - val_accuracy: 0.6786\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6512 - accuracy: 0.6231 - val_loss: 0.6148 - val_accuracy: 0.6811\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6507 - accuracy: 0.6246 - val_loss: 0.6151 - val_accuracy: 0.6837\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6489 - accuracy: 0.6236 - val_loss: 0.6124 - val_accuracy: 0.6824\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6482 - accuracy: 0.6261 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6527 - accuracy: 0.6211 - val_loss: 0.6122 - val_accuracy: 0.6824\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6487 - accuracy: 0.6261 - val_loss: 0.6131 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6521 - accuracy: 0.6279 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6498 - accuracy: 0.6305 - val_loss: 0.6132 - val_accuracy: 0.6798\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6493 - accuracy: 0.6300 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6498 - accuracy: 0.6276 - val_loss: 0.6148 - val_accuracy: 0.6824\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6496 - accuracy: 0.6260 - val_loss: 0.6148 - val_accuracy: 0.6824\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6492 - accuracy: 0.6264 - val_loss: 0.6119 - val_accuracy: 0.6849\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6482 - accuracy: 0.6326 - val_loss: 0.6118 - val_accuracy: 0.6849\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6517 - accuracy: 0.6290 - val_loss: 0.6117 - val_accuracy: 0.6824\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6499 - accuracy: 0.6279 - val_loss: 0.6116 - val_accuracy: 0.6837\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6495 - accuracy: 0.6309 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6508 - accuracy: 0.6250 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6509 - accuracy: 0.6264 - val_loss: 0.6157 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6477 - accuracy: 0.6335 - val_loss: 0.6129 - val_accuracy: 0.6811\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6486 - accuracy: 0.6272 - val_loss: 0.6129 - val_accuracy: 0.6773\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6466 - accuracy: 0.6326 - val_loss: 0.6103 - val_accuracy: 0.6811\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6429 - accuracy: 0.6368 - val_loss: 0.6094 - val_accuracy: 0.6811\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6430 - accuracy: 0.6334 - val_loss: 0.6119 - val_accuracy: 0.6786\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6477 - accuracy: 0.6266 - val_loss: 0.6156 - val_accuracy: 0.6824\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6483 - accuracy: 0.6246 - val_loss: 0.6130 - val_accuracy: 0.6811\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6475 - accuracy: 0.6356 - val_loss: 0.6113 - val_accuracy: 0.6798\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6495 - accuracy: 0.6246 - val_loss: 0.6153 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6470 - accuracy: 0.6309 - val_loss: 0.6111 - val_accuracy: 0.6837\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6440 - accuracy: 0.6309 - val_loss: 0.6124 - val_accuracy: 0.6811\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6462 - accuracy: 0.6324 - val_loss: 0.6113 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6479 - accuracy: 0.6323 - val_loss: 0.6152 - val_accuracy: 0.6849\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6457 - accuracy: 0.6295 - val_loss: 0.6124 - val_accuracy: 0.6862\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6442 - accuracy: 0.6318 - val_loss: 0.6112 - val_accuracy: 0.6849\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6459 - accuracy: 0.6345 - val_loss: 0.6113 - val_accuracy: 0.6837\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6461 - accuracy: 0.6291 - val_loss: 0.6092 - val_accuracy: 0.6875\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6438 - accuracy: 0.6348 - val_loss: 0.6107 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6447 - accuracy: 0.6380 - val_loss: 0.6118 - val_accuracy: 0.6837\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6425 - accuracy: 0.6339 - val_loss: 0.6101 - val_accuracy: 0.6824\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6494 - accuracy: 0.6237 - val_loss: 0.6123 - val_accuracy: 0.6837\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6427 - accuracy: 0.6405 - val_loss: 0.6117 - val_accuracy: 0.6837\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6484 - accuracy: 0.6358 - val_loss: 0.6118 - val_accuracy: 0.6811\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6439 - accuracy: 0.6368 - val_loss: 0.6089 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6437 - accuracy: 0.6344 - val_loss: 0.6086 - val_accuracy: 0.6862\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6452 - accuracy: 0.6392 - val_loss: 0.6094 - val_accuracy: 0.6888\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6434 - accuracy: 0.6408 - val_loss: 0.6098 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6464 - accuracy: 0.6346 - val_loss: 0.6118 - val_accuracy: 0.6875\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6434 - accuracy: 0.6361 - val_loss: 0.6104 - val_accuracy: 0.6849\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6415 - accuracy: 0.6368 - val_loss: 0.6110 - val_accuracy: 0.6888\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6436 - accuracy: 0.6331 - val_loss: 0.6090 - val_accuracy: 0.6875\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6452 - accuracy: 0.6356 - val_loss: 0.6096 - val_accuracy: 0.6862\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6442 - accuracy: 0.6372 - val_loss: 0.6117 - val_accuracy: 0.6811\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6479 - accuracy: 0.6318 - val_loss: 0.6114 - val_accuracy: 0.6875\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6420 - accuracy: 0.6355 - val_loss: 0.6085 - val_accuracy: 0.6901\n"
     ]
    }
   ],
   "source": [
    "# Lets explore some parameters:\n",
    "scores = {}\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30) # Early stop will stop training when the model has failed to improve\n",
    "\n",
    "dense1 = [700, 850, 1000]\n",
    "dense2 = [100, 250, 400]\n",
    "dropout = [0.5, 0.7, 0.9]\n",
    "optimizer = ['sgd']\n",
    "\n",
    "best_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "for d1 in dense1:\n",
    "    for d2 in dense2:\n",
    "        for dr in dropout:\n",
    "            for opt in optimizer:\n",
    "                print('Calculating for:', d1, d2, dr, opt)\n",
    "                keras_model = init_keras_model(dense1=d1, dense2=d2, dropout=dr, optimizer=opt)\n",
    "                keras_model.fit(X_train_tensor, \n",
    "                                y_train_tensor, \n",
    "                                epochs=n_epochs, \n",
    "                                validation_data=(X_val_tensor, y_val_tensor),\n",
    "                                callbacks=[es] \n",
    "                                )\n",
    "                score = keras_model.evaluate(X_test_tensor, y_test_tensor, verbose=0)[1]\n",
    "                scores[(d1, d2, dr, opt)] = score\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'dense1':d1, 'dense2':d2, 'dropout':dr, 'optimizer':opt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(700, 100, 0.5, 'sgd'): 0.6743295192718506, (700, 100, 0.7, 'sgd'): 0.6858237385749817, (700, 100, 0.9, 'sgd'): 0.6692209243774414, (700, 250, 0.5, 'sgd'): 0.671775221824646, (700, 250, 0.7, 'sgd'): 0.6615580916404724, (700, 250, 0.9, 'sgd'): 0.6704980731010437, (700, 400, 0.5, 'sgd'): 0.6730523705482483, (700, 400, 0.7, 'sgd'): 0.6628352403640747, (700, 400, 0.9, 'sgd'): 0.6692209243774414, (850, 100, 0.5, 'sgd'): 0.6743295192718506, (850, 100, 0.7, 'sgd'): 0.6602809429168701, (850, 100, 0.9, 'sgd'): 0.6743295192718506, (850, 250, 0.5, 'sgd'): 0.6513410210609436, (850, 250, 0.7, 'sgd'): 0.679438054561615, (850, 250, 0.9, 'sgd'): 0.671775221824646, (850, 400, 0.5, 'sgd'): 0.6602809429168701, (850, 400, 0.7, 'sgd'): 0.6704980731010437, (850, 400, 0.9, 'sgd'): 0.6756066679954529, (1000, 100, 0.5, 'sgd'): 0.6679438352584839, (1000, 100, 0.7, 'sgd'): 0.6666666865348816, (1000, 100, 0.9, 'sgd'): 0.6730523705482483, (1000, 250, 0.5, 'sgd'): 0.679438054561615, (1000, 250, 0.7, 'sgd'): 0.6768837571144104, (1000, 250, 0.9, 'sgd'): 0.679438054561615, (1000, 400, 0.5, 'sgd'): 0.656449556350708, (1000, 400, 0.7, 'sgd'): 0.6832695007324219, (1000, 400, 0.9, 'sgd'): 0.6768837571144104}\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'dense1': 700, 'dense2': 100, 'dropout': 0.7, 'optimizer': 'sgd'}, best_score: 0.6858237385749817\n"
     ]
    }
   ],
   "source": [
    "print(f'best_params: {best_params}, best_score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for: 650 50 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_240 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8221 - accuracy: 0.5309 - val_loss: 0.6546 - val_accuracy: 0.6467\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7168 - accuracy: 0.5504 - val_loss: 0.6543 - val_accuracy: 0.6454\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5605 - val_loss: 0.6454 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6806 - accuracy: 0.5835 - val_loss: 0.6437 - val_accuracy: 0.6607\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5816 - val_loss: 0.6476 - val_accuracy: 0.6543\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5867 - val_loss: 0.6417 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6681 - accuracy: 0.5956 - val_loss: 0.6451 - val_accuracy: 0.6645\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5944 - val_loss: 0.6425 - val_accuracy: 0.6633\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6081 - val_loss: 0.6371 - val_accuracy: 0.6671\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6084 - val_loss: 0.6369 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6094 - val_loss: 0.6402 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6042 - val_loss: 0.6389 - val_accuracy: 0.6658\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6564 - accuracy: 0.6110 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6076 - val_loss: 0.6358 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6207 - val_loss: 0.6359 - val_accuracy: 0.6620\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6149 - val_loss: 0.6328 - val_accuracy: 0.6760\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6222 - val_loss: 0.6268 - val_accuracy: 0.6747\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6248 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6207 - val_loss: 0.6339 - val_accuracy: 0.6531\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6306 - val_loss: 0.6329 - val_accuracy: 0.6786\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6257 - val_loss: 0.6329 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6483 - accuracy: 0.6284 - val_loss: 0.6305 - val_accuracy: 0.6747\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6266 - val_loss: 0.6280 - val_accuracy: 0.6671\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6277 - val_loss: 0.6280 - val_accuracy: 0.6824\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6307 - val_loss: 0.6270 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6237 - val_loss: 0.6319 - val_accuracy: 0.6760\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.6315 - val_loss: 0.6264 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6442 - accuracy: 0.6324 - val_loss: 0.6304 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6427 - accuracy: 0.6363 - val_loss: 0.6289 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6407 - accuracy: 0.6335 - val_loss: 0.6212 - val_accuracy: 0.6913\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6410 - accuracy: 0.6413 - val_loss: 0.6224 - val_accuracy: 0.6901\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6393 - val_loss: 0.6287 - val_accuracy: 0.6684\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6396 - accuracy: 0.6309 - val_loss: 0.6268 - val_accuracy: 0.6684\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6358 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6427 - val_loss: 0.6207 - val_accuracy: 0.6964\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6423 - val_loss: 0.6150 - val_accuracy: 0.6977\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6394 - accuracy: 0.6382 - val_loss: 0.6176 - val_accuracy: 0.6913\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6367 - accuracy: 0.6402 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6325 - accuracy: 0.6508 - val_loss: 0.6179 - val_accuracy: 0.6849\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6462 - val_loss: 0.6192 - val_accuracy: 0.6773\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6347 - accuracy: 0.6454 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6388 - accuracy: 0.6417 - val_loss: 0.6253 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6492 - val_loss: 0.6253 - val_accuracy: 0.6735\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6361 - accuracy: 0.6456 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6344 - accuracy: 0.6472 - val_loss: 0.6220 - val_accuracy: 0.6837\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6306 - accuracy: 0.6483 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6534 - val_loss: 0.6193 - val_accuracy: 0.6875\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6523 - val_loss: 0.6185 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6454 - val_loss: 0.6195 - val_accuracy: 0.6901\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6316 - accuracy: 0.6477 - val_loss: 0.6218 - val_accuracy: 0.6824\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6523 - val_loss: 0.6228 - val_accuracy: 0.6773\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6295 - accuracy: 0.6532 - val_loss: 0.6237 - val_accuracy: 0.6849\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6308 - accuracy: 0.6531 - val_loss: 0.6232 - val_accuracy: 0.6747\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6557 - val_loss: 0.6164 - val_accuracy: 0.6901\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6539 - val_loss: 0.6188 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6294 - accuracy: 0.6466 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6576 - val_loss: 0.6256 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6557 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6231 - accuracy: 0.6616 - val_loss: 0.6144 - val_accuracy: 0.6926\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6270 - accuracy: 0.6546 - val_loss: 0.6215 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6593 - val_loss: 0.6271 - val_accuracy: 0.6837\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6510 - val_loss: 0.6246 - val_accuracy: 0.6849\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6222 - accuracy: 0.6606 - val_loss: 0.6233 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6185 - accuracy: 0.6643 - val_loss: 0.6236 - val_accuracy: 0.6926\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6166 - accuracy: 0.6658 - val_loss: 0.6212 - val_accuracy: 0.6939\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6559 - val_loss: 0.6234 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6213 - accuracy: 0.6591 - val_loss: 0.6230 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6139 - accuracy: 0.6605 - val_loss: 0.6251 - val_accuracy: 0.6735\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.6704 - val_loss: 0.6260 - val_accuracy: 0.6849\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6173 - accuracy: 0.6682 - val_loss: 0.6219 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6608 - val_loss: 0.6206 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6129 - accuracy: 0.6647 - val_loss: 0.6223 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6706 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6118 - accuracy: 0.6694 - val_loss: 0.6240 - val_accuracy: 0.6849\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6709 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.6678 - val_loss: 0.6199 - val_accuracy: 0.6939\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6648 - val_loss: 0.6254 - val_accuracy: 0.6786\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6093 - accuracy: 0.6645 - val_loss: 0.6230 - val_accuracy: 0.6837\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6682 - val_loss: 0.6258 - val_accuracy: 0.6735\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6611 - val_loss: 0.6249 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6722 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6068 - accuracy: 0.6723 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6709 - val_loss: 0.6241 - val_accuracy: 0.6786\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6109 - accuracy: 0.6702 - val_loss: 0.6255 - val_accuracy: 0.6811\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6017 - accuracy: 0.6792 - val_loss: 0.6249 - val_accuracy: 0.6798\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6037 - accuracy: 0.6746 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6827 - val_loss: 0.6238 - val_accuracy: 0.6888\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6044 - accuracy: 0.6728 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6041 - accuracy: 0.6738 - val_loss: 0.6236 - val_accuracy: 0.6837\n",
      "Calculating for: 650 50 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_244 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8211 - accuracy: 0.5214 - val_loss: 0.6578 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7249 - accuracy: 0.5441 - val_loss: 0.6572 - val_accuracy: 0.6543\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6974 - accuracy: 0.5516 - val_loss: 0.6519 - val_accuracy: 0.6633\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6854 - accuracy: 0.5625 - val_loss: 0.6523 - val_accuracy: 0.6633\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6813 - accuracy: 0.5620 - val_loss: 0.6480 - val_accuracy: 0.6671\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5883 - val_loss: 0.6416 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6763 - accuracy: 0.5823 - val_loss: 0.6462 - val_accuracy: 0.6658\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.5853 - val_loss: 0.6451 - val_accuracy: 0.6709\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5918 - val_loss: 0.6471 - val_accuracy: 0.6760\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5936 - val_loss: 0.6398 - val_accuracy: 0.6722\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5888 - val_loss: 0.6386 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5934 - val_loss: 0.6390 - val_accuracy: 0.6760\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.5987 - val_loss: 0.6347 - val_accuracy: 0.6798\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6652 - accuracy: 0.6035 - val_loss: 0.6358 - val_accuracy: 0.6786\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.5958 - val_loss: 0.6401 - val_accuracy: 0.6760\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6011 - val_loss: 0.6417 - val_accuracy: 0.6722\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6600 - accuracy: 0.6076 - val_loss: 0.6322 - val_accuracy: 0.6862\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6093 - val_loss: 0.6344 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6107 - val_loss: 0.6421 - val_accuracy: 0.6684\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6152 - val_loss: 0.6347 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6203 - val_loss: 0.6291 - val_accuracy: 0.6875\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6591 - accuracy: 0.6078 - val_loss: 0.6325 - val_accuracy: 0.6798\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6194 - val_loss: 0.6284 - val_accuracy: 0.6837\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6619 - accuracy: 0.6145 - val_loss: 0.6336 - val_accuracy: 0.6862\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6554 - accuracy: 0.6168 - val_loss: 0.6338 - val_accuracy: 0.6811\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6553 - accuracy: 0.6144 - val_loss: 0.6302 - val_accuracy: 0.6837\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6155 - val_loss: 0.6322 - val_accuracy: 0.6849\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6568 - accuracy: 0.6138 - val_loss: 0.6318 - val_accuracy: 0.6913\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6124 - val_loss: 0.6334 - val_accuracy: 0.6875\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6559 - accuracy: 0.6095 - val_loss: 0.6311 - val_accuracy: 0.6849\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6230 - val_loss: 0.6275 - val_accuracy: 0.6837\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6220 - val_loss: 0.6250 - val_accuracy: 0.6824\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6518 - accuracy: 0.6264 - val_loss: 0.6290 - val_accuracy: 0.6901\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6252 - val_loss: 0.6283 - val_accuracy: 0.6888\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6526 - accuracy: 0.6238 - val_loss: 0.6263 - val_accuracy: 0.6913\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6276 - val_loss: 0.6295 - val_accuracy: 0.6901\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6508 - accuracy: 0.6265 - val_loss: 0.6233 - val_accuracy: 0.6913\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6236 - val_loss: 0.6315 - val_accuracy: 0.6888\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6515 - accuracy: 0.6296 - val_loss: 0.6221 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6493 - accuracy: 0.6252 - val_loss: 0.6206 - val_accuracy: 0.6913\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6467 - accuracy: 0.6346 - val_loss: 0.6157 - val_accuracy: 0.6952\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6306 - val_loss: 0.6220 - val_accuracy: 0.6926\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6346 - val_loss: 0.6239 - val_accuracy: 0.6926\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6496 - accuracy: 0.6243 - val_loss: 0.6257 - val_accuracy: 0.6939\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.6296 - val_loss: 0.6221 - val_accuracy: 0.6913\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6309 - val_loss: 0.6216 - val_accuracy: 0.6926\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6462 - accuracy: 0.6338 - val_loss: 0.6251 - val_accuracy: 0.6926\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6474 - accuracy: 0.6299 - val_loss: 0.6223 - val_accuracy: 0.6901\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6389 - val_loss: 0.6190 - val_accuracy: 0.7015\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6333 - val_loss: 0.6208 - val_accuracy: 0.6901\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6453 - accuracy: 0.6289 - val_loss: 0.6188 - val_accuracy: 0.7003\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6463 - accuracy: 0.6343 - val_loss: 0.6204 - val_accuracy: 0.7041\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6379 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6370 - val_loss: 0.6175 - val_accuracy: 0.7003\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6419 - accuracy: 0.6442 - val_loss: 0.6178 - val_accuracy: 0.6990\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6377 - val_loss: 0.6251 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6397 - val_loss: 0.6248 - val_accuracy: 0.6862\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6404 - accuracy: 0.6359 - val_loss: 0.6208 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6431 - accuracy: 0.6356 - val_loss: 0.6240 - val_accuracy: 0.6888\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6481 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6412 - accuracy: 0.6456 - val_loss: 0.6179 - val_accuracy: 0.6977\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6422 - val_loss: 0.6206 - val_accuracy: 0.6888\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6373 - accuracy: 0.6423 - val_loss: 0.6218 - val_accuracy: 0.6875\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6439 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6330 - accuracy: 0.6433 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6385 - accuracy: 0.6453 - val_loss: 0.6218 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6347 - accuracy: 0.6437 - val_loss: 0.6177 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6451 - val_loss: 0.6166 - val_accuracy: 0.6952\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6360 - accuracy: 0.6488 - val_loss: 0.6186 - val_accuracy: 0.6964\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6527 - val_loss: 0.6148 - val_accuracy: 0.6977\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6320 - accuracy: 0.6501 - val_loss: 0.6159 - val_accuracy: 0.6926\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6491 - val_loss: 0.6164 - val_accuracy: 0.6926\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6300 - accuracy: 0.6544 - val_loss: 0.6177 - val_accuracy: 0.6926\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6518 - val_loss: 0.6164 - val_accuracy: 0.6939\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6529 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6311 - accuracy: 0.6486 - val_loss: 0.6214 - val_accuracy: 0.6875\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6323 - accuracy: 0.6550 - val_loss: 0.6169 - val_accuracy: 0.6926\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6506 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6508 - val_loss: 0.6161 - val_accuracy: 0.6939\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6319 - accuracy: 0.6565 - val_loss: 0.6217 - val_accuracy: 0.6913\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6274 - accuracy: 0.6546 - val_loss: 0.6119 - val_accuracy: 0.6913\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6338 - accuracy: 0.6444 - val_loss: 0.6167 - val_accuracy: 0.6939\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6510 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6333 - accuracy: 0.6498 - val_loss: 0.6180 - val_accuracy: 0.6901\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6540 - val_loss: 0.6192 - val_accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6567 - val_loss: 0.6161 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6251 - accuracy: 0.6588 - val_loss: 0.6178 - val_accuracy: 0.6824\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6240 - accuracy: 0.6545 - val_loss: 0.6178 - val_accuracy: 0.6811\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6303 - accuracy: 0.6589 - val_loss: 0.6166 - val_accuracy: 0.6824\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6525 - val_loss: 0.6143 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6257 - accuracy: 0.6603 - val_loss: 0.6141 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6575 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6560 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6576 - val_loss: 0.6143 - val_accuracy: 0.6837\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6556 - val_loss: 0.6140 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6256 - accuracy: 0.6545 - val_loss: 0.6167 - val_accuracy: 0.6888\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6248 - accuracy: 0.6583 - val_loss: 0.6174 - val_accuracy: 0.6837\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6241 - accuracy: 0.6579 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6674 - val_loss: 0.6131 - val_accuracy: 0.6977\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6561 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6649 - val_loss: 0.6156 - val_accuracy: 0.6926\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6204 - accuracy: 0.6628 - val_loss: 0.6140 - val_accuracy: 0.6913\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6625 - val_loss: 0.6137 - val_accuracy: 0.6901\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6194 - accuracy: 0.6728 - val_loss: 0.6106 - val_accuracy: 0.6926\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6654 - val_loss: 0.6142 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6626 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6536 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6176 - accuracy: 0.6600 - val_loss: 0.6148 - val_accuracy: 0.6875\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6638 - val_loss: 0.6139 - val_accuracy: 0.6926\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6232 - accuracy: 0.6611 - val_loss: 0.6129 - val_accuracy: 0.6990\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6174 - accuracy: 0.6647 - val_loss: 0.6135 - val_accuracy: 0.6977\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6188 - accuracy: 0.6620 - val_loss: 0.6120 - val_accuracy: 0.6913\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6680 - val_loss: 0.6178 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6694 - val_loss: 0.6125 - val_accuracy: 0.6952\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6674 - val_loss: 0.6154 - val_accuracy: 0.6952\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6655 - val_loss: 0.6124 - val_accuracy: 0.6964\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6166 - accuracy: 0.6652 - val_loss: 0.6188 - val_accuracy: 0.6913\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6687 - val_loss: 0.6156 - val_accuracy: 0.6913\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6704 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6659 - val_loss: 0.6149 - val_accuracy: 0.6939\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6624 - val_loss: 0.6171 - val_accuracy: 0.6926\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6120 - accuracy: 0.6689 - val_loss: 0.6147 - val_accuracy: 0.6901\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6138 - accuracy: 0.6721 - val_loss: 0.6127 - val_accuracy: 0.6862\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6139 - accuracy: 0.6734 - val_loss: 0.6181 - val_accuracy: 0.6901\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6119 - accuracy: 0.6738 - val_loss: 0.6129 - val_accuracy: 0.6888\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6113 - accuracy: 0.6738 - val_loss: 0.6152 - val_accuracy: 0.6837\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6097 - accuracy: 0.6702 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6123 - accuracy: 0.6672 - val_loss: 0.6159 - val_accuracy: 0.6798\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6152 - accuracy: 0.6717 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6101 - accuracy: 0.6742 - val_loss: 0.6184 - val_accuracy: 0.6862\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6111 - accuracy: 0.6731 - val_loss: 0.6147 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6131 - accuracy: 0.6654 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6094 - accuracy: 0.6799 - val_loss: 0.6201 - val_accuracy: 0.6837\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6089 - accuracy: 0.6703 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Calculating for: 650 50 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_248 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8191 - accuracy: 0.5112 - val_loss: 0.6613 - val_accuracy: 0.6237\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7258 - accuracy: 0.5280 - val_loss: 0.6625 - val_accuracy: 0.6276\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7045 - accuracy: 0.5295 - val_loss: 0.6638 - val_accuracy: 0.6327\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5383 - val_loss: 0.6589 - val_accuracy: 0.6429\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5497 - val_loss: 0.6620 - val_accuracy: 0.6441\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6865 - accuracy: 0.5475 - val_loss: 0.6624 - val_accuracy: 0.6505\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6851 - accuracy: 0.5481 - val_loss: 0.6601 - val_accuracy: 0.6454\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6833 - accuracy: 0.5582 - val_loss: 0.6605 - val_accuracy: 0.6429\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6829 - accuracy: 0.5540 - val_loss: 0.6644 - val_accuracy: 0.6620\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.5578 - val_loss: 0.6613 - val_accuracy: 0.6582\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5604 - val_loss: 0.6557 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6798 - accuracy: 0.5668 - val_loss: 0.6527 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.5673 - val_loss: 0.6524 - val_accuracy: 0.6569\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5766 - val_loss: 0.6531 - val_accuracy: 0.6607\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5764 - val_loss: 0.6496 - val_accuracy: 0.6492\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.5772 - val_loss: 0.6510 - val_accuracy: 0.6582\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6751 - accuracy: 0.5787 - val_loss: 0.6525 - val_accuracy: 0.6594\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6732 - accuracy: 0.5864 - val_loss: 0.6501 - val_accuracy: 0.6594\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6750 - accuracy: 0.5815 - val_loss: 0.6537 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5774 - val_loss: 0.6448 - val_accuracy: 0.6582\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5801 - val_loss: 0.6448 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6710 - accuracy: 0.5863 - val_loss: 0.6500 - val_accuracy: 0.6658\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5880 - val_loss: 0.6377 - val_accuracy: 0.6569\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6702 - accuracy: 0.5907 - val_loss: 0.6423 - val_accuracy: 0.6671\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6708 - accuracy: 0.5934 - val_loss: 0.6440 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6704 - accuracy: 0.5864 - val_loss: 0.6422 - val_accuracy: 0.6709\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5960 - val_loss: 0.6484 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6705 - accuracy: 0.5956 - val_loss: 0.6423 - val_accuracy: 0.6722\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5841 - val_loss: 0.6471 - val_accuracy: 0.6760\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.5985 - val_loss: 0.6454 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6679 - accuracy: 0.5892 - val_loss: 0.6425 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6677 - accuracy: 0.5968 - val_loss: 0.6435 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5968 - val_loss: 0.6382 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5918 - val_loss: 0.6369 - val_accuracy: 0.6722\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6675 - accuracy: 0.5957 - val_loss: 0.6418 - val_accuracy: 0.6773\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6629 - accuracy: 0.6032 - val_loss: 0.6403 - val_accuracy: 0.6735\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6658 - accuracy: 0.6065 - val_loss: 0.6438 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.5978 - val_loss: 0.6373 - val_accuracy: 0.6709\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6069 - val_loss: 0.6395 - val_accuracy: 0.6747\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6017 - val_loss: 0.6388 - val_accuracy: 0.6786\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6622 - accuracy: 0.6035 - val_loss: 0.6396 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6649 - accuracy: 0.5990 - val_loss: 0.6393 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6054 - val_loss: 0.6412 - val_accuracy: 0.6722\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6047 - val_loss: 0.6354 - val_accuracy: 0.6837\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6086 - val_loss: 0.6352 - val_accuracy: 0.6786\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6125 - val_loss: 0.6358 - val_accuracy: 0.6747\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6088 - val_loss: 0.6385 - val_accuracy: 0.6824\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6623 - accuracy: 0.6107 - val_loss: 0.6411 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6124 - val_loss: 0.6328 - val_accuracy: 0.6747\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6610 - accuracy: 0.6150 - val_loss: 0.6386 - val_accuracy: 0.6811\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6130 - val_loss: 0.6341 - val_accuracy: 0.6824\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.6115 - val_loss: 0.6332 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6061 - val_loss: 0.6363 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6147 - val_loss: 0.6383 - val_accuracy: 0.6722\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6589 - accuracy: 0.6158 - val_loss: 0.6337 - val_accuracy: 0.6786\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6089 - val_loss: 0.6326 - val_accuracy: 0.6735\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6613 - accuracy: 0.6027 - val_loss: 0.6339 - val_accuracy: 0.6773\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6173 - val_loss: 0.6336 - val_accuracy: 0.6837\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6130 - val_loss: 0.6334 - val_accuracy: 0.6837\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6564 - accuracy: 0.6167 - val_loss: 0.6298 - val_accuracy: 0.6747\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6601 - accuracy: 0.6099 - val_loss: 0.6380 - val_accuracy: 0.6862\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6110 - val_loss: 0.6378 - val_accuracy: 0.6875\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6144 - val_loss: 0.6360 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6572 - accuracy: 0.6118 - val_loss: 0.6356 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6113 - val_loss: 0.6304 - val_accuracy: 0.6811\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6294 - val_loss: 0.6328 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6600 - accuracy: 0.6153 - val_loss: 0.6328 - val_accuracy: 0.6875\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6137 - val_loss: 0.6286 - val_accuracy: 0.6798\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6230 - val_loss: 0.6302 - val_accuracy: 0.6849\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6218 - val_loss: 0.6264 - val_accuracy: 0.6862\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6550 - accuracy: 0.6245 - val_loss: 0.6272 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6208 - val_loss: 0.6334 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6184 - val_loss: 0.6408 - val_accuracy: 0.6531\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6558 - accuracy: 0.6243 - val_loss: 0.6341 - val_accuracy: 0.6824\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6212 - val_loss: 0.6350 - val_accuracy: 0.6875\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6201 - val_loss: 0.6293 - val_accuracy: 0.6862\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6232 - val_loss: 0.6301 - val_accuracy: 0.6798\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6188 - val_loss: 0.6270 - val_accuracy: 0.6798\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6196 - val_loss: 0.6305 - val_accuracy: 0.6888\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6226 - val_loss: 0.6258 - val_accuracy: 0.6837\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6546 - accuracy: 0.6230 - val_loss: 0.6348 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6221 - val_loss: 0.6334 - val_accuracy: 0.6875\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6553 - accuracy: 0.6204 - val_loss: 0.6323 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6199 - val_loss: 0.6261 - val_accuracy: 0.6849\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6246 - val_loss: 0.6295 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6503 - accuracy: 0.6264 - val_loss: 0.6298 - val_accuracy: 0.6837\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6510 - accuracy: 0.6251 - val_loss: 0.6272 - val_accuracy: 0.6837\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6504 - accuracy: 0.6262 - val_loss: 0.6230 - val_accuracy: 0.6888\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6255 - val_loss: 0.6251 - val_accuracy: 0.6875\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6513 - accuracy: 0.6246 - val_loss: 0.6317 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6265 - val_loss: 0.6308 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6307 - val_loss: 0.6264 - val_accuracy: 0.6875\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6513 - accuracy: 0.6247 - val_loss: 0.6331 - val_accuracy: 0.6888\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6312 - val_loss: 0.6252 - val_accuracy: 0.6811\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6299 - val_loss: 0.6251 - val_accuracy: 0.6824\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6502 - accuracy: 0.6284 - val_loss: 0.6229 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6487 - accuracy: 0.6363 - val_loss: 0.6251 - val_accuracy: 0.6862\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6487 - accuracy: 0.6295 - val_loss: 0.6268 - val_accuracy: 0.6849\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6272 - val_loss: 0.6256 - val_accuracy: 0.6862\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6492 - accuracy: 0.6335 - val_loss: 0.6284 - val_accuracy: 0.6875\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6479 - accuracy: 0.6311 - val_loss: 0.6270 - val_accuracy: 0.6901\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6499 - accuracy: 0.6315 - val_loss: 0.6273 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6328 - val_loss: 0.6308 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6294 - val_loss: 0.6290 - val_accuracy: 0.6913\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6473 - accuracy: 0.6314 - val_loss: 0.6273 - val_accuracy: 0.6888\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6294 - val_loss: 0.6270 - val_accuracy: 0.6939\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6316 - val_loss: 0.6205 - val_accuracy: 0.6824\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6277 - val_loss: 0.6276 - val_accuracy: 0.6837\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6457 - accuracy: 0.6296 - val_loss: 0.6230 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6349 - val_loss: 0.6302 - val_accuracy: 0.6849\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6393 - val_loss: 0.6264 - val_accuracy: 0.6849\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6339 - val_loss: 0.6269 - val_accuracy: 0.6849\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6305 - val_loss: 0.6201 - val_accuracy: 0.6773\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6282 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6370 - val_loss: 0.6232 - val_accuracy: 0.6913\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6323 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6370 - val_loss: 0.6264 - val_accuracy: 0.6888\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6448 - accuracy: 0.6340 - val_loss: 0.6266 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6349 - val_loss: 0.6184 - val_accuracy: 0.6811\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6389 - val_loss: 0.6233 - val_accuracy: 0.6811\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6420 - accuracy: 0.6382 - val_loss: 0.6244 - val_accuracy: 0.6862\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6330 - val_loss: 0.6304 - val_accuracy: 0.6849\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6331 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6436 - accuracy: 0.6387 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6385 - val_loss: 0.6251 - val_accuracy: 0.6862\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6378 - val_loss: 0.6153 - val_accuracy: 0.6811\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6393 - val_loss: 0.6251 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6335 - val_loss: 0.6340 - val_accuracy: 0.6773\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6379 - val_loss: 0.6206 - val_accuracy: 0.6837\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6354 - val_loss: 0.6249 - val_accuracy: 0.6837\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6393 - val_loss: 0.6274 - val_accuracy: 0.6901\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6414 - val_loss: 0.6165 - val_accuracy: 0.6849\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6270 - val_loss: 0.6193 - val_accuracy: 0.6875\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6408 - val_loss: 0.6205 - val_accuracy: 0.6862\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6339 - val_loss: 0.6220 - val_accuracy: 0.6849\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6446 - val_loss: 0.6234 - val_accuracy: 0.6837\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6418 - val_loss: 0.6191 - val_accuracy: 0.6849\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6407 - val_loss: 0.6271 - val_accuracy: 0.6837\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6375 - val_loss: 0.6235 - val_accuracy: 0.6862\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6392 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6389 - val_loss: 0.6193 - val_accuracy: 0.6901\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6458 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6380 - val_loss: 0.6211 - val_accuracy: 0.6964\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6512 - val_loss: 0.6174 - val_accuracy: 0.6952\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6375 - val_loss: 0.6190 - val_accuracy: 0.6913\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6356 - accuracy: 0.6485 - val_loss: 0.6181 - val_accuracy: 0.6888\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6387 - accuracy: 0.6468 - val_loss: 0.6142 - val_accuracy: 0.6913\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6462 - val_loss: 0.6153 - val_accuracy: 0.6875\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6453 - val_loss: 0.6201 - val_accuracy: 0.6875\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.6477 - val_loss: 0.6230 - val_accuracy: 0.6875\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6391 - accuracy: 0.6473 - val_loss: 0.6158 - val_accuracy: 0.6952\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6393 - accuracy: 0.6463 - val_loss: 0.6219 - val_accuracy: 0.6926\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6453 - val_loss: 0.6202 - val_accuracy: 0.6926\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6476 - val_loss: 0.6176 - val_accuracy: 0.6977\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6449 - val_loss: 0.6192 - val_accuracy: 0.6939\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6472 - val_loss: 0.6215 - val_accuracy: 0.6939\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6438 - accuracy: 0.6412 - val_loss: 0.6178 - val_accuracy: 0.6964\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6462 - val_loss: 0.6150 - val_accuracy: 0.7003\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6444 - val_loss: 0.6299 - val_accuracy: 0.6773\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6531 - val_loss: 0.6195 - val_accuracy: 0.6952\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6343 - accuracy: 0.6516 - val_loss: 0.6198 - val_accuracy: 0.6952\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6363 - accuracy: 0.6466 - val_loss: 0.6174 - val_accuracy: 0.7054\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6409 - val_loss: 0.6151 - val_accuracy: 0.7028\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6378 - accuracy: 0.6423 - val_loss: 0.6161 - val_accuracy: 0.6977\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6531 - val_loss: 0.6205 - val_accuracy: 0.6926\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6356 - accuracy: 0.6500 - val_loss: 0.6178 - val_accuracy: 0.6913\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6510 - val_loss: 0.6155 - val_accuracy: 0.7041\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6300 - accuracy: 0.6545 - val_loss: 0.6118 - val_accuracy: 0.7066\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6590 - val_loss: 0.6123 - val_accuracy: 0.7028\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6338 - accuracy: 0.6520 - val_loss: 0.6158 - val_accuracy: 0.7003\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6532 - val_loss: 0.6141 - val_accuracy: 0.7041\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6349 - accuracy: 0.6449 - val_loss: 0.6189 - val_accuracy: 0.6977\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6338 - accuracy: 0.6501 - val_loss: 0.6160 - val_accuracy: 0.7028\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6506 - val_loss: 0.6144 - val_accuracy: 0.6977\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6574 - val_loss: 0.6164 - val_accuracy: 0.6990\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6473 - val_loss: 0.6154 - val_accuracy: 0.6939\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6304 - accuracy: 0.6544 - val_loss: 0.6152 - val_accuracy: 0.6926\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6614 - val_loss: 0.6158 - val_accuracy: 0.6964\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6567 - val_loss: 0.6124 - val_accuracy: 0.6926\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6332 - accuracy: 0.6529 - val_loss: 0.6143 - val_accuracy: 0.6952\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6550 - val_loss: 0.6149 - val_accuracy: 0.6977\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6459 - val_loss: 0.6129 - val_accuracy: 0.7028\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6515 - val_loss: 0.6177 - val_accuracy: 0.6977\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6279 - accuracy: 0.6600 - val_loss: 0.6156 - val_accuracy: 0.6913\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6576 - val_loss: 0.6113 - val_accuracy: 0.6990\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6523 - val_loss: 0.6207 - val_accuracy: 0.6849\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6505 - val_loss: 0.6200 - val_accuracy: 0.6849\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6585 - val_loss: 0.6116 - val_accuracy: 0.7015\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6554 - val_loss: 0.6183 - val_accuracy: 0.7003\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6567 - val_loss: 0.6145 - val_accuracy: 0.6977\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6556 - val_loss: 0.6150 - val_accuracy: 0.7015\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6596 - val_loss: 0.6129 - val_accuracy: 0.6977\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6600 - val_loss: 0.6121 - val_accuracy: 0.6939\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6511 - val_loss: 0.6151 - val_accuracy: 0.6964\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6534 - val_loss: 0.6192 - val_accuracy: 0.6964\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6520 - val_loss: 0.6153 - val_accuracy: 0.7003\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6595 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6255 - accuracy: 0.6601 - val_loss: 0.6130 - val_accuracy: 0.6977\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6585 - val_loss: 0.6150 - val_accuracy: 0.6977\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6610 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Calculating for: 650 100 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_252 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8317 - accuracy: 0.5348 - val_loss: 0.7068 - val_accuracy: 0.4694\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7164 - accuracy: 0.5653 - val_loss: 0.6667 - val_accuracy: 0.6276\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6852 - accuracy: 0.5769 - val_loss: 0.6486 - val_accuracy: 0.6633\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6790 - accuracy: 0.5760 - val_loss: 0.6503 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5855 - val_loss: 0.6494 - val_accuracy: 0.6722\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5933 - val_loss: 0.6470 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6056 - val_loss: 0.6449 - val_accuracy: 0.6696\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6022 - val_loss: 0.6407 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6069 - val_loss: 0.6448 - val_accuracy: 0.6556\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6065 - val_loss: 0.6444 - val_accuracy: 0.6607\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6118 - val_loss: 0.6396 - val_accuracy: 0.6722\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6167 - val_loss: 0.6384 - val_accuracy: 0.6696\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6220 - val_loss: 0.6377 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6201 - val_loss: 0.6392 - val_accuracy: 0.6620\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6197 - val_loss: 0.6358 - val_accuracy: 0.6645\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6183 - val_loss: 0.6341 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6197 - val_loss: 0.6390 - val_accuracy: 0.6480\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6276 - val_loss: 0.6346 - val_accuracy: 0.6696\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6250 - val_loss: 0.6358 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6265 - val_loss: 0.6340 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6326 - val_loss: 0.6365 - val_accuracy: 0.6671\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6282 - val_loss: 0.6344 - val_accuracy: 0.6620\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6276 - val_loss: 0.6275 - val_accuracy: 0.6786\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6449 - accuracy: 0.6380 - val_loss: 0.6275 - val_accuracy: 0.6926\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6320 - val_loss: 0.6298 - val_accuracy: 0.6811\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6340 - val_loss: 0.6311 - val_accuracy: 0.6798\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6316 - val_loss: 0.6354 - val_accuracy: 0.6735\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6370 - val_loss: 0.6319 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6309 - val_loss: 0.6293 - val_accuracy: 0.6760\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6383 - val_loss: 0.6290 - val_accuracy: 0.6837\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6379 - val_loss: 0.6253 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6378 - val_loss: 0.6257 - val_accuracy: 0.6811\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6419 - val_loss: 0.6256 - val_accuracy: 0.6786\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6408 - val_loss: 0.6226 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6459 - val_loss: 0.6256 - val_accuracy: 0.6722\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6375 - val_loss: 0.6312 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6408 - val_loss: 0.6291 - val_accuracy: 0.6735\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6417 - val_loss: 0.6227 - val_accuracy: 0.6786\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6449 - val_loss: 0.6272 - val_accuracy: 0.6658\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6522 - val_loss: 0.6299 - val_accuracy: 0.6543\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6503 - val_loss: 0.6295 - val_accuracy: 0.6633\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6510 - val_loss: 0.6198 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6513 - val_loss: 0.6197 - val_accuracy: 0.6773\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6487 - val_loss: 0.6217 - val_accuracy: 0.6773\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6478 - val_loss: 0.6216 - val_accuracy: 0.6747\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6520 - val_loss: 0.6228 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6487 - val_loss: 0.6251 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6502 - val_loss: 0.6210 - val_accuracy: 0.6824\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6530 - val_loss: 0.6220 - val_accuracy: 0.6849\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6598 - val_loss: 0.6220 - val_accuracy: 0.6888\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6523 - val_loss: 0.6185 - val_accuracy: 0.6926\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6521 - val_loss: 0.6214 - val_accuracy: 0.6747\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6567 - val_loss: 0.6236 - val_accuracy: 0.6722\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6571 - val_loss: 0.6239 - val_accuracy: 0.6696\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6210 - accuracy: 0.6590 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6593 - val_loss: 0.6283 - val_accuracy: 0.6658\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6603 - val_loss: 0.6248 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6628 - val_loss: 0.6233 - val_accuracy: 0.6696\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6642 - val_loss: 0.6310 - val_accuracy: 0.6543\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6580 - val_loss: 0.6226 - val_accuracy: 0.6684\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6614 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6574 - val_loss: 0.6214 - val_accuracy: 0.6735\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6633 - val_loss: 0.6210 - val_accuracy: 0.6760\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6642 - val_loss: 0.6219 - val_accuracy: 0.6722\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6125 - accuracy: 0.6677 - val_loss: 0.6212 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6120 - accuracy: 0.6653 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6667 - val_loss: 0.6254 - val_accuracy: 0.6658\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6121 - accuracy: 0.6707 - val_loss: 0.6257 - val_accuracy: 0.6658\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6134 - accuracy: 0.6698 - val_loss: 0.6280 - val_accuracy: 0.6620\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6726 - val_loss: 0.6249 - val_accuracy: 0.6684\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6678 - val_loss: 0.6267 - val_accuracy: 0.6633\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6643 - val_loss: 0.6343 - val_accuracy: 0.6518\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6111 - accuracy: 0.6732 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.6698 - val_loss: 0.6309 - val_accuracy: 0.6569\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6104 - accuracy: 0.6745 - val_loss: 0.6246 - val_accuracy: 0.6671\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6076 - accuracy: 0.6780 - val_loss: 0.6280 - val_accuracy: 0.6556\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6747 - val_loss: 0.6270 - val_accuracy: 0.6633\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6691 - val_loss: 0.6273 - val_accuracy: 0.6645\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6727 - val_loss: 0.6293 - val_accuracy: 0.6594\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6076 - accuracy: 0.6736 - val_loss: 0.6305 - val_accuracy: 0.6645\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6087 - accuracy: 0.6702 - val_loss: 0.6323 - val_accuracy: 0.6645\n",
      "Calculating for: 650 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_256 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8219 - accuracy: 0.5234 - val_loss: 0.6943 - val_accuracy: 0.5013\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7247 - accuracy: 0.5485 - val_loss: 0.6612 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5578 - val_loss: 0.6571 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6818 - accuracy: 0.5747 - val_loss: 0.6543 - val_accuracy: 0.6518\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6782 - accuracy: 0.5678 - val_loss: 0.6519 - val_accuracy: 0.6620\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6763 - accuracy: 0.5804 - val_loss: 0.6497 - val_accuracy: 0.6633\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.5863 - val_loss: 0.6521 - val_accuracy: 0.6696\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5849 - val_loss: 0.6532 - val_accuracy: 0.6582\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6712 - accuracy: 0.5874 - val_loss: 0.6500 - val_accuracy: 0.6594\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5960 - val_loss: 0.6468 - val_accuracy: 0.6633\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6015 - val_loss: 0.6432 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6007 - val_loss: 0.6502 - val_accuracy: 0.6594\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6016 - val_loss: 0.6446 - val_accuracy: 0.6671\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6066 - val_loss: 0.6449 - val_accuracy: 0.6607\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6055 - val_loss: 0.6473 - val_accuracy: 0.6645\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5987 - val_loss: 0.6390 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6626 - accuracy: 0.6034 - val_loss: 0.6431 - val_accuracy: 0.6582\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6068 - val_loss: 0.6418 - val_accuracy: 0.6594\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6112 - val_loss: 0.6416 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6153 - val_loss: 0.6387 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6187 - val_loss: 0.6389 - val_accuracy: 0.6518\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6107 - val_loss: 0.6334 - val_accuracy: 0.6671\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6142 - val_loss: 0.6402 - val_accuracy: 0.6556\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6171 - val_loss: 0.6412 - val_accuracy: 0.6531\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6148 - val_loss: 0.6409 - val_accuracy: 0.6505\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6226 - val_loss: 0.6365 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6252 - val_loss: 0.6326 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6206 - val_loss: 0.6351 - val_accuracy: 0.6556\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6223 - val_loss: 0.6378 - val_accuracy: 0.6594\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6197 - val_loss: 0.6396 - val_accuracy: 0.6569\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6236 - val_loss: 0.6343 - val_accuracy: 0.6620\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6223 - val_loss: 0.6366 - val_accuracy: 0.6492\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6197 - val_loss: 0.6330 - val_accuracy: 0.6518\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6512 - accuracy: 0.6232 - val_loss: 0.6357 - val_accuracy: 0.6620\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6233 - val_loss: 0.6334 - val_accuracy: 0.6607\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6256 - val_loss: 0.6323 - val_accuracy: 0.6633\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6492 - accuracy: 0.6274 - val_loss: 0.6311 - val_accuracy: 0.6594\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6309 - val_loss: 0.6361 - val_accuracy: 0.6569\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6301 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6291 - val_loss: 0.6323 - val_accuracy: 0.6607\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6294 - val_loss: 0.6330 - val_accuracy: 0.6454\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6361 - val_loss: 0.6304 - val_accuracy: 0.6645\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6267 - val_loss: 0.6325 - val_accuracy: 0.6556\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6304 - val_loss: 0.6336 - val_accuracy: 0.6594\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6330 - val_loss: 0.6343 - val_accuracy: 0.6582\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6363 - val_loss: 0.6237 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6360 - val_loss: 0.6249 - val_accuracy: 0.6696\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6349 - val_loss: 0.6234 - val_accuracy: 0.6747\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6380 - val_loss: 0.6287 - val_accuracy: 0.6671\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6382 - val_loss: 0.6266 - val_accuracy: 0.6645\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6349 - val_loss: 0.6321 - val_accuracy: 0.6594\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6414 - accuracy: 0.6373 - val_loss: 0.6270 - val_accuracy: 0.6569\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6387 - val_loss: 0.6273 - val_accuracy: 0.6505\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6375 - val_loss: 0.6290 - val_accuracy: 0.6543\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6407 - val_loss: 0.6250 - val_accuracy: 0.6531\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6408 - val_loss: 0.6250 - val_accuracy: 0.6658\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6355 - val_loss: 0.6226 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6393 - val_loss: 0.6332 - val_accuracy: 0.6594\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6449 - val_loss: 0.6286 - val_accuracy: 0.6569\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6453 - val_loss: 0.6306 - val_accuracy: 0.6569\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6485 - val_loss: 0.6231 - val_accuracy: 0.6684\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6420 - val_loss: 0.6200 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6458 - val_loss: 0.6288 - val_accuracy: 0.6594\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6497 - val_loss: 0.6287 - val_accuracy: 0.6543\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6467 - val_loss: 0.6257 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6427 - val_loss: 0.6286 - val_accuracy: 0.6607\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6395 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6447 - val_loss: 0.6269 - val_accuracy: 0.6556\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6335 - accuracy: 0.6508 - val_loss: 0.6186 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6507 - val_loss: 0.6249 - val_accuracy: 0.6709\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6497 - val_loss: 0.6329 - val_accuracy: 0.6582\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6534 - val_loss: 0.6233 - val_accuracy: 0.6760\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6501 - val_loss: 0.6236 - val_accuracy: 0.6684\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6559 - val_loss: 0.6201 - val_accuracy: 0.6722\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6488 - val_loss: 0.6221 - val_accuracy: 0.6747\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6523 - val_loss: 0.6260 - val_accuracy: 0.6671\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6496 - val_loss: 0.6206 - val_accuracy: 0.6696\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6515 - val_loss: 0.6288 - val_accuracy: 0.6543\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6555 - val_loss: 0.6243 - val_accuracy: 0.6658\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6498 - val_loss: 0.6259 - val_accuracy: 0.6620\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6526 - val_loss: 0.6257 - val_accuracy: 0.6684\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6516 - val_loss: 0.6242 - val_accuracy: 0.6722\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6522 - val_loss: 0.6246 - val_accuracy: 0.6735\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6610 - val_loss: 0.6221 - val_accuracy: 0.6760\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6560 - val_loss: 0.6218 - val_accuracy: 0.6735\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6620 - val_loss: 0.6228 - val_accuracy: 0.6709\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6561 - val_loss: 0.6254 - val_accuracy: 0.6671\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6544 - val_loss: 0.6266 - val_accuracy: 0.6645\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6224 - accuracy: 0.6596 - val_loss: 0.6309 - val_accuracy: 0.6569\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6603 - val_loss: 0.6220 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6593 - val_loss: 0.6219 - val_accuracy: 0.6722\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6530 - val_loss: 0.6274 - val_accuracy: 0.6722\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6531 - val_loss: 0.6266 - val_accuracy: 0.6722\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6243 - accuracy: 0.6579 - val_loss: 0.6248 - val_accuracy: 0.6684\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6614 - val_loss: 0.6247 - val_accuracy: 0.6696\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6566 - val_loss: 0.6213 - val_accuracy: 0.6811\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6217 - accuracy: 0.6576 - val_loss: 0.6228 - val_accuracy: 0.6773\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6613 - val_loss: 0.6230 - val_accuracy: 0.6658\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6701 - val_loss: 0.6244 - val_accuracy: 0.6658\n",
      "Calculating for: 650 100 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_260 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8143 - accuracy: 0.5225 - val_loss: 0.6581 - val_accuracy: 0.6339\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7274 - accuracy: 0.5261 - val_loss: 0.6553 - val_accuracy: 0.6390\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7005 - accuracy: 0.5226 - val_loss: 0.6565 - val_accuracy: 0.6352\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.5369 - val_loss: 0.6601 - val_accuracy: 0.6429\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6888 - accuracy: 0.5426 - val_loss: 0.6616 - val_accuracy: 0.6467\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6851 - accuracy: 0.5482 - val_loss: 0.6574 - val_accuracy: 0.6480\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.5561 - val_loss: 0.6531 - val_accuracy: 0.6454\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.5662 - val_loss: 0.6579 - val_accuracy: 0.6543\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5688 - val_loss: 0.6504 - val_accuracy: 0.6441\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.5653 - val_loss: 0.6542 - val_accuracy: 0.6505\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6807 - accuracy: 0.5587 - val_loss: 0.6509 - val_accuracy: 0.6441\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5643 - val_loss: 0.6508 - val_accuracy: 0.6556\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6790 - accuracy: 0.5703 - val_loss: 0.6453 - val_accuracy: 0.6492\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5787 - val_loss: 0.6455 - val_accuracy: 0.6633\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5826 - val_loss: 0.6439 - val_accuracy: 0.6645\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6764 - accuracy: 0.5764 - val_loss: 0.6419 - val_accuracy: 0.6607\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6738 - accuracy: 0.5804 - val_loss: 0.6431 - val_accuracy: 0.6607\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6723 - accuracy: 0.5809 - val_loss: 0.6398 - val_accuracy: 0.6658\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5833 - val_loss: 0.6363 - val_accuracy: 0.6633\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5947 - val_loss: 0.6373 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5845 - val_loss: 0.6389 - val_accuracy: 0.6684\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5977 - val_loss: 0.6356 - val_accuracy: 0.6696\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6728 - accuracy: 0.5816 - val_loss: 0.6374 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5878 - val_loss: 0.6420 - val_accuracy: 0.6671\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6717 - accuracy: 0.5923 - val_loss: 0.6392 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6030 - val_loss: 0.6330 - val_accuracy: 0.6747\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.6016 - val_loss: 0.6347 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.5998 - val_loss: 0.6343 - val_accuracy: 0.6696\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5957 - val_loss: 0.6341 - val_accuracy: 0.6658\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6702 - accuracy: 0.5926 - val_loss: 0.6401 - val_accuracy: 0.6684\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5997 - val_loss: 0.6387 - val_accuracy: 0.6684\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6010 - val_loss: 0.6345 - val_accuracy: 0.6735\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.6001 - val_loss: 0.6368 - val_accuracy: 0.6671\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6639 - accuracy: 0.5993 - val_loss: 0.6305 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.6014 - val_loss: 0.6356 - val_accuracy: 0.6709\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.5985 - val_loss: 0.6362 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6009 - val_loss: 0.6315 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6063 - val_loss: 0.6334 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6068 - val_loss: 0.6297 - val_accuracy: 0.6735\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6030 - val_loss: 0.6306 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6085 - val_loss: 0.6274 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6064 - val_loss: 0.6258 - val_accuracy: 0.6696\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.6075 - val_loss: 0.6309 - val_accuracy: 0.6709\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6076 - val_loss: 0.6280 - val_accuracy: 0.6722\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6004 - val_loss: 0.6275 - val_accuracy: 0.6735\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6117 - val_loss: 0.6301 - val_accuracy: 0.6747\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6196 - val_loss: 0.6267 - val_accuracy: 0.6798\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6124 - val_loss: 0.6306 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6091 - val_loss: 0.6330 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6148 - val_loss: 0.6251 - val_accuracy: 0.6798\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6124 - val_loss: 0.6296 - val_accuracy: 0.6824\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6567 - accuracy: 0.6172 - val_loss: 0.6285 - val_accuracy: 0.6837\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6235 - val_loss: 0.6293 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6187 - val_loss: 0.6233 - val_accuracy: 0.6849\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6251 - val_loss: 0.6237 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6110 - val_loss: 0.6272 - val_accuracy: 0.6824\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6182 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6137 - val_loss: 0.6266 - val_accuracy: 0.6773\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6242 - val_loss: 0.6269 - val_accuracy: 0.6849\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6177 - val_loss: 0.6244 - val_accuracy: 0.6888\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6125 - val_loss: 0.6278 - val_accuracy: 0.6811\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6172 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6248 - val_loss: 0.6197 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6186 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6306 - val_loss: 0.6252 - val_accuracy: 0.6913\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6264 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6164 - val_loss: 0.6219 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6199 - val_loss: 0.6218 - val_accuracy: 0.6939\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6225 - val_accuracy: 0.6849\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6216 - val_loss: 0.6224 - val_accuracy: 0.6837\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6267 - val_loss: 0.6252 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6242 - val_loss: 0.6211 - val_accuracy: 0.6849\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6252 - val_loss: 0.6258 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6208 - val_loss: 0.6240 - val_accuracy: 0.6786\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6240 - val_loss: 0.6250 - val_accuracy: 0.6798\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6230 - val_loss: 0.6235 - val_accuracy: 0.6875\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6547 - accuracy: 0.6248 - val_loss: 0.6208 - val_accuracy: 0.6862\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6295 - val_loss: 0.6233 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6238 - val_loss: 0.6210 - val_accuracy: 0.6837\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6243 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6305 - val_loss: 0.6182 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6134 - val_loss: 0.6204 - val_accuracy: 0.6913\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6301 - val_loss: 0.6226 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6284 - val_loss: 0.6197 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6311 - val_loss: 0.6229 - val_accuracy: 0.6888\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6296 - val_loss: 0.6208 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6310 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6474 - accuracy: 0.6331 - val_loss: 0.6219 - val_accuracy: 0.6862\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6294 - val_loss: 0.6174 - val_accuracy: 0.6888\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6331 - val_loss: 0.6160 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6226 - val_loss: 0.6169 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6355 - val_loss: 0.6199 - val_accuracy: 0.6811\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6365 - val_loss: 0.6184 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6299 - val_loss: 0.6189 - val_accuracy: 0.6849\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6315 - val_loss: 0.6217 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6315 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6284 - val_loss: 0.6197 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6402 - val_loss: 0.6183 - val_accuracy: 0.6913\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6323 - val_loss: 0.6150 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6329 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6400 - val_loss: 0.6205 - val_accuracy: 0.6849\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6402 - val_loss: 0.6209 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6353 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6413 - val_loss: 0.6185 - val_accuracy: 0.6875\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6355 - val_loss: 0.6213 - val_accuracy: 0.6786\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6438 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6312 - val_loss: 0.6211 - val_accuracy: 0.6786\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6364 - val_loss: 0.6226 - val_accuracy: 0.6837\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6409 - val_loss: 0.6175 - val_accuracy: 0.6875\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6330 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6424 - val_loss: 0.6165 - val_accuracy: 0.6837\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6447 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6385 - val_loss: 0.6188 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6426 - val_loss: 0.6190 - val_accuracy: 0.6849\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6452 - val_loss: 0.6141 - val_accuracy: 0.6849\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6372 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6416 - accuracy: 0.6353 - val_loss: 0.6164 - val_accuracy: 0.6837\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6495 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6382 - val_loss: 0.6139 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6417 - val_loss: 0.6149 - val_accuracy: 0.6824\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6375 - val_loss: 0.6147 - val_accuracy: 0.6837\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6475 - val_loss: 0.6188 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6414 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6388 - accuracy: 0.6429 - val_loss: 0.6211 - val_accuracy: 0.6798\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6436 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6429 - val_loss: 0.6142 - val_accuracy: 0.6811\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6436 - val_loss: 0.6188 - val_accuracy: 0.6837\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6438 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6363 - accuracy: 0.6482 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6467 - val_loss: 0.6107 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6390 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6444 - val_loss: 0.6157 - val_accuracy: 0.6849\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6486 - val_loss: 0.6196 - val_accuracy: 0.6811\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6518 - val_loss: 0.6155 - val_accuracy: 0.6824\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6371 - accuracy: 0.6477 - val_loss: 0.6158 - val_accuracy: 0.6875\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6428 - val_loss: 0.6182 - val_accuracy: 0.6888\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6540 - val_loss: 0.6119 - val_accuracy: 0.6939\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6472 - val_loss: 0.6129 - val_accuracy: 0.6939\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6493 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6449 - val_loss: 0.6135 - val_accuracy: 0.6875\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6495 - val_loss: 0.6167 - val_accuracy: 0.6913\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6459 - val_loss: 0.6166 - val_accuracy: 0.6862\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6517 - val_loss: 0.6125 - val_accuracy: 0.6875\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6471 - val_loss: 0.6122 - val_accuracy: 0.6888\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6336 - accuracy: 0.6461 - val_loss: 0.6120 - val_accuracy: 0.6875\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6516 - val_loss: 0.6139 - val_accuracy: 0.6862\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6588 - val_loss: 0.6120 - val_accuracy: 0.6862\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6475 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6468 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6486 - val_loss: 0.6132 - val_accuracy: 0.6862\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6550 - val_loss: 0.6160 - val_accuracy: 0.6849\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6525 - val_loss: 0.6140 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6526 - val_loss: 0.6132 - val_accuracy: 0.6888\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6488 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6495 - val_loss: 0.6157 - val_accuracy: 0.6888\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6472 - val_loss: 0.6136 - val_accuracy: 0.6888\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6579 - val_loss: 0.6108 - val_accuracy: 0.6849\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6510 - val_loss: 0.6128 - val_accuracy: 0.6849\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6536 - val_loss: 0.6135 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6541 - val_loss: 0.6112 - val_accuracy: 0.6837\n",
      "Calculating for: 650 150 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_264 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8317 - accuracy: 0.5373 - val_loss: 0.6446 - val_accuracy: 0.6531\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7177 - accuracy: 0.5574 - val_loss: 0.6401 - val_accuracy: 0.6671\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6846 - accuracy: 0.5808 - val_loss: 0.6405 - val_accuracy: 0.6556\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5831 - val_loss: 0.6409 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5911 - val_loss: 0.6389 - val_accuracy: 0.6658\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5933 - val_loss: 0.6390 - val_accuracy: 0.6773\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.5970 - val_loss: 0.6360 - val_accuracy: 0.6709\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6049 - val_loss: 0.6351 - val_accuracy: 0.6773\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6606 - accuracy: 0.6074 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6148 - val_loss: 0.6310 - val_accuracy: 0.6849\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6066 - val_loss: 0.6358 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6169 - val_loss: 0.6314 - val_accuracy: 0.6798\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6147 - val_loss: 0.6295 - val_accuracy: 0.6798\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6153 - val_loss: 0.6289 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6207 - val_loss: 0.6324 - val_accuracy: 0.6684\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6218 - val_loss: 0.6310 - val_accuracy: 0.6671\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6504 - accuracy: 0.6220 - val_loss: 0.6314 - val_accuracy: 0.6722\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6208 - val_loss: 0.6289 - val_accuracy: 0.6645\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6238 - val_loss: 0.6272 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6258 - val_loss: 0.6280 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6454 - accuracy: 0.6329 - val_loss: 0.6237 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6270 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6250 - val_loss: 0.6275 - val_accuracy: 0.6875\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6260 - val_loss: 0.6301 - val_accuracy: 0.6722\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6272 - val_loss: 0.6281 - val_accuracy: 0.6709\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6373 - val_loss: 0.6284 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6372 - val_loss: 0.6253 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6356 - val_loss: 0.6216 - val_accuracy: 0.6798\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6315 - val_loss: 0.6265 - val_accuracy: 0.6684\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6331 - val_loss: 0.6237 - val_accuracy: 0.6594\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6400 - val_loss: 0.6221 - val_accuracy: 0.6696\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6355 - val_loss: 0.6259 - val_accuracy: 0.6696\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6356 - val_loss: 0.6191 - val_accuracy: 0.6735\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6374 - val_loss: 0.6291 - val_accuracy: 0.6658\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6379 - val_loss: 0.6272 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6478 - val_loss: 0.6209 - val_accuracy: 0.6684\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6390 - val_loss: 0.6244 - val_accuracy: 0.6645\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6478 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6409 - val_loss: 0.6221 - val_accuracy: 0.6671\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6493 - val_loss: 0.6201 - val_accuracy: 0.6735\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6486 - val_loss: 0.6191 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6478 - val_loss: 0.6152 - val_accuracy: 0.6811\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6443 - val_loss: 0.6179 - val_accuracy: 0.6786\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6418 - val_loss: 0.6252 - val_accuracy: 0.6722\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6426 - val_loss: 0.6198 - val_accuracy: 0.6747\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6546 - val_loss: 0.6162 - val_accuracy: 0.6888\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6478 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6591 - val_loss: 0.6182 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6532 - val_loss: 0.6193 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6539 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6614 - val_loss: 0.6186 - val_accuracy: 0.6849\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6555 - val_loss: 0.6182 - val_accuracy: 0.6862\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6556 - val_loss: 0.6208 - val_accuracy: 0.6849\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6521 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6569 - val_loss: 0.6219 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6603 - val_loss: 0.6154 - val_accuracy: 0.6952\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6653 - val_loss: 0.6151 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6606 - val_loss: 0.6173 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6561 - val_loss: 0.6160 - val_accuracy: 0.6862\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6611 - val_loss: 0.6129 - val_accuracy: 0.6990\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6572 - val_loss: 0.6187 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6616 - val_loss: 0.6158 - val_accuracy: 0.6888\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6611 - val_loss: 0.6189 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6598 - val_loss: 0.6204 - val_accuracy: 0.6773\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6166 - accuracy: 0.6648 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6652 - val_loss: 0.6222 - val_accuracy: 0.6735\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6126 - accuracy: 0.6655 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6642 - val_loss: 0.6239 - val_accuracy: 0.6709\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6166 - accuracy: 0.6640 - val_loss: 0.6212 - val_accuracy: 0.6786\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6112 - accuracy: 0.6673 - val_loss: 0.6181 - val_accuracy: 0.6786\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6642 - val_loss: 0.6227 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6649 - val_loss: 0.6204 - val_accuracy: 0.6760\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6075 - accuracy: 0.6733 - val_loss: 0.6199 - val_accuracy: 0.6773\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6724 - val_loss: 0.6223 - val_accuracy: 0.6747\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6084 - accuracy: 0.6686 - val_loss: 0.6211 - val_accuracy: 0.6760\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5995 - accuracy: 0.6834 - val_loss: 0.6227 - val_accuracy: 0.6760\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6062 - accuracy: 0.6763 - val_loss: 0.6210 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6745 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6771 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6712 - val_loss: 0.6209 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6060 - accuracy: 0.6772 - val_loss: 0.6209 - val_accuracy: 0.6709\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6000 - accuracy: 0.6811 - val_loss: 0.6230 - val_accuracy: 0.6709\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6035 - accuracy: 0.6726 - val_loss: 0.6237 - val_accuracy: 0.6696\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6811 - val_loss: 0.6205 - val_accuracy: 0.6786\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6810 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5997 - accuracy: 0.6829 - val_loss: 0.6215 - val_accuracy: 0.6786\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6049 - accuracy: 0.6760 - val_loss: 0.6235 - val_accuracy: 0.6722\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6020 - accuracy: 0.6806 - val_loss: 0.6239 - val_accuracy: 0.6747\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6001 - accuracy: 0.6767 - val_loss: 0.6230 - val_accuracy: 0.6773\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5978 - accuracy: 0.6805 - val_loss: 0.6285 - val_accuracy: 0.6709\n",
      "Calculating for: 650 150 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_268 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8212 - accuracy: 0.5259 - val_loss: 0.6860 - val_accuracy: 0.5446\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7199 - accuracy: 0.5446 - val_loss: 0.6692 - val_accuracy: 0.6148\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5540 - val_loss: 0.6551 - val_accuracy: 0.6543\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6799 - accuracy: 0.5678 - val_loss: 0.6526 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.5761 - val_loss: 0.6495 - val_accuracy: 0.6722\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5844 - val_loss: 0.6504 - val_accuracy: 0.6658\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5896 - val_loss: 0.6476 - val_accuracy: 0.6684\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5814 - val_loss: 0.6504 - val_accuracy: 0.6722\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6700 - accuracy: 0.5960 - val_loss: 0.6432 - val_accuracy: 0.6760\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5967 - val_loss: 0.6450 - val_accuracy: 0.6633\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5853 - val_loss: 0.6439 - val_accuracy: 0.6684\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5987 - val_loss: 0.6481 - val_accuracy: 0.6696\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6029 - val_loss: 0.6464 - val_accuracy: 0.6620\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.6029 - val_loss: 0.6461 - val_accuracy: 0.6633\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6017 - val_loss: 0.6387 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6103 - val_loss: 0.6387 - val_accuracy: 0.6709\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6045 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6584 - accuracy: 0.6179 - val_loss: 0.6477 - val_accuracy: 0.6454\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6133 - val_loss: 0.6398 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6108 - val_loss: 0.6443 - val_accuracy: 0.6531\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6184 - val_loss: 0.6363 - val_accuracy: 0.6709\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6085 - val_loss: 0.6360 - val_accuracy: 0.6709\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6187 - val_loss: 0.6352 - val_accuracy: 0.6645\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6216 - val_loss: 0.6368 - val_accuracy: 0.6684\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6208 - val_loss: 0.6317 - val_accuracy: 0.6709\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6133 - val_loss: 0.6422 - val_accuracy: 0.6633\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6191 - val_loss: 0.6393 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6240 - val_loss: 0.6355 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6291 - val_loss: 0.6384 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6150 - val_loss: 0.6431 - val_accuracy: 0.6454\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6265 - val_loss: 0.6308 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6189 - val_loss: 0.6357 - val_accuracy: 0.6671\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6235 - val_loss: 0.6410 - val_accuracy: 0.6531\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6237 - val_loss: 0.6340 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6248 - val_loss: 0.6321 - val_accuracy: 0.6747\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6173 - val_loss: 0.6338 - val_accuracy: 0.6722\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6307 - val_loss: 0.6378 - val_accuracy: 0.6492\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6297 - val_loss: 0.6317 - val_accuracy: 0.6722\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6251 - val_loss: 0.6335 - val_accuracy: 0.6773\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6271 - val_loss: 0.6306 - val_accuracy: 0.6709\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6305 - val_loss: 0.6310 - val_accuracy: 0.6709\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6232 - val_loss: 0.6308 - val_accuracy: 0.6773\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6439 - val_loss: 0.6266 - val_accuracy: 0.6798\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6280 - val_loss: 0.6263 - val_accuracy: 0.6760\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6294 - val_loss: 0.6299 - val_accuracy: 0.6696\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6426 - val_loss: 0.6263 - val_accuracy: 0.6747\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6384 - val_loss: 0.6307 - val_accuracy: 0.6709\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6306 - val_loss: 0.6300 - val_accuracy: 0.6633\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6359 - val_loss: 0.6282 - val_accuracy: 0.6735\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6345 - val_loss: 0.6225 - val_accuracy: 0.6747\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6366 - val_loss: 0.6269 - val_accuracy: 0.6747\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6338 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6354 - val_loss: 0.6286 - val_accuracy: 0.6620\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6302 - val_loss: 0.6254 - val_accuracy: 0.6735\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6405 - val_loss: 0.6248 - val_accuracy: 0.6696\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6454 - val_loss: 0.6274 - val_accuracy: 0.6633\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6359 - val_loss: 0.6253 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6360 - val_loss: 0.6239 - val_accuracy: 0.6760\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6394 - val_loss: 0.6231 - val_accuracy: 0.6862\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6372 - val_loss: 0.6305 - val_accuracy: 0.6645\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6400 - val_loss: 0.6262 - val_accuracy: 0.6709\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6398 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6384 - val_loss: 0.6265 - val_accuracy: 0.6722\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6473 - val_loss: 0.6287 - val_accuracy: 0.6607\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6419 - val_loss: 0.6292 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6384 - val_loss: 0.6255 - val_accuracy: 0.6798\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6350 - val_loss: 0.6317 - val_accuracy: 0.6709\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6478 - val_loss: 0.6265 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6443 - val_loss: 0.6193 - val_accuracy: 0.6811\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6415 - val_loss: 0.6235 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6487 - val_loss: 0.6189 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6486 - val_loss: 0.6190 - val_accuracy: 0.6747\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6483 - val_loss: 0.6248 - val_accuracy: 0.6773\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6547 - val_loss: 0.6221 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6446 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6547 - val_loss: 0.6164 - val_accuracy: 0.6735\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6426 - val_loss: 0.6194 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6501 - val_loss: 0.6250 - val_accuracy: 0.6824\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6518 - val_loss: 0.6224 - val_accuracy: 0.6786\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6517 - val_loss: 0.6198 - val_accuracy: 0.6773\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6549 - val_loss: 0.6263 - val_accuracy: 0.6888\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6522 - val_loss: 0.6254 - val_accuracy: 0.6888\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6574 - val_loss: 0.6227 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6512 - val_loss: 0.6272 - val_accuracy: 0.6824\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6515 - val_loss: 0.6230 - val_accuracy: 0.6837\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6575 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6537 - val_loss: 0.6200 - val_accuracy: 0.6888\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6527 - val_loss: 0.6186 - val_accuracy: 0.6862\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6510 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6541 - val_loss: 0.6231 - val_accuracy: 0.6888\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6551 - val_loss: 0.6188 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6540 - val_loss: 0.6196 - val_accuracy: 0.6952\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6596 - val_loss: 0.6204 - val_accuracy: 0.6862\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6570 - val_loss: 0.6202 - val_accuracy: 0.6862\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6570 - val_loss: 0.6224 - val_accuracy: 0.6862\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6616 - val_loss: 0.6201 - val_accuracy: 0.6862\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6660 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6680 - val_loss: 0.6212 - val_accuracy: 0.6901\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6620 - val_loss: 0.6213 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6658 - val_loss: 0.6214 - val_accuracy: 0.6875\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6598 - val_loss: 0.6215 - val_accuracy: 0.6901\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6556 - val_loss: 0.6185 - val_accuracy: 0.6862\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6593 - val_loss: 0.6145 - val_accuracy: 0.6939\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6560 - val_loss: 0.6174 - val_accuracy: 0.6875\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6613 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6645 - val_loss: 0.6231 - val_accuracy: 0.6798\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6624 - val_loss: 0.6186 - val_accuracy: 0.6888\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6588 - val_loss: 0.6231 - val_accuracy: 0.6709\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6689 - val_loss: 0.6209 - val_accuracy: 0.6862\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6615 - val_loss: 0.6225 - val_accuracy: 0.6786\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6604 - val_loss: 0.6212 - val_accuracy: 0.6837\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6583 - val_loss: 0.6235 - val_accuracy: 0.6722\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6694 - val_loss: 0.6202 - val_accuracy: 0.6837\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6113 - accuracy: 0.6702 - val_loss: 0.6257 - val_accuracy: 0.6709\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6140 - accuracy: 0.6714 - val_loss: 0.6230 - val_accuracy: 0.6747\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6182 - accuracy: 0.6561 - val_loss: 0.6184 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6655 - val_loss: 0.6183 - val_accuracy: 0.6888\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6645 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6114 - accuracy: 0.6653 - val_loss: 0.6212 - val_accuracy: 0.6824\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6109 - accuracy: 0.6682 - val_loss: 0.6290 - val_accuracy: 0.6671\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6101 - accuracy: 0.6752 - val_loss: 0.6248 - val_accuracy: 0.6722\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6674 - val_loss: 0.6198 - val_accuracy: 0.6862\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6729 - val_loss: 0.6237 - val_accuracy: 0.6773\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6091 - accuracy: 0.6699 - val_loss: 0.6243 - val_accuracy: 0.6735\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6089 - accuracy: 0.6726 - val_loss: 0.6224 - val_accuracy: 0.6760\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6112 - accuracy: 0.6677 - val_loss: 0.6233 - val_accuracy: 0.6747\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6733 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6091 - accuracy: 0.6734 - val_loss: 0.6268 - val_accuracy: 0.6645\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6708 - val_loss: 0.6233 - val_accuracy: 0.6671\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6680 - val_loss: 0.6228 - val_accuracy: 0.6684\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6075 - accuracy: 0.6718 - val_loss: 0.6226 - val_accuracy: 0.6760\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6092 - accuracy: 0.6763 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6734 - val_loss: 0.6261 - val_accuracy: 0.6620\n",
      "Calculating for: 650 150 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_272 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8116 - accuracy: 0.5207 - val_loss: 0.6662 - val_accuracy: 0.6505\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7272 - accuracy: 0.5261 - val_loss: 0.6593 - val_accuracy: 0.6492\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5466 - val_loss: 0.6674 - val_accuracy: 0.6556\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5453 - val_loss: 0.6637 - val_accuracy: 0.6492\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5540 - val_loss: 0.6638 - val_accuracy: 0.6454\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6823 - accuracy: 0.5566 - val_loss: 0.6614 - val_accuracy: 0.6480\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.5544 - val_loss: 0.6611 - val_accuracy: 0.6582\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6802 - accuracy: 0.5668 - val_loss: 0.6624 - val_accuracy: 0.6658\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6776 - accuracy: 0.5713 - val_loss: 0.6503 - val_accuracy: 0.6454\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5692 - val_loss: 0.6530 - val_accuracy: 0.6531\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5757 - val_loss: 0.6532 - val_accuracy: 0.6684\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6797 - accuracy: 0.5686 - val_loss: 0.6583 - val_accuracy: 0.6556\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5808 - val_loss: 0.6532 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5799 - val_loss: 0.6557 - val_accuracy: 0.6441\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6719 - accuracy: 0.5872 - val_loss: 0.6446 - val_accuracy: 0.6696\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5759 - val_loss: 0.6423 - val_accuracy: 0.6658\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5850 - val_loss: 0.6506 - val_accuracy: 0.6531\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6728 - accuracy: 0.5865 - val_loss: 0.6498 - val_accuracy: 0.6543\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5978 - val_loss: 0.6419 - val_accuracy: 0.6709\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5841 - val_loss: 0.6475 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5907 - val_loss: 0.6471 - val_accuracy: 0.6607\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5923 - val_loss: 0.6473 - val_accuracy: 0.6454\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5850 - val_loss: 0.6434 - val_accuracy: 0.6633\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6019 - val_loss: 0.6409 - val_accuracy: 0.6658\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5922 - val_loss: 0.6422 - val_accuracy: 0.6671\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.6009 - val_loss: 0.6398 - val_accuracy: 0.6645\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5952 - val_loss: 0.6399 - val_accuracy: 0.6671\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5960 - val_loss: 0.6432 - val_accuracy: 0.6658\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5981 - val_loss: 0.6438 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6010 - val_loss: 0.6423 - val_accuracy: 0.6620\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6022 - val_loss: 0.6395 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6026 - val_loss: 0.6395 - val_accuracy: 0.6645\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6104 - val_loss: 0.6403 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6000 - val_loss: 0.6348 - val_accuracy: 0.6696\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6035 - val_loss: 0.6431 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6089 - val_loss: 0.6365 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.5996 - val_loss: 0.6359 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6118 - val_loss: 0.6482 - val_accuracy: 0.6339\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6140 - val_loss: 0.6365 - val_accuracy: 0.6645\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6108 - val_loss: 0.6400 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6074 - val_loss: 0.6338 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6100 - val_loss: 0.6392 - val_accuracy: 0.6607\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6066 - val_loss: 0.6397 - val_accuracy: 0.6594\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6172 - val_loss: 0.6382 - val_accuracy: 0.6633\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6060 - val_loss: 0.6346 - val_accuracy: 0.6696\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6153 - val_loss: 0.6361 - val_accuracy: 0.6658\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6182 - val_loss: 0.6322 - val_accuracy: 0.6645\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6140 - val_loss: 0.6341 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6150 - val_loss: 0.6353 - val_accuracy: 0.6658\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6173 - val_loss: 0.6369 - val_accuracy: 0.6709\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6138 - val_loss: 0.6344 - val_accuracy: 0.6684\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6128 - val_loss: 0.6366 - val_accuracy: 0.6696\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6216 - val_loss: 0.6352 - val_accuracy: 0.6709\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6289 - val_loss: 0.6305 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6159 - val_loss: 0.6347 - val_accuracy: 0.6696\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6230 - val_loss: 0.6360 - val_accuracy: 0.6620\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6184 - val_loss: 0.6386 - val_accuracy: 0.6582\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6237 - val_loss: 0.6390 - val_accuracy: 0.6429\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6282 - val_loss: 0.6320 - val_accuracy: 0.6709\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6220 - val_loss: 0.6255 - val_accuracy: 0.6824\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6194 - val_loss: 0.6318 - val_accuracy: 0.6747\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6228 - val_loss: 0.6281 - val_accuracy: 0.6671\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6221 - val_loss: 0.6270 - val_accuracy: 0.6811\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6211 - val_loss: 0.6287 - val_accuracy: 0.6798\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6218 - val_loss: 0.6306 - val_accuracy: 0.6709\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6271 - val_loss: 0.6352 - val_accuracy: 0.6543\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6235 - val_loss: 0.6330 - val_accuracy: 0.6671\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6339 - val_loss: 0.6275 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6218 - val_loss: 0.6328 - val_accuracy: 0.6671\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6276 - val_loss: 0.6315 - val_accuracy: 0.6569\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6324 - val_loss: 0.6288 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6296 - val_loss: 0.6288 - val_accuracy: 0.6696\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6202 - val_loss: 0.6367 - val_accuracy: 0.6518\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6256 - val_loss: 0.6309 - val_accuracy: 0.6696\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6304 - val_loss: 0.6394 - val_accuracy: 0.6390\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6218 - val_loss: 0.6329 - val_accuracy: 0.6607\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6349 - val_loss: 0.6314 - val_accuracy: 0.6671\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6280 - val_loss: 0.6252 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6331 - val_loss: 0.6282 - val_accuracy: 0.6709\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6241 - val_loss: 0.6264 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6344 - val_loss: 0.6271 - val_accuracy: 0.6824\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6297 - val_loss: 0.6299 - val_accuracy: 0.6696\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6272 - val_loss: 0.6317 - val_accuracy: 0.6594\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6328 - val_loss: 0.6264 - val_accuracy: 0.6786\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6316 - val_loss: 0.6288 - val_accuracy: 0.6773\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6306 - val_loss: 0.6243 - val_accuracy: 0.6849\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6279 - val_loss: 0.6287 - val_accuracy: 0.6786\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6292 - val_loss: 0.6306 - val_accuracy: 0.6696\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6324 - val_loss: 0.6323 - val_accuracy: 0.6684\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6351 - val_loss: 0.6269 - val_accuracy: 0.6773\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6418 - val_loss: 0.6337 - val_accuracy: 0.6429\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6335 - val_loss: 0.6320 - val_accuracy: 0.6645\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6354 - val_loss: 0.6269 - val_accuracy: 0.6735\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6330 - val_loss: 0.6298 - val_accuracy: 0.6735\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6311 - val_loss: 0.6262 - val_accuracy: 0.6824\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6315 - val_loss: 0.6201 - val_accuracy: 0.6913\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6360 - val_loss: 0.6227 - val_accuracy: 0.6875\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6297 - val_loss: 0.6269 - val_accuracy: 0.6620\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6350 - val_loss: 0.6267 - val_accuracy: 0.6747\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6350 - val_loss: 0.6281 - val_accuracy: 0.6684\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6429 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6452 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6380 - val_loss: 0.6214 - val_accuracy: 0.6862\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6379 - val_loss: 0.6235 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6383 - val_loss: 0.6254 - val_accuracy: 0.6824\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6377 - val_loss: 0.6199 - val_accuracy: 0.6926\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6439 - val_loss: 0.6298 - val_accuracy: 0.6518\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6453 - val_loss: 0.6275 - val_accuracy: 0.6684\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6404 - val_loss: 0.6327 - val_accuracy: 0.6403\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6492 - val_loss: 0.6262 - val_accuracy: 0.6633\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6394 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6412 - val_loss: 0.6231 - val_accuracy: 0.6786\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6431 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6426 - val_loss: 0.6238 - val_accuracy: 0.6760\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6400 - val_loss: 0.6276 - val_accuracy: 0.6760\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6472 - val_loss: 0.6265 - val_accuracy: 0.6696\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6404 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6369 - val_loss: 0.6253 - val_accuracy: 0.6837\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6456 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6480 - val_loss: 0.6246 - val_accuracy: 0.6684\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6437 - val_loss: 0.6258 - val_accuracy: 0.6696\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6438 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6420 - accuracy: 0.6392 - val_loss: 0.6262 - val_accuracy: 0.6747\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6409 - val_loss: 0.6229 - val_accuracy: 0.6798\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6459 - val_loss: 0.6265 - val_accuracy: 0.6722\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6545 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6454 - val_loss: 0.6194 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6449 - val_loss: 0.6216 - val_accuracy: 0.6773\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6505 - val_loss: 0.6219 - val_accuracy: 0.6747\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6537 - val_loss: 0.6217 - val_accuracy: 0.6773\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6443 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6555 - val_loss: 0.6237 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6485 - val_loss: 0.6225 - val_accuracy: 0.6849\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6446 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6472 - val_loss: 0.6190 - val_accuracy: 0.6888\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6505 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6507 - val_loss: 0.6157 - val_accuracy: 0.6939\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6503 - val_loss: 0.6190 - val_accuracy: 0.6837\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6518 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6532 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6464 - val_loss: 0.6201 - val_accuracy: 0.6747\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6456 - val_loss: 0.6245 - val_accuracy: 0.6709\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6444 - val_loss: 0.6260 - val_accuracy: 0.6671\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6491 - val_loss: 0.6189 - val_accuracy: 0.6824\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6555 - val_loss: 0.6185 - val_accuracy: 0.6862\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6513 - val_loss: 0.6248 - val_accuracy: 0.6760\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6497 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6461 - val_loss: 0.6284 - val_accuracy: 0.6671\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6472 - val_loss: 0.6222 - val_accuracy: 0.6773\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6555 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6534 - val_loss: 0.6243 - val_accuracy: 0.6722\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6463 - val_loss: 0.6205 - val_accuracy: 0.6875\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6525 - val_loss: 0.6174 - val_accuracy: 0.6952\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6542 - val_loss: 0.6202 - val_accuracy: 0.6862\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6576 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6575 - val_loss: 0.6211 - val_accuracy: 0.6824\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6594 - val_loss: 0.6192 - val_accuracy: 0.6837\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6510 - val_loss: 0.6205 - val_accuracy: 0.6875\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6570 - val_loss: 0.6185 - val_accuracy: 0.6862\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6554 - val_loss: 0.6207 - val_accuracy: 0.6849\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6551 - val_loss: 0.6229 - val_accuracy: 0.6786\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6551 - val_loss: 0.6237 - val_accuracy: 0.6798\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6571 - val_loss: 0.6180 - val_accuracy: 0.6837\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6571 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6619 - val_loss: 0.6224 - val_accuracy: 0.6747\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6576 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6596 - val_loss: 0.6239 - val_accuracy: 0.6696\n",
      "Calculating for: 700 50 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_276 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8237 - accuracy: 0.5201 - val_loss: 0.6837 - val_accuracy: 0.5663\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7357 - accuracy: 0.5324 - val_loss: 0.6550 - val_accuracy: 0.6339\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7012 - accuracy: 0.5588 - val_loss: 0.6547 - val_accuracy: 0.6288\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6807 - accuracy: 0.5809 - val_loss: 0.6514 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5806 - val_loss: 0.6553 - val_accuracy: 0.6327\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5801 - val_loss: 0.6486 - val_accuracy: 0.6531\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5868 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6652 - accuracy: 0.5944 - val_loss: 0.6417 - val_accuracy: 0.6569\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6060 - val_loss: 0.6474 - val_accuracy: 0.6429\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6032 - val_loss: 0.6363 - val_accuracy: 0.6671\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6046 - val_loss: 0.6362 - val_accuracy: 0.6645\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6606 - accuracy: 0.6104 - val_loss: 0.6380 - val_accuracy: 0.6582\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6095 - val_loss: 0.6350 - val_accuracy: 0.6773\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6168 - val_loss: 0.6326 - val_accuracy: 0.6645\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6602 - accuracy: 0.6128 - val_loss: 0.6357 - val_accuracy: 0.6671\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6105 - val_loss: 0.6323 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6155 - val_loss: 0.6320 - val_accuracy: 0.6671\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6186 - val_loss: 0.6327 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6560 - accuracy: 0.6153 - val_loss: 0.6330 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6191 - val_loss: 0.6268 - val_accuracy: 0.6786\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6197 - val_loss: 0.6344 - val_accuracy: 0.6594\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6515 - accuracy: 0.6217 - val_loss: 0.6304 - val_accuracy: 0.6811\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6505 - accuracy: 0.6237 - val_loss: 0.6294 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6285 - val_loss: 0.6322 - val_accuracy: 0.6556\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6449 - accuracy: 0.6255 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6281 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6480 - accuracy: 0.6294 - val_loss: 0.6304 - val_accuracy: 0.6773\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6256 - val_loss: 0.6301 - val_accuracy: 0.6760\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6453 - accuracy: 0.6321 - val_loss: 0.6283 - val_accuracy: 0.6773\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6436 - accuracy: 0.6316 - val_loss: 0.6239 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6445 - accuracy: 0.6335 - val_loss: 0.6237 - val_accuracy: 0.6811\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6410 - val_loss: 0.6244 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6385 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6408 - accuracy: 0.6373 - val_loss: 0.6223 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6368 - val_loss: 0.6234 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6370 - accuracy: 0.6389 - val_loss: 0.6173 - val_accuracy: 0.6798\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6431 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6420 - accuracy: 0.6358 - val_loss: 0.6213 - val_accuracy: 0.6786\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6356 - val_loss: 0.6201 - val_accuracy: 0.6798\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6434 - val_loss: 0.6164 - val_accuracy: 0.6824\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6367 - accuracy: 0.6395 - val_loss: 0.6154 - val_accuracy: 0.6849\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6426 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6437 - val_loss: 0.6152 - val_accuracy: 0.6952\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6433 - val_loss: 0.6175 - val_accuracy: 0.6849\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6478 - val_loss: 0.6164 - val_accuracy: 0.6913\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6434 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6500 - val_loss: 0.6159 - val_accuracy: 0.6888\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6490 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6518 - val_loss: 0.6143 - val_accuracy: 0.6901\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6261 - accuracy: 0.6556 - val_loss: 0.6188 - val_accuracy: 0.6773\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6464 - val_loss: 0.6158 - val_accuracy: 0.6939\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6261 - accuracy: 0.6581 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6529 - val_loss: 0.6108 - val_accuracy: 0.6990\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6517 - val_loss: 0.6151 - val_accuracy: 0.6901\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6539 - val_loss: 0.6118 - val_accuracy: 0.6964\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6277 - accuracy: 0.6488 - val_loss: 0.6086 - val_accuracy: 0.6964\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6562 - val_loss: 0.6110 - val_accuracy: 0.6926\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6277 - accuracy: 0.6526 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6583 - val_loss: 0.6120 - val_accuracy: 0.6952\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6222 - accuracy: 0.6534 - val_loss: 0.6090 - val_accuracy: 0.6939\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6577 - val_loss: 0.6131 - val_accuracy: 0.6926\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6536 - val_loss: 0.6129 - val_accuracy: 0.6939\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6249 - accuracy: 0.6614 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6626 - val_loss: 0.6127 - val_accuracy: 0.6913\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6195 - accuracy: 0.6596 - val_loss: 0.6093 - val_accuracy: 0.6990\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6189 - accuracy: 0.6650 - val_loss: 0.6111 - val_accuracy: 0.6952\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6585 - val_loss: 0.6103 - val_accuracy: 0.6939\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6589 - val_loss: 0.6100 - val_accuracy: 0.6977\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6616 - val_loss: 0.6119 - val_accuracy: 0.6913\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6139 - accuracy: 0.6693 - val_loss: 0.6098 - val_accuracy: 0.6952\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6643 - val_loss: 0.6107 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6173 - accuracy: 0.6678 - val_loss: 0.6127 - val_accuracy: 0.6888\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6650 - val_loss: 0.6083 - val_accuracy: 0.6990\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6125 - accuracy: 0.6714 - val_loss: 0.6122 - val_accuracy: 0.6875\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6167 - accuracy: 0.6655 - val_loss: 0.6116 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6082 - accuracy: 0.6755 - val_loss: 0.6120 - val_accuracy: 0.6862\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6756 - val_loss: 0.6106 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6116 - accuracy: 0.6686 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6722 - val_loss: 0.6150 - val_accuracy: 0.6811\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6109 - accuracy: 0.6743 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6083 - accuracy: 0.6718 - val_loss: 0.6141 - val_accuracy: 0.6849\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6126 - accuracy: 0.6663 - val_loss: 0.6112 - val_accuracy: 0.6849\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6084 - accuracy: 0.6753 - val_loss: 0.6113 - val_accuracy: 0.6849\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6047 - accuracy: 0.6755 - val_loss: 0.6131 - val_accuracy: 0.6811\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6046 - accuracy: 0.6837 - val_loss: 0.6155 - val_accuracy: 0.6760\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6751 - val_loss: 0.6134 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6047 - accuracy: 0.6723 - val_loss: 0.6168 - val_accuracy: 0.6773\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6016 - accuracy: 0.6795 - val_loss: 0.6166 - val_accuracy: 0.6798\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6078 - accuracy: 0.6733 - val_loss: 0.6129 - val_accuracy: 0.6862\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6026 - accuracy: 0.6781 - val_loss: 0.6157 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6025 - accuracy: 0.6816 - val_loss: 0.6125 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6071 - accuracy: 0.6743 - val_loss: 0.6118 - val_accuracy: 0.6811\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6073 - accuracy: 0.6745 - val_loss: 0.6141 - val_accuracy: 0.6811\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6044 - accuracy: 0.6794 - val_loss: 0.6153 - val_accuracy: 0.6798\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6001 - accuracy: 0.6859 - val_loss: 0.6160 - val_accuracy: 0.6747\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6015 - accuracy: 0.6799 - val_loss: 0.6199 - val_accuracy: 0.6760\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6008 - accuracy: 0.6794 - val_loss: 0.6156 - val_accuracy: 0.6811\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6015 - accuracy: 0.6815 - val_loss: 0.6180 - val_accuracy: 0.6824\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6020 - accuracy: 0.6778 - val_loss: 0.6147 - val_accuracy: 0.6837\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.6850 - val_loss: 0.6172 - val_accuracy: 0.6824\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6002 - accuracy: 0.6819 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5945 - accuracy: 0.6874 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5943 - accuracy: 0.6855 - val_loss: 0.6177 - val_accuracy: 0.6786\n",
      "Calculating for: 700 50 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_280 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7986 - accuracy: 0.5325 - val_loss: 0.6468 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7241 - accuracy: 0.5383 - val_loss: 0.6496 - val_accuracy: 0.6467\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6976 - accuracy: 0.5534 - val_loss: 0.6521 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5510 - val_loss: 0.6510 - val_accuracy: 0.6594\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6801 - accuracy: 0.5745 - val_loss: 0.6499 - val_accuracy: 0.6620\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.5736 - val_loss: 0.6466 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5833 - val_loss: 0.6511 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5872 - val_loss: 0.6426 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6727 - accuracy: 0.5872 - val_loss: 0.6421 - val_accuracy: 0.6633\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5854 - val_loss: 0.6432 - val_accuracy: 0.6633\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5824 - val_loss: 0.6442 - val_accuracy: 0.6722\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5973 - val_loss: 0.6410 - val_accuracy: 0.6633\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6667 - accuracy: 0.5958 - val_loss: 0.6423 - val_accuracy: 0.6722\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6060 - val_loss: 0.6405 - val_accuracy: 0.6658\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6039 - val_loss: 0.6432 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6080 - val_loss: 0.6359 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6046 - val_loss: 0.6389 - val_accuracy: 0.6709\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6031 - val_loss: 0.6396 - val_accuracy: 0.6747\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6071 - val_loss: 0.6346 - val_accuracy: 0.6735\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6049 - val_loss: 0.6345 - val_accuracy: 0.6722\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6098 - val_loss: 0.6328 - val_accuracy: 0.6709\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6035 - val_loss: 0.6399 - val_accuracy: 0.6786\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6138 - val_loss: 0.6343 - val_accuracy: 0.6798\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6105 - val_loss: 0.6351 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6130 - val_loss: 0.6346 - val_accuracy: 0.6837\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6187 - val_loss: 0.6303 - val_accuracy: 0.6811\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6103 - val_loss: 0.6309 - val_accuracy: 0.6862\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6154 - val_loss: 0.6346 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6218 - val_loss: 0.6284 - val_accuracy: 0.6849\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6203 - val_loss: 0.6288 - val_accuracy: 0.6837\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6119 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6217 - val_loss: 0.6326 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6266 - val_loss: 0.6332 - val_accuracy: 0.6671\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6172 - val_loss: 0.6313 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6238 - val_loss: 0.6320 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6280 - val_loss: 0.6289 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6265 - val_loss: 0.6290 - val_accuracy: 0.6901\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6266 - val_loss: 0.6311 - val_accuracy: 0.6875\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6212 - val_loss: 0.6282 - val_accuracy: 0.6862\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6216 - val_loss: 0.6238 - val_accuracy: 0.6875\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6265 - val_loss: 0.6317 - val_accuracy: 0.6696\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6274 - val_loss: 0.6266 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6326 - val_loss: 0.6273 - val_accuracy: 0.6824\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6300 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6361 - val_loss: 0.6196 - val_accuracy: 0.6888\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6374 - val_loss: 0.6237 - val_accuracy: 0.6862\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6361 - val_loss: 0.6195 - val_accuracy: 0.6862\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6296 - val_loss: 0.6300 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6343 - val_loss: 0.6247 - val_accuracy: 0.6837\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6368 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6302 - val_loss: 0.6237 - val_accuracy: 0.6837\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6436 - val_loss: 0.6291 - val_accuracy: 0.6760\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6427 - val_loss: 0.6218 - val_accuracy: 0.6849\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6369 - val_loss: 0.6198 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6409 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6353 - val_loss: 0.6197 - val_accuracy: 0.6888\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6350 - val_loss: 0.6241 - val_accuracy: 0.6786\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6364 - val_loss: 0.6209 - val_accuracy: 0.6875\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6441 - val_loss: 0.6191 - val_accuracy: 0.6888\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6410 - val_loss: 0.6223 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6384 - accuracy: 0.6441 - val_loss: 0.6179 - val_accuracy: 0.6862\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6365 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6372 - val_loss: 0.6159 - val_accuracy: 0.6875\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6443 - val_loss: 0.6185 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6456 - val_loss: 0.6228 - val_accuracy: 0.6837\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6420 - val_loss: 0.6209 - val_accuracy: 0.6760\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6458 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6434 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6373 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6413 - val_loss: 0.6181 - val_accuracy: 0.6875\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6477 - val_loss: 0.6188 - val_accuracy: 0.6901\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6457 - val_loss: 0.6207 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6415 - val_loss: 0.6201 - val_accuracy: 0.6875\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6466 - val_loss: 0.6224 - val_accuracy: 0.6862\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6527 - val_loss: 0.6194 - val_accuracy: 0.6901\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6522 - val_loss: 0.6180 - val_accuracy: 0.6913\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6451 - val_loss: 0.6205 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6480 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6546 - val_loss: 0.6207 - val_accuracy: 0.6926\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6511 - val_loss: 0.6201 - val_accuracy: 0.6939\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6476 - val_loss: 0.6199 - val_accuracy: 0.6964\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6477 - val_loss: 0.6209 - val_accuracy: 0.6939\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6521 - val_loss: 0.6195 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6599 - val_loss: 0.6215 - val_accuracy: 0.6875\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6532 - val_loss: 0.6215 - val_accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6554 - val_loss: 0.6158 - val_accuracy: 0.6901\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6520 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6536 - val_loss: 0.6168 - val_accuracy: 0.6952\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6497 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6603 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6600 - val_loss: 0.6195 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6581 - val_loss: 0.6138 - val_accuracy: 0.6888\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6559 - val_loss: 0.6166 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6606 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6588 - val_loss: 0.6163 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6541 - val_loss: 0.6193 - val_accuracy: 0.6939\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6629 - val_loss: 0.6147 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6598 - val_loss: 0.6175 - val_accuracy: 0.6837\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6580 - val_loss: 0.6147 - val_accuracy: 0.6913\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6557 - val_loss: 0.6138 - val_accuracy: 0.6913\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6628 - val_loss: 0.6181 - val_accuracy: 0.6888\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6626 - val_loss: 0.6150 - val_accuracy: 0.6888\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6616 - val_loss: 0.6175 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6576 - val_loss: 0.6199 - val_accuracy: 0.6888\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6590 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6623 - val_loss: 0.6197 - val_accuracy: 0.6824\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6583 - val_loss: 0.6155 - val_accuracy: 0.6901\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6146 - accuracy: 0.6683 - val_loss: 0.6197 - val_accuracy: 0.6888\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6571 - val_loss: 0.6152 - val_accuracy: 0.6913\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6731 - val_loss: 0.6132 - val_accuracy: 0.6888\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6650 - val_loss: 0.6133 - val_accuracy: 0.6901\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6635 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6623 - val_loss: 0.6160 - val_accuracy: 0.6888\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6717 - val_loss: 0.6186 - val_accuracy: 0.6888\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6669 - val_loss: 0.6129 - val_accuracy: 0.6901\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6136 - accuracy: 0.6664 - val_loss: 0.6130 - val_accuracy: 0.6888\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6128 - accuracy: 0.6729 - val_loss: 0.6155 - val_accuracy: 0.6901\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6603 - val_loss: 0.6175 - val_accuracy: 0.6926\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6703 - val_loss: 0.6190 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6648 - val_loss: 0.6201 - val_accuracy: 0.6862\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6117 - accuracy: 0.6697 - val_loss: 0.6176 - val_accuracy: 0.6926\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.6703 - val_loss: 0.6190 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6630 - val_loss: 0.6212 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6683 - val_loss: 0.6214 - val_accuracy: 0.6862\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6631 - val_loss: 0.6151 - val_accuracy: 0.6862\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6689 - val_loss: 0.6160 - val_accuracy: 0.6901\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6721 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6699 - val_loss: 0.6205 - val_accuracy: 0.6901\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6650 - val_loss: 0.6166 - val_accuracy: 0.6888\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6122 - accuracy: 0.6706 - val_loss: 0.6185 - val_accuracy: 0.6798\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6131 - accuracy: 0.6723 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6139 - accuracy: 0.6644 - val_loss: 0.6195 - val_accuracy: 0.6837\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6768 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6100 - accuracy: 0.6756 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6063 - accuracy: 0.6727 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6755 - val_loss: 0.6170 - val_accuracy: 0.6849\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6737 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6047 - accuracy: 0.6763 - val_loss: 0.6169 - val_accuracy: 0.6888\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6751 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6736 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6822 - val_loss: 0.6192 - val_accuracy: 0.6862\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6087 - accuracy: 0.6708 - val_loss: 0.6202 - val_accuracy: 0.6837\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6083 - accuracy: 0.6791 - val_loss: 0.6160 - val_accuracy: 0.6913\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6804 - val_loss: 0.6225 - val_accuracy: 0.6849\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6048 - accuracy: 0.6791 - val_loss: 0.6212 - val_accuracy: 0.6875\n",
      "Calculating for: 700 50 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_284 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8244 - accuracy: 0.5211 - val_loss: 0.6552 - val_accuracy: 0.6250\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7430 - accuracy: 0.5225 - val_loss: 0.6606 - val_accuracy: 0.6390\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7099 - accuracy: 0.5252 - val_loss: 0.6643 - val_accuracy: 0.6339\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6995 - accuracy: 0.5275 - val_loss: 0.6620 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5315 - val_loss: 0.6621 - val_accuracy: 0.6403\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5453 - val_loss: 0.6576 - val_accuracy: 0.6429\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6841 - accuracy: 0.5472 - val_loss: 0.6606 - val_accuracy: 0.6467\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5549 - val_loss: 0.6545 - val_accuracy: 0.6378\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5649 - val_loss: 0.6592 - val_accuracy: 0.6454\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6797 - accuracy: 0.5697 - val_loss: 0.6569 - val_accuracy: 0.6531\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5710 - val_loss: 0.6582 - val_accuracy: 0.6518\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6797 - accuracy: 0.5752 - val_loss: 0.6540 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6803 - accuracy: 0.5722 - val_loss: 0.6545 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6798 - accuracy: 0.5739 - val_loss: 0.6569 - val_accuracy: 0.6556\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5764 - val_loss: 0.6484 - val_accuracy: 0.6505\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5849 - val_loss: 0.6521 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.5819 - val_loss: 0.6496 - val_accuracy: 0.6543\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.5804 - val_loss: 0.6483 - val_accuracy: 0.6531\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6757 - accuracy: 0.5784 - val_loss: 0.6475 - val_accuracy: 0.6543\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6769 - accuracy: 0.5715 - val_loss: 0.6553 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5844 - val_loss: 0.6474 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5850 - val_loss: 0.6513 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5854 - val_loss: 0.6431 - val_accuracy: 0.6684\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5919 - val_loss: 0.6444 - val_accuracy: 0.6709\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6691 - accuracy: 0.5956 - val_loss: 0.6454 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5867 - val_loss: 0.6423 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6684 - accuracy: 0.5943 - val_loss: 0.6429 - val_accuracy: 0.6709\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5913 - val_loss: 0.6436 - val_accuracy: 0.6671\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5941 - val_loss: 0.6517 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.6034 - val_loss: 0.6393 - val_accuracy: 0.6633\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6021 - val_loss: 0.6391 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5936 - val_loss: 0.6428 - val_accuracy: 0.6684\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5996 - val_loss: 0.6434 - val_accuracy: 0.6684\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5977 - val_loss: 0.6410 - val_accuracy: 0.6747\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6001 - val_loss: 0.6364 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.5976 - val_loss: 0.6384 - val_accuracy: 0.6696\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6006 - val_loss: 0.6373 - val_accuracy: 0.6773\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6024 - val_loss: 0.6342 - val_accuracy: 0.6658\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.5992 - val_loss: 0.6400 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.5996 - val_loss: 0.6407 - val_accuracy: 0.6735\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6039 - val_loss: 0.6383 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6036 - val_loss: 0.6402 - val_accuracy: 0.6735\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6056 - val_loss: 0.6370 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6049 - val_loss: 0.6362 - val_accuracy: 0.6722\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6045 - val_loss: 0.6411 - val_accuracy: 0.6709\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6010 - val_loss: 0.6385 - val_accuracy: 0.6709\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6107 - val_loss: 0.6365 - val_accuracy: 0.6747\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6099 - val_loss: 0.6343 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6039 - val_loss: 0.6395 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6118 - val_loss: 0.6333 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6012 - val_loss: 0.6372 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6133 - val_loss: 0.6344 - val_accuracy: 0.6773\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6113 - val_loss: 0.6354 - val_accuracy: 0.6747\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6134 - val_loss: 0.6404 - val_accuracy: 0.6645\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6138 - val_loss: 0.6361 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6149 - val_loss: 0.6303 - val_accuracy: 0.6786\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6113 - val_loss: 0.6305 - val_accuracy: 0.6798\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6230 - val_loss: 0.6280 - val_accuracy: 0.6786\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6271 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6099 - val_loss: 0.6302 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6150 - val_loss: 0.6335 - val_accuracy: 0.6735\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6144 - val_loss: 0.6296 - val_accuracy: 0.6824\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6236 - val_loss: 0.6295 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6114 - val_loss: 0.6340 - val_accuracy: 0.6837\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6203 - val_loss: 0.6301 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6218 - val_loss: 0.6314 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6222 - val_loss: 0.6264 - val_accuracy: 0.6824\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6245 - val_loss: 0.6281 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6184 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6183 - val_loss: 0.6305 - val_accuracy: 0.6786\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6242 - val_loss: 0.6293 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6245 - val_loss: 0.6284 - val_accuracy: 0.6862\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6174 - val_loss: 0.6260 - val_accuracy: 0.6811\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6222 - val_loss: 0.6301 - val_accuracy: 0.6798\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6245 - val_loss: 0.6282 - val_accuracy: 0.6837\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6162 - val_loss: 0.6278 - val_accuracy: 0.6849\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6193 - val_loss: 0.6280 - val_accuracy: 0.6875\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6261 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6261 - val_loss: 0.6356 - val_accuracy: 0.6760\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6251 - val_loss: 0.6269 - val_accuracy: 0.6862\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6211 - val_loss: 0.6290 - val_accuracy: 0.6888\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6250 - val_loss: 0.6267 - val_accuracy: 0.6901\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6227 - val_loss: 0.6247 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6228 - val_loss: 0.6259 - val_accuracy: 0.6888\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6481 - accuracy: 0.6302 - val_loss: 0.6256 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6275 - val_loss: 0.6269 - val_accuracy: 0.6862\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6301 - val_loss: 0.6236 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6277 - val_loss: 0.6244 - val_accuracy: 0.6913\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6266 - val_loss: 0.6278 - val_accuracy: 0.6824\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6260 - val_loss: 0.6313 - val_accuracy: 0.6901\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6297 - val_loss: 0.6238 - val_accuracy: 0.6862\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6272 - val_loss: 0.6237 - val_accuracy: 0.6926\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6294 - val_loss: 0.6226 - val_accuracy: 0.6875\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6258 - val_loss: 0.6190 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6355 - val_loss: 0.6233 - val_accuracy: 0.6901\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6504 - accuracy: 0.6319 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6415 - val_loss: 0.6179 - val_accuracy: 0.6952\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6321 - val_loss: 0.6226 - val_accuracy: 0.6837\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6280 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6380 - val_loss: 0.6174 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6296 - val_loss: 0.6215 - val_accuracy: 0.6824\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6353 - val_loss: 0.6220 - val_accuracy: 0.6862\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6378 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6272 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6377 - val_loss: 0.6214 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6319 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.6284 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6349 - val_loss: 0.6195 - val_accuracy: 0.6888\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6422 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6310 - val_loss: 0.6268 - val_accuracy: 0.6837\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6304 - val_loss: 0.6283 - val_accuracy: 0.6888\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6350 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6390 - val_loss: 0.6226 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6321 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6374 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6344 - val_loss: 0.6217 - val_accuracy: 0.6849\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6417 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6373 - val_loss: 0.6204 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6428 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6395 - val_loss: 0.6205 - val_accuracy: 0.6862\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6400 - val_loss: 0.6208 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.6310 - val_loss: 0.6238 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6392 - val_loss: 0.6196 - val_accuracy: 0.6875\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6321 - val_loss: 0.6178 - val_accuracy: 0.6875\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6412 - val_loss: 0.6191 - val_accuracy: 0.6862\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6379 - val_loss: 0.6181 - val_accuracy: 0.6875\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6420 - val_loss: 0.6171 - val_accuracy: 0.6888\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6358 - val_loss: 0.6163 - val_accuracy: 0.6913\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6413 - val_loss: 0.6220 - val_accuracy: 0.6913\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6424 - val_loss: 0.6190 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6423 - val_loss: 0.6152 - val_accuracy: 0.6926\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6433 - val_loss: 0.6178 - val_accuracy: 0.6913\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6417 - accuracy: 0.6387 - val_loss: 0.6229 - val_accuracy: 0.6888\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6417 - val_loss: 0.6152 - val_accuracy: 0.6939\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6374 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6387 - val_loss: 0.6195 - val_accuracy: 0.6901\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6516 - val_loss: 0.6142 - val_accuracy: 0.6888\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6433 - val_loss: 0.6155 - val_accuracy: 0.6952\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6461 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6418 - val_loss: 0.6182 - val_accuracy: 0.6913\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6431 - val_loss: 0.6185 - val_accuracy: 0.6901\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6429 - val_loss: 0.6151 - val_accuracy: 0.6913\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6473 - val_loss: 0.6166 - val_accuracy: 0.6888\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6471 - val_loss: 0.6135 - val_accuracy: 0.6888\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6487 - val_loss: 0.6168 - val_accuracy: 0.6926\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6508 - val_loss: 0.6163 - val_accuracy: 0.6926\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6475 - val_loss: 0.6164 - val_accuracy: 0.6926\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6501 - val_loss: 0.6166 - val_accuracy: 0.6952\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6408 - val_loss: 0.6158 - val_accuracy: 0.6913\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6468 - val_loss: 0.6146 - val_accuracy: 0.7003\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6530 - val_loss: 0.6168 - val_accuracy: 0.6964\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6427 - val_loss: 0.6183 - val_accuracy: 0.6862\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6535 - val_loss: 0.6118 - val_accuracy: 0.7015\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6516 - val_loss: 0.6132 - val_accuracy: 0.6977\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6512 - val_loss: 0.6144 - val_accuracy: 0.6977\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6518 - val_loss: 0.6137 - val_accuracy: 0.6990\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6518 - val_loss: 0.6136 - val_accuracy: 0.6952\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6507 - val_loss: 0.6104 - val_accuracy: 0.6964\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6544 - val_loss: 0.6125 - val_accuracy: 0.7015\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6472 - val_loss: 0.6148 - val_accuracy: 0.7015\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6534 - val_loss: 0.6130 - val_accuracy: 0.7015\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6517 - val_loss: 0.6147 - val_accuracy: 0.6952\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6593 - val_loss: 0.6219 - val_accuracy: 0.6952\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6527 - val_loss: 0.6169 - val_accuracy: 0.7015\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6410 - val_loss: 0.6150 - val_accuracy: 0.6964\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6525 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6462 - val_loss: 0.6116 - val_accuracy: 0.6926\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6595 - val_loss: 0.6108 - val_accuracy: 0.6964\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6575 - val_loss: 0.6147 - val_accuracy: 0.7003\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6541 - val_loss: 0.6113 - val_accuracy: 0.7003\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6497 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6600 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6515 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6603 - val_loss: 0.6123 - val_accuracy: 0.6939\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6600 - val_loss: 0.6187 - val_accuracy: 0.6849\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6604 - val_loss: 0.6147 - val_accuracy: 0.6862\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6606 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6532 - val_loss: 0.6116 - val_accuracy: 0.6849\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6561 - val_loss: 0.6126 - val_accuracy: 0.6913\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6580 - val_loss: 0.6096 - val_accuracy: 0.6913\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6551 - val_loss: 0.6085 - val_accuracy: 0.6913\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6556 - val_loss: 0.6153 - val_accuracy: 0.6901\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6565 - val_loss: 0.6161 - val_accuracy: 0.6888\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6626 - val_loss: 0.6155 - val_accuracy: 0.6875\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6594 - val_loss: 0.6094 - val_accuracy: 0.6888\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6559 - val_loss: 0.6163 - val_accuracy: 0.6926\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6552 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6579 - val_loss: 0.6104 - val_accuracy: 0.6901\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6637 - val_loss: 0.6133 - val_accuracy: 0.6875\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6605 - val_loss: 0.6132 - val_accuracy: 0.6875\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6586 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6229 - accuracy: 0.6614 - val_loss: 0.6145 - val_accuracy: 0.6913\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6596 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6611 - val_loss: 0.6184 - val_accuracy: 0.6875\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6567 - val_loss: 0.6129 - val_accuracy: 0.6888\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6516 - val_loss: 0.6159 - val_accuracy: 0.6926\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6606 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6613 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6583 - val_loss: 0.6179 - val_accuracy: 0.6913\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6604 - val_loss: 0.6165 - val_accuracy: 0.6875\n",
      "Calculating for: 700 100 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_288 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8017 - accuracy: 0.5308 - val_loss: 0.6415 - val_accuracy: 0.6480\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7085 - accuracy: 0.5605 - val_loss: 0.6405 - val_accuracy: 0.6696\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.5718 - val_loss: 0.6413 - val_accuracy: 0.6658\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5823 - val_loss: 0.6432 - val_accuracy: 0.6671\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.5824 - val_loss: 0.6390 - val_accuracy: 0.6709\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.5987 - val_loss: 0.6398 - val_accuracy: 0.6786\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.5901 - val_loss: 0.6462 - val_accuracy: 0.6747\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6025 - val_loss: 0.6360 - val_accuracy: 0.6773\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6017 - val_loss: 0.6360 - val_accuracy: 0.6747\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6616 - accuracy: 0.6074 - val_loss: 0.6364 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6070 - val_loss: 0.6388 - val_accuracy: 0.6747\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6098 - val_loss: 0.6361 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6184 - val_loss: 0.6289 - val_accuracy: 0.6773\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6179 - val_loss: 0.6326 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6147 - val_loss: 0.6330 - val_accuracy: 0.6722\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6184 - val_loss: 0.6305 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6193 - val_loss: 0.6327 - val_accuracy: 0.6760\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6152 - val_loss: 0.6303 - val_accuracy: 0.6747\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6253 - val_loss: 0.6208 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6228 - val_loss: 0.6290 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6233 - val_loss: 0.6291 - val_accuracy: 0.6811\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6251 - val_loss: 0.6242 - val_accuracy: 0.6824\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6287 - val_loss: 0.6259 - val_accuracy: 0.6811\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6261 - val_loss: 0.6273 - val_accuracy: 0.6837\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6304 - val_loss: 0.6253 - val_accuracy: 0.6862\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6353 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6246 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6384 - val_loss: 0.6185 - val_accuracy: 0.6811\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6305 - val_loss: 0.6158 - val_accuracy: 0.6798\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6383 - val_loss: 0.6239 - val_accuracy: 0.6824\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6419 - val_loss: 0.6273 - val_accuracy: 0.6658\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6365 - accuracy: 0.6400 - val_loss: 0.6223 - val_accuracy: 0.6684\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6309 - val_loss: 0.6248 - val_accuracy: 0.6735\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6356 - val_loss: 0.6175 - val_accuracy: 0.6798\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6379 - val_loss: 0.6223 - val_accuracy: 0.6811\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6387 - val_loss: 0.6214 - val_accuracy: 0.6824\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6418 - val_loss: 0.6206 - val_accuracy: 0.6811\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6500 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6497 - val_loss: 0.6168 - val_accuracy: 0.6811\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6451 - val_loss: 0.6178 - val_accuracy: 0.6760\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6461 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6518 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6506 - val_loss: 0.6168 - val_accuracy: 0.6811\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6555 - val_loss: 0.6183 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6525 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6541 - val_loss: 0.6182 - val_accuracy: 0.6786\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6570 - val_loss: 0.6129 - val_accuracy: 0.6901\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6485 - val_loss: 0.6143 - val_accuracy: 0.6849\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6462 - val_loss: 0.6135 - val_accuracy: 0.6811\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6569 - val_loss: 0.6170 - val_accuracy: 0.6824\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6571 - val_loss: 0.6183 - val_accuracy: 0.6760\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6614 - val_loss: 0.6150 - val_accuracy: 0.6837\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6580 - val_loss: 0.6165 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6521 - val_loss: 0.6125 - val_accuracy: 0.6824\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6645 - val_loss: 0.6162 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6621 - val_loss: 0.6132 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6654 - val_loss: 0.6139 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6610 - val_loss: 0.6142 - val_accuracy: 0.6888\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6630 - val_loss: 0.6161 - val_accuracy: 0.6798\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6600 - val_loss: 0.6164 - val_accuracy: 0.6824\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6621 - val_loss: 0.6156 - val_accuracy: 0.6773\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6679 - val_loss: 0.6147 - val_accuracy: 0.6811\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6672 - val_loss: 0.6179 - val_accuracy: 0.6837\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6166 - accuracy: 0.6640 - val_loss: 0.6184 - val_accuracy: 0.6849\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6154 - accuracy: 0.6668 - val_loss: 0.6175 - val_accuracy: 0.6862\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6621 - val_loss: 0.6150 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6643 - val_loss: 0.6193 - val_accuracy: 0.6798\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6137 - accuracy: 0.6689 - val_loss: 0.6204 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6729 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6097 - accuracy: 0.6736 - val_loss: 0.6201 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6067 - accuracy: 0.6760 - val_loss: 0.6207 - val_accuracy: 0.6786\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6704 - val_loss: 0.6195 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6103 - accuracy: 0.6675 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6079 - accuracy: 0.6757 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6111 - accuracy: 0.6697 - val_loss: 0.6163 - val_accuracy: 0.6824\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6112 - accuracy: 0.6741 - val_loss: 0.6124 - val_accuracy: 0.6875\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6144 - accuracy: 0.6782 - val_loss: 0.6104 - val_accuracy: 0.6901\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6760 - val_loss: 0.6139 - val_accuracy: 0.6811\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6113 - accuracy: 0.6719 - val_loss: 0.6157 - val_accuracy: 0.6862\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6040 - accuracy: 0.6845 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5971 - accuracy: 0.6858 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6030 - accuracy: 0.6861 - val_loss: 0.6182 - val_accuracy: 0.6862\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5971 - accuracy: 0.6806 - val_loss: 0.6144 - val_accuracy: 0.6901\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6052 - accuracy: 0.6775 - val_loss: 0.6156 - val_accuracy: 0.6837\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6009 - accuracy: 0.6830 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6035 - accuracy: 0.6732 - val_loss: 0.6170 - val_accuracy: 0.6837\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5969 - accuracy: 0.6858 - val_loss: 0.6175 - val_accuracy: 0.6824\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6827 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5980 - accuracy: 0.6767 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5992 - accuracy: 0.6786 - val_loss: 0.6206 - val_accuracy: 0.6837\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5967 - accuracy: 0.6874 - val_loss: 0.6180 - val_accuracy: 0.6875\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6009 - accuracy: 0.6868 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5954 - accuracy: 0.6868 - val_loss: 0.6231 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5974 - accuracy: 0.6820 - val_loss: 0.6201 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5924 - accuracy: 0.6893 - val_loss: 0.6240 - val_accuracy: 0.6786\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5919 - accuracy: 0.6869 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5952 - accuracy: 0.6904 - val_loss: 0.6215 - val_accuracy: 0.6824\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5966 - accuracy: 0.6863 - val_loss: 0.6151 - val_accuracy: 0.6888\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5950 - accuracy: 0.6952 - val_loss: 0.6182 - val_accuracy: 0.6849\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5942 - accuracy: 0.6908 - val_loss: 0.6189 - val_accuracy: 0.6862\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5883 - accuracy: 0.6958 - val_loss: 0.6183 - val_accuracy: 0.6849\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5890 - accuracy: 0.6930 - val_loss: 0.6193 - val_accuracy: 0.6824\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5926 - accuracy: 0.6898 - val_loss: 0.6233 - val_accuracy: 0.6811\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5862 - accuracy: 0.6958 - val_loss: 0.6265 - val_accuracy: 0.6786\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5961 - accuracy: 0.6874 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5819 - accuracy: 0.6971 - val_loss: 0.6248 - val_accuracy: 0.6773\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5856 - accuracy: 0.6962 - val_loss: 0.6228 - val_accuracy: 0.6875\n",
      "Calculating for: 700 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_292 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8087 - accuracy: 0.5252 - val_loss: 0.6748 - val_accuracy: 0.6148\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7191 - accuracy: 0.5455 - val_loss: 0.6628 - val_accuracy: 0.6352\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6921 - accuracy: 0.5491 - val_loss: 0.6558 - val_accuracy: 0.6467\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.5649 - val_loss: 0.6525 - val_accuracy: 0.6658\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6767 - accuracy: 0.5696 - val_loss: 0.6507 - val_accuracy: 0.6709\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5820 - val_loss: 0.6483 - val_accuracy: 0.6696\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5840 - val_loss: 0.6486 - val_accuracy: 0.6607\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5860 - val_loss: 0.6448 - val_accuracy: 0.6645\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.5985 - val_loss: 0.6452 - val_accuracy: 0.6467\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5899 - val_loss: 0.6447 - val_accuracy: 0.6531\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5973 - val_loss: 0.6473 - val_accuracy: 0.6416\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6703 - accuracy: 0.5906 - val_loss: 0.6470 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5916 - val_loss: 0.6378 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6042 - val_loss: 0.6399 - val_accuracy: 0.6492\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5971 - val_loss: 0.6425 - val_accuracy: 0.6518\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6004 - val_loss: 0.6427 - val_accuracy: 0.6518\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.5987 - val_loss: 0.6363 - val_accuracy: 0.6607\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6040 - val_loss: 0.6420 - val_accuracy: 0.6429\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6598 - accuracy: 0.6114 - val_loss: 0.6401 - val_accuracy: 0.6518\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6105 - val_loss: 0.6373 - val_accuracy: 0.6543\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6124 - val_loss: 0.6376 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6135 - val_loss: 0.6418 - val_accuracy: 0.6480\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6112 - val_loss: 0.6395 - val_accuracy: 0.6645\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6099 - val_loss: 0.6395 - val_accuracy: 0.6556\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6184 - val_loss: 0.6389 - val_accuracy: 0.6633\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6149 - val_loss: 0.6346 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6194 - val_loss: 0.6347 - val_accuracy: 0.6645\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6198 - val_loss: 0.6332 - val_accuracy: 0.6582\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6148 - val_loss: 0.6308 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6204 - val_loss: 0.6368 - val_accuracy: 0.6518\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6289 - val_loss: 0.6429 - val_accuracy: 0.6365\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6204 - val_loss: 0.6382 - val_accuracy: 0.6531\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6202 - val_loss: 0.6326 - val_accuracy: 0.6658\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6222 - val_loss: 0.6343 - val_accuracy: 0.6620\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6281 - val_loss: 0.6325 - val_accuracy: 0.6607\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6207 - val_loss: 0.6303 - val_accuracy: 0.6773\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6267 - val_loss: 0.6284 - val_accuracy: 0.6722\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6216 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6476 - accuracy: 0.6282 - val_loss: 0.6322 - val_accuracy: 0.6607\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6285 - val_loss: 0.6352 - val_accuracy: 0.6569\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6246 - val_loss: 0.6298 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6474 - accuracy: 0.6256 - val_loss: 0.6274 - val_accuracy: 0.6594\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6515 - accuracy: 0.6235 - val_loss: 0.6355 - val_accuracy: 0.6684\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6428 - accuracy: 0.6330 - val_loss: 0.6307 - val_accuracy: 0.6696\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6481 - accuracy: 0.6335 - val_loss: 0.6248 - val_accuracy: 0.6773\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6282 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6333 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6356 - val_loss: 0.6291 - val_accuracy: 0.6658\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6448 - accuracy: 0.6377 - val_loss: 0.6280 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6334 - val_loss: 0.6304 - val_accuracy: 0.6607\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6356 - val_loss: 0.6274 - val_accuracy: 0.6645\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6379 - val_loss: 0.6327 - val_accuracy: 0.6671\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.6353 - val_loss: 0.6271 - val_accuracy: 0.6684\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6412 - accuracy: 0.6321 - val_loss: 0.6289 - val_accuracy: 0.6696\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6420 - accuracy: 0.6368 - val_loss: 0.6225 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6414 - accuracy: 0.6392 - val_loss: 0.6262 - val_accuracy: 0.6684\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6400 - accuracy: 0.6432 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6417 - accuracy: 0.6395 - val_loss: 0.6240 - val_accuracy: 0.6735\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6400 - accuracy: 0.6387 - val_loss: 0.6249 - val_accuracy: 0.6735\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6388 - accuracy: 0.6402 - val_loss: 0.6273 - val_accuracy: 0.6722\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6390 - accuracy: 0.6413 - val_loss: 0.6214 - val_accuracy: 0.6760\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6369 - accuracy: 0.6429 - val_loss: 0.6267 - val_accuracy: 0.6633\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6418 - accuracy: 0.6368 - val_loss: 0.6278 - val_accuracy: 0.6735\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6401 - accuracy: 0.6377 - val_loss: 0.6315 - val_accuracy: 0.6671\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6316 - accuracy: 0.6464 - val_loss: 0.6272 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6361 - accuracy: 0.6434 - val_loss: 0.6246 - val_accuracy: 0.6786\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6343 - accuracy: 0.6461 - val_loss: 0.6259 - val_accuracy: 0.6773\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6377 - accuracy: 0.6438 - val_loss: 0.6264 - val_accuracy: 0.6760\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6358 - accuracy: 0.6454 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6372 - accuracy: 0.6442 - val_loss: 0.6211 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6487 - val_loss: 0.6235 - val_accuracy: 0.6722\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6447 - val_loss: 0.6264 - val_accuracy: 0.6786\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6393 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6512 - val_loss: 0.6152 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6332 - accuracy: 0.6490 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6313 - accuracy: 0.6486 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6335 - accuracy: 0.6490 - val_loss: 0.6244 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6306 - accuracy: 0.6473 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6327 - accuracy: 0.6466 - val_loss: 0.6246 - val_accuracy: 0.6773\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6310 - accuracy: 0.6569 - val_loss: 0.6261 - val_accuracy: 0.6798\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6273 - accuracy: 0.6566 - val_loss: 0.6224 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6274 - accuracy: 0.6531 - val_loss: 0.6256 - val_accuracy: 0.6773\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6596 - val_loss: 0.6247 - val_accuracy: 0.6786\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6559 - val_loss: 0.6275 - val_accuracy: 0.6709\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6527 - val_loss: 0.6241 - val_accuracy: 0.6824\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6505 - val_loss: 0.6219 - val_accuracy: 0.6862\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6547 - val_loss: 0.6212 - val_accuracy: 0.6901\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6238 - accuracy: 0.6572 - val_loss: 0.6224 - val_accuracy: 0.6901\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6577 - val_loss: 0.6282 - val_accuracy: 0.6811\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6567 - val_loss: 0.6235 - val_accuracy: 0.6875\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6552 - val_loss: 0.6251 - val_accuracy: 0.6786\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6610 - val_loss: 0.6197 - val_accuracy: 0.6901\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6193 - accuracy: 0.6664 - val_loss: 0.6251 - val_accuracy: 0.6811\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6187 - accuracy: 0.6606 - val_loss: 0.6228 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6267 - accuracy: 0.6547 - val_loss: 0.6194 - val_accuracy: 0.6875\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6583 - val_loss: 0.6159 - val_accuracy: 0.6952\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6643 - val_loss: 0.6213 - val_accuracy: 0.6888\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6610 - val_loss: 0.6209 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6634 - val_loss: 0.6216 - val_accuracy: 0.6837\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6580 - val_loss: 0.6255 - val_accuracy: 0.6798\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6631 - val_loss: 0.6273 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6576 - val_loss: 0.6203 - val_accuracy: 0.6913\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6645 - val_loss: 0.6240 - val_accuracy: 0.6837\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6165 - accuracy: 0.6613 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Calculating for: 700 100 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_296 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.8482 - accuracy: 0.5014 - val_loss: 0.6598 - val_accuracy: 0.6390\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.7272 - accuracy: 0.5387 - val_loss: 0.6581 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7087 - accuracy: 0.5203 - val_loss: 0.6567 - val_accuracy: 0.6416\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5296 - val_loss: 0.6654 - val_accuracy: 0.6518\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6914 - accuracy: 0.5396 - val_loss: 0.6673 - val_accuracy: 0.6492\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6894 - accuracy: 0.5389 - val_loss: 0.6618 - val_accuracy: 0.6403\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6841 - accuracy: 0.5558 - val_loss: 0.6534 - val_accuracy: 0.6390\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6849 - accuracy: 0.5551 - val_loss: 0.6569 - val_accuracy: 0.6403\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6849 - accuracy: 0.5494 - val_loss: 0.6592 - val_accuracy: 0.6441\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.5664 - val_loss: 0.6529 - val_accuracy: 0.6390\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.5649 - val_loss: 0.6532 - val_accuracy: 0.6441\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.5575 - val_loss: 0.6547 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6800 - accuracy: 0.5707 - val_loss: 0.6538 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5622 - val_loss: 0.6568 - val_accuracy: 0.6645\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6784 - accuracy: 0.5732 - val_loss: 0.6486 - val_accuracy: 0.6492\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.5678 - val_loss: 0.6547 - val_accuracy: 0.6645\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6750 - accuracy: 0.5726 - val_loss: 0.6500 - val_accuracy: 0.6594\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6790 - accuracy: 0.5723 - val_loss: 0.6498 - val_accuracy: 0.6620\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5779 - val_loss: 0.6486 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6752 - accuracy: 0.5794 - val_loss: 0.6458 - val_accuracy: 0.6671\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5865 - val_loss: 0.6456 - val_accuracy: 0.6671\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5907 - val_loss: 0.6430 - val_accuracy: 0.6671\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5815 - val_loss: 0.6404 - val_accuracy: 0.6645\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5921 - val_loss: 0.6428 - val_accuracy: 0.6696\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5917 - val_loss: 0.6420 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.5907 - val_loss: 0.6450 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5927 - val_loss: 0.6391 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5971 - val_loss: 0.6365 - val_accuracy: 0.6684\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6684 - accuracy: 0.5961 - val_loss: 0.6375 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6657 - accuracy: 0.5980 - val_loss: 0.6378 - val_accuracy: 0.6671\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6099 - val_loss: 0.6387 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6639 - accuracy: 0.6011 - val_loss: 0.6344 - val_accuracy: 0.6709\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.6017 - val_loss: 0.6380 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.6035 - val_loss: 0.6431 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5955 - val_loss: 0.6404 - val_accuracy: 0.6735\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.6024 - val_loss: 0.6434 - val_accuracy: 0.6684\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.5992 - val_loss: 0.6400 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6017 - val_loss: 0.6353 - val_accuracy: 0.6760\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5967 - val_loss: 0.6382 - val_accuracy: 0.6722\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6081 - val_loss: 0.6457 - val_accuracy: 0.6658\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6617 - accuracy: 0.6107 - val_loss: 0.6386 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6604 - accuracy: 0.6118 - val_loss: 0.6370 - val_accuracy: 0.6811\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6042 - val_loss: 0.6362 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6110 - val_loss: 0.6326 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6091 - val_loss: 0.6343 - val_accuracy: 0.6837\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6569 - accuracy: 0.6162 - val_loss: 0.6321 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6115 - val_loss: 0.6399 - val_accuracy: 0.6696\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6041 - val_loss: 0.6394 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6091 - val_loss: 0.6355 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6168 - val_loss: 0.6331 - val_accuracy: 0.6862\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6587 - accuracy: 0.6113 - val_loss: 0.6305 - val_accuracy: 0.6798\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6123 - val_loss: 0.6323 - val_accuracy: 0.6862\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6212 - val_loss: 0.6303 - val_accuracy: 0.6773\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6153 - val_loss: 0.6278 - val_accuracy: 0.6824\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6152 - val_loss: 0.6387 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6125 - val_loss: 0.6289 - val_accuracy: 0.6837\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6564 - accuracy: 0.6158 - val_loss: 0.6349 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6162 - val_loss: 0.6358 - val_accuracy: 0.6798\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6541 - accuracy: 0.6194 - val_loss: 0.6334 - val_accuracy: 0.6798\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6163 - val_loss: 0.6362 - val_accuracy: 0.6773\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6173 - val_loss: 0.6281 - val_accuracy: 0.6875\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6232 - val_loss: 0.6306 - val_accuracy: 0.6773\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6194 - val_loss: 0.6297 - val_accuracy: 0.6786\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6222 - val_loss: 0.6275 - val_accuracy: 0.6773\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6204 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6238 - val_loss: 0.6335 - val_accuracy: 0.6849\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6184 - val_loss: 0.6370 - val_accuracy: 0.6747\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.6228 - val_loss: 0.6348 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6267 - val_loss: 0.6319 - val_accuracy: 0.6811\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6252 - val_loss: 0.6289 - val_accuracy: 0.6901\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6232 - val_loss: 0.6315 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6256 - val_loss: 0.6271 - val_accuracy: 0.6862\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6543 - accuracy: 0.6251 - val_loss: 0.6277 - val_accuracy: 0.6888\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.6216 - val_loss: 0.6280 - val_accuracy: 0.6824\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6495 - accuracy: 0.6266 - val_loss: 0.6257 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6509 - accuracy: 0.6324 - val_loss: 0.6263 - val_accuracy: 0.6939\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6285 - val_loss: 0.6301 - val_accuracy: 0.6939\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6247 - val_loss: 0.6300 - val_accuracy: 0.6901\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6260 - val_loss: 0.6246 - val_accuracy: 0.6875\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6248 - val_loss: 0.6310 - val_accuracy: 0.6658\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6280 - val_loss: 0.6251 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6285 - val_loss: 0.6304 - val_accuracy: 0.6747\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6295 - val_loss: 0.6268 - val_accuracy: 0.6862\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6292 - val_loss: 0.6261 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6296 - val_loss: 0.6238 - val_accuracy: 0.6964\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6294 - val_loss: 0.6220 - val_accuracy: 0.6901\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6368 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6356 - val_loss: 0.6236 - val_accuracy: 0.6926\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6369 - val_loss: 0.6226 - val_accuracy: 0.6901\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6344 - val_loss: 0.6236 - val_accuracy: 0.6773\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6331 - val_loss: 0.6251 - val_accuracy: 0.6773\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6349 - val_loss: 0.6203 - val_accuracy: 0.6901\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6349 - val_loss: 0.6252 - val_accuracy: 0.6888\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6340 - val_loss: 0.6226 - val_accuracy: 0.6888\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6346 - val_loss: 0.6261 - val_accuracy: 0.6862\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6331 - val_loss: 0.6275 - val_accuracy: 0.6875\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6323 - val_loss: 0.6250 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6441 - accuracy: 0.6353 - val_loss: 0.6251 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6432 - accuracy: 0.6358 - val_loss: 0.6237 - val_accuracy: 0.6862\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6437 - accuracy: 0.6372 - val_loss: 0.6213 - val_accuracy: 0.6901\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6369 - val_loss: 0.6210 - val_accuracy: 0.6952\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6452 - accuracy: 0.6290 - val_loss: 0.6267 - val_accuracy: 0.6875\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6424 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6419 - val_loss: 0.6212 - val_accuracy: 0.6862\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6414 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6363 - val_loss: 0.6216 - val_accuracy: 0.6837\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6418 - val_loss: 0.6168 - val_accuracy: 0.6901\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6328 - val_loss: 0.6198 - val_accuracy: 0.6862\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6405 - val_loss: 0.6231 - val_accuracy: 0.6824\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6378 - val_loss: 0.6235 - val_accuracy: 0.6811\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6404 - accuracy: 0.6428 - val_loss: 0.6189 - val_accuracy: 0.6901\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6382 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6413 - val_loss: 0.6247 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6392 - accuracy: 0.6427 - val_loss: 0.6151 - val_accuracy: 0.6913\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6452 - val_loss: 0.6198 - val_accuracy: 0.6824\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6355 - val_loss: 0.6195 - val_accuracy: 0.6875\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6427 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6446 - val_loss: 0.6250 - val_accuracy: 0.6811\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6448 - val_loss: 0.6266 - val_accuracy: 0.6760\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6393 - accuracy: 0.6429 - val_loss: 0.6176 - val_accuracy: 0.6926\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6461 - val_loss: 0.6173 - val_accuracy: 0.6913\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6428 - val_loss: 0.6186 - val_accuracy: 0.6901\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6463 - val_loss: 0.6174 - val_accuracy: 0.6849\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6417 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6446 - val_loss: 0.6179 - val_accuracy: 0.6888\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6399 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6437 - val_loss: 0.6163 - val_accuracy: 0.6901\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6431 - val_loss: 0.6197 - val_accuracy: 0.6926\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6339 - accuracy: 0.6442 - val_loss: 0.6203 - val_accuracy: 0.6862\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6469 - val_loss: 0.6192 - val_accuracy: 0.6901\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6496 - val_loss: 0.6160 - val_accuracy: 0.6913\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6529 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6442 - val_loss: 0.6192 - val_accuracy: 0.6888\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6410 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6373 - accuracy: 0.6467 - val_loss: 0.6172 - val_accuracy: 0.6913\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6351 - accuracy: 0.6497 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6327 - accuracy: 0.6493 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6427 - val_loss: 0.6184 - val_accuracy: 0.6913\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6463 - val_loss: 0.6133 - val_accuracy: 0.6939\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6467 - val_loss: 0.6145 - val_accuracy: 0.6952\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6506 - val_loss: 0.6141 - val_accuracy: 0.6939\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6497 - val_loss: 0.6137 - val_accuracy: 0.6913\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6541 - val_loss: 0.6155 - val_accuracy: 0.6913\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6547 - val_loss: 0.6172 - val_accuracy: 0.6939\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6498 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6503 - val_loss: 0.6137 - val_accuracy: 0.6952\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6502 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6518 - val_loss: 0.6149 - val_accuracy: 0.6939\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6559 - val_loss: 0.6167 - val_accuracy: 0.6964\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6541 - val_loss: 0.6191 - val_accuracy: 0.6875\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6488 - val_loss: 0.6184 - val_accuracy: 0.6875\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6505 - val_loss: 0.6155 - val_accuracy: 0.7003\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6513 - val_loss: 0.6135 - val_accuracy: 0.6977\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6561 - val_loss: 0.6119 - val_accuracy: 0.6926\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6550 - val_loss: 0.6169 - val_accuracy: 0.6888\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6522 - val_loss: 0.6152 - val_accuracy: 0.6913\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6311 - accuracy: 0.6556 - val_loss: 0.6165 - val_accuracy: 0.6901\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6536 - val_loss: 0.6166 - val_accuracy: 0.6926\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6279 - accuracy: 0.6567 - val_loss: 0.6137 - val_accuracy: 0.6939\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6232 - accuracy: 0.6624 - val_loss: 0.6104 - val_accuracy: 0.6913\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6277 - accuracy: 0.6554 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6546 - val_loss: 0.6122 - val_accuracy: 0.6888\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6541 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6593 - val_loss: 0.6105 - val_accuracy: 0.6926\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6572 - val_loss: 0.6182 - val_accuracy: 0.6901\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6235 - accuracy: 0.6640 - val_loss: 0.6126 - val_accuracy: 0.6964\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6536 - val_loss: 0.6143 - val_accuracy: 0.6913\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6248 - accuracy: 0.6606 - val_loss: 0.6121 - val_accuracy: 0.6901\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6262 - accuracy: 0.6598 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6562 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6532 - val_loss: 0.6174 - val_accuracy: 0.6913\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6626 - val_loss: 0.6162 - val_accuracy: 0.6926\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6545 - val_loss: 0.6141 - val_accuracy: 0.6990\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6608 - val_loss: 0.6130 - val_accuracy: 0.6939\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6657 - val_loss: 0.6098 - val_accuracy: 0.7015\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6619 - val_loss: 0.6150 - val_accuracy: 0.6901\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6594 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6577 - val_loss: 0.6213 - val_accuracy: 0.6888\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6618 - val_loss: 0.6211 - val_accuracy: 0.6888\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6257 - accuracy: 0.6577 - val_loss: 0.6170 - val_accuracy: 0.6875\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6182 - accuracy: 0.6686 - val_loss: 0.6185 - val_accuracy: 0.6875\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6644 - val_loss: 0.6177 - val_accuracy: 0.6862\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6216 - accuracy: 0.6609 - val_loss: 0.6193 - val_accuracy: 0.6875\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6174 - accuracy: 0.6689 - val_loss: 0.6121 - val_accuracy: 0.6862\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6619 - val_loss: 0.6168 - val_accuracy: 0.6875\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6240 - accuracy: 0.6586 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6649 - val_loss: 0.6128 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6613 - val_loss: 0.6173 - val_accuracy: 0.6837\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6583 - val_loss: 0.6182 - val_accuracy: 0.6837\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6570 - val_loss: 0.6176 - val_accuracy: 0.6913\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6223 - accuracy: 0.6628 - val_loss: 0.6192 - val_accuracy: 0.6901\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6191 - accuracy: 0.6668 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6630 - val_loss: 0.6180 - val_accuracy: 0.6837\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6180 - accuracy: 0.6699 - val_loss: 0.6189 - val_accuracy: 0.6862\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6686 - val_loss: 0.6162 - val_accuracy: 0.6849\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6201 - accuracy: 0.6648 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6624 - val_loss: 0.6139 - val_accuracy: 0.6875\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6148 - accuracy: 0.6609 - val_loss: 0.6148 - val_accuracy: 0.6862\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6586 - val_loss: 0.6148 - val_accuracy: 0.6875\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6200 - accuracy: 0.6652 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Calculating for: 700 150 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_300 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7952 - accuracy: 0.5344 - val_loss: 0.6661 - val_accuracy: 0.6301\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7031 - accuracy: 0.5698 - val_loss: 0.6623 - val_accuracy: 0.6237\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6816 - accuracy: 0.5740 - val_loss: 0.6517 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6753 - accuracy: 0.5841 - val_loss: 0.6435 - val_accuracy: 0.6645\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6679 - accuracy: 0.5968 - val_loss: 0.6424 - val_accuracy: 0.6658\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6642 - accuracy: 0.6034 - val_loss: 0.6431 - val_accuracy: 0.6454\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6654 - accuracy: 0.6002 - val_loss: 0.6393 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6621 - accuracy: 0.6040 - val_loss: 0.6412 - val_accuracy: 0.6480\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6045 - val_loss: 0.6395 - val_accuracy: 0.6543\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6597 - accuracy: 0.6088 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6568 - accuracy: 0.6194 - val_loss: 0.6365 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6162 - val_loss: 0.6318 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6577 - accuracy: 0.6143 - val_loss: 0.6314 - val_accuracy: 0.6658\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.6186 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6228 - val_loss: 0.6287 - val_accuracy: 0.6747\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6535 - accuracy: 0.6221 - val_loss: 0.6333 - val_accuracy: 0.6518\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6199 - val_loss: 0.6333 - val_accuracy: 0.6505\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6255 - val_loss: 0.6308 - val_accuracy: 0.6594\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6490 - accuracy: 0.6226 - val_loss: 0.6304 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6470 - accuracy: 0.6260 - val_loss: 0.6219 - val_accuracy: 0.6824\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6500 - accuracy: 0.6291 - val_loss: 0.6294 - val_accuracy: 0.6594\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6442 - accuracy: 0.6297 - val_loss: 0.6262 - val_accuracy: 0.6620\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6455 - accuracy: 0.6281 - val_loss: 0.6257 - val_accuracy: 0.6607\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6361 - val_loss: 0.6266 - val_accuracy: 0.6607\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6402 - accuracy: 0.6408 - val_loss: 0.6268 - val_accuracy: 0.6633\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6457 - accuracy: 0.6305 - val_loss: 0.6285 - val_accuracy: 0.6620\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6377 - val_loss: 0.6276 - val_accuracy: 0.6607\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6394 - val_loss: 0.6270 - val_accuracy: 0.6607\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6373 - accuracy: 0.6442 - val_loss: 0.6250 - val_accuracy: 0.6658\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6388 - val_loss: 0.6254 - val_accuracy: 0.6735\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6390 - accuracy: 0.6374 - val_loss: 0.6262 - val_accuracy: 0.6594\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6380 - accuracy: 0.6426 - val_loss: 0.6278 - val_accuracy: 0.6594\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6414 - accuracy: 0.6385 - val_loss: 0.6254 - val_accuracy: 0.6620\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6369 - val_loss: 0.6256 - val_accuracy: 0.6696\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6466 - val_loss: 0.6252 - val_accuracy: 0.6658\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6523 - val_loss: 0.6239 - val_accuracy: 0.6735\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6377 - accuracy: 0.6368 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6394 - val_loss: 0.6203 - val_accuracy: 0.6760\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6290 - accuracy: 0.6481 - val_loss: 0.6176 - val_accuracy: 0.6747\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6448 - val_loss: 0.6209 - val_accuracy: 0.6671\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6448 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6307 - accuracy: 0.6500 - val_loss: 0.6227 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6269 - accuracy: 0.6486 - val_loss: 0.6258 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6535 - val_loss: 0.6293 - val_accuracy: 0.6569\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6280 - accuracy: 0.6477 - val_loss: 0.6214 - val_accuracy: 0.6760\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6545 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6537 - val_loss: 0.6247 - val_accuracy: 0.6747\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6501 - val_loss: 0.6254 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6236 - accuracy: 0.6537 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6215 - accuracy: 0.6580 - val_loss: 0.6206 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6216 - accuracy: 0.6560 - val_loss: 0.6226 - val_accuracy: 0.6709\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6230 - accuracy: 0.6555 - val_loss: 0.6222 - val_accuracy: 0.6735\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6221 - accuracy: 0.6566 - val_loss: 0.6205 - val_accuracy: 0.6760\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6537 - val_loss: 0.6242 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6628 - val_loss: 0.6208 - val_accuracy: 0.6696\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6186 - accuracy: 0.6579 - val_loss: 0.6219 - val_accuracy: 0.6747\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6644 - val_loss: 0.6255 - val_accuracy: 0.6722\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6626 - val_loss: 0.6263 - val_accuracy: 0.6747\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6163 - accuracy: 0.6644 - val_loss: 0.6227 - val_accuracy: 0.6709\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6158 - accuracy: 0.6673 - val_loss: 0.6225 - val_accuracy: 0.6798\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6668 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6664 - val_loss: 0.6254 - val_accuracy: 0.6773\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6114 - accuracy: 0.6721 - val_loss: 0.6264 - val_accuracy: 0.6722\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6144 - accuracy: 0.6678 - val_loss: 0.6238 - val_accuracy: 0.6798\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6142 - accuracy: 0.6703 - val_loss: 0.6239 - val_accuracy: 0.6786\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6682 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6654 - val_loss: 0.6216 - val_accuracy: 0.6901\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6625 - val_loss: 0.6222 - val_accuracy: 0.6811\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6109 - accuracy: 0.6652 - val_loss: 0.6218 - val_accuracy: 0.6888\n",
      "Calculating for: 700 150 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_304 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8170 - accuracy: 0.5280 - val_loss: 0.6689 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7183 - accuracy: 0.5659 - val_loss: 0.6428 - val_accuracy: 0.6633\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6892 - accuracy: 0.5732 - val_loss: 0.6450 - val_accuracy: 0.6645\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6794 - accuracy: 0.5723 - val_loss: 0.6477 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6762 - accuracy: 0.5737 - val_loss: 0.6410 - val_accuracy: 0.6633\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6722 - accuracy: 0.5855 - val_loss: 0.6425 - val_accuracy: 0.6556\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5962 - val_loss: 0.6426 - val_accuracy: 0.6645\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5883 - val_loss: 0.6398 - val_accuracy: 0.6645\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.5967 - val_loss: 0.6373 - val_accuracy: 0.6696\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6031 - val_loss: 0.6334 - val_accuracy: 0.6684\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6052 - val_loss: 0.6377 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6039 - val_loss: 0.6389 - val_accuracy: 0.6582\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.6095 - val_loss: 0.6311 - val_accuracy: 0.6760\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6638 - accuracy: 0.6058 - val_loss: 0.6359 - val_accuracy: 0.6696\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6140 - val_loss: 0.6331 - val_accuracy: 0.6696\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.5958 - val_loss: 0.6331 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6611 - accuracy: 0.6020 - val_loss: 0.6362 - val_accuracy: 0.6747\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6096 - val_loss: 0.6305 - val_accuracy: 0.6811\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6601 - accuracy: 0.6107 - val_loss: 0.6281 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6582 - accuracy: 0.6078 - val_loss: 0.6269 - val_accuracy: 0.6760\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6179 - val_loss: 0.6283 - val_accuracy: 0.6773\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6570 - accuracy: 0.6076 - val_loss: 0.6297 - val_accuracy: 0.6798\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6093 - val_loss: 0.6289 - val_accuracy: 0.6773\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6552 - accuracy: 0.6202 - val_loss: 0.6290 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6174 - val_loss: 0.6260 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6546 - accuracy: 0.6178 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6218 - val_loss: 0.6243 - val_accuracy: 0.6798\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6187 - val_loss: 0.6260 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6258 - val_loss: 0.6267 - val_accuracy: 0.6824\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6498 - accuracy: 0.6242 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6473 - accuracy: 0.6321 - val_loss: 0.6198 - val_accuracy: 0.6798\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6289 - val_loss: 0.6301 - val_accuracy: 0.6837\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6236 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6235 - val_loss: 0.6226 - val_accuracy: 0.6875\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6227 - val_loss: 0.6247 - val_accuracy: 0.6837\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.6299 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6471 - accuracy: 0.6299 - val_loss: 0.6212 - val_accuracy: 0.6888\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6453 - accuracy: 0.6312 - val_loss: 0.6240 - val_accuracy: 0.6888\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6486 - accuracy: 0.6301 - val_loss: 0.6217 - val_accuracy: 0.6913\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6439 - accuracy: 0.6316 - val_loss: 0.6196 - val_accuracy: 0.6952\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6277 - val_loss: 0.6249 - val_accuracy: 0.6760\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6385 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6290 - val_loss: 0.6199 - val_accuracy: 0.6888\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6320 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6450 - accuracy: 0.6319 - val_loss: 0.6216 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6459 - accuracy: 0.6335 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6341 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6354 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6422 - accuracy: 0.6387 - val_loss: 0.6168 - val_accuracy: 0.6849\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6394 - val_loss: 0.6195 - val_accuracy: 0.6837\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6382 - val_loss: 0.6189 - val_accuracy: 0.6875\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6404 - val_loss: 0.6183 - val_accuracy: 0.6888\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6389 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6395 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6388 - val_loss: 0.6207 - val_accuracy: 0.6862\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6409 - val_loss: 0.6172 - val_accuracy: 0.6901\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6373 - val_loss: 0.6214 - val_accuracy: 0.6875\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6468 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6399 - val_loss: 0.6157 - val_accuracy: 0.6862\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6386 - accuracy: 0.6418 - val_loss: 0.6134 - val_accuracy: 0.6888\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6486 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6383 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6463 - val_loss: 0.6162 - val_accuracy: 0.6875\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6468 - val_loss: 0.6196 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6495 - val_loss: 0.6201 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6476 - val_loss: 0.6174 - val_accuracy: 0.6811\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6510 - val_loss: 0.6125 - val_accuracy: 0.6926\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6382 - accuracy: 0.6443 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6463 - val_loss: 0.6155 - val_accuracy: 0.6926\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6502 - val_loss: 0.6116 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6443 - val_loss: 0.6133 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6492 - val_loss: 0.6187 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6544 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6463 - val_loss: 0.6157 - val_accuracy: 0.6901\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6526 - val_loss: 0.6134 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6521 - val_loss: 0.6179 - val_accuracy: 0.6837\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6539 - val_loss: 0.6137 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6505 - val_loss: 0.6144 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6570 - val_loss: 0.6172 - val_accuracy: 0.6862\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6510 - val_loss: 0.6126 - val_accuracy: 0.6901\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6557 - val_loss: 0.6097 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6539 - val_loss: 0.6155 - val_accuracy: 0.6888\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6572 - val_loss: 0.6122 - val_accuracy: 0.6964\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6546 - val_loss: 0.6143 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6536 - val_loss: 0.6176 - val_accuracy: 0.6824\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6628 - val_loss: 0.6137 - val_accuracy: 0.6811\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6503 - val_loss: 0.6125 - val_accuracy: 0.6913\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6598 - val_loss: 0.6153 - val_accuracy: 0.6837\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6642 - val_loss: 0.6135 - val_accuracy: 0.6926\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6556 - val_loss: 0.6132 - val_accuracy: 0.6888\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6550 - val_loss: 0.6124 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6539 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6583 - val_loss: 0.6167 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6624 - val_loss: 0.6146 - val_accuracy: 0.6811\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6620 - val_loss: 0.6131 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6611 - val_loss: 0.6126 - val_accuracy: 0.6798\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6605 - val_loss: 0.6163 - val_accuracy: 0.6786\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6585 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6637 - val_loss: 0.6143 - val_accuracy: 0.6811\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6590 - val_loss: 0.6143 - val_accuracy: 0.6760\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6168 - accuracy: 0.6613 - val_loss: 0.6134 - val_accuracy: 0.6798\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6176 - accuracy: 0.6650 - val_loss: 0.6138 - val_accuracy: 0.6798\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6644 - val_loss: 0.6149 - val_accuracy: 0.6773\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6192 - accuracy: 0.6556 - val_loss: 0.6131 - val_accuracy: 0.6824\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6629 - val_loss: 0.6161 - val_accuracy: 0.6811\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6140 - accuracy: 0.6663 - val_loss: 0.6163 - val_accuracy: 0.6760\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6669 - val_loss: 0.6166 - val_accuracy: 0.6747\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6625 - val_loss: 0.6167 - val_accuracy: 0.6747\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6669 - val_loss: 0.6162 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6134 - accuracy: 0.6729 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6672 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Calculating for: 700 150 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_308 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8167 - accuracy: 0.5200 - val_loss: 0.6532 - val_accuracy: 0.6378\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7285 - accuracy: 0.5359 - val_loss: 0.6523 - val_accuracy: 0.6390\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7020 - accuracy: 0.5418 - val_loss: 0.6525 - val_accuracy: 0.6429\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5446 - val_loss: 0.6553 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6789 - accuracy: 0.5667 - val_loss: 0.6480 - val_accuracy: 0.6480\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6840 - accuracy: 0.5609 - val_loss: 0.6531 - val_accuracy: 0.6480\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.5676 - val_loss: 0.6524 - val_accuracy: 0.6492\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6787 - accuracy: 0.5656 - val_loss: 0.6466 - val_accuracy: 0.6556\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5666 - val_loss: 0.6479 - val_accuracy: 0.6582\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6805 - accuracy: 0.5623 - val_loss: 0.6471 - val_accuracy: 0.6582\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6774 - accuracy: 0.5725 - val_loss: 0.6436 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5782 - val_loss: 0.6435 - val_accuracy: 0.6594\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6756 - accuracy: 0.5759 - val_loss: 0.6423 - val_accuracy: 0.6556\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6735 - accuracy: 0.5775 - val_loss: 0.6399 - val_accuracy: 0.6582\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5946 - val_loss: 0.6369 - val_accuracy: 0.6607\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5830 - val_loss: 0.6422 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6717 - accuracy: 0.5823 - val_loss: 0.6441 - val_accuracy: 0.6696\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5879 - val_loss: 0.6461 - val_accuracy: 0.6658\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5889 - val_loss: 0.6390 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5962 - val_loss: 0.6390 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5909 - val_loss: 0.6349 - val_accuracy: 0.6696\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6693 - accuracy: 0.5941 - val_loss: 0.6369 - val_accuracy: 0.6709\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5981 - val_loss: 0.6372 - val_accuracy: 0.6747\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.5948 - val_loss: 0.6336 - val_accuracy: 0.6735\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6029 - val_loss: 0.6340 - val_accuracy: 0.6773\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5990 - val_loss: 0.6326 - val_accuracy: 0.6747\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6061 - val_loss: 0.6335 - val_accuracy: 0.6786\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.6022 - val_loss: 0.6336 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5937 - val_loss: 0.6325 - val_accuracy: 0.6747\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6026 - val_loss: 0.6356 - val_accuracy: 0.6735\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6009 - val_loss: 0.6312 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6636 - accuracy: 0.6065 - val_loss: 0.6332 - val_accuracy: 0.6658\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.6039 - val_loss: 0.6307 - val_accuracy: 0.6773\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6597 - accuracy: 0.6085 - val_loss: 0.6278 - val_accuracy: 0.6747\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.6010 - val_loss: 0.6308 - val_accuracy: 0.6760\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6060 - val_loss: 0.6301 - val_accuracy: 0.6735\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6078 - val_loss: 0.6284 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6115 - val_loss: 0.6320 - val_accuracy: 0.6811\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6622 - accuracy: 0.6058 - val_loss: 0.6312 - val_accuracy: 0.6811\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6598 - accuracy: 0.6085 - val_loss: 0.6284 - val_accuracy: 0.6811\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6602 - accuracy: 0.6114 - val_loss: 0.6276 - val_accuracy: 0.6811\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6122 - val_loss: 0.6289 - val_accuracy: 0.6709\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6133 - val_loss: 0.6258 - val_accuracy: 0.6824\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6100 - val_loss: 0.6262 - val_accuracy: 0.6837\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6117 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6117 - val_loss: 0.6260 - val_accuracy: 0.6849\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6161 - val_loss: 0.6236 - val_accuracy: 0.6798\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6143 - val_loss: 0.6285 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6166 - val_loss: 0.6279 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6588 - accuracy: 0.6058 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6150 - val_loss: 0.6250 - val_accuracy: 0.6849\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6562 - accuracy: 0.6167 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6222 - val_loss: 0.6264 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6551 - accuracy: 0.6172 - val_loss: 0.6308 - val_accuracy: 0.6824\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6117 - val_loss: 0.6239 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6216 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6191 - val_loss: 0.6232 - val_accuracy: 0.6837\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6184 - val_loss: 0.6223 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6153 - val_loss: 0.6255 - val_accuracy: 0.6875\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6177 - val_loss: 0.6273 - val_accuracy: 0.6786\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6233 - val_loss: 0.6254 - val_accuracy: 0.6837\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6196 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6548 - accuracy: 0.6194 - val_loss: 0.6210 - val_accuracy: 0.6837\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6235 - val_loss: 0.6217 - val_accuracy: 0.6811\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.6266 - val_loss: 0.6231 - val_accuracy: 0.6849\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6305 - val_loss: 0.6270 - val_accuracy: 0.6735\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6275 - val_loss: 0.6225 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6269 - val_loss: 0.6225 - val_accuracy: 0.6875\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6226 - val_loss: 0.6265 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6294 - val_loss: 0.6224 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6482 - accuracy: 0.6272 - val_loss: 0.6228 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6271 - val_loss: 0.6227 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6307 - val_loss: 0.6239 - val_accuracy: 0.6849\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6266 - val_loss: 0.6241 - val_accuracy: 0.6862\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6257 - val_loss: 0.6188 - val_accuracy: 0.6824\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6483 - accuracy: 0.6257 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6323 - val_loss: 0.6232 - val_accuracy: 0.6849\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6252 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6499 - accuracy: 0.6270 - val_loss: 0.6209 - val_accuracy: 0.6824\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6320 - val_loss: 0.6204 - val_accuracy: 0.6875\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6287 - val_loss: 0.6218 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6279 - val_loss: 0.6215 - val_accuracy: 0.6849\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6336 - val_loss: 0.6145 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6436 - accuracy: 0.6349 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6477 - accuracy: 0.6285 - val_loss: 0.6250 - val_accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6445 - accuracy: 0.6295 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6370 - val_loss: 0.6223 - val_accuracy: 0.6849\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6295 - val_loss: 0.6207 - val_accuracy: 0.6837\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6393 - val_loss: 0.6176 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6339 - val_loss: 0.6191 - val_accuracy: 0.6798\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6484 - accuracy: 0.6300 - val_loss: 0.6170 - val_accuracy: 0.6862\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6325 - val_loss: 0.6137 - val_accuracy: 0.6888\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6442 - accuracy: 0.6324 - val_loss: 0.6155 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6379 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6388 - val_loss: 0.6155 - val_accuracy: 0.6837\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6400 - val_loss: 0.6184 - val_accuracy: 0.6888\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6432 - accuracy: 0.6346 - val_loss: 0.6144 - val_accuracy: 0.6837\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6452 - accuracy: 0.6385 - val_loss: 0.6166 - val_accuracy: 0.6862\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6415 - accuracy: 0.6407 - val_loss: 0.6170 - val_accuracy: 0.6862\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6418 - accuracy: 0.6390 - val_loss: 0.6153 - val_accuracy: 0.6837\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6361 - val_loss: 0.6157 - val_accuracy: 0.6875\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6449 - val_loss: 0.6192 - val_accuracy: 0.6888\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6370 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6389 - val_loss: 0.6173 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6427 - val_loss: 0.6162 - val_accuracy: 0.6888\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6437 - val_loss: 0.6172 - val_accuracy: 0.6811\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6441 - accuracy: 0.6387 - val_loss: 0.6203 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6381 - accuracy: 0.6404 - val_loss: 0.6138 - val_accuracy: 0.6875\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6439 - val_loss: 0.6167 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6372 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6387 - accuracy: 0.6432 - val_loss: 0.6176 - val_accuracy: 0.6875\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6448 - val_loss: 0.6148 - val_accuracy: 0.6862\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.6384 - val_loss: 0.6181 - val_accuracy: 0.6862\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6353 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6375 - val_loss: 0.6157 - val_accuracy: 0.6875\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6399 - val_loss: 0.6181 - val_accuracy: 0.6875\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6377 - val_loss: 0.6167 - val_accuracy: 0.6901\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6437 - val_loss: 0.6226 - val_accuracy: 0.6786\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6457 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6382 - val_loss: 0.6178 - val_accuracy: 0.6824\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6429 - val_loss: 0.6132 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6432 - val_loss: 0.6152 - val_accuracy: 0.6888\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6462 - val_loss: 0.6184 - val_accuracy: 0.6849\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6467 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6461 - val_loss: 0.6118 - val_accuracy: 0.6926\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6544 - val_loss: 0.6137 - val_accuracy: 0.6888\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6467 - val_loss: 0.6136 - val_accuracy: 0.6901\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6408 - val_loss: 0.6152 - val_accuracy: 0.6939\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6516 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6367 - accuracy: 0.6476 - val_loss: 0.6151 - val_accuracy: 0.6926\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6463 - val_loss: 0.6160 - val_accuracy: 0.6849\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6468 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6368 - accuracy: 0.6477 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.6500 - val_loss: 0.6137 - val_accuracy: 0.6875\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6560 - val_loss: 0.6146 - val_accuracy: 0.6901\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6510 - val_loss: 0.6145 - val_accuracy: 0.6939\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6439 - val_loss: 0.6209 - val_accuracy: 0.6824\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6350 - accuracy: 0.6448 - val_loss: 0.6142 - val_accuracy: 0.6939\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6546 - val_loss: 0.6092 - val_accuracy: 0.6990\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6525 - val_loss: 0.6118 - val_accuracy: 0.6939\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6511 - val_loss: 0.6140 - val_accuracy: 0.6875\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6541 - val_loss: 0.6074 - val_accuracy: 0.6990\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6442 - val_loss: 0.6149 - val_accuracy: 0.6875\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6491 - val_loss: 0.6119 - val_accuracy: 0.7054\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6463 - val_loss: 0.6149 - val_accuracy: 0.6964\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6502 - val_loss: 0.6119 - val_accuracy: 0.6952\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6500 - val_loss: 0.6116 - val_accuracy: 0.6952\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6530 - val_loss: 0.6152 - val_accuracy: 0.6901\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6529 - val_loss: 0.6145 - val_accuracy: 0.6862\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6601 - val_loss: 0.6131 - val_accuracy: 0.6901\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6480 - val_loss: 0.6102 - val_accuracy: 0.6990\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6595 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6523 - val_loss: 0.6135 - val_accuracy: 0.6888\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6510 - val_loss: 0.6115 - val_accuracy: 0.7028\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6550 - val_loss: 0.6101 - val_accuracy: 0.6990\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6626 - val_loss: 0.6117 - val_accuracy: 0.6977\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6581 - val_loss: 0.6134 - val_accuracy: 0.6875\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6541 - val_loss: 0.6158 - val_accuracy: 0.6875\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6629 - val_loss: 0.6117 - val_accuracy: 0.6926\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6601 - val_loss: 0.6145 - val_accuracy: 0.6888\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6537 - val_loss: 0.6127 - val_accuracy: 0.6990\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6588 - val_loss: 0.6113 - val_accuracy: 0.6901\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6566 - val_loss: 0.6112 - val_accuracy: 0.6862\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6620 - val_loss: 0.6096 - val_accuracy: 0.7003\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6614 - val_loss: 0.6100 - val_accuracy: 0.6926\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6588 - val_loss: 0.6108 - val_accuracy: 0.6926\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6531 - val_loss: 0.6148 - val_accuracy: 0.6939\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6593 - val_loss: 0.6120 - val_accuracy: 0.6901\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6275 - accuracy: 0.6584 - val_loss: 0.6121 - val_accuracy: 0.6939\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6523 - val_loss: 0.6100 - val_accuracy: 0.6913\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.6615 - val_loss: 0.6085 - val_accuracy: 0.6990\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6589 - val_loss: 0.6144 - val_accuracy: 0.6888\n",
      "Calculating for: 750 50 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_312 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7893 - accuracy: 0.5373 - val_loss: 0.6534 - val_accuracy: 0.6429\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7096 - accuracy: 0.5528 - val_loss: 0.6440 - val_accuracy: 0.6531\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5705 - val_loss: 0.6411 - val_accuracy: 0.6696\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.5811 - val_loss: 0.6435 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6741 - accuracy: 0.5909 - val_loss: 0.6415 - val_accuracy: 0.6760\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.5958 - val_loss: 0.6428 - val_accuracy: 0.6735\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.5911 - val_loss: 0.6365 - val_accuracy: 0.6722\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6015 - val_loss: 0.6378 - val_accuracy: 0.6760\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6088 - val_loss: 0.6364 - val_accuracy: 0.6722\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6090 - val_loss: 0.6333 - val_accuracy: 0.6747\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6133 - val_loss: 0.6361 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6047 - val_loss: 0.6360 - val_accuracy: 0.6735\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6596 - accuracy: 0.6075 - val_loss: 0.6325 - val_accuracy: 0.6837\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6152 - val_loss: 0.6290 - val_accuracy: 0.6811\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6159 - val_loss: 0.6269 - val_accuracy: 0.6798\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6177 - val_loss: 0.6306 - val_accuracy: 0.6760\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6261 - val_loss: 0.6299 - val_accuracy: 0.6837\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6236 - val_loss: 0.6257 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6204 - val_loss: 0.6314 - val_accuracy: 0.6735\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6260 - val_loss: 0.6270 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6256 - val_loss: 0.6275 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6243 - val_loss: 0.6272 - val_accuracy: 0.6875\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6276 - val_loss: 0.6239 - val_accuracy: 0.6913\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6311 - val_loss: 0.6274 - val_accuracy: 0.6888\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6282 - val_loss: 0.6236 - val_accuracy: 0.6862\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6330 - val_loss: 0.6258 - val_accuracy: 0.6875\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6325 - val_loss: 0.6202 - val_accuracy: 0.6875\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6343 - val_loss: 0.6232 - val_accuracy: 0.6837\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6325 - val_loss: 0.6263 - val_accuracy: 0.6875\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6359 - val_loss: 0.6229 - val_accuracy: 0.6849\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6334 - val_loss: 0.6267 - val_accuracy: 0.6837\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6438 - val_loss: 0.6232 - val_accuracy: 0.6875\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6405 - accuracy: 0.6385 - val_loss: 0.6223 - val_accuracy: 0.6888\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6415 - val_loss: 0.6213 - val_accuracy: 0.6862\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6427 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6457 - val_loss: 0.6241 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6461 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6532 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6535 - val_loss: 0.6118 - val_accuracy: 0.6901\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6545 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6561 - val_loss: 0.6172 - val_accuracy: 0.6849\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6488 - val_loss: 0.6172 - val_accuracy: 0.6849\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6496 - val_loss: 0.6200 - val_accuracy: 0.6862\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6507 - val_loss: 0.6217 - val_accuracy: 0.6888\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6339 - accuracy: 0.6469 - val_loss: 0.6194 - val_accuracy: 0.6735\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6466 - val_loss: 0.6173 - val_accuracy: 0.6786\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6512 - val_loss: 0.6236 - val_accuracy: 0.6709\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6567 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6561 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6501 - val_loss: 0.6197 - val_accuracy: 0.6824\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6557 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6520 - val_loss: 0.6197 - val_accuracy: 0.6901\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6529 - val_loss: 0.6193 - val_accuracy: 0.6901\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6662 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6550 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6549 - val_loss: 0.6189 - val_accuracy: 0.6849\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6213 - accuracy: 0.6640 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6601 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6668 - val_loss: 0.6144 - val_accuracy: 0.6913\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6584 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6199 - accuracy: 0.6619 - val_loss: 0.6219 - val_accuracy: 0.6798\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6619 - val_loss: 0.6151 - val_accuracy: 0.6913\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6643 - val_loss: 0.6171 - val_accuracy: 0.6824\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6166 - accuracy: 0.6683 - val_loss: 0.6227 - val_accuracy: 0.6760\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6616 - val_loss: 0.6168 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.6672 - val_loss: 0.6181 - val_accuracy: 0.6849\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6645 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6203 - accuracy: 0.6630 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6074 - accuracy: 0.6776 - val_loss: 0.6189 - val_accuracy: 0.6824\n",
      "Calculating for: 750 50 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_316 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8143 - accuracy: 0.5136 - val_loss: 0.6771 - val_accuracy: 0.6365\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7340 - accuracy: 0.5286 - val_loss: 0.6625 - val_accuracy: 0.6569\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7002 - accuracy: 0.5504 - val_loss: 0.6564 - val_accuracy: 0.6543\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5671 - val_loss: 0.6543 - val_accuracy: 0.6658\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6815 - accuracy: 0.5703 - val_loss: 0.6532 - val_accuracy: 0.6607\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6781 - accuracy: 0.5777 - val_loss: 0.6512 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6723 - accuracy: 0.5829 - val_loss: 0.6521 - val_accuracy: 0.6569\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5814 - val_loss: 0.6501 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5923 - val_loss: 0.6457 - val_accuracy: 0.6607\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5845 - val_loss: 0.6475 - val_accuracy: 0.6556\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6005 - val_loss: 0.6432 - val_accuracy: 0.6620\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5916 - val_loss: 0.6459 - val_accuracy: 0.6594\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5988 - val_loss: 0.6425 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5917 - val_loss: 0.6374 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6022 - val_loss: 0.6344 - val_accuracy: 0.6862\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5985 - val_loss: 0.6396 - val_accuracy: 0.6684\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6045 - val_loss: 0.6358 - val_accuracy: 0.6837\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6029 - val_loss: 0.6379 - val_accuracy: 0.6773\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.6000 - val_loss: 0.6319 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6640 - accuracy: 0.6027 - val_loss: 0.6428 - val_accuracy: 0.6620\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6154 - val_loss: 0.6423 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6137 - val_loss: 0.6353 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6107 - val_loss: 0.6363 - val_accuracy: 0.6684\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6591 - accuracy: 0.6154 - val_loss: 0.6342 - val_accuracy: 0.6811\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6047 - val_loss: 0.6362 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6158 - val_loss: 0.6428 - val_accuracy: 0.6620\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6150 - val_loss: 0.6385 - val_accuracy: 0.6658\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6182 - val_loss: 0.6351 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6183 - val_loss: 0.6325 - val_accuracy: 0.6722\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6223 - val_loss: 0.6326 - val_accuracy: 0.6773\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6207 - val_loss: 0.6303 - val_accuracy: 0.6811\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6269 - val_loss: 0.6303 - val_accuracy: 0.6811\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6555 - accuracy: 0.6196 - val_loss: 0.6278 - val_accuracy: 0.6837\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6171 - val_loss: 0.6338 - val_accuracy: 0.6658\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6201 - val_loss: 0.6317 - val_accuracy: 0.6760\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6211 - val_loss: 0.6287 - val_accuracy: 0.6862\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6323 - val_accuracy: 0.6773\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6262 - val_loss: 0.6289 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6281 - val_loss: 0.6266 - val_accuracy: 0.6849\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6262 - val_loss: 0.6279 - val_accuracy: 0.6875\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6233 - val_loss: 0.6265 - val_accuracy: 0.6901\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6482 - accuracy: 0.6285 - val_loss: 0.6287 - val_accuracy: 0.6862\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6329 - val_loss: 0.6276 - val_accuracy: 0.6773\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6500 - accuracy: 0.6297 - val_loss: 0.6271 - val_accuracy: 0.6901\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6306 - val_loss: 0.6338 - val_accuracy: 0.6798\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6304 - val_loss: 0.6280 - val_accuracy: 0.6849\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6402 - val_loss: 0.6270 - val_accuracy: 0.6862\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6320 - val_loss: 0.6254 - val_accuracy: 0.6875\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6326 - val_loss: 0.6271 - val_accuracy: 0.6990\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6358 - val_loss: 0.6243 - val_accuracy: 0.6913\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6271 - val_loss: 0.6240 - val_accuracy: 0.6811\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6404 - val_loss: 0.6216 - val_accuracy: 0.6875\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6326 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6366 - val_loss: 0.6209 - val_accuracy: 0.6811\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6366 - val_loss: 0.6210 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6319 - val_loss: 0.6229 - val_accuracy: 0.6837\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6351 - val_loss: 0.6194 - val_accuracy: 0.6926\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6393 - val_loss: 0.6243 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6408 - accuracy: 0.6323 - val_loss: 0.6242 - val_accuracy: 0.6837\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6420 - val_loss: 0.6205 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6377 - val_loss: 0.6215 - val_accuracy: 0.6811\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6403 - val_loss: 0.6235 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6482 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6487 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6393 - accuracy: 0.6402 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6448 - val_loss: 0.6288 - val_accuracy: 0.6645\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6390 - val_loss: 0.6194 - val_accuracy: 0.6875\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6319 - accuracy: 0.6517 - val_loss: 0.6157 - val_accuracy: 0.6926\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6458 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6469 - val_loss: 0.6209 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6480 - val_loss: 0.6153 - val_accuracy: 0.6926\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6447 - val_loss: 0.6189 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6518 - val_loss: 0.6204 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6487 - val_loss: 0.6195 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6550 - val_loss: 0.6179 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6546 - val_loss: 0.6150 - val_accuracy: 0.6952\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6522 - val_loss: 0.6180 - val_accuracy: 0.6990\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6500 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6501 - val_loss: 0.6175 - val_accuracy: 0.6926\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6535 - val_loss: 0.6196 - val_accuracy: 0.6837\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6545 - val_loss: 0.6185 - val_accuracy: 0.6926\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6523 - val_loss: 0.6130 - val_accuracy: 0.6977\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6541 - val_loss: 0.6188 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6560 - val_loss: 0.6159 - val_accuracy: 0.6875\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6508 - val_loss: 0.6180 - val_accuracy: 0.6747\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6259 - accuracy: 0.6518 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6557 - val_loss: 0.6150 - val_accuracy: 0.6939\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6552 - val_loss: 0.6159 - val_accuracy: 0.6901\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6520 - val_loss: 0.6132 - val_accuracy: 0.6964\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6644 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6575 - val_loss: 0.6153 - val_accuracy: 0.6939\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6604 - val_loss: 0.6149 - val_accuracy: 0.6939\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6611 - val_loss: 0.6161 - val_accuracy: 0.6926\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6628 - val_loss: 0.6178 - val_accuracy: 0.6837\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6613 - val_loss: 0.6177 - val_accuracy: 0.6837\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6629 - val_loss: 0.6145 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6631 - val_loss: 0.6155 - val_accuracy: 0.6888\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6554 - val_loss: 0.6154 - val_accuracy: 0.6849\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6225 - accuracy: 0.6647 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6640 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6577 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6179 - accuracy: 0.6683 - val_loss: 0.6148 - val_accuracy: 0.6901\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6228 - accuracy: 0.6633 - val_loss: 0.6156 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6658 - val_loss: 0.6180 - val_accuracy: 0.6875\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6667 - val_loss: 0.6152 - val_accuracy: 0.6811\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6717 - val_loss: 0.6155 - val_accuracy: 0.6849\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6693 - val_loss: 0.6168 - val_accuracy: 0.6849\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6624 - val_loss: 0.6174 - val_accuracy: 0.6837\n",
      "Calculating for: 750 50 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_320 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8224 - accuracy: 0.5231 - val_loss: 0.6602 - val_accuracy: 0.6390\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7400 - accuracy: 0.5262 - val_loss: 0.6585 - val_accuracy: 0.6441\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7060 - accuracy: 0.5377 - val_loss: 0.6631 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5456 - val_loss: 0.6551 - val_accuracy: 0.6454\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5522 - val_loss: 0.6593 - val_accuracy: 0.6352\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5496 - val_loss: 0.6585 - val_accuracy: 0.6467\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.5666 - val_loss: 0.6572 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5676 - val_loss: 0.6502 - val_accuracy: 0.6416\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6795 - accuracy: 0.5667 - val_loss: 0.6545 - val_accuracy: 0.6531\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6792 - accuracy: 0.5688 - val_loss: 0.6520 - val_accuracy: 0.6505\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6793 - accuracy: 0.5656 - val_loss: 0.6557 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5776 - val_loss: 0.6454 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6817 - accuracy: 0.5695 - val_loss: 0.6523 - val_accuracy: 0.6543\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6746 - accuracy: 0.5845 - val_loss: 0.6466 - val_accuracy: 0.6556\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5756 - val_loss: 0.6457 - val_accuracy: 0.6531\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6722 - accuracy: 0.5813 - val_loss: 0.6442 - val_accuracy: 0.6709\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5737 - val_loss: 0.6449 - val_accuracy: 0.6696\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5838 - val_loss: 0.6485 - val_accuracy: 0.6607\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6712 - accuracy: 0.5904 - val_loss: 0.6421 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6717 - accuracy: 0.5928 - val_loss: 0.6426 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.5894 - val_loss: 0.6485 - val_accuracy: 0.6505\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6705 - accuracy: 0.5884 - val_loss: 0.6455 - val_accuracy: 0.6633\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5869 - val_loss: 0.6434 - val_accuracy: 0.6671\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.5944 - val_loss: 0.6404 - val_accuracy: 0.6696\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6691 - accuracy: 0.5943 - val_loss: 0.6391 - val_accuracy: 0.6684\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5922 - val_loss: 0.6436 - val_accuracy: 0.6658\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5929 - val_loss: 0.6422 - val_accuracy: 0.6671\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5990 - val_loss: 0.6447 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5948 - val_loss: 0.6388 - val_accuracy: 0.6722\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6693 - accuracy: 0.5929 - val_loss: 0.6406 - val_accuracy: 0.6722\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.6012 - val_loss: 0.6399 - val_accuracy: 0.6633\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.6009 - val_loss: 0.6391 - val_accuracy: 0.6607\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6683 - accuracy: 0.5968 - val_loss: 0.6372 - val_accuracy: 0.6658\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.5941 - val_loss: 0.6379 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6015 - val_loss: 0.6353 - val_accuracy: 0.6658\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6078 - val_loss: 0.6398 - val_accuracy: 0.6658\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6093 - val_loss: 0.6358 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6034 - val_loss: 0.6384 - val_accuracy: 0.6658\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6630 - accuracy: 0.6109 - val_loss: 0.6343 - val_accuracy: 0.6722\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6638 - accuracy: 0.5997 - val_loss: 0.6386 - val_accuracy: 0.6620\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6631 - accuracy: 0.6068 - val_loss: 0.6337 - val_accuracy: 0.6671\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6084 - val_loss: 0.6341 - val_accuracy: 0.6645\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6004 - val_loss: 0.6342 - val_accuracy: 0.6658\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6599 - accuracy: 0.6120 - val_loss: 0.6358 - val_accuracy: 0.6671\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6576 - accuracy: 0.6181 - val_loss: 0.6354 - val_accuracy: 0.6696\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6576 - accuracy: 0.6138 - val_loss: 0.6336 - val_accuracy: 0.6645\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6076 - val_loss: 0.6346 - val_accuracy: 0.6722\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6108 - val_loss: 0.6358 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6583 - accuracy: 0.6177 - val_loss: 0.6334 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6606 - accuracy: 0.6129 - val_loss: 0.6313 - val_accuracy: 0.6747\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6117 - val_loss: 0.6339 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6600 - accuracy: 0.6098 - val_loss: 0.6316 - val_accuracy: 0.6633\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6159 - val_loss: 0.6289 - val_accuracy: 0.6722\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6571 - accuracy: 0.6127 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6267 - val_loss: 0.6303 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6071 - val_loss: 0.6359 - val_accuracy: 0.6696\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6186 - val_loss: 0.6268 - val_accuracy: 0.6709\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6112 - val_loss: 0.6344 - val_accuracy: 0.6760\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6187 - val_loss: 0.6255 - val_accuracy: 0.6735\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6162 - val_loss: 0.6296 - val_accuracy: 0.6773\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6242 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6197 - val_loss: 0.6301 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6230 - val_loss: 0.6283 - val_accuracy: 0.6735\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6149 - val_loss: 0.6338 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6133 - val_loss: 0.6322 - val_accuracy: 0.6811\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6208 - val_loss: 0.6290 - val_accuracy: 0.6824\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6257 - val_loss: 0.6255 - val_accuracy: 0.6709\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6262 - val_loss: 0.6297 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6196 - val_loss: 0.6292 - val_accuracy: 0.6837\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6238 - val_loss: 0.6273 - val_accuracy: 0.6760\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6183 - val_loss: 0.6338 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6194 - val_loss: 0.6309 - val_accuracy: 0.6722\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6230 - val_loss: 0.6283 - val_accuracy: 0.6811\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6262 - val_loss: 0.6233 - val_accuracy: 0.6773\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6201 - val_loss: 0.6256 - val_accuracy: 0.6875\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6247 - val_loss: 0.6306 - val_accuracy: 0.6773\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6267 - val_loss: 0.6277 - val_accuracy: 0.6824\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6264 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6287 - val_loss: 0.6239 - val_accuracy: 0.6824\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6257 - val_loss: 0.6251 - val_accuracy: 0.6824\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6305 - val_loss: 0.6251 - val_accuracy: 0.6837\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6335 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6506 - accuracy: 0.6305 - val_loss: 0.6253 - val_accuracy: 0.6837\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6336 - val_loss: 0.6221 - val_accuracy: 0.6849\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6296 - val_loss: 0.6221 - val_accuracy: 0.6849\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6286 - val_loss: 0.6267 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6296 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6275 - val_loss: 0.6237 - val_accuracy: 0.6849\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6334 - val_loss: 0.6214 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6265 - val_loss: 0.6235 - val_accuracy: 0.6837\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6325 - val_loss: 0.6256 - val_accuracy: 0.6798\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6306 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6338 - val_loss: 0.6248 - val_accuracy: 0.6786\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6407 - val_loss: 0.6238 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6364 - val_loss: 0.6209 - val_accuracy: 0.6913\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6494 - accuracy: 0.6330 - val_loss: 0.6229 - val_accuracy: 0.6862\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6384 - val_loss: 0.6203 - val_accuracy: 0.6901\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6324 - val_loss: 0.6212 - val_accuracy: 0.6901\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6339 - val_loss: 0.6192 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6445 - accuracy: 0.6321 - val_loss: 0.6250 - val_accuracy: 0.6824\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6355 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6355 - val_loss: 0.6199 - val_accuracy: 0.6760\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6410 - accuracy: 0.6379 - val_loss: 0.6160 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6479 - accuracy: 0.6333 - val_loss: 0.6218 - val_accuracy: 0.6849\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6418 - val_loss: 0.6236 - val_accuracy: 0.6684\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6405 - val_loss: 0.6222 - val_accuracy: 0.6862\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6319 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6390 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6453 - val_loss: 0.6166 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6370 - val_loss: 0.6208 - val_accuracy: 0.6811\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6427 - accuracy: 0.6366 - val_loss: 0.6197 - val_accuracy: 0.6811\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6403 - val_loss: 0.6201 - val_accuracy: 0.6798\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6338 - val_loss: 0.6188 - val_accuracy: 0.6837\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6395 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6452 - val_loss: 0.6172 - val_accuracy: 0.6773\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6404 - val_loss: 0.6196 - val_accuracy: 0.6849\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6463 - val_loss: 0.6212 - val_accuracy: 0.6862\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6428 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6477 - val_loss: 0.6177 - val_accuracy: 0.6824\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6443 - val_loss: 0.6200 - val_accuracy: 0.6824\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6420 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6452 - val_loss: 0.6172 - val_accuracy: 0.6875\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6366 - val_loss: 0.6192 - val_accuracy: 0.6849\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6511 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6441 - val_loss: 0.6147 - val_accuracy: 0.6849\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6452 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6439 - val_loss: 0.6147 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6481 - val_loss: 0.6128 - val_accuracy: 0.6888\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6531 - val_loss: 0.6143 - val_accuracy: 0.6837\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6478 - val_loss: 0.6166 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6453 - val_loss: 0.6170 - val_accuracy: 0.6913\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6375 - val_loss: 0.6150 - val_accuracy: 0.6875\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6490 - val_loss: 0.6133 - val_accuracy: 0.6837\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6413 - val_loss: 0.6148 - val_accuracy: 0.6913\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6536 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6459 - val_loss: 0.6163 - val_accuracy: 0.6913\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6483 - val_loss: 0.6141 - val_accuracy: 0.6901\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6456 - val_loss: 0.6181 - val_accuracy: 0.6964\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6501 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6354 - accuracy: 0.6488 - val_loss: 0.6135 - val_accuracy: 0.6901\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6500 - val_loss: 0.6141 - val_accuracy: 0.6952\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6468 - val_loss: 0.6134 - val_accuracy: 0.6990\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6513 - val_loss: 0.6167 - val_accuracy: 0.6913\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6486 - val_loss: 0.6201 - val_accuracy: 0.6901\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6472 - val_loss: 0.6157 - val_accuracy: 0.6990\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6463 - val_loss: 0.6118 - val_accuracy: 0.6913\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6492 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6503 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6510 - val_loss: 0.6114 - val_accuracy: 0.6939\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6506 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6560 - val_loss: 0.6103 - val_accuracy: 0.6977\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6531 - val_loss: 0.6166 - val_accuracy: 0.6926\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6555 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6535 - val_loss: 0.6135 - val_accuracy: 0.6939\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6590 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6567 - val_loss: 0.6104 - val_accuracy: 0.6913\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6595 - val_loss: 0.6117 - val_accuracy: 0.6901\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6544 - val_loss: 0.6088 - val_accuracy: 0.6888\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6527 - val_loss: 0.6154 - val_accuracy: 0.6837\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6554 - val_loss: 0.6106 - val_accuracy: 0.6888\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6544 - val_loss: 0.6143 - val_accuracy: 0.6888\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6546 - val_loss: 0.6107 - val_accuracy: 0.6913\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6527 - val_loss: 0.6133 - val_accuracy: 0.6888\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6605 - val_loss: 0.6098 - val_accuracy: 0.6875\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6588 - val_loss: 0.6108 - val_accuracy: 0.6926\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6585 - val_loss: 0.6117 - val_accuracy: 0.6901\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6589 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6536 - val_loss: 0.6151 - val_accuracy: 0.6939\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6645 - val_loss: 0.6080 - val_accuracy: 0.6888\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6569 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6625 - val_loss: 0.6114 - val_accuracy: 0.6939\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6633 - val_loss: 0.6107 - val_accuracy: 0.6913\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6511 - val_loss: 0.6087 - val_accuracy: 0.6952\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6603 - val_loss: 0.6101 - val_accuracy: 0.6875\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6539 - val_loss: 0.6127 - val_accuracy: 0.6888\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6575 - val_loss: 0.6100 - val_accuracy: 0.6913\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6541 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6614 - val_loss: 0.6111 - val_accuracy: 0.6926\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6491 - val_loss: 0.6143 - val_accuracy: 0.6862\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6574 - val_loss: 0.6135 - val_accuracy: 0.6849\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6667 - val_loss: 0.6111 - val_accuracy: 0.6913\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6571 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6605 - val_loss: 0.6144 - val_accuracy: 0.6849\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6562 - val_loss: 0.6151 - val_accuracy: 0.6901\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6546 - val_loss: 0.6125 - val_accuracy: 0.6952\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6576 - val_loss: 0.6116 - val_accuracy: 0.6901\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6584 - val_loss: 0.6082 - val_accuracy: 0.6901\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6667 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6658 - val_loss: 0.6115 - val_accuracy: 0.6901\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6585 - val_loss: 0.6131 - val_accuracy: 0.6926\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6677 - val_loss: 0.6135 - val_accuracy: 0.6862\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6635 - val_loss: 0.6124 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6640 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6702 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6652 - val_loss: 0.6087 - val_accuracy: 0.6875\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6581 - val_loss: 0.6115 - val_accuracy: 0.6913\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6594 - val_loss: 0.6118 - val_accuracy: 0.6939\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6601 - val_loss: 0.6096 - val_accuracy: 0.6926\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6584 - val_loss: 0.6118 - val_accuracy: 0.7028\n",
      "Calculating for: 750 100 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_324 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8121 - accuracy: 0.5428 - val_loss: 0.6473 - val_accuracy: 0.6518\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7294 - accuracy: 0.5534 - val_loss: 0.6378 - val_accuracy: 0.6607\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.5672 - val_loss: 0.6415 - val_accuracy: 0.6671\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5786 - val_loss: 0.6410 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5836 - val_loss: 0.6384 - val_accuracy: 0.6633\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5955 - val_loss: 0.6428 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6656 - accuracy: 0.5978 - val_loss: 0.6379 - val_accuracy: 0.6658\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6086 - val_loss: 0.6423 - val_accuracy: 0.6658\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6040 - val_loss: 0.6360 - val_accuracy: 0.6671\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6091 - val_loss: 0.6357 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6598 - accuracy: 0.6086 - val_loss: 0.6374 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6578 - accuracy: 0.6133 - val_loss: 0.6359 - val_accuracy: 0.6709\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6150 - val_loss: 0.6370 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6207 - val_loss: 0.6310 - val_accuracy: 0.6735\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6144 - val_loss: 0.6267 - val_accuracy: 0.6824\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6157 - val_loss: 0.6302 - val_accuracy: 0.6824\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6479 - accuracy: 0.6236 - val_loss: 0.6278 - val_accuracy: 0.6849\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6168 - val_loss: 0.6304 - val_accuracy: 0.6798\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6261 - val_loss: 0.6314 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6289 - val_loss: 0.6295 - val_accuracy: 0.6747\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6248 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6274 - val_loss: 0.6247 - val_accuracy: 0.6773\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6286 - val_loss: 0.6256 - val_accuracy: 0.6837\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6320 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6290 - val_loss: 0.6279 - val_accuracy: 0.6735\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6305 - val_loss: 0.6271 - val_accuracy: 0.6760\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6378 - val_loss: 0.6251 - val_accuracy: 0.6811\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6419 - accuracy: 0.6316 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6392 - accuracy: 0.6378 - val_loss: 0.6224 - val_accuracy: 0.6824\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6390 - val_loss: 0.6228 - val_accuracy: 0.6837\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6304 - val_loss: 0.6225 - val_accuracy: 0.6837\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6377 - val_loss: 0.6207 - val_accuracy: 0.6798\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6394 - val_loss: 0.6225 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6510 - val_loss: 0.6194 - val_accuracy: 0.6837\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6466 - val_loss: 0.6221 - val_accuracy: 0.6735\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6395 - val_loss: 0.6227 - val_accuracy: 0.6786\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6424 - val_loss: 0.6216 - val_accuracy: 0.6798\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6477 - val_loss: 0.6189 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6433 - val_loss: 0.6179 - val_accuracy: 0.6773\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6482 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6545 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6487 - val_loss: 0.6245 - val_accuracy: 0.6747\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6547 - val_loss: 0.6223 - val_accuracy: 0.6735\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6535 - val_loss: 0.6188 - val_accuracy: 0.6786\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6547 - val_loss: 0.6182 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6520 - val_loss: 0.6193 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6495 - val_loss: 0.6210 - val_accuracy: 0.6773\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6520 - val_loss: 0.6185 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6590 - val_loss: 0.6190 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6647 - val_loss: 0.6217 - val_accuracy: 0.6735\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6581 - val_loss: 0.6168 - val_accuracy: 0.6786\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6660 - val_loss: 0.6220 - val_accuracy: 0.6773\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6609 - val_loss: 0.6239 - val_accuracy: 0.6747\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6625 - val_loss: 0.6197 - val_accuracy: 0.6773\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6177 - accuracy: 0.6576 - val_loss: 0.6248 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6137 - accuracy: 0.6590 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6615 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6621 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6637 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6724 - val_loss: 0.6253 - val_accuracy: 0.6735\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6693 - val_loss: 0.6181 - val_accuracy: 0.6735\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6106 - accuracy: 0.6691 - val_loss: 0.6189 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6120 - accuracy: 0.6652 - val_loss: 0.6207 - val_accuracy: 0.6747\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6090 - accuracy: 0.6747 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6083 - accuracy: 0.6768 - val_loss: 0.6245 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6083 - accuracy: 0.6740 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6092 - accuracy: 0.6740 - val_loss: 0.6221 - val_accuracy: 0.6735\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6716 - val_loss: 0.6205 - val_accuracy: 0.6773\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6746 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6093 - accuracy: 0.6707 - val_loss: 0.6265 - val_accuracy: 0.6594\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6025 - accuracy: 0.6762 - val_loss: 0.6226 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6038 - accuracy: 0.6768 - val_loss: 0.6252 - val_accuracy: 0.6696\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6066 - accuracy: 0.6707 - val_loss: 0.6218 - val_accuracy: 0.6722\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6796 - val_loss: 0.6233 - val_accuracy: 0.6773\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5997 - accuracy: 0.6822 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5982 - accuracy: 0.6807 - val_loss: 0.6288 - val_accuracy: 0.6633\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5986 - accuracy: 0.6806 - val_loss: 0.6276 - val_accuracy: 0.6658\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6032 - accuracy: 0.6804 - val_loss: 0.6265 - val_accuracy: 0.6722\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6008 - accuracy: 0.6817 - val_loss: 0.6243 - val_accuracy: 0.6760\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6001 - accuracy: 0.6855 - val_loss: 0.6291 - val_accuracy: 0.6760\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6022 - accuracy: 0.6796 - val_loss: 0.6253 - val_accuracy: 0.6824\n",
      "Calculating for: 750 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_328 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8021 - accuracy: 0.5197 - val_loss: 0.6681 - val_accuracy: 0.6416\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7135 - accuracy: 0.5467 - val_loss: 0.6592 - val_accuracy: 0.6518\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5574 - val_loss: 0.6618 - val_accuracy: 0.6390\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6837 - accuracy: 0.5673 - val_loss: 0.6586 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.5671 - val_loss: 0.6620 - val_accuracy: 0.6390\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6765 - accuracy: 0.5850 - val_loss: 0.6543 - val_accuracy: 0.6543\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5841 - val_loss: 0.6507 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5814 - val_loss: 0.6519 - val_accuracy: 0.6454\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5893 - val_loss: 0.6546 - val_accuracy: 0.6543\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5899 - val_loss: 0.6538 - val_accuracy: 0.6594\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5948 - val_loss: 0.6539 - val_accuracy: 0.6492\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.5927 - val_loss: 0.6502 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5953 - val_loss: 0.6492 - val_accuracy: 0.6492\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.5986 - val_loss: 0.6444 - val_accuracy: 0.6671\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6616 - accuracy: 0.6042 - val_loss: 0.6403 - val_accuracy: 0.6735\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6143 - val_loss: 0.6469 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6099 - val_loss: 0.6493 - val_accuracy: 0.6518\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6063 - val_loss: 0.6463 - val_accuracy: 0.6582\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6052 - val_loss: 0.6456 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6607 - accuracy: 0.6039 - val_loss: 0.6400 - val_accuracy: 0.6684\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6591 - accuracy: 0.6109 - val_loss: 0.6472 - val_accuracy: 0.6492\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6178 - val_loss: 0.6457 - val_accuracy: 0.6531\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6132 - val_loss: 0.6427 - val_accuracy: 0.6582\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6129 - val_loss: 0.6435 - val_accuracy: 0.6607\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6177 - val_loss: 0.6416 - val_accuracy: 0.6645\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6139 - val_loss: 0.6346 - val_accuracy: 0.6773\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6144 - val_loss: 0.6424 - val_accuracy: 0.6518\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6138 - val_loss: 0.6363 - val_accuracy: 0.6684\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6173 - val_loss: 0.6379 - val_accuracy: 0.6645\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6235 - val_loss: 0.6352 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6198 - val_loss: 0.6332 - val_accuracy: 0.6786\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6243 - val_loss: 0.6414 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6150 - val_loss: 0.6386 - val_accuracy: 0.6633\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6250 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6207 - val_loss: 0.6310 - val_accuracy: 0.6773\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6222 - val_loss: 0.6316 - val_accuracy: 0.6747\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6284 - val_loss: 0.6345 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6262 - val_loss: 0.6293 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6310 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6368 - val_loss: 0.6354 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6240 - val_loss: 0.6306 - val_accuracy: 0.6798\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6302 - val_loss: 0.6357 - val_accuracy: 0.6658\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6291 - val_loss: 0.6343 - val_accuracy: 0.6722\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6272 - val_loss: 0.6350 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6354 - val_loss: 0.6371 - val_accuracy: 0.6582\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6351 - val_loss: 0.6317 - val_accuracy: 0.6760\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6310 - val_loss: 0.6329 - val_accuracy: 0.6760\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6359 - val_loss: 0.6326 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6336 - val_loss: 0.6320 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6306 - val_loss: 0.6264 - val_accuracy: 0.6798\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6359 - val_loss: 0.6293 - val_accuracy: 0.6747\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6412 - val_loss: 0.6304 - val_accuracy: 0.6760\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6292 - val_loss: 0.6249 - val_accuracy: 0.6837\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6404 - accuracy: 0.6398 - val_loss: 0.6305 - val_accuracy: 0.6747\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6301 - val_loss: 0.6345 - val_accuracy: 0.6709\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6372 - val_loss: 0.6322 - val_accuracy: 0.6684\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6384 - val_loss: 0.6270 - val_accuracy: 0.6696\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6389 - val_loss: 0.6233 - val_accuracy: 0.6786\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6355 - val_loss: 0.6294 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6423 - val_loss: 0.6217 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6365 - val_loss: 0.6238 - val_accuracy: 0.6747\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6429 - val_loss: 0.6278 - val_accuracy: 0.6709\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6354 - val_loss: 0.6244 - val_accuracy: 0.6773\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6433 - val_loss: 0.6314 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6433 - val_loss: 0.6276 - val_accuracy: 0.6709\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6442 - val_loss: 0.6313 - val_accuracy: 0.6709\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6463 - val_loss: 0.6277 - val_accuracy: 0.6722\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6517 - val_loss: 0.6233 - val_accuracy: 0.6722\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6481 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6345 - accuracy: 0.6477 - val_loss: 0.6282 - val_accuracy: 0.6735\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6448 - val_loss: 0.6285 - val_accuracy: 0.6735\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6518 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6473 - val_loss: 0.6239 - val_accuracy: 0.6735\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6520 - val_loss: 0.6219 - val_accuracy: 0.6709\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6550 - val_loss: 0.6230 - val_accuracy: 0.6760\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6510 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6476 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6506 - val_loss: 0.6269 - val_accuracy: 0.6773\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6575 - val_loss: 0.6179 - val_accuracy: 0.6824\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6539 - val_loss: 0.6228 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6508 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6531 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6611 - val_loss: 0.6252 - val_accuracy: 0.6773\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6559 - val_loss: 0.6238 - val_accuracy: 0.6760\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6549 - val_loss: 0.6236 - val_accuracy: 0.6722\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6551 - val_loss: 0.6296 - val_accuracy: 0.6735\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6566 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6522 - val_loss: 0.6256 - val_accuracy: 0.6747\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6566 - val_loss: 0.6251 - val_accuracy: 0.6722\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6541 - val_loss: 0.6207 - val_accuracy: 0.6722\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6520 - val_loss: 0.6279 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6680 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6188 - accuracy: 0.6555 - val_loss: 0.6246 - val_accuracy: 0.6798\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6626 - val_loss: 0.6194 - val_accuracy: 0.6747\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6529 - val_loss: 0.6283 - val_accuracy: 0.6798\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6583 - val_loss: 0.6209 - val_accuracy: 0.6786\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6590 - val_loss: 0.6221 - val_accuracy: 0.6786\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6588 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6643 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6629 - val_loss: 0.6235 - val_accuracy: 0.6798\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6648 - val_loss: 0.6221 - val_accuracy: 0.6773\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6598 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6615 - val_loss: 0.6215 - val_accuracy: 0.6760\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6219 - accuracy: 0.6613 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6214 - accuracy: 0.6631 - val_loss: 0.6235 - val_accuracy: 0.6773\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6609 - val_loss: 0.6240 - val_accuracy: 0.6824\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6618 - val_loss: 0.6174 - val_accuracy: 0.6837\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6623 - val_loss: 0.6266 - val_accuracy: 0.6773\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6598 - val_loss: 0.6220 - val_accuracy: 0.6798\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6614 - val_loss: 0.6232 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6618 - val_loss: 0.6261 - val_accuracy: 0.6747\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6663 - val_loss: 0.6242 - val_accuracy: 0.6773\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6117 - accuracy: 0.6728 - val_loss: 0.6323 - val_accuracy: 0.6760\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6134 - accuracy: 0.6677 - val_loss: 0.6290 - val_accuracy: 0.6709\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6122 - accuracy: 0.6729 - val_loss: 0.6258 - val_accuracy: 0.6735\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6731 - val_loss: 0.6322 - val_accuracy: 0.6735\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6121 - accuracy: 0.6719 - val_loss: 0.6296 - val_accuracy: 0.6696\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6152 - accuracy: 0.6688 - val_loss: 0.6272 - val_accuracy: 0.6773\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6084 - accuracy: 0.6740 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6148 - accuracy: 0.6670 - val_loss: 0.6263 - val_accuracy: 0.6786\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6757 - val_loss: 0.6280 - val_accuracy: 0.6773\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6649 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6775 - val_loss: 0.6282 - val_accuracy: 0.6722\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6108 - accuracy: 0.6729 - val_loss: 0.6295 - val_accuracy: 0.6786\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6084 - accuracy: 0.6734 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6086 - accuracy: 0.6763 - val_loss: 0.6242 - val_accuracy: 0.6798\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6740 - val_loss: 0.6276 - val_accuracy: 0.6798\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6072 - accuracy: 0.6762 - val_loss: 0.6287 - val_accuracy: 0.6837\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6083 - accuracy: 0.6738 - val_loss: 0.6303 - val_accuracy: 0.6684\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6057 - accuracy: 0.6772 - val_loss: 0.6261 - val_accuracy: 0.6824\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6018 - accuracy: 0.6826 - val_loss: 0.6247 - val_accuracy: 0.6837\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6094 - accuracy: 0.6726 - val_loss: 0.6235 - val_accuracy: 0.6849\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6067 - accuracy: 0.6746 - val_loss: 0.6232 - val_accuracy: 0.6862\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6041 - accuracy: 0.6809 - val_loss: 0.6283 - val_accuracy: 0.6824\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6056 - accuracy: 0.6787 - val_loss: 0.6248 - val_accuracy: 0.6824\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6080 - accuracy: 0.6766 - val_loss: 0.6248 - val_accuracy: 0.6824\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6037 - accuracy: 0.6766 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Calculating for: 750 100 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_332 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8436 - accuracy: 0.5099 - val_loss: 0.6710 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7321 - accuracy: 0.5328 - val_loss: 0.6658 - val_accuracy: 0.6454\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6997 - accuracy: 0.5349 - val_loss: 0.6622 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6882 - accuracy: 0.5525 - val_loss: 0.6644 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.5593 - val_loss: 0.6621 - val_accuracy: 0.6645\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.5568 - val_loss: 0.6627 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5657 - val_loss: 0.6628 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6844 - accuracy: 0.5587 - val_loss: 0.6604 - val_accuracy: 0.6569\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5629 - val_loss: 0.6609 - val_accuracy: 0.6594\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6808 - accuracy: 0.5687 - val_loss: 0.6564 - val_accuracy: 0.6594\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5732 - val_loss: 0.6533 - val_accuracy: 0.6671\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.5723 - val_loss: 0.6529 - val_accuracy: 0.6620\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.5767 - val_loss: 0.6493 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.5869 - val_loss: 0.6519 - val_accuracy: 0.6658\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5782 - val_loss: 0.6512 - val_accuracy: 0.6671\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5796 - val_loss: 0.6513 - val_accuracy: 0.6569\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5809 - val_loss: 0.6464 - val_accuracy: 0.6658\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6730 - accuracy: 0.5831 - val_loss: 0.6491 - val_accuracy: 0.6556\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6710 - accuracy: 0.5970 - val_loss: 0.6526 - val_accuracy: 0.6531\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5904 - val_loss: 0.6424 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5864 - val_loss: 0.6468 - val_accuracy: 0.6543\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.6010 - val_loss: 0.6445 - val_accuracy: 0.6543\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5948 - val_loss: 0.6484 - val_accuracy: 0.6518\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6667 - accuracy: 0.5972 - val_loss: 0.6433 - val_accuracy: 0.6607\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5950 - val_loss: 0.6466 - val_accuracy: 0.6569\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6017 - val_loss: 0.6462 - val_accuracy: 0.6633\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5955 - val_loss: 0.6407 - val_accuracy: 0.6633\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6032 - val_loss: 0.6477 - val_accuracy: 0.6607\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6069 - val_loss: 0.6419 - val_accuracy: 0.6594\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5966 - val_loss: 0.6411 - val_accuracy: 0.6633\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6027 - val_loss: 0.6430 - val_accuracy: 0.6620\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6010 - val_loss: 0.6445 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6086 - val_loss: 0.6364 - val_accuracy: 0.6658\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6073 - val_loss: 0.6401 - val_accuracy: 0.6684\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6105 - val_loss: 0.6350 - val_accuracy: 0.6735\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6054 - val_loss: 0.6408 - val_accuracy: 0.6645\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6027 - val_loss: 0.6412 - val_accuracy: 0.6760\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6089 - val_loss: 0.6412 - val_accuracy: 0.6645\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6105 - val_loss: 0.6393 - val_accuracy: 0.6671\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6138 - val_loss: 0.6362 - val_accuracy: 0.6709\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6104 - val_loss: 0.6350 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6164 - val_loss: 0.6351 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6595 - accuracy: 0.6172 - val_loss: 0.6382 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6095 - val_loss: 0.6361 - val_accuracy: 0.6786\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6154 - val_loss: 0.6403 - val_accuracy: 0.6722\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6129 - val_loss: 0.6383 - val_accuracy: 0.6722\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6166 - val_loss: 0.6358 - val_accuracy: 0.6747\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6118 - val_loss: 0.6479 - val_accuracy: 0.6276\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6135 - val_loss: 0.6366 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6133 - val_loss: 0.6340 - val_accuracy: 0.6837\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6198 - val_loss: 0.6433 - val_accuracy: 0.6416\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6139 - val_loss: 0.6323 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6235 - val_loss: 0.6375 - val_accuracy: 0.6658\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6162 - val_loss: 0.6313 - val_accuracy: 0.6773\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6178 - val_loss: 0.6323 - val_accuracy: 0.6798\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6109 - val_loss: 0.6342 - val_accuracy: 0.6811\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6212 - val_loss: 0.6292 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6179 - val_loss: 0.6363 - val_accuracy: 0.6620\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6257 - val_loss: 0.6307 - val_accuracy: 0.6811\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6245 - val_loss: 0.6315 - val_accuracy: 0.6837\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6206 - val_loss: 0.6431 - val_accuracy: 0.6416\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6199 - val_loss: 0.6369 - val_accuracy: 0.6620\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6246 - val_loss: 0.6315 - val_accuracy: 0.6773\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6227 - val_loss: 0.6358 - val_accuracy: 0.6531\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6230 - val_loss: 0.6313 - val_accuracy: 0.6786\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6258 - val_loss: 0.6250 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6267 - val_loss: 0.6288 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6290 - val_loss: 0.6264 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6250 - val_loss: 0.6284 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6285 - val_loss: 0.6284 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6192 - val_loss: 0.6296 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6198 - val_loss: 0.6339 - val_accuracy: 0.6684\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6284 - val_loss: 0.6332 - val_accuracy: 0.6684\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6290 - val_loss: 0.6284 - val_accuracy: 0.6811\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6285 - val_loss: 0.6325 - val_accuracy: 0.6722\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.6329 - val_loss: 0.6322 - val_accuracy: 0.6760\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6328 - val_loss: 0.6284 - val_accuracy: 0.6798\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6292 - val_loss: 0.6323 - val_accuracy: 0.6684\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6339 - val_loss: 0.6249 - val_accuracy: 0.6811\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6307 - val_loss: 0.6268 - val_accuracy: 0.6735\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6282 - val_loss: 0.6286 - val_accuracy: 0.6773\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6272 - val_loss: 0.6284 - val_accuracy: 0.6760\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6306 - val_loss: 0.6273 - val_accuracy: 0.6760\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6363 - val_loss: 0.6307 - val_accuracy: 0.6722\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6257 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6330 - val_loss: 0.6252 - val_accuracy: 0.6798\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6312 - val_loss: 0.6319 - val_accuracy: 0.6620\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6351 - val_loss: 0.6300 - val_accuracy: 0.6645\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6462 - accuracy: 0.6326 - val_loss: 0.6277 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6330 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6348 - val_loss: 0.6297 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6360 - val_loss: 0.6270 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6311 - val_loss: 0.6310 - val_accuracy: 0.6671\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6274 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6331 - val_loss: 0.6290 - val_accuracy: 0.6709\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.6315 - val_loss: 0.6254 - val_accuracy: 0.6709\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6392 - val_loss: 0.6305 - val_accuracy: 0.6645\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6353 - val_loss: 0.6285 - val_accuracy: 0.6684\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6393 - accuracy: 0.6410 - val_loss: 0.6208 - val_accuracy: 0.6735\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6411 - accuracy: 0.6359 - val_loss: 0.6276 - val_accuracy: 0.6658\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6296 - val_loss: 0.6295 - val_accuracy: 0.6684\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6493 - val_loss: 0.6307 - val_accuracy: 0.6607\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6378 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6374 - val_loss: 0.6234 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6372 - val_loss: 0.6237 - val_accuracy: 0.6747\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6421 - accuracy: 0.6387 - val_loss: 0.6270 - val_accuracy: 0.6671\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6432 - val_loss: 0.6227 - val_accuracy: 0.6824\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6456 - val_loss: 0.6253 - val_accuracy: 0.6786\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6444 - val_loss: 0.6250 - val_accuracy: 0.6747\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6451 - val_loss: 0.6214 - val_accuracy: 0.6798\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6491 - val_loss: 0.6242 - val_accuracy: 0.6773\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6410 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6380 - accuracy: 0.6437 - val_loss: 0.6195 - val_accuracy: 0.6773\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6371 - accuracy: 0.6412 - val_loss: 0.6234 - val_accuracy: 0.6773\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6449 - val_loss: 0.6226 - val_accuracy: 0.6773\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6451 - val_loss: 0.6193 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6418 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6434 - val_loss: 0.6186 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6404 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6405 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6510 - val_loss: 0.6234 - val_accuracy: 0.6798\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6471 - val_loss: 0.6202 - val_accuracy: 0.6798\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6402 - val_loss: 0.6183 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6409 - val_loss: 0.6206 - val_accuracy: 0.6811\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6478 - val_loss: 0.6168 - val_accuracy: 0.6875\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6517 - val_loss: 0.6178 - val_accuracy: 0.6875\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6449 - val_loss: 0.6180 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6466 - val_loss: 0.6210 - val_accuracy: 0.6811\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6457 - val_loss: 0.6233 - val_accuracy: 0.6811\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6457 - val_loss: 0.6171 - val_accuracy: 0.6837\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6525 - val_loss: 0.6180 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6466 - val_loss: 0.6196 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6505 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6478 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6513 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6555 - val_loss: 0.6203 - val_accuracy: 0.6798\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6507 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6493 - val_loss: 0.6197 - val_accuracy: 0.6811\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6493 - val_loss: 0.6129 - val_accuracy: 0.6913\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6467 - val_loss: 0.6212 - val_accuracy: 0.6773\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6505 - val_loss: 0.6180 - val_accuracy: 0.6824\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6467 - val_loss: 0.6156 - val_accuracy: 0.6798\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6503 - val_loss: 0.6173 - val_accuracy: 0.6798\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6512 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6515 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6574 - val_loss: 0.6221 - val_accuracy: 0.6837\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6556 - val_loss: 0.6181 - val_accuracy: 0.6811\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6566 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6537 - val_loss: 0.6207 - val_accuracy: 0.6849\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6537 - val_loss: 0.6183 - val_accuracy: 0.6875\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6540 - val_loss: 0.6156 - val_accuracy: 0.6888\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6549 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6565 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6498 - val_loss: 0.6233 - val_accuracy: 0.6786\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.6611 - val_loss: 0.6168 - val_accuracy: 0.6862\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6591 - val_loss: 0.6233 - val_accuracy: 0.6773\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6576 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6575 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6601 - val_loss: 0.6168 - val_accuracy: 0.6888\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6536 - val_loss: 0.6174 - val_accuracy: 0.6901\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6559 - val_loss: 0.6149 - val_accuracy: 0.6913\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6559 - val_loss: 0.6199 - val_accuracy: 0.6875\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6580 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6595 - val_loss: 0.6205 - val_accuracy: 0.6849\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6620 - val_loss: 0.6173 - val_accuracy: 0.6913\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6487 - val_loss: 0.6166 - val_accuracy: 0.6862\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6561 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6616 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6642 - val_loss: 0.6156 - val_accuracy: 0.6913\n",
      "Calculating for: 750 150 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_336 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8099 - accuracy: 0.5481 - val_loss: 0.6704 - val_accuracy: 0.6224\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7152 - accuracy: 0.5565 - val_loss: 0.6458 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5642 - val_loss: 0.6404 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5977 - val_loss: 0.6449 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6692 - accuracy: 0.5883 - val_loss: 0.6400 - val_accuracy: 0.6658\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6677 - accuracy: 0.6002 - val_loss: 0.6372 - val_accuracy: 0.6671\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6058 - val_loss: 0.6390 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6068 - val_loss: 0.6379 - val_accuracy: 0.6633\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6135 - val_loss: 0.6357 - val_accuracy: 0.6633\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6171 - val_loss: 0.6336 - val_accuracy: 0.6607\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6124 - val_loss: 0.6354 - val_accuracy: 0.6658\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6066 - val_loss: 0.6359 - val_accuracy: 0.6722\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6140 - val_loss: 0.6365 - val_accuracy: 0.6684\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6142 - val_loss: 0.6318 - val_accuracy: 0.6633\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6238 - val_loss: 0.6324 - val_accuracy: 0.6696\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6226 - val_loss: 0.6345 - val_accuracy: 0.6620\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6270 - val_loss: 0.6297 - val_accuracy: 0.6633\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6359 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6338 - val_loss: 0.6270 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6296 - val_loss: 0.6249 - val_accuracy: 0.6735\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6334 - val_loss: 0.6301 - val_accuracy: 0.6633\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6436 - accuracy: 0.6364 - val_loss: 0.6306 - val_accuracy: 0.6607\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6356 - val_loss: 0.6314 - val_accuracy: 0.6582\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6265 - val_loss: 0.6294 - val_accuracy: 0.6658\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6392 - val_loss: 0.6269 - val_accuracy: 0.6684\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6375 - val_loss: 0.6253 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6395 - val_loss: 0.6254 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6409 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.6420 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6334 - accuracy: 0.6461 - val_loss: 0.6229 - val_accuracy: 0.6696\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6372 - accuracy: 0.6378 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6334 - accuracy: 0.6462 - val_loss: 0.6261 - val_accuracy: 0.6735\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6490 - val_loss: 0.6266 - val_accuracy: 0.6722\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6330 - accuracy: 0.6448 - val_loss: 0.6291 - val_accuracy: 0.6607\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6471 - val_loss: 0.6247 - val_accuracy: 0.6747\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6441 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6303 - accuracy: 0.6515 - val_loss: 0.6236 - val_accuracy: 0.6684\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6521 - val_loss: 0.6244 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6518 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6530 - val_loss: 0.6307 - val_accuracy: 0.6543\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6556 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6591 - val_loss: 0.6245 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6570 - val_loss: 0.6242 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6599 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6564 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6591 - val_loss: 0.6281 - val_accuracy: 0.6684\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6206 - accuracy: 0.6589 - val_loss: 0.6252 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6630 - val_loss: 0.6223 - val_accuracy: 0.6824\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6591 - val_loss: 0.6199 - val_accuracy: 0.6760\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6614 - val_loss: 0.6239 - val_accuracy: 0.6747\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6541 - val_loss: 0.6247 - val_accuracy: 0.6709\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6659 - val_loss: 0.6223 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6570 - val_loss: 0.6236 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6590 - val_loss: 0.6248 - val_accuracy: 0.6747\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6665 - val_loss: 0.6236 - val_accuracy: 0.6798\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6668 - val_loss: 0.6252 - val_accuracy: 0.6722\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6674 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6072 - accuracy: 0.6727 - val_loss: 0.6251 - val_accuracy: 0.6709\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6667 - val_loss: 0.6310 - val_accuracy: 0.6658\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6767 - val_loss: 0.6271 - val_accuracy: 0.6684\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6108 - accuracy: 0.6688 - val_loss: 0.6238 - val_accuracy: 0.6747\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6140 - accuracy: 0.6688 - val_loss: 0.6255 - val_accuracy: 0.6671\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6659 - val_loss: 0.6229 - val_accuracy: 0.6747\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6679 - val_loss: 0.6235 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6665 - val_loss: 0.6277 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6118 - accuracy: 0.6680 - val_loss: 0.6252 - val_accuracy: 0.6696\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6096 - accuracy: 0.6721 - val_loss: 0.6264 - val_accuracy: 0.6722\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6018 - accuracy: 0.6758 - val_loss: 0.6291 - val_accuracy: 0.6684\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6041 - accuracy: 0.6783 - val_loss: 0.6283 - val_accuracy: 0.6696\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6063 - accuracy: 0.6805 - val_loss: 0.6278 - val_accuracy: 0.6696\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6726 - val_loss: 0.6247 - val_accuracy: 0.6709\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6068 - accuracy: 0.6714 - val_loss: 0.6301 - val_accuracy: 0.6620\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6085 - accuracy: 0.6742 - val_loss: 0.6293 - val_accuracy: 0.6709\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6003 - accuracy: 0.6788 - val_loss: 0.6335 - val_accuracy: 0.6671\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6029 - accuracy: 0.6863 - val_loss: 0.6278 - val_accuracy: 0.6735\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6031 - accuracy: 0.6786 - val_loss: 0.6274 - val_accuracy: 0.6849\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6017 - accuracy: 0.6786 - val_loss: 0.6316 - val_accuracy: 0.6633\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6025 - accuracy: 0.6802 - val_loss: 0.6336 - val_accuracy: 0.6696\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5973 - accuracy: 0.6809 - val_loss: 0.6359 - val_accuracy: 0.6645\n",
      "Calculating for: 750 150 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_340 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8125 - accuracy: 0.5309 - val_loss: 0.6549 - val_accuracy: 0.6339\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7156 - accuracy: 0.5501 - val_loss: 0.6424 - val_accuracy: 0.6416\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5522 - val_loss: 0.6460 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5736 - val_loss: 0.6452 - val_accuracy: 0.6492\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5772 - val_loss: 0.6451 - val_accuracy: 0.6556\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6730 - accuracy: 0.5786 - val_loss: 0.6419 - val_accuracy: 0.6505\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.5791 - val_loss: 0.6431 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5903 - val_loss: 0.6393 - val_accuracy: 0.6531\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5880 - val_loss: 0.6384 - val_accuracy: 0.6556\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6693 - accuracy: 0.5904 - val_loss: 0.6357 - val_accuracy: 0.6518\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6632 - accuracy: 0.5976 - val_loss: 0.6334 - val_accuracy: 0.6607\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6650 - accuracy: 0.5990 - val_loss: 0.6374 - val_accuracy: 0.6798\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.5996 - val_loss: 0.6337 - val_accuracy: 0.6760\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.5982 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6634 - accuracy: 0.5997 - val_loss: 0.6332 - val_accuracy: 0.6760\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6071 - val_loss: 0.6286 - val_accuracy: 0.6773\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6039 - val_loss: 0.6303 - val_accuracy: 0.6798\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6101 - val_loss: 0.6299 - val_accuracy: 0.6760\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6181 - val_loss: 0.6255 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6155 - val_loss: 0.6275 - val_accuracy: 0.6773\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6112 - val_loss: 0.6264 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6174 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6098 - val_loss: 0.6280 - val_accuracy: 0.6811\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6211 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6198 - val_loss: 0.6262 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6197 - val_loss: 0.6264 - val_accuracy: 0.6760\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6242 - val_loss: 0.6214 - val_accuracy: 0.6786\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6237 - val_loss: 0.6267 - val_accuracy: 0.6798\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6233 - val_loss: 0.6260 - val_accuracy: 0.6786\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6551 - accuracy: 0.6179 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6204 - val_loss: 0.6221 - val_accuracy: 0.6747\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6497 - accuracy: 0.6226 - val_loss: 0.6212 - val_accuracy: 0.6747\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6255 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6348 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6475 - accuracy: 0.6255 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6500 - accuracy: 0.6230 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6252 - val_loss: 0.6221 - val_accuracy: 0.6824\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6301 - val_loss: 0.6228 - val_accuracy: 0.6837\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6479 - accuracy: 0.6294 - val_loss: 0.6214 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6289 - val_loss: 0.6204 - val_accuracy: 0.6849\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6255 - val_loss: 0.6223 - val_accuracy: 0.6849\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6445 - accuracy: 0.6320 - val_loss: 0.6216 - val_accuracy: 0.6913\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6364 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6344 - val_loss: 0.6188 - val_accuracy: 0.6862\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6374 - val_loss: 0.6158 - val_accuracy: 0.6888\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6360 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6349 - val_loss: 0.6143 - val_accuracy: 0.6888\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6404 - accuracy: 0.6413 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6436 - val_loss: 0.6193 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6399 - val_loss: 0.6153 - val_accuracy: 0.6875\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6394 - accuracy: 0.6363 - val_loss: 0.6125 - val_accuracy: 0.6849\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6382 - val_loss: 0.6120 - val_accuracy: 0.6913\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6350 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6335 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6389 - val_loss: 0.6123 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6380 - val_loss: 0.6133 - val_accuracy: 0.6888\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6444 - val_loss: 0.6173 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6436 - val_loss: 0.6150 - val_accuracy: 0.6913\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6412 - val_loss: 0.6155 - val_accuracy: 0.6926\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6457 - val_loss: 0.6157 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6472 - val_loss: 0.6111 - val_accuracy: 0.6939\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6423 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6448 - val_loss: 0.6115 - val_accuracy: 0.6926\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6372 - accuracy: 0.6451 - val_loss: 0.6166 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6448 - val_loss: 0.6127 - val_accuracy: 0.6888\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6426 - val_loss: 0.6121 - val_accuracy: 0.6952\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6537 - val_loss: 0.6084 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6452 - val_loss: 0.6109 - val_accuracy: 0.6926\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6393 - val_loss: 0.6136 - val_accuracy: 0.6990\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6451 - val_loss: 0.6083 - val_accuracy: 0.6939\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6555 - val_loss: 0.6080 - val_accuracy: 0.6977\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6525 - val_loss: 0.6110 - val_accuracy: 0.7015\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6485 - val_loss: 0.6112 - val_accuracy: 0.6977\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6527 - val_loss: 0.6115 - val_accuracy: 0.6990\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6517 - val_loss: 0.6138 - val_accuracy: 0.6977\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6537 - val_loss: 0.6121 - val_accuracy: 0.6964\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6539 - val_loss: 0.6112 - val_accuracy: 0.6964\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6604 - val_loss: 0.6138 - val_accuracy: 0.6990\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6551 - val_loss: 0.6122 - val_accuracy: 0.6990\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6564 - val_loss: 0.6123 - val_accuracy: 0.6926\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6224 - accuracy: 0.6511 - val_loss: 0.6114 - val_accuracy: 0.6926\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6495 - val_loss: 0.6111 - val_accuracy: 0.6926\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6561 - val_loss: 0.6145 - val_accuracy: 0.6952\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6596 - val_loss: 0.6152 - val_accuracy: 0.6964\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6584 - val_loss: 0.6126 - val_accuracy: 0.6952\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6570 - val_loss: 0.6108 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6567 - val_loss: 0.6105 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6171 - accuracy: 0.6665 - val_loss: 0.6086 - val_accuracy: 0.6939\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6570 - val_loss: 0.6115 - val_accuracy: 0.6939\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6619 - val_loss: 0.6122 - val_accuracy: 0.6913\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6220 - accuracy: 0.6598 - val_loss: 0.6112 - val_accuracy: 0.6964\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6624 - val_loss: 0.6137 - val_accuracy: 0.6913\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6624 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6596 - val_loss: 0.6135 - val_accuracy: 0.6926\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6173 - accuracy: 0.6649 - val_loss: 0.6159 - val_accuracy: 0.6913\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6640 - val_loss: 0.6125 - val_accuracy: 0.6964\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6197 - accuracy: 0.6684 - val_loss: 0.6158 - val_accuracy: 0.6913\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6635 - val_loss: 0.6127 - val_accuracy: 0.6926\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6693 - val_loss: 0.6142 - val_accuracy: 0.6952\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6668 - val_loss: 0.6154 - val_accuracy: 0.6926\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6689 - val_loss: 0.6173 - val_accuracy: 0.6849\n",
      "Calculating for: 750 150 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_344 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8212 - accuracy: 0.5134 - val_loss: 0.6767 - val_accuracy: 0.6288\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7347 - accuracy: 0.5226 - val_loss: 0.6676 - val_accuracy: 0.6480\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7010 - accuracy: 0.5398 - val_loss: 0.6620 - val_accuracy: 0.6441\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5408 - val_loss: 0.6577 - val_accuracy: 0.6441\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.5534 - val_loss: 0.6565 - val_accuracy: 0.6441\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5525 - val_loss: 0.6553 - val_accuracy: 0.6429\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6844 - accuracy: 0.5561 - val_loss: 0.6573 - val_accuracy: 0.6467\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6817 - accuracy: 0.5641 - val_loss: 0.6551 - val_accuracy: 0.6454\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6834 - accuracy: 0.5580 - val_loss: 0.6574 - val_accuracy: 0.6441\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5674 - val_loss: 0.6524 - val_accuracy: 0.6480\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5623 - val_loss: 0.6532 - val_accuracy: 0.6467\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6804 - accuracy: 0.5564 - val_loss: 0.6516 - val_accuracy: 0.6492\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6780 - accuracy: 0.5766 - val_loss: 0.6500 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5776 - val_loss: 0.6495 - val_accuracy: 0.6492\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5785 - val_loss: 0.6436 - val_accuracy: 0.6531\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.5833 - val_loss: 0.6409 - val_accuracy: 0.6531\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5870 - val_loss: 0.6474 - val_accuracy: 0.6569\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6726 - accuracy: 0.5838 - val_loss: 0.6423 - val_accuracy: 0.6518\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.5800 - val_loss: 0.6450 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5898 - val_loss: 0.6465 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5894 - val_loss: 0.6443 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5904 - val_loss: 0.6443 - val_accuracy: 0.6645\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5852 - val_loss: 0.6450 - val_accuracy: 0.6684\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5873 - val_loss: 0.6404 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.5977 - val_loss: 0.6348 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6683 - accuracy: 0.5944 - val_loss: 0.6352 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5963 - val_loss: 0.6356 - val_accuracy: 0.6671\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5967 - val_loss: 0.6393 - val_accuracy: 0.6633\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5967 - val_loss: 0.6352 - val_accuracy: 0.6735\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6069 - val_loss: 0.6374 - val_accuracy: 0.6747\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.5976 - val_loss: 0.6386 - val_accuracy: 0.6760\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.6016 - val_loss: 0.6369 - val_accuracy: 0.6747\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6025 - val_loss: 0.6345 - val_accuracy: 0.6722\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5966 - val_loss: 0.6358 - val_accuracy: 0.6709\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6063 - val_loss: 0.6333 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.5939 - val_loss: 0.6335 - val_accuracy: 0.6709\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6052 - val_loss: 0.6342 - val_accuracy: 0.6735\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6110 - val_loss: 0.6359 - val_accuracy: 0.6735\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6098 - val_loss: 0.6308 - val_accuracy: 0.6811\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6061 - val_loss: 0.6329 - val_accuracy: 0.6773\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6050 - val_loss: 0.6391 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6155 - val_loss: 0.6364 - val_accuracy: 0.6824\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6093 - val_loss: 0.6343 - val_accuracy: 0.6786\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6036 - val_loss: 0.6345 - val_accuracy: 0.6798\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6162 - val_loss: 0.6295 - val_accuracy: 0.6773\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6174 - val_loss: 0.6337 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6079 - val_loss: 0.6385 - val_accuracy: 0.6760\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6088 - val_loss: 0.6417 - val_accuracy: 0.6645\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6163 - val_loss: 0.6357 - val_accuracy: 0.6786\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6134 - val_loss: 0.6329 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6101 - val_loss: 0.6297 - val_accuracy: 0.6837\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6145 - val_loss: 0.6286 - val_accuracy: 0.6811\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6130 - val_loss: 0.6342 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6183 - val_loss: 0.6279 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6193 - val_loss: 0.6302 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6228 - val_loss: 0.6312 - val_accuracy: 0.6811\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6238 - val_loss: 0.6305 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6227 - val_loss: 0.6325 - val_accuracy: 0.6811\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6226 - val_loss: 0.6312 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6545 - accuracy: 0.6189 - val_loss: 0.6337 - val_accuracy: 0.6811\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6538 - accuracy: 0.6237 - val_loss: 0.6326 - val_accuracy: 0.6786\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6218 - val_loss: 0.6286 - val_accuracy: 0.6837\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6543 - accuracy: 0.6150 - val_loss: 0.6293 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6520 - accuracy: 0.6212 - val_loss: 0.6313 - val_accuracy: 0.6798\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6212 - val_loss: 0.6299 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6529 - accuracy: 0.6178 - val_loss: 0.6242 - val_accuracy: 0.6849\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6286 - val_loss: 0.6276 - val_accuracy: 0.6824\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6238 - val_loss: 0.6245 - val_accuracy: 0.6849\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6264 - val_loss: 0.6260 - val_accuracy: 0.6862\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6528 - accuracy: 0.6237 - val_loss: 0.6262 - val_accuracy: 0.6862\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6488 - accuracy: 0.6247 - val_loss: 0.6279 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.6168 - val_loss: 0.6237 - val_accuracy: 0.6824\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6161 - val_loss: 0.6264 - val_accuracy: 0.6811\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6274 - val_accuracy: 0.6875\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6242 - val_loss: 0.6241 - val_accuracy: 0.6888\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6186 - val_loss: 0.6258 - val_accuracy: 0.6862\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6267 - val_loss: 0.6215 - val_accuracy: 0.6875\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6325 - val_loss: 0.6213 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6291 - val_loss: 0.6228 - val_accuracy: 0.6849\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6304 - val_loss: 0.6251 - val_accuracy: 0.6837\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6318 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6366 - val_loss: 0.6276 - val_accuracy: 0.6811\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6304 - val_loss: 0.6226 - val_accuracy: 0.6862\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6338 - val_loss: 0.6217 - val_accuracy: 0.6837\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6350 - val_loss: 0.6209 - val_accuracy: 0.6811\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6390 - val_loss: 0.6192 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6326 - val_loss: 0.6181 - val_accuracy: 0.6939\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6316 - val_loss: 0.6276 - val_accuracy: 0.6798\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6488 - accuracy: 0.6311 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6447 - accuracy: 0.6311 - val_loss: 0.6159 - val_accuracy: 0.6837\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6359 - val_loss: 0.6199 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6366 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6266 - val_loss: 0.6248 - val_accuracy: 0.6760\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6356 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6280 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6331 - val_loss: 0.6216 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6446 - accuracy: 0.6341 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6394 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6439 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6338 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6323 - val_loss: 0.6230 - val_accuracy: 0.6760\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6358 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6338 - val_loss: 0.6216 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6461 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6441 - accuracy: 0.6364 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6404 - accuracy: 0.6382 - val_loss: 0.6187 - val_accuracy: 0.6837\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6420 - val_loss: 0.6222 - val_accuracy: 0.6849\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6429 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6346 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6426 - val_loss: 0.6193 - val_accuracy: 0.6849\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6372 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6402 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6431 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6423 - val_loss: 0.6193 - val_accuracy: 0.6786\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6344 - val_loss: 0.6175 - val_accuracy: 0.6798\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6459 - val_loss: 0.6139 - val_accuracy: 0.6888\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6382 - val_loss: 0.6171 - val_accuracy: 0.6901\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6417 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6350 - val_loss: 0.6148 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6447 - val_loss: 0.6179 - val_accuracy: 0.6811\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6457 - val_loss: 0.6153 - val_accuracy: 0.6862\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6447 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6357 - accuracy: 0.6477 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6492 - val_loss: 0.6110 - val_accuracy: 0.6926\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6458 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6530 - val_loss: 0.6171 - val_accuracy: 0.6849\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6462 - val_loss: 0.6167 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6564 - val_loss: 0.6181 - val_accuracy: 0.6862\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6500 - val_loss: 0.6165 - val_accuracy: 0.6837\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6444 - val_loss: 0.6181 - val_accuracy: 0.6811\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6542 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6381 - accuracy: 0.6443 - val_loss: 0.6146 - val_accuracy: 0.6913\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6482 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6478 - val_loss: 0.6141 - val_accuracy: 0.6862\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6510 - val_loss: 0.6199 - val_accuracy: 0.6837\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6483 - val_loss: 0.6178 - val_accuracy: 0.6913\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6491 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6467 - val_loss: 0.6105 - val_accuracy: 0.6888\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6458 - val_loss: 0.6136 - val_accuracy: 0.6888\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6477 - val_loss: 0.6124 - val_accuracy: 0.6926\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6485 - val_loss: 0.6145 - val_accuracy: 0.6888\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6464 - val_loss: 0.6124 - val_accuracy: 0.6939\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6527 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6545 - val_loss: 0.6164 - val_accuracy: 0.6849\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6503 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6497 - val_loss: 0.6167 - val_accuracy: 0.6875\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6594 - val_loss: 0.6106 - val_accuracy: 0.6939\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6496 - val_loss: 0.6199 - val_accuracy: 0.6875\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6590 - val_loss: 0.6159 - val_accuracy: 0.6913\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6508 - val_loss: 0.6121 - val_accuracy: 0.6926\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6502 - val_loss: 0.6139 - val_accuracy: 0.6952\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6545 - val_loss: 0.6131 - val_accuracy: 0.6939\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6574 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6608 - val_loss: 0.6113 - val_accuracy: 0.6849\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6541 - val_loss: 0.6101 - val_accuracy: 0.6875\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6542 - val_loss: 0.6097 - val_accuracy: 0.6875\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6540 - val_loss: 0.6118 - val_accuracy: 0.6888\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6634 - val_loss: 0.6114 - val_accuracy: 0.6875\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6536 - val_loss: 0.6136 - val_accuracy: 0.6849\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6472 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6532 - val_loss: 0.6138 - val_accuracy: 0.6901\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6546 - val_loss: 0.6110 - val_accuracy: 0.6875\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6542 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6541 - val_loss: 0.6109 - val_accuracy: 0.6901\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6517 - val_loss: 0.6114 - val_accuracy: 0.6952\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6609 - val_loss: 0.6140 - val_accuracy: 0.6888\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6593 - val_loss: 0.6068 - val_accuracy: 0.6939\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6561 - val_loss: 0.6124 - val_accuracy: 0.6862\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6569 - val_loss: 0.6134 - val_accuracy: 0.6849\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6579 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6565 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6701 - val_loss: 0.6126 - val_accuracy: 0.6849\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6570 - val_loss: 0.6138 - val_accuracy: 0.6862\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6589 - val_loss: 0.6122 - val_accuracy: 0.6875\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6586 - val_loss: 0.6151 - val_accuracy: 0.6862\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6670 - val_loss: 0.6144 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6569 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6624 - val_loss: 0.6181 - val_accuracy: 0.6837\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6645 - val_loss: 0.6133 - val_accuracy: 0.6849\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6638 - val_loss: 0.6105 - val_accuracy: 0.6849\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6590 - val_loss: 0.6098 - val_accuracy: 0.6862\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6653 - val_loss: 0.6121 - val_accuracy: 0.6888\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6628 - val_loss: 0.6113 - val_accuracy: 0.6875\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6644 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6618 - val_loss: 0.6138 - val_accuracy: 0.6798\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6639 - val_loss: 0.6110 - val_accuracy: 0.6875\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6591 - val_loss: 0.6157 - val_accuracy: 0.6786\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6619 - val_loss: 0.6130 - val_accuracy: 0.6786\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6187 - accuracy: 0.6697 - val_loss: 0.6135 - val_accuracy: 0.6798\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6644 - val_loss: 0.6134 - val_accuracy: 0.6849\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6193 - accuracy: 0.6624 - val_loss: 0.6136 - val_accuracy: 0.6824\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6703 - val_loss: 0.6101 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6599 - val_loss: 0.6089 - val_accuracy: 0.6837\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6694 - val_loss: 0.6127 - val_accuracy: 0.6837\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6689 - val_loss: 0.6092 - val_accuracy: 0.6824\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6701 - val_loss: 0.6090 - val_accuracy: 0.6862\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6649 - val_loss: 0.6129 - val_accuracy: 0.6824\n"
     ]
    }
   ],
   "source": [
    "dense1 = [650, 700, 750]\n",
    "dense2 = [50, 100, 150]\n",
    "dropout = [0.6, 0.7, 0.8]\n",
    "\n",
    "\n",
    "for d1 in dense1:\n",
    "    for d2 in dense2:\n",
    "        for dr in dropout:\n",
    "            print('Calculating for:', d1, d2, dr, opt)\n",
    "            keras_model = init_keras_model(dense1=d1, dense2=d2, dropout=dr, optimizer=opt)\n",
    "            keras_model.fit(X_train_tensor, \n",
    "                            y_train_tensor, \n",
    "                            epochs=n_epochs, \n",
    "                            validation_data=(X_val_tensor, y_val_tensor),\n",
    "                            callbacks=[es] \n",
    "                            )\n",
    "            score = keras_model.evaluate(X_test_tensor, y_test_tensor, verbose=0)[1]\n",
    "            scores[(d1, d2, dr, opt)] = score\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'dense1':d1, 'dense2':d2, 'dropout':dr, 'optimizer':opt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'dense1': 750, 'dense2': 50, 'dropout': 0.8, 'optimizer': 'sgd'}, best_score: 0.6883780360221863\n"
     ]
    }
   ],
   "source": [
    "print(f'best_params: {best_params}, best_score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_348 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8526 - accuracy: 0.5255 - val_loss: 0.6639 - val_accuracy: 0.6492\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7355 - accuracy: 0.5318 - val_loss: 0.6589 - val_accuracy: 0.6531\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7034 - accuracy: 0.5442 - val_loss: 0.6535 - val_accuracy: 0.6467\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6875 - accuracy: 0.5612 - val_loss: 0.6512 - val_accuracy: 0.6454\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6836 - accuracy: 0.5604 - val_loss: 0.6529 - val_accuracy: 0.6441\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6857 - accuracy: 0.5594 - val_loss: 0.6518 - val_accuracy: 0.6441\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6837 - accuracy: 0.5628 - val_loss: 0.6521 - val_accuracy: 0.6390\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6834 - accuracy: 0.5644 - val_loss: 0.6545 - val_accuracy: 0.6441\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6799 - accuracy: 0.5686 - val_loss: 0.6526 - val_accuracy: 0.6441\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6781 - accuracy: 0.5722 - val_loss: 0.6474 - val_accuracy: 0.6441\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6768 - accuracy: 0.5749 - val_loss: 0.6476 - val_accuracy: 0.6505\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.5809 - val_loss: 0.6462 - val_accuracy: 0.6492\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.5720 - val_loss: 0.6487 - val_accuracy: 0.6620\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.5755 - val_loss: 0.6466 - val_accuracy: 0.6658\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5801 - val_loss: 0.6433 - val_accuracy: 0.6607\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6712 - accuracy: 0.5890 - val_loss: 0.6455 - val_accuracy: 0.6709\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5779 - val_loss: 0.6486 - val_accuracy: 0.6786\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.5858 - val_loss: 0.6417 - val_accuracy: 0.6684\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6744 - accuracy: 0.5874 - val_loss: 0.6429 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6690 - accuracy: 0.5948 - val_loss: 0.6393 - val_accuracy: 0.6658\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6702 - accuracy: 0.5898 - val_loss: 0.6405 - val_accuracy: 0.6747\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6704 - accuracy: 0.5914 - val_loss: 0.6375 - val_accuracy: 0.6747\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5927 - val_loss: 0.6391 - val_accuracy: 0.6760\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5908 - val_loss: 0.6385 - val_accuracy: 0.6773\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5922 - val_loss: 0.6376 - val_accuracy: 0.6786\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6675 - accuracy: 0.5972 - val_loss: 0.6359 - val_accuracy: 0.6760\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6629 - accuracy: 0.6007 - val_loss: 0.6345 - val_accuracy: 0.6786\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6671 - accuracy: 0.5926 - val_loss: 0.6414 - val_accuracy: 0.6837\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.6031 - val_loss: 0.6338 - val_accuracy: 0.6786\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6639 - accuracy: 0.6024 - val_loss: 0.6338 - val_accuracy: 0.6824\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6647 - accuracy: 0.6065 - val_loss: 0.6298 - val_accuracy: 0.6696\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.6025 - val_loss: 0.6383 - val_accuracy: 0.6837\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6075 - val_loss: 0.6357 - val_accuracy: 0.6837\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6643 - accuracy: 0.5972 - val_loss: 0.6361 - val_accuracy: 0.6875\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.6050 - val_loss: 0.6358 - val_accuracy: 0.6786\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6089 - val_loss: 0.6356 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6040 - val_loss: 0.6320 - val_accuracy: 0.6798\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6616 - accuracy: 0.6108 - val_loss: 0.6335 - val_accuracy: 0.6849\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6138 - val_loss: 0.6292 - val_accuracy: 0.6824\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6099 - val_loss: 0.6291 - val_accuracy: 0.6862\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6094 - val_loss: 0.6337 - val_accuracy: 0.6862\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6602 - accuracy: 0.6081 - val_loss: 0.6311 - val_accuracy: 0.6862\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6616 - accuracy: 0.6090 - val_loss: 0.6283 - val_accuracy: 0.6837\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6597 - accuracy: 0.6109 - val_loss: 0.6285 - val_accuracy: 0.6837\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6128 - val_loss: 0.6327 - val_accuracy: 0.6888\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6164 - val_loss: 0.6288 - val_accuracy: 0.6901\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6101 - val_loss: 0.6276 - val_accuracy: 0.6888\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6211 - val_loss: 0.6241 - val_accuracy: 0.6862\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6545 - accuracy: 0.6194 - val_loss: 0.6275 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6172 - val_loss: 0.6254 - val_accuracy: 0.6849\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6193 - val_loss: 0.6251 - val_accuracy: 0.6875\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6584 - accuracy: 0.6137 - val_loss: 0.6317 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6154 - val_loss: 0.6225 - val_accuracy: 0.6862\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6153 - val_loss: 0.6237 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6176 - val_loss: 0.6280 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6152 - val_loss: 0.6255 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6198 - val_loss: 0.6259 - val_accuracy: 0.6926\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6129 - val_loss: 0.6255 - val_accuracy: 0.6875\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6563 - accuracy: 0.6177 - val_loss: 0.6305 - val_accuracy: 0.6773\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6255 - val_loss: 0.6224 - val_accuracy: 0.6913\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6198 - val_loss: 0.6235 - val_accuracy: 0.6837\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6496 - accuracy: 0.6270 - val_loss: 0.6232 - val_accuracy: 0.6875\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6543 - accuracy: 0.6208 - val_loss: 0.6246 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6182 - val_loss: 0.6248 - val_accuracy: 0.6824\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6179 - val_loss: 0.6262 - val_accuracy: 0.6811\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6509 - accuracy: 0.6231 - val_loss: 0.6237 - val_accuracy: 0.6824\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6189 - val_loss: 0.6220 - val_accuracy: 0.6926\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6228 - val_loss: 0.6247 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6304 - val_loss: 0.6233 - val_accuracy: 0.6849\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6505 - accuracy: 0.6226 - val_loss: 0.6242 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6226 - val_loss: 0.6190 - val_accuracy: 0.6888\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6489 - accuracy: 0.6241 - val_loss: 0.6236 - val_accuracy: 0.6798\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6506 - accuracy: 0.6314 - val_loss: 0.6253 - val_accuracy: 0.6875\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6475 - accuracy: 0.6285 - val_loss: 0.6204 - val_accuracy: 0.6901\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6280 - val_loss: 0.6223 - val_accuracy: 0.6849\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6252 - val_loss: 0.6227 - val_accuracy: 0.6875\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6311 - val_loss: 0.6193 - val_accuracy: 0.6901\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6294 - val_loss: 0.6213 - val_accuracy: 0.6862\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6274 - val_loss: 0.6257 - val_accuracy: 0.6658\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6484 - accuracy: 0.6287 - val_loss: 0.6220 - val_accuracy: 0.6875\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6321 - val_loss: 0.6250 - val_accuracy: 0.6888\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6297 - val_loss: 0.6192 - val_accuracy: 0.6926\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6465 - accuracy: 0.6379 - val_loss: 0.6203 - val_accuracy: 0.6849\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6469 - accuracy: 0.6318 - val_loss: 0.6210 - val_accuracy: 0.6888\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6373 - val_loss: 0.6190 - val_accuracy: 0.6849\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6276 - val_loss: 0.6219 - val_accuracy: 0.6798\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6426 - accuracy: 0.6377 - val_loss: 0.6215 - val_accuracy: 0.6824\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6484 - accuracy: 0.6260 - val_loss: 0.6293 - val_accuracy: 0.6696\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6297 - val_loss: 0.6248 - val_accuracy: 0.6773\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6445 - accuracy: 0.6343 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6469 - accuracy: 0.6301 - val_loss: 0.6192 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6465 - accuracy: 0.6310 - val_loss: 0.6238 - val_accuracy: 0.6696\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6395 - val_loss: 0.6189 - val_accuracy: 0.6875\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6427 - accuracy: 0.6368 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6458 - accuracy: 0.6400 - val_loss: 0.6199 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6438 - accuracy: 0.6341 - val_loss: 0.6186 - val_accuracy: 0.6888\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6452 - accuracy: 0.6351 - val_loss: 0.6184 - val_accuracy: 0.6875\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6449 - accuracy: 0.6385 - val_loss: 0.6251 - val_accuracy: 0.6773\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6390 - accuracy: 0.6379 - val_loss: 0.6219 - val_accuracy: 0.6747\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6420 - accuracy: 0.6355 - val_loss: 0.6204 - val_accuracy: 0.6849\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6403 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6425 - accuracy: 0.6387 - val_loss: 0.6192 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6396 - accuracy: 0.6438 - val_loss: 0.6163 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6400 - accuracy: 0.6412 - val_loss: 0.6210 - val_accuracy: 0.6824\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6387 - accuracy: 0.6439 - val_loss: 0.6159 - val_accuracy: 0.6926\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6388 - accuracy: 0.6446 - val_loss: 0.6210 - val_accuracy: 0.6849\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6421 - accuracy: 0.6388 - val_loss: 0.6170 - val_accuracy: 0.6849\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6355 - accuracy: 0.6446 - val_loss: 0.6153 - val_accuracy: 0.6849\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6366 - accuracy: 0.6441 - val_loss: 0.6168 - val_accuracy: 0.6875\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6382 - accuracy: 0.6441 - val_loss: 0.6166 - val_accuracy: 0.6875\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6469 - val_loss: 0.6150 - val_accuracy: 0.6913\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6447 - val_loss: 0.6142 - val_accuracy: 0.6926\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6394 - accuracy: 0.6404 - val_loss: 0.6182 - val_accuracy: 0.6824\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6380 - accuracy: 0.6444 - val_loss: 0.6155 - val_accuracy: 0.6926\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6357 - accuracy: 0.6418 - val_loss: 0.6169 - val_accuracy: 0.6913\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6456 - val_loss: 0.6148 - val_accuracy: 0.6939\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6375 - accuracy: 0.6409 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6443 - val_loss: 0.6160 - val_accuracy: 0.6901\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6371 - accuracy: 0.6400 - val_loss: 0.6182 - val_accuracy: 0.6862\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6389 - accuracy: 0.6412 - val_loss: 0.6129 - val_accuracy: 0.6926\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6324 - accuracy: 0.6492 - val_loss: 0.6141 - val_accuracy: 0.6888\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6371 - accuracy: 0.6439 - val_loss: 0.6178 - val_accuracy: 0.6849\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6437 - val_loss: 0.6152 - val_accuracy: 0.6901\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6385 - accuracy: 0.6476 - val_loss: 0.6175 - val_accuracy: 0.6875\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6457 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6491 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6358 - accuracy: 0.6420 - val_loss: 0.6130 - val_accuracy: 0.6888\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6329 - accuracy: 0.6480 - val_loss: 0.6173 - val_accuracy: 0.6811\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6343 - accuracy: 0.6485 - val_loss: 0.6145 - val_accuracy: 0.6888\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6529 - val_loss: 0.6147 - val_accuracy: 0.6849\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6302 - accuracy: 0.6485 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6506 - val_loss: 0.6148 - val_accuracy: 0.6888\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6497 - val_loss: 0.6148 - val_accuracy: 0.6824\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6576 - val_loss: 0.6124 - val_accuracy: 0.6849\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6371 - accuracy: 0.6442 - val_loss: 0.6155 - val_accuracy: 0.6824\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6329 - accuracy: 0.6512 - val_loss: 0.6204 - val_accuracy: 0.6786\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6491 - val_loss: 0.6166 - val_accuracy: 0.6849\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6508 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6492 - val_loss: 0.6146 - val_accuracy: 0.6901\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6487 - val_loss: 0.6164 - val_accuracy: 0.6837\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6497 - val_loss: 0.6163 - val_accuracy: 0.6875\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6289 - accuracy: 0.6512 - val_loss: 0.6144 - val_accuracy: 0.6837\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6506 - val_loss: 0.6188 - val_accuracy: 0.6824\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6520 - val_loss: 0.6152 - val_accuracy: 0.6837\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6565 - val_loss: 0.6112 - val_accuracy: 0.6901\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6482 - val_loss: 0.6167 - val_accuracy: 0.6888\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6316 - accuracy: 0.6508 - val_loss: 0.6143 - val_accuracy: 0.6913\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6516 - val_loss: 0.6158 - val_accuracy: 0.6849\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6525 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6506 - val_loss: 0.6147 - val_accuracy: 0.6862\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6596 - val_loss: 0.6108 - val_accuracy: 0.6913\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6490 - val_loss: 0.6143 - val_accuracy: 0.6952\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6544 - val_loss: 0.6121 - val_accuracy: 0.6901\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6547 - val_loss: 0.6123 - val_accuracy: 0.6901\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6634 - val_loss: 0.6174 - val_accuracy: 0.6760\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6507 - val_loss: 0.6124 - val_accuracy: 0.6888\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6564 - val_loss: 0.6166 - val_accuracy: 0.6760\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6523 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6284 - accuracy: 0.6498 - val_loss: 0.6146 - val_accuracy: 0.6837\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6243 - accuracy: 0.6581 - val_loss: 0.6145 - val_accuracy: 0.6875\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6554 - val_loss: 0.6148 - val_accuracy: 0.6913\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6588 - val_loss: 0.6144 - val_accuracy: 0.6888\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6292 - accuracy: 0.6545 - val_loss: 0.6112 - val_accuracy: 0.6913\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6254 - accuracy: 0.6535 - val_loss: 0.6111 - val_accuracy: 0.6862\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6667 - val_loss: 0.6109 - val_accuracy: 0.6901\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6203 - accuracy: 0.6652 - val_loss: 0.6146 - val_accuracy: 0.6798\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6220 - accuracy: 0.6593 - val_loss: 0.6130 - val_accuracy: 0.6837\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6554 - val_loss: 0.6123 - val_accuracy: 0.6862\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6537 - val_loss: 0.6137 - val_accuracy: 0.6837\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6276 - accuracy: 0.6586 - val_loss: 0.6134 - val_accuracy: 0.6849\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6250 - accuracy: 0.6590 - val_loss: 0.6127 - val_accuracy: 0.6901\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6199 - accuracy: 0.6637 - val_loss: 0.6084 - val_accuracy: 0.6875\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6216 - accuracy: 0.6631 - val_loss: 0.6088 - val_accuracy: 0.6939\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6199 - accuracy: 0.6625 - val_loss: 0.6103 - val_accuracy: 0.6913\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6224 - accuracy: 0.6624 - val_loss: 0.6082 - val_accuracy: 0.6913\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6598 - val_loss: 0.6088 - val_accuracy: 0.6939\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6564 - val_loss: 0.6098 - val_accuracy: 0.6926\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6257 - accuracy: 0.6531 - val_loss: 0.6109 - val_accuracy: 0.6939\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6199 - accuracy: 0.6672 - val_loss: 0.6112 - val_accuracy: 0.6901\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6228 - accuracy: 0.6628 - val_loss: 0.6108 - val_accuracy: 0.6926\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6246 - accuracy: 0.6620 - val_loss: 0.6144 - val_accuracy: 0.6849\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6660 - val_loss: 0.6105 - val_accuracy: 0.6901\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6639 - val_loss: 0.6119 - val_accuracy: 0.6849\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6633 - val_loss: 0.6128 - val_accuracy: 0.6862\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6188 - accuracy: 0.6640 - val_loss: 0.6118 - val_accuracy: 0.6849\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6192 - accuracy: 0.6665 - val_loss: 0.6111 - val_accuracy: 0.6875\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6605 - val_loss: 0.6120 - val_accuracy: 0.6888\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6716 - val_loss: 0.6127 - val_accuracy: 0.6849\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6606 - val_loss: 0.6142 - val_accuracy: 0.6901\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6655 - val_loss: 0.6145 - val_accuracy: 0.6901\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6206 - accuracy: 0.6664 - val_loss: 0.6116 - val_accuracy: 0.6888\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6186 - accuracy: 0.6658 - val_loss: 0.6124 - val_accuracy: 0.6888\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6560 - val_loss: 0.6098 - val_accuracy: 0.6888\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6639 - val_loss: 0.6108 - val_accuracy: 0.6862\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6615 - val_loss: 0.6131 - val_accuracy: 0.6913\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6195 - accuracy: 0.6649 - val_loss: 0.6099 - val_accuracy: 0.6862\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6644 - val_loss: 0.6127 - val_accuracy: 0.6862\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6579 - val_loss: 0.6138 - val_accuracy: 0.6837\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6167 - accuracy: 0.6731 - val_loss: 0.6157 - val_accuracy: 0.6849\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6182 - accuracy: 0.6640 - val_loss: 0.6152 - val_accuracy: 0.6786\n"
     ]
    }
   ],
   "source": [
    "model = init_keras_model(dense1=750, dense2=150, dropout=0.8, optimizer='sgd')\n",
    "\n",
    "history = model.fit(X_train_tensor,\n",
    "          y_train_tensor,\n",
    "          epochs=n_epochs,\n",
    "          validation_data=(X_val_tensor, y_val_tensor),\n",
    "          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(history):\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax1.plot(history.history['loss'], label='Training Loss', c='red')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy', c='blue')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Training Loss/Accuracy')\n",
    "\n",
    "\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='center right')\n",
    "\n",
    "\n",
    "\n",
    "    ax3 = plt.subplot(1, 2, 2)\n",
    "    ax3.plot(history.history['val_accuracy'], label='Validation Accuracy', c='purple')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "\n",
    "    ax3.plot(history.history['accuracy'], label='Training Accuracy', c='orange')\n",
    "    ax3.set_title('Training Accuracy/Validation Accuracy')\n",
    "    ax3.legend(loc='lower right')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_for_chemists_tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
