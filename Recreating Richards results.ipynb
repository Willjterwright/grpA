{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import tensorflow as tf\n",
    "from rdkit.Chem import Descriptors as ds\n",
    "from rdkit.Chem import Fragments as fr\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is not important, it just gets the number of threads on your CPU and returns that number.\n",
    "import threading\n",
    "\n",
    "total_threads =threading.active_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure TensorFlow to use multiple CPU cores - This will speed up the training (hopefully)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(total_threads)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(total_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets import the dataset - the featurizer and splitter dont matter as we will be combining the datasets and\n",
    "#designing our own featurizer\n",
    "tasks, datasets, transformers = dc.molnet.load_tox21(\n",
    "    featurizer='GraphConv', \n",
    "    splitter='random')\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X', 'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11',\n",
       "       'y12', 'w1', 'w2', 'w3', 'w4', 'w5', 'w6', 'w7', 'w8', 'w9', 'w10',\n",
       "       'w11', 'w12', 'ids'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets convert them to dataframes because deepchem is a piece of shit and it cant handle numpy arrays <-codeium generated this sentence\n",
    "\n",
    "train_df = train_dataset.to_dataframe()\n",
    "test_dataset = test_dataset.to_dataframe()\n",
    "valid_dataset = valid_dataset.to_dataframe()\n",
    "\n",
    "#This line concatenates all the dataframes meaning all the data is in one dataframe, we will resplit them later with SKLEARN\n",
    "dataset = pd.concat([train_df, test_dataset, valid_dataset])\n",
    "\n",
    "\n",
    "#print the columns\n",
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>...</th>\n",
       "      <th>w4</th>\n",
       "      <th>w5</th>\n",
       "      <th>w6</th>\n",
       "      <th>w7</th>\n",
       "      <th>w8</th>\n",
       "      <th>w9</th>\n",
       "      <th>w10</th>\n",
       "      <th>w11</th>\n",
       "      <th>w12</th>\n",
       "      <th>ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.186419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>1.186419</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>1.186419</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>1.182927</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.182927</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>6.364256</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>6.466667</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.364256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.182927</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>6.364256</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>1.182927</td>\n",
       "      <td>16.767081</td>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>1.186419</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.767081</td>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>&lt;deepchem.feat.mol_graphs.ConvMol object at 0x...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.054158</td>\n",
       "      <td>1.145708</td>\n",
       "      <td>1.050492</td>\n",
       "      <td>1.029118</td>\n",
       "      <td>6.364256</td>\n",
       "      <td>1.038037</td>\n",
       "      <td>1.058113</td>\n",
       "      <td>1.182927</td>\n",
       "      <td>1.063423</td>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     X   y1   y2   y3   y4  \\\n",
       "0    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "1    <deepchem.feat.mol_graphs.ConvMol object at 0x...  1.0  0.0  0.0  0.0   \n",
       "2    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "3    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "4    <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  1.0  0.0   \n",
       "..                                                 ...  ...  ...  ...  ...   \n",
       "778  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "779  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "780  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  1.0  0.0  0.0   \n",
       "781  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "782  <deepchem.feat.mol_graphs.ConvMol object at 0x...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      y5   y6   y7   y8   y9  ...        w4        w5        w6        w7  \\\n",
       "0    0.0  0.0  0.0  0.0  0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1    0.0  0.0  0.0  0.0  0.0  ...  0.000000  1.145708  1.050492  1.029118   \n",
       "2    0.0  0.0  0.0  0.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "3    0.0  0.0  0.0  0.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "4    0.0  0.0  0.0  1.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "..   ...  ...  ...  ...  ...  ...       ...       ...       ...       ...   \n",
       "778  0.0  0.0  0.0  1.0  0.0  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "779  0.0  0.0  0.0  0.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "780  0.0  0.0  0.0  1.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "781  0.0  0.0  0.0  0.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "782  0.0  0.0  0.0  1.0  0.0  ...  1.054158  1.145708  1.050492  1.029118   \n",
       "\n",
       "           w8        w9       w10       w11        w12  \\\n",
       "0    1.186419  0.000000  1.058113  0.000000   0.000000   \n",
       "1    1.186419  1.038037  1.058113  0.000000   1.063423   \n",
       "2    1.186419  1.038037  1.058113  1.182927   1.063423   \n",
       "3    0.000000  1.038037  0.000000  1.182927   1.063423   \n",
       "4    6.364256  1.038037  1.058113  6.466667   1.063423   \n",
       "..        ...       ...       ...       ...        ...   \n",
       "778  6.364256  0.000000  0.000000  0.000000   0.000000   \n",
       "779  0.000000  1.038037  0.000000  1.182927   1.063423   \n",
       "780  6.364256  1.038037  1.058113  1.182927  16.767081   \n",
       "781  1.186419  1.038037  1.058113  0.000000  16.767081   \n",
       "782  6.364256  1.038037  1.058113  1.182927   1.063423   \n",
       "\n",
       "                                                   ids  \n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  \n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  \n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  \n",
       "3                                   O=c1oc2cc(O)ccc2s1  \n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  \n",
       "..                                                 ...  \n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  \n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  \n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  \n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  \n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  \n",
       "\n",
       "[7831 rows x 26 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>y10</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids   y1   y2   y3   y4  \\\n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  0.0  0.0  0.0  0.0   \n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  1.0  0.0  0.0  0.0   \n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  0.0  0.0  0.0  0.0   \n",
       "3                                   O=c1oc2cc(O)ccc2s1  0.0  0.0  0.0  0.0   \n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  0.0  0.0  1.0  0.0   \n",
       "..                                                 ...  ...  ...  ...  ...   \n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  0.0  0.0  0.0  0.0   \n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  0.0  0.0  0.0  0.0   \n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  0.0  1.0  0.0  0.0   \n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  0.0  0.0  0.0  0.0   \n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      y5   y6   y7   y8   y9  y10  y11  y12  tox  \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  3.0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "778  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  \n",
       "779  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "780  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  3.0  \n",
       "781  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0  \n",
       "782  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0  \n",
       "\n",
       "[7831 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets first select only the useful information, ie the y values (toxicity)\n",
    "#As well as the ids - ie the smiles strings\n",
    "data = dataset[['ids', 'y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12']]\n",
    "\n",
    "# Lets add up all the y values into one 'toxicity measure\n",
    "col_to_sum = ['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12']\n",
    "\n",
    "#This adds a new column called 'tox' which is the sum of each row \n",
    "data['tox'] = data[col_to_sum].sum(axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y4</th>\n",
       "      <th>y5</th>\n",
       "      <th>y6</th>\n",
       "      <th>y7</th>\n",
       "      <th>y8</th>\n",
       "      <th>y9</th>\n",
       "      <th>y10</th>\n",
       "      <th>y11</th>\n",
       "      <th>y12</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids   y1   y2   y3   y4  \\\n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  0.0  0.0  0.0  0.0   \n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  1.0  0.0  0.0  0.0   \n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  0.0  0.0  0.0  0.0   \n",
       "3                                   O=c1oc2cc(O)ccc2s1  0.0  0.0  0.0  0.0   \n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  0.0  0.0  1.0  0.0   \n",
       "..                                                 ...  ...  ...  ...  ...   \n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  0.0  0.0  0.0  0.0   \n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  0.0  0.0  0.0  0.0   \n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  0.0  1.0  0.0  0.0   \n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  0.0  0.0  0.0  0.0   \n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      y5   y6   y7   y8   y9  y10  y11  y12  tox  tox_bin  \n",
       "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0        1  \n",
       "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "4    0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  3.0        1  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...      ...  \n",
       "778  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0        1  \n",
       "779  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0        0  \n",
       "780  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  3.0        1  \n",
       "781  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0        1  \n",
       "782  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  1.0        1  \n",
       "\n",
       "[7831 rows x 15 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets add another row with a binary (1 or 0) that tells us if the compound is toxic against one or more of the assays or not\n",
    "data['tox_bin'] = data['tox'].apply(lambda x: 1 if x>0 else 0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of the y columns - we dont need them anymore\n",
    "data.drop(columns=['y1', 'y2', 'y3', 'y4', 'y5', 'y6', 'y7', 'y8', 'y9', 'y10', 'y11', 'y12'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids  tox  tox_bin\n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  0.0        0\n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  1.0        1\n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  0.0        0\n",
       "3                                   O=c1oc2cc(O)ccc2s1  0.0        0\n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  3.0        1\n",
       "..                                                 ...  ...      ...\n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  1.0        1\n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  0.0        0\n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  3.0        1\n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  1.0        1\n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  1.0        1\n",
       "\n",
       "[7831 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurisation of the dataset\n",
    "\n",
    "Lets get all the features we think we need from the dataset, I have copied richard here but feel free to have a play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fr_Al_COO',\n",
       " 'fr_Al_OH',\n",
       " 'fr_Al_OH_noTert',\n",
       " 'fr_ArN',\n",
       " 'fr_Ar_COO',\n",
       " 'fr_Ar_N',\n",
       " 'fr_Ar_NH',\n",
       " 'fr_Ar_OH',\n",
       " 'fr_COO',\n",
       " 'fr_COO2',\n",
       " 'fr_C_O',\n",
       " 'fr_C_O_noCOO',\n",
       " 'fr_C_S',\n",
       " 'fr_HOCCN',\n",
       " 'fr_Imine',\n",
       " 'fr_NH0',\n",
       " 'fr_NH1',\n",
       " 'fr_NH2',\n",
       " 'fr_N_O',\n",
       " 'fr_Ndealkylation1',\n",
       " 'fr_Ndealkylation2',\n",
       " 'fr_Nhpyrrole',\n",
       " 'fr_SH',\n",
       " 'fr_aldehyde',\n",
       " 'fr_alkyl_carbamate',\n",
       " 'fr_alkyl_halide',\n",
       " 'fr_allylic_oxid',\n",
       " 'fr_amide',\n",
       " 'fr_amidine',\n",
       " 'fr_aniline',\n",
       " 'fr_aryl_methyl',\n",
       " 'fr_azide',\n",
       " 'fr_azo',\n",
       " 'fr_barbitur',\n",
       " 'fr_benzene',\n",
       " 'fr_benzodiazepine',\n",
       " 'fr_bicyclic',\n",
       " 'fr_diazo',\n",
       " 'fr_dihydropyridine',\n",
       " 'fr_epoxide',\n",
       " 'fr_ester',\n",
       " 'fr_ether',\n",
       " 'fr_furan',\n",
       " 'fr_guanido',\n",
       " 'fr_halogen',\n",
       " 'fr_hdrzine',\n",
       " 'fr_hdrzone',\n",
       " 'fr_imidazole',\n",
       " 'fr_imide',\n",
       " 'fr_isocyan',\n",
       " 'fr_isothiocyan',\n",
       " 'fr_ketone',\n",
       " 'fr_ketone_Topliss',\n",
       " 'fr_lactam',\n",
       " 'fr_lactone',\n",
       " 'fr_methoxy',\n",
       " 'fr_morpholine',\n",
       " 'fr_nitrile',\n",
       " 'fr_nitro',\n",
       " 'fr_nitro_arom',\n",
       " 'fr_nitro_arom_nonortho',\n",
       " 'fr_nitroso',\n",
       " 'fr_oxazole',\n",
       " 'fr_oxime',\n",
       " 'fr_para_hydroxylation',\n",
       " 'fr_phenol',\n",
       " 'fr_phenol_noOrthoHbond',\n",
       " 'fr_phos_acid',\n",
       " 'fr_phos_ester',\n",
       " 'fr_piperdine',\n",
       " 'fr_piperzine',\n",
       " 'fr_priamide',\n",
       " 'fr_prisulfonamd',\n",
       " 'fr_pyridine',\n",
       " 'fr_quatN',\n",
       " 'fr_sulfide',\n",
       " 'fr_sulfonamd',\n",
       " 'fr_sulfone',\n",
       " 'fr_term_acetylene',\n",
       " 'fr_tetrazole',\n",
       " 'fr_thiazole',\n",
       " 'fr_thiocyan',\n",
       " 'fr_thiophene',\n",
       " 'fr_unbrch_alkane',\n",
       " 'fr_urea']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#Get all the possible functions within the fr(fragments) module and put them into a list\n",
    "#This is called a list comprehension it says 'for each element in this list, do this'\n",
    "#E.g for each name in the directory of the fr module, if the function is callable (i.e. it has a value) then append it to the list\n",
    "function_names = [name for name in dir(fr) if callable(getattr(fr, name))]\n",
    "\n",
    "#Remove the first 2 functions (they are not important for us)\n",
    "function_names.pop(0)\n",
    "function_names.pop(0)\n",
    "\n",
    "function_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09:55:21] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_sulfide</th>\n",
       "      <th>fr_sulfonamd</th>\n",
       "      <th>fr_sulfone</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids  tox  tox_bin  \\\n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  0.0        0   \n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  1.0        1   \n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  0.0        0   \n",
       "3                                   O=c1oc2cc(O)ccc2s1  0.0        0   \n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  3.0        1   \n",
       "..                                                 ...  ...      ...   \n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  1.0        1   \n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  0.0        0   \n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  3.0        1   \n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  1.0        1   \n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  1.0        1   \n",
       "\n",
       "     fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  fr_ArN  fr_Ar_COO  fr_Ar_N  \\\n",
       "0          0.0       0.0              0.0     0.0        1.0      0.0   \n",
       "1          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "2          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "3          0.0       4.0              4.0     0.0        0.0      0.0   \n",
       "4          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "..         ...       ...              ...     ...        ...      ...   \n",
       "778        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "779        0.0       4.0              4.0     0.0        2.0      0.0   \n",
       "780        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "781        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "782        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "\n",
       "     fr_Ar_NH  ...  fr_sulfide  fr_sulfonamd  fr_sulfone  fr_term_acetylene  \\\n",
       "0         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "1         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "2         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "3         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "4         0.0  ...         0.0           0.0         0.0                0.0   \n",
       "..        ...  ...         ...           ...         ...                ...   \n",
       "778       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "779       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "780       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "781       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "782       0.0  ...         0.0           0.0         0.0                0.0   \n",
       "\n",
       "     fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  fr_unbrch_alkane  \\\n",
       "0             0.0          0.0          0.0           0.0               0.0   \n",
       "1             0.0          0.0          0.0           0.0               3.0   \n",
       "2             0.0          0.0          0.0           0.0               3.0   \n",
       "3             0.0          0.0          0.0           0.0               5.0   \n",
       "4             0.0          0.0          0.0           0.0               0.0   \n",
       "..            ...          ...          ...           ...               ...   \n",
       "778           0.0          0.0          0.0           0.0               0.0   \n",
       "779           0.0          0.0          0.0           0.0               0.0   \n",
       "780           0.0          0.0          0.0           0.0               0.0   \n",
       "781           0.0          0.0          0.0           0.0               0.0   \n",
       "782           0.0          0.0          0.0           0.0               0.0   \n",
       "\n",
       "     fr_urea  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        0.0  \n",
       "..       ...  \n",
       "778      0.0  \n",
       "779      0.0  \n",
       "780      0.0  \n",
       "781      0.0  \n",
       "782      1.0  \n",
       "\n",
       "[7831 rows x 88 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Iterate through the rows in the dataframe\n",
    "# For each row, create a molecule object from the smiles strings\n",
    "# For each function in the list of functions, get the value for that function (in this case the total number of each functional group)\n",
    "#Then add that value to the dataframe for that row with the column heading as the name of the function\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    mol = Chem.MolFromSmiles(row['ids']) # Create molecule object\n",
    "    for function in function_names: # For each function in functions\n",
    "        data.at[index, function] = getattr(fr, function)(mol) # Set the value in the dataframe at the molecule index and column name to the value of the function\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09:57:19] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "#We have a list of functional groups, lets also get a binary value for if the molecule matches any of the tests below:\n",
    "#PAINS, BRENK, NIH\n",
    "\n",
    "from rdkit.Chem.FilterCatalog import FilterCatalog, FilterCatalogParams\n",
    "from rdkit.Chem import Descriptors as ds\n",
    "\n",
    "params_pains = FilterCatalogParams()\n",
    "params_pains.AddCatalog(FilterCatalogParams.FilterCatalogs.PAINS)\n",
    "pains = FilterCatalog(params_pains)\n",
    "\n",
    "params_brenk = FilterCatalogParams()\n",
    "params_brenk.AddCatalog(FilterCatalogParams.FilterCatalogs.BRENK)\n",
    "brenk = FilterCatalog(params_brenk)\n",
    "\n",
    "params_nih = FilterCatalogParams()\n",
    "params_nih.AddCatalog(FilterCatalogParams.FilterCatalogs.NIH)\n",
    "nih = FilterCatalog(params_nih)\n",
    "\n",
    "\n",
    "SMILES_strings = data['ids'] # Get smiles strings\n",
    "mol = [Chem.MolFromSmiles(formula) for formula in SMILES_strings] # Get a list of molecule objects\n",
    "\n",
    "for row in data.index: # For each row\n",
    "    molecule = mol[row] # Get the molecule\n",
    "    data.loc[row, 'pain'] = 1 if pains.HasMatch(molecule) else 0 # Does it match the PAINS filter?\n",
    "    data.loc[row, 'brenk'] = 1 if pains.HasMatch(molecule) else 0 # Does it match the BRENK filter?\n",
    "    data.loc[row, 'nih'] = 1 if pains.HasMatch(molecule) else 0 # Does it match the NIH filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>tox</th>\n",
       "      <th>tox_bin</th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "      <th>pain</th>\n",
       "      <th>brenk</th>\n",
       "      <th>nih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOC(=O)C(=O)c1csc(NC=O)n1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCCCCCCCCCCCCC(=O)OCC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O=c1oc2cc(O)ccc2s1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>O=Cc1ccccc1[N+](=O)[O-]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>CC(C)C(Br)C(=O)NC(N)=O</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   ids  tox  tox_bin  \\\n",
       "0                           CCOC(=O)C(=O)c1csc(NC=O)n1  0.0        0   \n",
       "1    CN(CCOc1ccc(NS(C)(=O)=O)cc1)CCc1ccc(NS(C)(=O)=...  1.0        1   \n",
       "2                              CCCCCCCCCCCCCCCC(=O)OCC  0.0        0   \n",
       "3                                   O=c1oc2cc(O)ccc2s1  0.0        0   \n",
       "4                     CCOP(=S)(OCC)Oc1nc(Cl)c(Cl)cc1Cl  3.0        1   \n",
       "..                                                 ...  ...      ...   \n",
       "778               NC(=O)C(=O)NN=Cc1ccc([N+](=O)[O-])o1  1.0        1   \n",
       "779  CC(=O)OC[C@H]1O[C@@H](O)[C@H](OC(=O)c2ccccc2C(...  0.0        0   \n",
       "780               c1cc2ccc3ccc4ccc5cccc6c(c1)c2c3c4c56  3.0        1   \n",
       "781                            O=Cc1ccccc1[N+](=O)[O-]  1.0        1   \n",
       "782                             CC(C)C(Br)C(=O)NC(N)=O  1.0        1   \n",
       "\n",
       "     fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  fr_ArN  fr_Ar_COO  fr_Ar_N  \\\n",
       "0          0.0       0.0              0.0     0.0        1.0      0.0   \n",
       "1          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "2          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "3          0.0       4.0              4.0     0.0        0.0      0.0   \n",
       "4          0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "..         ...       ...              ...     ...        ...      ...   \n",
       "778        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "779        0.0       4.0              4.0     0.0        2.0      0.0   \n",
       "780        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "781        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "782        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "\n",
       "     fr_Ar_NH  ...  fr_term_acetylene  fr_tetrazole  fr_thiazole  fr_thiocyan  \\\n",
       "0         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "1         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "2         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "3         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "4         0.0  ...                0.0           0.0          0.0          0.0   \n",
       "..        ...  ...                ...           ...          ...          ...   \n",
       "778       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "779       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "780       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "781       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "782       0.0  ...                0.0           0.0          0.0          0.0   \n",
       "\n",
       "     fr_thiophene  fr_unbrch_alkane  fr_urea  pain  brenk  nih  \n",
       "0             0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "1             0.0               3.0      0.0   0.0    0.0  0.0  \n",
       "2             0.0               3.0      0.0   0.0    0.0  0.0  \n",
       "3             0.0               5.0      0.0   0.0    0.0  0.0  \n",
       "4             0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "..            ...               ...      ...   ...    ...  ...  \n",
       "778           0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "779           0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "780           0.0               0.0      0.0   1.0    1.0  1.0  \n",
       "781           0.0               0.0      0.0   0.0    0.0  0.0  \n",
       "782           0.0               0.0      1.0   0.0    0.0  0.0  \n",
       "\n",
       "[7831 rows x 91 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x212ec3e0128>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets have a look at the distribution of the data:\n",
    "\n",
    "percent_toxic = len(data[data['tox_bin'] == 1]) / len(data) # Divides the length of the dataframe where tox_bin == 1 (ie toxic) by the length of the dataframe (all mols)\n",
    "percent_non_toxic = 1 - percent_toxic\n",
    "\n",
    "\n",
    "#Lets plot the train_df as a stacked bar graph:\n",
    "ax=plt.bar('Toxicity',percent_toxic, label='Toxic')\n",
    "plt.bar('Toxicity',percent_non_toxic, bottom= percent_toxic, label='Non-Toxic')\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.ylabel(\"Molecules %\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this dataset is unbalanced so we will need to deal with this before training- but first we need to split the data so we can balance ONLY the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fr_Al_COO</th>\n",
       "      <th>fr_Al_OH</th>\n",
       "      <th>fr_Al_OH_noTert</th>\n",
       "      <th>fr_ArN</th>\n",
       "      <th>fr_Ar_COO</th>\n",
       "      <th>fr_Ar_N</th>\n",
       "      <th>fr_Ar_NH</th>\n",
       "      <th>fr_Ar_OH</th>\n",
       "      <th>fr_COO</th>\n",
       "      <th>fr_COO2</th>\n",
       "      <th>...</th>\n",
       "      <th>fr_term_acetylene</th>\n",
       "      <th>fr_tetrazole</th>\n",
       "      <th>fr_thiazole</th>\n",
       "      <th>fr_thiocyan</th>\n",
       "      <th>fr_thiophene</th>\n",
       "      <th>fr_unbrch_alkane</th>\n",
       "      <th>fr_urea</th>\n",
       "      <th>pain</th>\n",
       "      <th>brenk</th>\n",
       "      <th>nih</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4817</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1204</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2312</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6264 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fr_Al_COO  fr_Al_OH  fr_Al_OH_noTert  fr_ArN  fr_Ar_COO  fr_Ar_N  \\\n",
       "4817        0.0       0.0              0.0     0.0        0.0      1.0   \n",
       "91          0.0       4.0              4.0     0.0        0.0      0.0   \n",
       "1212        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "176         0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "1204        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "...         ...       ...              ...     ...        ...      ...   \n",
       "1973        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "4375        0.0       0.0              0.0     0.0        0.0      0.0   \n",
       "3375        0.0       0.0              0.0     1.0        0.0      2.0   \n",
       "1958        0.0       0.0              0.0     0.0        0.0      4.0   \n",
       "2312        0.0       0.0              0.0     0.0        0.0      3.0   \n",
       "\n",
       "      fr_Ar_NH  fr_Ar_OH  fr_COO  fr_COO2  ...  fr_term_acetylene  \\\n",
       "4817       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "91         0.0       0.0     0.0      0.0  ...                0.0   \n",
       "1212       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "176        0.0       0.0     0.0      0.0  ...                0.0   \n",
       "1204       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "...        ...       ...     ...      ...  ...                ...   \n",
       "1973       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "4375       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "3375       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "1958       1.0       1.0     0.0      0.0  ...                0.0   \n",
       "2312       0.0       0.0     0.0      0.0  ...                0.0   \n",
       "\n",
       "      fr_tetrazole  fr_thiazole  fr_thiocyan  fr_thiophene  fr_unbrch_alkane  \\\n",
       "4817           0.0          0.0          0.0           0.0               0.0   \n",
       "91             0.0          0.0          0.0           0.0               0.0   \n",
       "1212           0.0          0.0          0.0           0.0               0.0   \n",
       "176            0.0          0.0          0.0           0.0               0.0   \n",
       "1204           0.0          0.0          0.0           0.0               0.0   \n",
       "...            ...          ...          ...           ...               ...   \n",
       "1973           0.0          0.0          0.0           0.0               0.0   \n",
       "4375           0.0          0.0          0.0           0.0               0.0   \n",
       "3375           0.0          0.0          0.0           0.0               0.0   \n",
       "1958           1.0          0.0          0.0           0.0               2.0   \n",
       "2312           0.0          0.0          0.0           0.0               0.0   \n",
       "\n",
       "      fr_urea  pain  brenk  nih  \n",
       "4817      0.0   0.0    0.0  0.0  \n",
       "91        0.0   0.0    0.0  0.0  \n",
       "1212      0.0   0.0    0.0  0.0  \n",
       "176       0.0   0.0    0.0  0.0  \n",
       "1204      0.0   0.0    0.0  0.0  \n",
       "...       ...   ...    ...  ...  \n",
       "1973      0.0   0.0    0.0  0.0  \n",
       "4375      1.0   0.0    0.0  0.0  \n",
       "3375      0.0   0.0    0.0  0.0  \n",
       "1958      0.0   0.0    0.0  0.0  \n",
       "2312      0.0   0.0    0.0  0.0  \n",
       "\n",
       "[6264 rows x 88 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data['tox_bin'], test_size=0.2) # Split the entire dataset into train and test sets\n",
    "\n",
    "X_train = X_train.drop(['ids', 'tox', 'tox_bin'], axis=1) # Drop the targets (toxicity indicator) from the X values of the training set\n",
    "X_test = X_test.drop(['ids', 'tox', 'tox_bin'], axis=1) # Drop the targets (toxicity indicator) from the X values of the test set\n",
    "\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set:  (6264, 88)\n",
      "Shape of test set:  (1567, 88)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of train set: ', X_train.shape)\n",
    "print('Shape of test set: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x212ec487d68>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets resample our data - there are a number of ways of doing this but the simplest is to jsut upsample the minority class\n",
    "#Ie duplicate the samples\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#First concatenate the training X and y into one dataframe\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "#Select the data in the majority and minority class\n",
    "df_majority = train_df[train_df['tox_bin']==0]\n",
    "df_minority = train_df[train_df['tox_bin']==1]\n",
    "\n",
    "#Then resample the minority class - this will duplicate randomly the training data in the minority class to match the number of samples in the majority class\n",
    "df_minority_upsampled = resample(df_minority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=len(df_majority),    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "#Then combine the majority and upsampled minority class\n",
    "train_df = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "#Split them back into X and y values\n",
    "y_train = train_df['tox_bin']\n",
    "X_train = train_df.drop(['tox_bin'], axis=1)\n",
    "\n",
    "#Then lets do what we did above to check it has worked!\n",
    "percent_toxic = len(y_train[y_train == 1]) / len(y_train)\n",
    "percent_non_toxic = 1 - percent_toxic\n",
    "\n",
    "\n",
    "#Lets plot the train_df as a stacked bar graph:\n",
    "ax=plt.bar('Toxicity',percent_toxic, label='Toxic')\n",
    "plt.bar('Toxicity',percent_non_toxic, bottom= percent_toxic, label='Non-Toxic')\n",
    "plt.xticks(rotation=-45, ha='left')\n",
    "plt.ylabel(\"Molecules %\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect, this looks perfectly balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test set:  (783, 88)\n",
      "Shape of validation set:  (784, 88)\n"
     ]
    }
   ],
   "source": [
    "#Lets now split our test set into vaL and test sets\n",
    "#Test will be used to test our model at the END of training\n",
    "#Whereas validation will be used to test the model DURING training\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5)\n",
    "\n",
    "print('Shape of test set: ', X_test.shape)\n",
    "print('Shape of validation set: ', X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order for the model to work, we first have to convert our dataframes into tensors:\n",
    "\n",
    "X_train_array = X_train.values\n",
    "X_test_array = X_test.values\n",
    "X_val_array = X_val.values\n",
    "\n",
    "y_train_array = y_train.values\n",
    "y_test_array = y_test.values\n",
    "y_val_array = y_val.values\n",
    "\n",
    "X_train_tensor = tf.convert_to_tensor(X_train_array)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test_array)\n",
    "X_val_tensor = tf.convert_to_tensor(X_val_array)\n",
    "\n",
    "y_train_tensor = tf.convert_to_tensor(y_train_array)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test_array)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is simply creating a tensorflow keras model for our machine learning\n",
    "#I have initialised it inside a function as it will allow me to search through all different combinations of parameters\n",
    "# e.g number of dense neurones, dropout rate, etc.\n",
    "\n",
    "def init_keras_model(dense1=700, dense2=100, dropout=0.7, optimizer='adam', callbacks=[]):\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dense1, activation='relu'), # Create a fully connected dense layer with dense1 number of neurons\n",
    "        tf.keras.layers.BatchNormalization(), # Batch normalisation reduces variability of our model\n",
    "        tf.keras.layers.Dropout(rate=dropout), # Dropout will reduce overtraining\n",
    "        tf.keras.layers.Dense(dense2, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(rate=dropout),\n",
    "        tf.keras.layers.Dense(50, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(rate=0.3),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid') # output layer\n",
    "    ])\n",
    "\n",
    "    # Call backs are a way of monitoring our model during training, for example to reduce the learning rate or save the best model as we go\n",
    "    #If callbacks is empty, compile the model without them\n",
    "    if callbacks == []: \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'],\n",
    "        )\n",
    "    else: # If we specify callbacks the model will be compiled with them\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy'],\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7803 - accuracy: 0.5458 - val_loss: 0.6621 - val_accuracy: 0.6314\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7147 - accuracy: 0.5570 - val_loss: 0.6428 - val_accuracy: 0.6454\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.5867 - val_loss: 0.6389 - val_accuracy: 0.6429\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.6005 - val_loss: 0.6401 - val_accuracy: 0.6327\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6091 - val_loss: 0.6356 - val_accuracy: 0.6531\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6202 - val_loss: 0.6343 - val_accuracy: 0.6518\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6320 - val_loss: 0.6242 - val_accuracy: 0.6696\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6451 - val_loss: 0.6164 - val_accuracy: 0.6798\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6493 - val_loss: 0.6218 - val_accuracy: 0.6773\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6467 - val_loss: 0.6127 - val_accuracy: 0.6824\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6467 - val_loss: 0.6163 - val_accuracy: 0.6824\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6521 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6493 - val_loss: 0.6210 - val_accuracy: 0.6722\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6211 - accuracy: 0.6672 - val_loss: 0.6151 - val_accuracy: 0.6811\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6152 - accuracy: 0.6653 - val_loss: 0.6134 - val_accuracy: 0.6837\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6593 - val_loss: 0.6176 - val_accuracy: 0.6735\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6159 - accuracy: 0.6675 - val_loss: 0.6164 - val_accuracy: 0.6760\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6085 - accuracy: 0.6740 - val_loss: 0.6229 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6106 - accuracy: 0.6694 - val_loss: 0.6233 - val_accuracy: 0.6760\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6038 - accuracy: 0.6756 - val_loss: 0.6258 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6742 - val_loss: 0.6237 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6848 - val_loss: 0.6304 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5989 - accuracy: 0.6834 - val_loss: 0.6251 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5961 - accuracy: 0.6889 - val_loss: 0.6246 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5984 - accuracy: 0.6816 - val_loss: 0.6276 - val_accuracy: 0.6671\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5934 - accuracy: 0.6863 - val_loss: 0.6344 - val_accuracy: 0.6671\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5954 - accuracy: 0.6853 - val_loss: 0.6311 - val_accuracy: 0.6607\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5867 - accuracy: 0.6917 - val_loss: 0.6307 - val_accuracy: 0.6735\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5861 - accuracy: 0.6909 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5796 - accuracy: 0.6945 - val_loss: 0.6334 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5844 - accuracy: 0.6937 - val_loss: 0.6338 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5781 - accuracy: 0.7001 - val_loss: 0.6367 - val_accuracy: 0.6696\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5781 - accuracy: 0.7038 - val_loss: 0.6317 - val_accuracy: 0.6735\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5741 - accuracy: 0.6989 - val_loss: 0.6296 - val_accuracy: 0.6798\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5688 - accuracy: 0.7069 - val_loss: 0.6358 - val_accuracy: 0.6798\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5727 - accuracy: 0.7085 - val_loss: 0.6384 - val_accuracy: 0.6645\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5719 - accuracy: 0.7105 - val_loss: 0.6442 - val_accuracy: 0.6531\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5672 - accuracy: 0.7100 - val_loss: 0.6372 - val_accuracy: 0.6709\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5662 - accuracy: 0.7163 - val_loss: 0.6321 - val_accuracy: 0.6773\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5652 - accuracy: 0.7114 - val_loss: 0.6380 - val_accuracy: 0.6556\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5611 - accuracy: 0.7154 - val_loss: 0.6366 - val_accuracy: 0.6607\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.5580 - accuracy: 0.7209 - val_loss: 0.6410 - val_accuracy: 0.6773\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.5597 - accuracy: 0.7100 - val_loss: 0.6384 - val_accuracy: 0.6811\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5597 - accuracy: 0.7170 - val_loss: 0.6413 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5508 - accuracy: 0.7254 - val_loss: 0.6396 - val_accuracy: 0.6888\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5571 - accuracy: 0.7160 - val_loss: 0.6348 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5527 - accuracy: 0.7199 - val_loss: 0.6444 - val_accuracy: 0.6684\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5431 - accuracy: 0.7302 - val_loss: 0.6600 - val_accuracy: 0.6594\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5496 - accuracy: 0.7189 - val_loss: 0.6437 - val_accuracy: 0.6786\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5477 - accuracy: 0.7209 - val_loss: 0.6526 - val_accuracy: 0.6671\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5387 - accuracy: 0.7287 - val_loss: 0.6493 - val_accuracy: 0.6696\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5449 - accuracy: 0.7222 - val_loss: 0.6490 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5387 - accuracy: 0.7286 - val_loss: 0.6645 - val_accuracy: 0.6696\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5411 - accuracy: 0.7268 - val_loss: 0.6523 - val_accuracy: 0.6696\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5379 - accuracy: 0.7265 - val_loss: 0.6479 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5355 - accuracy: 0.7352 - val_loss: 0.6568 - val_accuracy: 0.6684\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5292 - accuracy: 0.7347 - val_loss: 0.6636 - val_accuracy: 0.6633\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5392 - accuracy: 0.7277 - val_loss: 0.6638 - val_accuracy: 0.6773\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5344 - accuracy: 0.7311 - val_loss: 0.6546 - val_accuracy: 0.6773\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5300 - accuracy: 0.7352 - val_loss: 0.6631 - val_accuracy: 0.6722\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5317 - accuracy: 0.7350 - val_loss: 0.6756 - val_accuracy: 0.6709\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5316 - accuracy: 0.7379 - val_loss: 0.6531 - val_accuracy: 0.6773\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5296 - accuracy: 0.7391 - val_loss: 0.6617 - val_accuracy: 0.6684\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5252 - accuracy: 0.7381 - val_loss: 0.6653 - val_accuracy: 0.6811\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5276 - accuracy: 0.7404 - val_loss: 0.6536 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5215 - accuracy: 0.7427 - val_loss: 0.6588 - val_accuracy: 0.6862\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5260 - accuracy: 0.7386 - val_loss: 0.6617 - val_accuracy: 0.6811\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5235 - accuracy: 0.7464 - val_loss: 0.6492 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5219 - accuracy: 0.7415 - val_loss: 0.6626 - val_accuracy: 0.6786\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5225 - accuracy: 0.7443 - val_loss: 0.6551 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5237 - accuracy: 0.7375 - val_loss: 0.6554 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5213 - accuracy: 0.7386 - val_loss: 0.6648 - val_accuracy: 0.6811\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5147 - accuracy: 0.7482 - val_loss: 0.6570 - val_accuracy: 0.6849\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5124 - accuracy: 0.7469 - val_loss: 0.6504 - val_accuracy: 0.6786\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5145 - accuracy: 0.7481 - val_loss: 0.6633 - val_accuracy: 0.6862\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5117 - accuracy: 0.7477 - val_loss: 0.6601 - val_accuracy: 0.6824\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5102 - accuracy: 0.7513 - val_loss: 0.6641 - val_accuracy: 0.6786\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5121 - accuracy: 0.7498 - val_loss: 0.6624 - val_accuracy: 0.6798\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5081 - accuracy: 0.7504 - val_loss: 0.6797 - val_accuracy: 0.6901\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5093 - accuracy: 0.7496 - val_loss: 0.6747 - val_accuracy: 0.6773\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5096 - accuracy: 0.7496 - val_loss: 0.6727 - val_accuracy: 0.6786\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5079 - accuracy: 0.7521 - val_loss: 0.6673 - val_accuracy: 0.6824\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5119 - accuracy: 0.7491 - val_loss: 0.6774 - val_accuracy: 0.6709\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5094 - accuracy: 0.7509 - val_loss: 0.6662 - val_accuracy: 0.6849\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5026 - accuracy: 0.7566 - val_loss: 0.6740 - val_accuracy: 0.6798\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5077 - accuracy: 0.7492 - val_loss: 0.6824 - val_accuracy: 0.6658\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5050 - accuracy: 0.7516 - val_loss: 0.6870 - val_accuracy: 0.6811\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5002 - accuracy: 0.7575 - val_loss: 0.6815 - val_accuracy: 0.6645\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4974 - accuracy: 0.7538 - val_loss: 0.6879 - val_accuracy: 0.6760\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4993 - accuracy: 0.7573 - val_loss: 0.6770 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5039 - accuracy: 0.7557 - val_loss: 0.6658 - val_accuracy: 0.6786\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5136 - accuracy: 0.7455 - val_loss: 0.6724 - val_accuracy: 0.6747\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5022 - accuracy: 0.7504 - val_loss: 0.6885 - val_accuracy: 0.6824\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5008 - accuracy: 0.7567 - val_loss: 0.6848 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4892 - accuracy: 0.7589 - val_loss: 0.6902 - val_accuracy: 0.6747\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4907 - accuracy: 0.7615 - val_loss: 0.6891 - val_accuracy: 0.6798\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.4939 - accuracy: 0.7601 - val_loss: 0.6898 - val_accuracy: 0.6811\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4999 - accuracy: 0.7514 - val_loss: 0.6801 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4976 - accuracy: 0.7576 - val_loss: 0.6888 - val_accuracy: 0.6747\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4988 - accuracy: 0.7538 - val_loss: 0.6827 - val_accuracy: 0.6722\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4930 - accuracy: 0.7591 - val_loss: 0.6919 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5003 - accuracy: 0.7561 - val_loss: 0.6926 - val_accuracy: 0.6798\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4944 - accuracy: 0.7581 - val_loss: 0.6865 - val_accuracy: 0.6786\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4962 - accuracy: 0.7625 - val_loss: 0.6811 - val_accuracy: 0.6786\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4908 - accuracy: 0.7631 - val_loss: 0.6873 - val_accuracy: 0.6735\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4877 - accuracy: 0.7638 - val_loss: 0.6944 - val_accuracy: 0.6747\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4933 - accuracy: 0.7601 - val_loss: 0.6824 - val_accuracy: 0.6952\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4854 - accuracy: 0.7681 - val_loss: 0.6907 - val_accuracy: 0.6849\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4933 - accuracy: 0.7606 - val_loss: 0.7027 - val_accuracy: 0.6773\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4818 - accuracy: 0.7689 - val_loss: 0.6933 - val_accuracy: 0.6849\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4871 - accuracy: 0.7679 - val_loss: 0.6870 - val_accuracy: 0.6773\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4826 - accuracy: 0.7633 - val_loss: 0.6995 - val_accuracy: 0.6747\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4874 - accuracy: 0.7619 - val_loss: 0.6759 - val_accuracy: 0.6913\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4903 - accuracy: 0.7621 - val_loss: 0.6899 - val_accuracy: 0.6849\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4818 - accuracy: 0.7654 - val_loss: 0.6986 - val_accuracy: 0.6658\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4870 - accuracy: 0.7646 - val_loss: 0.6984 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4802 - accuracy: 0.7679 - val_loss: 0.6919 - val_accuracy: 0.6607\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4861 - accuracy: 0.7639 - val_loss: 0.6975 - val_accuracy: 0.6798\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4849 - accuracy: 0.7625 - val_loss: 0.7122 - val_accuracy: 0.6837\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4871 - accuracy: 0.7705 - val_loss: 0.6863 - val_accuracy: 0.6798\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4861 - accuracy: 0.7665 - val_loss: 0.6945 - val_accuracy: 0.6735\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4765 - accuracy: 0.7695 - val_loss: 0.7005 - val_accuracy: 0.6824\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4761 - accuracy: 0.7670 - val_loss: 0.6981 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4773 - accuracy: 0.7719 - val_loss: 0.7054 - val_accuracy: 0.6747\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4748 - accuracy: 0.7764 - val_loss: 0.7081 - val_accuracy: 0.6747\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.4710 - accuracy: 0.7747 - val_loss: 0.7034 - val_accuracy: 0.6633\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4795 - accuracy: 0.7664 - val_loss: 0.7019 - val_accuracy: 0.6696\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4784 - accuracy: 0.7615 - val_loss: 0.7027 - val_accuracy: 0.6607\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4799 - accuracy: 0.7615 - val_loss: 0.6953 - val_accuracy: 0.6875\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4801 - accuracy: 0.7665 - val_loss: 0.6966 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4757 - accuracy: 0.7720 - val_loss: 0.6982 - val_accuracy: 0.6786\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4776 - accuracy: 0.7695 - val_loss: 0.7109 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4799 - accuracy: 0.7699 - val_loss: 0.7027 - val_accuracy: 0.6747\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4780 - accuracy: 0.7714 - val_loss: 0.6958 - val_accuracy: 0.6811\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4729 - accuracy: 0.7743 - val_loss: 0.7001 - val_accuracy: 0.6709\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4685 - accuracy: 0.7807 - val_loss: 0.7035 - val_accuracy: 0.6760\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4684 - accuracy: 0.7738 - val_loss: 0.7007 - val_accuracy: 0.6735\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4742 - accuracy: 0.7742 - val_loss: 0.6828 - val_accuracy: 0.6811\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4728 - accuracy: 0.7729 - val_loss: 0.6899 - val_accuracy: 0.6722\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4701 - accuracy: 0.7702 - val_loss: 0.6956 - val_accuracy: 0.6735\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4729 - accuracy: 0.7710 - val_loss: 0.6898 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4738 - accuracy: 0.7733 - val_loss: 0.7072 - val_accuracy: 0.6735\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4625 - accuracy: 0.7779 - val_loss: 0.7234 - val_accuracy: 0.6582\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4722 - accuracy: 0.7754 - val_loss: 0.7059 - val_accuracy: 0.6773\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4621 - accuracy: 0.7784 - val_loss: 0.7184 - val_accuracy: 0.6798\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4698 - accuracy: 0.7753 - val_loss: 0.7035 - val_accuracy: 0.6696\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4700 - accuracy: 0.7719 - val_loss: 0.7051 - val_accuracy: 0.6735\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.4732 - accuracy: 0.7754 - val_loss: 0.6977 - val_accuracy: 0.6722\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4550 - accuracy: 0.7817 - val_loss: 0.7193 - val_accuracy: 0.6786\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4677 - accuracy: 0.7781 - val_loss: 0.7012 - val_accuracy: 0.6722\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4619 - accuracy: 0.7795 - val_loss: 0.7149 - val_accuracy: 0.6760\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4626 - accuracy: 0.7787 - val_loss: 0.7196 - val_accuracy: 0.6735\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4732 - accuracy: 0.7783 - val_loss: 0.6933 - val_accuracy: 0.6862\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4586 - accuracy: 0.7779 - val_loss: 0.7088 - val_accuracy: 0.6824\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4568 - accuracy: 0.7837 - val_loss: 0.7249 - val_accuracy: 0.6786\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4707 - accuracy: 0.7670 - val_loss: 0.7104 - val_accuracy: 0.6747\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4628 - accuracy: 0.7772 - val_loss: 0.6942 - val_accuracy: 0.6837\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4632 - accuracy: 0.7798 - val_loss: 0.7152 - val_accuracy: 0.6722\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4542 - accuracy: 0.7846 - val_loss: 0.7328 - val_accuracy: 0.6786\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.4543 - accuracy: 0.7782 - val_loss: 0.7311 - val_accuracy: 0.6798\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4674 - accuracy: 0.7744 - val_loss: 0.7126 - val_accuracy: 0.6811\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4627 - accuracy: 0.7796 - val_loss: 0.7154 - val_accuracy: 0.6760\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4644 - accuracy: 0.7724 - val_loss: 0.7265 - val_accuracy: 0.6837\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4617 - accuracy: 0.7796 - val_loss: 0.7146 - val_accuracy: 0.6798\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4562 - accuracy: 0.7865 - val_loss: 0.7211 - val_accuracy: 0.6747\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4578 - accuracy: 0.7817 - val_loss: 0.7149 - val_accuracy: 0.6722\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4651 - accuracy: 0.7806 - val_loss: 0.7036 - val_accuracy: 0.6798\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.4579 - accuracy: 0.7821 - val_loss: 0.7130 - val_accuracy: 0.6875\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.4563 - accuracy: 0.7805 - val_loss: 0.7136 - val_accuracy: 0.6760\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4545 - accuracy: 0.7802 - val_loss: 0.7282 - val_accuracy: 0.6760\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4651 - accuracy: 0.7803 - val_loss: 0.7095 - val_accuracy: 0.6824\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4636 - accuracy: 0.7748 - val_loss: 0.7023 - val_accuracy: 0.6798\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4643 - accuracy: 0.7778 - val_loss: 0.7175 - val_accuracy: 0.6837\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4489 - accuracy: 0.7870 - val_loss: 0.7332 - val_accuracy: 0.6735\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4598 - accuracy: 0.7807 - val_loss: 0.6960 - val_accuracy: 0.6786\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4521 - accuracy: 0.7826 - val_loss: 0.7245 - val_accuracy: 0.6620\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4591 - accuracy: 0.7812 - val_loss: 0.7073 - val_accuracy: 0.6709\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4585 - accuracy: 0.7815 - val_loss: 0.7242 - val_accuracy: 0.6735\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4507 - accuracy: 0.7842 - val_loss: 0.7112 - val_accuracy: 0.6786\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4576 - accuracy: 0.7802 - val_loss: 0.7235 - val_accuracy: 0.6709\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4541 - accuracy: 0.7859 - val_loss: 0.7069 - val_accuracy: 0.6569\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4490 - accuracy: 0.7827 - val_loss: 0.7435 - val_accuracy: 0.6633\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4569 - accuracy: 0.7840 - val_loss: 0.7163 - val_accuracy: 0.6773\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4527 - accuracy: 0.7881 - val_loss: 0.7241 - val_accuracy: 0.6722\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4603 - accuracy: 0.7812 - val_loss: 0.7226 - val_accuracy: 0.6645\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4547 - accuracy: 0.7838 - val_loss: 0.7368 - val_accuracy: 0.6747\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4469 - accuracy: 0.7919 - val_loss: 0.7261 - val_accuracy: 0.6696\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4607 - accuracy: 0.7820 - val_loss: 0.7299 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4493 - accuracy: 0.7860 - val_loss: 0.6996 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4573 - accuracy: 0.7793 - val_loss: 0.7262 - val_accuracy: 0.6811\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4500 - accuracy: 0.7865 - val_loss: 0.7145 - val_accuracy: 0.6645\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4548 - accuracy: 0.7838 - val_loss: 0.7198 - val_accuracy: 0.6786\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4553 - accuracy: 0.7820 - val_loss: 0.7138 - val_accuracy: 0.6709\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4495 - accuracy: 0.7841 - val_loss: 0.7345 - val_accuracy: 0.6709\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4511 - accuracy: 0.7828 - val_loss: 0.7189 - val_accuracy: 0.6709\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4518 - accuracy: 0.7859 - val_loss: 0.7160 - val_accuracy: 0.6684\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4541 - accuracy: 0.7894 - val_loss: 0.7244 - val_accuracy: 0.6786\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.4503 - accuracy: 0.7895 - val_loss: 0.7086 - val_accuracy: 0.6735\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4410 - accuracy: 0.7940 - val_loss: 0.7284 - val_accuracy: 0.6709\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.4463 - accuracy: 0.7869 - val_loss: 0.7193 - val_accuracy: 0.6811\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001 # This is how fast the model should learn\n",
    "n_epochs = 200 # This is how many times to train the model \n",
    "# (more epochs may lead to better accuracy but will also lead to overfitting)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE) # The optimizer is Adam and is the algorithm used to train the model\n",
    "lrr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=15, verbose=1, factor=0.5, min_lr=0.00001) # Reduce learning rate on plateau tells the model to reduce the speed at which it learns when it gets close to converging.\n",
    "mc = tf.keras.callbacks.ModelCheckpoint(filepath='weights_chk.h5',\n",
    "                                        monitor='val_accuracy',\n",
    "                                        verbose=0,\n",
    "                                        save_best_only=True,\n",
    "                                        save_weights_only=True,\n",
    "                                        mode='max') # Model Checkpoint is used to save the best model during training\n",
    "\n",
    "\n",
    "\n",
    "#precision = tf.keras.metrics.Precision()\n",
    "#recall = tf.keras.metrics.Recall()\n",
    "\n",
    "keras_model = init_keras_model() # Initialise the model\n",
    "\n",
    "\n",
    "history = keras_model.fit(X_train_tensor, \n",
    "                          y_train_tensor, \n",
    "                          epochs=n_epochs, \n",
    "                          validation_data=(X_val_tensor, y_val_tensor), \n",
    "                          ) # Fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss & accuracy\n",
    "\n",
    "This will plot the loss (how far each prediction is from the truth) - lower is better\n",
    "And the accuracy measures the % of correct predictions - higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "# Plot the training loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss', c='red')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "\n",
    "ax2=ax1.twinx()\n",
    "\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', c='blue')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "plt.title('Training Loss/Accuracy')\n",
    "\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='center right')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the training accuracy and validation accuracy\n",
    "\n",
    "This will tell us if the model is overfitting (training accuracy increases and val accuracy decreases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e55952ba58>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAACIRElEQVR4nO3dd3gUdf4H8PfuprdN7yGEEGqooTcLGkRROE/BctjArieHeup5nqdX+KmnZwUbiO0UFVFUWlCqVOk1BAgkkF43vezO749vZnYn2SS7ySabhPfrefLMZHZ2dmbLzGc+36aRJEkCERERURemdfYOEBEREbWGAQsRERF1eQxYiIiIqMtjwEJERERdHgMWIiIi6vIYsBAREVGXx4CFiIiIujwGLERERNTluTh7BxzFZDIhKysLvr6+0Gg0zt4dIiIisoEkSSgrK0NkZCS02ubzKD0mYMnKykJMTIyzd4OIiIjaIDMzE9HR0c0+3mMCFl9fXwDigP38/Jy8N0RERGQLg8GAmJgY5TrenB4TsMjFQH5+fgxYiIiIupnWqnOw0i0RERF1eW0KWBYvXoy4uDh4eHggKSkJ27Zta3H9zz//HMOGDYOXlxciIiJw9913o7CwULXOypUrMWjQILi7u2PQoEFYtWpVW3aNiIiIeiC7A5YVK1ZgwYIFePbZZ3HgwAFMnjwZ06dPR0ZGhtX1t2/fjjvuuAPz5s3DsWPH8PXXX2Pv3r2YP3++ss7OnTsxZ84czJ07F4cOHcLcuXMxe/Zs7N69u+1HRkRERD2GRpIkyZ4njB07FiNHjsSSJUuUZQMHDsSsWbOwaNGiJuv/5z//wZIlS3DmzBll2VtvvYWXX34ZmZmZAIA5c+bAYDBg7dq1yjrXXHMNAgIC8MUXX9i0XwaDAXq9HqWlpazDQkRE1E3Yev22K8NSW1uLffv2ITk5WbU8OTkZO3bssPqcCRMm4MKFC1izZg0kSUJubi6++eYbXHfddco6O3fubLLNadOmNbtNAKipqYHBYFD9ERERUc9kV8BSUFAAo9GIsLAw1fKwsDDk5ORYfc6ECRPw+eefY86cOXBzc0N4eDj8/f3x1ltvKevk5OTYtU0AWLRoEfR6vfLHPliIiIh6rjZVum3c9EiSpGabIx0/fhx//OMf8be//Q379u3DunXrkJ6ejgceeKDN2wSAZ555BqWlpcqfXLxEREREPY9d/bAEBwdDp9M1yXzk5eU1yZDIFi1ahIkTJ+LJJ58EAAwdOhTe3t6YPHky/vnPfyIiIgLh4eF2bRMA3N3d4e7ubs/uExERUTdlV4bFzc0NSUlJSElJUS1PSUnBhAkTrD6nsrKyydgAOp0OgMiiAMD48eObbHPDhg3NbpOIiIguLXb3dLtw4ULMnTsXo0aNwvjx4/H+++8jIyNDKeJ55plncPHiRXzyyScAgOuvvx733nsvlixZgmnTpiE7OxsLFizAmDFjEBkZCQB47LHHMGXKFLz00kuYOXMmvv/+e2zcuBHbt2934KESERFRd2V3wDJnzhwUFhbixRdfRHZ2NhITE7FmzRrExsYCALKzs1V9stx1110oKyvD22+/jccffxz+/v648sor8dJLLynrTJgwAV9++SX++te/4rnnnkN8fDxWrFiBsWPHOuAQiYiIqLuzux+Wror9sBAREXU/HdIPCxEREV1Czn0B5Pzi7L0A0INGayYiIiIHKtwL7LgNcPEBfl8I6NycujvMsBAREVFTZ5eLaX05ULzfqbsCMGAhIiLqPoy1wMG/APnND13jmNepAc5bjOWX7/xWuwxYiIiIuovzXwLHFwH7FnTs61z8EagtNv/PgIWIiIhsVrhbTEuPApKp414n/WMxDb1MTPN/BZzcqJgBCxERUXdRuEdMjVVAebpjt22sBrI3AGc+ArLWimVJrwM6D6CmACg75djXsxMDFiIiou7AWAOUHDL/X3rMsdvfcz+waRqw+x5AqgcCRwEBw4GgMeJxJxcLMWAhIiLqas79T2Q7LJUcBkx15v8dGbCUnQbOfSbmw68Cet8OjF4i/g+ZJKZODljYDwsREVFXUrAb2HE7oPMCbioURTKAuThI5siA5fjLok5M5LXA5T+pHwueKKZ5zLAQERF1vOJDQEWm47Z36DlgVTRQfs5x2wSA0++KqbFSHaQU7hXTgOFiWnK0+W3k7wSq82x7vcqLQPpyMT/4L00fDxkPQAOUnwaqcmzbZgdgwEJERD1f+Vlg/Whg42WAyeiYbZ79CKi6qO6vxB4Fu4GMlepltSXA+RXm//O2mueLGgKWuLvE1HBSHEvORuDAn0UfLYAIbFImABvGA/WVre/HyddEUVPoFCBkYtPH3QIA/8SGfe7g/l9awICFiIh6vpyN4qJckQ7kbW7/9moKRbACADkp9j/fVAdsvhbYfhNwcY15efqnogWQTA5Y6sqA0hNivtfNgM4TMNUAhuPAr7cBJ14BMr4Sj8t1X8rPAkdeaHk/Ki8CaQ11VQZZya7IRi0GrjsORM+y+RAdjQELERF1L+dXANtuEtkIW+VtM8+f+1/796HksHk+/9fWMxmSJDIfxpqG52wHaovE/KFnRP0RSTIXB/V9QEwLdojgpmgfAAnwigG8IgH9IPH4kReBmnwxLwdilpVjT74KFB1Q70t9hXn+8HMiQAqZBEQkN7//oZMA/UBA47ywgQELERF1Hfm/Aidebb6TMlM9sO+PQOZKIP0zO7ZrEbBkrjQHDm1VbNG82FTbeguaEy8D68eYe6i9aFGxteSwGBX5zFKg9LiobDt8EeAWKIKLogPmuixyE2P94IZj+ca8ndxNoohILrYJGA5IRmD3fPPxpr0HfK0HtswU68vjBY34D6DR2PkmdC4GLERE1DVIErB9DnDgCXWAYSn3F3Nl0tyfbdtuRSZQcR7Q6ACPcKCu1NwxWlvJ/aHIGYecjWJq2exYVn7OXDSTvlwUJ2X9KP4PGieme+4D9twr5vveB7j5A6GTxf9ZPwKpb4r5kIZlcsACABoXcWzlZ4HstUCdQYywfNmPgKu/GLhw5x2ilc9vj4gg5uJq4OcrAUhA7C1A8Nh2vR2dgQELERF1DSWHzPVCmmt5Y1mck7vZtgq0cvATMEL0LwIA5y22Y6oHLqy2vVUNYM6wRN8opjkpIiD4NgxImQRUZpnX3feYuV6KsRo49CxgSBWBxpRvRRBlrASgAYb8XWQ7ACBkipge+7d4X3ziRTADqAOWmN8DgUli/ug/xTR4POAVBUz+BtC6ivotv1wpOoSLmA74Joj1tG7AsH/bftxOxICFiIgcz3AKWJsEZHxt+3OyLCqfVmU1fby+Csj8VsxrtEBdicgetEYurgmZDPS+Tcxf/AEo2COKXLbOFH/rRokO1Fpjqjf3gTLgT2JafBDYcr0YMDD/V9Ei6ewnYmTli6tFcDLgcbHu6ffENHQK4BkBjP9EBBFXpgBDnge0uobHG7IpUkNQNuZdwMVTzMutdgCg30NA2BViXh5rSO7sLXwqMPajhv2uE3VfJn0FXPMbMOQFYNLXgE9c68fcBbDjOCKinq5wr7gQ9761814zbYkIJn77IxB1A6Bzb/05qoAl28rjPwH1ZYB3LOA/VAQdOT8DQaNb3q5c4TZ0ksiyBI4WTYRTxgPefUT/IgBQmQlsnAJc9gPgP9wcOEgmdWVTQ6qot+LiAwSPE/tSclgEUEFjgPpyURdl153m5wxYKIKRM0vFegAQNUNMI64Wf40FjABcvEVQFXen6IFW5tUL6P+YKEYLmSwq/R5/yfy4HLAAQNztokVRxjdA0huAq49YPuRvLb9vXQwzLEREPZlkEtmDHbeJfj86i1ynozrH3OV7S2qKgIKd5v+tZVjk4qDYW80X79bqsdQUiZGNAXER12iAK9YBsbeJ96b8tKjcOuU7wH+ICJTWjQK+8gJWRQErPIEVXo0qyTYUB/kPFYFMxDTxv28CcNlPQPJOIH4eEDAS6DUHGPkaMPQfgIsX0Odu83Yir2t537UuwJAXxXojX1U/ptGIgQlHvSHmQyaJeiyAmDaukxJ/D3DFGsAvoeXX7MKYYSEi6slKjpqzFbm/dE7lyqpsc5AAiD5CYm8TzXdLjop6GDE3iguyLHuDCCCUbTQKWMpOiwwLIAIW+eKcv120gNG5N1TavUm8/tRNYln+r2I9v/6AR6iYdw8EJn4O9LoJuPA9MOgp0WQ3eCKwc25Da5sa9T6kLQGiGgIMuUmz/1AxHfS02HbsrYBHsFg29kPr702/h4EzH4g6KH79Wn8vBy4Uf61x9RGZo8JdIlBy8W79Od0MAxYiop7MMgORtxUY/Ez7t1lfKbImMTeJi39jOQ2v6TdQXPQNqcCaRNGKRd4nr14iMxE8HgiZYG41EzJJBCGWRUKSJEYSNtWKzIr/ELHcI1xkcAp2AmGXAyVHzHVc5GVyhVu5dY2lmN+JP5lHMHDFWlGRtzJDtOapLRIjGOf+LIpmXLzNFW4DhompeyAw8Anb3jvfeOD60x0TUERdJwKWlvpT6cZYJERE1JPlWAQs+b+KCqP2OvgM8Nuj5hY5+xaIAGLnndbXl4uDom8AEh4U8+VnAVc90O+PgHuwCAjOfADsvgf4cQBw7nOxXvx8Ma3KMvfFkv6xyA7pPIHR74oiEI0GCLtSPJ69TkwzLbq5lyva5rUQsDRHqxMVUYNGAeFXA969Rese+biUIqFhtm/TkmeYuR6JIw38syjaSvyr47fdBTBgISLqqUx1QN4WMa/Rigqr8sXWVsWHgeP/B5x6G0h9QxTpnF0qHsv6sekIvpJkvrCHXyUqhnrFiCKQabtFnYuZGeLCOugpEUjIoxF7RYsmuoAokqktFlmO/Q2ta4b8XWQoZHJ25MyyhhZEjQKW+kqg6Dfxf6gdAYsljQaIul7MX/wByN0isj8aF3VLna5A5wZEzzS/nz0Mi4SIiJzp3BeidczIV811LBylcK9oseIeBASOEZ2K5W0199lh0/5ZVJg9/CyQsULUNdF5NPQp8jRw1TZzL6mGVNFniNZd1Alx8RRFIFpX8zounuLCGj1T/G+sFXVePCNF5sEtUBTFVGWL5sK1RaIOityEWBY9SxQtVWYAR/5mbmoMiCKhgp2i3xHPKJElaavoG4BTb4mApfigWNb3XsDVt+3bJLsxw0JE5CzGamDvQyIo+OVq0aLFkeTioNArRH0OQD36b2tMRnPLHM9Isb+Fe0TwccV6EbTk/2quDAuYsyshk8x9hujcWu72XecGBI4EPMMbXitCTKuyxOB+ABB6uXhdS1oXkcEBgBMNna2FXw24+IreXtMWNzx3cvu6nQ+ZArj6iY7livaJ7Q/5e9u3R23CgIWIyFku/mjuk6PkMLB5uhiV11HkCrfhU0UnZYCohGrZGqcleVtEtsQtAJi6WVyoASDhYbE9OVg49BfzNi80VHq17DPEXp6RYlqVJfozAcyD/TUWP0/0hyLrNVtU4gWAzFViak/9FWt0bkDENeb/Bz/t+GwYtYoBCxGRs5z9WExjbhLFIIV7xMB/jlBXbu7XJOxK0dRV5yXqhBx8Gth1N5CxsuVtnPtUTHvNFv13TP4G6PcIMLRhXJxBT4nMQ8kR4MJ3op+X3E2ifkd7OqlTApbs1gMWN725oq5GK4qZgic2PNhQabet9VcsRc8SU69ooP+C9m+P7MY6LEREzlCVK+qUAKJTsbypwN4HWx/111Yn/iOaAfv0FR2aaTSiCXHuz6JfFECM1Bt/r+j9VC6+kdVXmgOa3n8Q04hkdZNZtwDR6ufYP4EjLwLevcTyuD+I3mjbSi4Sqkg390Lr10zAAoheZDNXimbSHiGiR1vLfbQcd6etYueIQRNDJosO4KjTMWAhInKG8/8TY8QEjQH0AxoGv4OoIyFJ7atzUXEeONHQTfvwReZtDVgIVOeKAMY9GDjzoWhaXHIYuHqbuo7IsX83dIPfGwiZ2OQlFAP+BKS+LloflRwCoAEGPtX2fQfMGZa8LaKoyVVvDmKs8Y4BZmWY/w8aI7I8Ur3ItmgcUJig0QIJD7R/O9RmLBIiInKG9E/ENO4OMdUnipFz60rMHaw1x1gN/Hp788VHB/4s1gm9zNxMGACirgWuOyJGCB77vhhszy1ADJiX+oZ5vewUEbAAwPCXWg6e3AOBfo+a/4/5vQjA2kMOWAypYqofZF8A5+ItKvECjikOoi6BAQsRUWcrOSaax2pdgdhbxDKdm7kjMrnvkOZc+F5kaA48YW6VI8v4Gsj4SmQEkt5o+UIfPhUY0dC65sjfgcoLIjuz8w8AJNGFfuzs1o9nwMKGiq8ax/Sk2zib0lz9lZYM/z9RlNX33vbvD3UJDFiIiBxJkoD9TwAnX29+nYwVYho+TfSRIpP7Ryna1/JrXPzBPL9rnrllUdq7wK8NAVDCw+au41vS5y5Rt6W+QnRB/+MA0XxXnwiMbOEYLHkEiyKlqb+YMxvtIWdYZC3VX2lO2BXAhE9FBol6BNZhISJypKLfgJOvAmjoIdU3XgQU2RtEB2QaF+B8Q8AiZ1dkQaOA02g5w2KqFx3NAaKFTmUGsGWGCJTkcXP63idGCLaFRguMXgysSzK3yAmZCIz/pGlF3JYEDLd93dbI/bHI2pJhoR6HGRYiujQV7QOOvyK6f3ekwr0NM5IY4RcAdt0lRhH+7RFRFFR2SnS6Fn2D+rmBo8z71lxfKfm/in12DwYmN7TiydtqDlYSnxfj7WjtuB8NGA4kvQlETAcu+0n0XOvTx/bnO5rOQzTzljFgITDDQkSXAskkxpsJnQL49RPLds8XwcOJV0TF0j532teapL4CMNY0Ha24cI95/uwy0QxYHkH49PtA6QkxH3ld067d9YPExbrOAJSdNu+rpYurzc8PvwoY8wFQtFe0jAmZIvpLaYt+D4u/rsIzUnTJ7+IjxiKiSx4zLETU82WuAvbcC+yeJ/6vrxRNeQGgJl+MGGzZSqY1pjpg/ThgdbyoqGqpqCHDotGKTMi2hlY6cl0VORMSO6fpdrWuFhVvrdRjkSTgQkPAIg/I13c+MOY90eNrW4OVrkiueGtvCyHqsRiwEFHPJ3fGVrhbNPctPiSyLh5hwMA/i8dOLRYBgS3OfiQG66srEVkTWV2ZOYPS749iWl8usgTJuwHvOLHMxVtkSKxRioWs1GMxpIqO1LRu6g7ceiK54i2Lg6gBAxYi6vnkrIepDijabw4GAkcDic+JLuvLT6uLc5pjrAaO/sP8/+n3xWjDQENWRBJFGIP/IgILABj0tKh8O/5jMR5PwoPN95Ya1BCwFO5u+tjF78U07IqeP1JwRLIY8VnOJNElj3VYiKhnM9WLIEVWsNNcHBSYBLj6ADG/A859LkZNDh7bdBuGU8D+hUDwOJGFqbwAeEYBMInxbjK/BXrfYq5wGzRGdBE/erEIggYsFMtDJwM3FQNaXfP7G3qZeT+rctQtZjK+FtPo37XprehWet8G9Lq56QjNdMlihoWIerbSY4Cxyvx/wU5z/RA5m9F7rpie/1JkYRr77VEg6yfg8HPAkb+JZYl/BeLvE/Npi8VUzuQEjRbT+Hmifoll8+CWghUA8IkTAY9kAjK+MS8vPyv2W6MVAdalgMEKWWDAQkQ9S22JKKZJfUtc9OWsh6teTPO2AoaGeiZyR23hU0V9lpoCIHu9ent524CcDeLiKfc14psA9LlH9Hei0YmKtPk7zEVKgaPbdwy9Girkyh3MAebgJfQKwCO0fdsn6oYYsBBR92cyAlnrgV9vBb4NB/bcD+z7o8iYyEFE3FwRXNTki0DGM9LcEkXrAsTeKuZPvi6aKwOi+Ofwc2K+zzzgmv3A9APA1b+KrvS9IkWxBQD8crXo1h4wB0JtJXeHn7/d3Aop4ysxlV+P6BLDgIWIurfydODHfsDmaxqKdGrMTYhPvGIupgm93NxkGDC3xpHFzxMBTe7PwM9XAiVHRD8qeVtE5c/EZ0Xz2oDhon6KbPRi0R+KPNqy3wDATd++Y/KKBkImifnzX12axUFEjTBgISLnqC0VTYkb92Nir+MviQu6WwDQ7xHgmt+AGadEy5/ig+IPEPVCgsebn9c4YPFPBC5fI4qOCnYAa4aKzuUAoO/9Ioiwxi0AuHytuRlzxPT2HY9M7rY/bQnwW8O2Qy9ncRBdshiwEFHnkyRgx+3Abw+LMWzyf23bduorgfNfiPnJK4FRb4niGPdAIH6+eT2PMBFwqAIWK8U2EcnAtN0ii6JxAfz6i/okQ/7W8n5oXYBRbwCzLgAj/9O2Y2ks5iaR8Sk/LSr8AkAvG0ZOJuqh2KyZiDpfxtfmi3B1HvDzFcCEL4Bev7dvO5mrRDf23r3NzYFlA/4EpL0DSEZRCVajAUJaCVgAEaRMPyDqxbTWoqcxryj71m+JZxgw6RtRJAWIYqg+dztu+0TdDAMWIupctcWiQiwADHxSFOdkrgQOPNlywGJIE8UvHsHmZWc/EtM+dzcdB8int6hIe+4zIOzyhmV9gGGLRIdunmEt76e9wUpHiJkl/oiIAQsRdbJ9fwKqc0UmY+g/RB8pmSuBinSgpqjpYIKACFbWJIoeZK87KgYILD8nKshCIwYutGbMu6Kn1OhZ5mWDn+6AgyKijsY6LERkP8nUtued/hBI/1hkQ8Z8AOjcATd/kfkAzBVkjdVA2Rnz8zK+Aky1QPkZIPVNsUzurC18KuAda/31XLxFE2GdW9v2l4i6DAYsRGSfk28AKzyB7A3if2M1sOkaYPtsoL6i+ecV7hWVbAFg6D9FN/WygBFiWtzQhf7eh4Ef+gIXfxT/X1hlXvfYv4Azy4ATDZVbEx5s/zERUZfHgIWImjKkAuvHAxd/avrY6XdFtuPQs6K1z9mPRe+wGV8Dm68VIxY3ZqoHts8Rz4ueCQx6Sv144EgxLTog1s1s6NX16L+AioyGrvQ1gN9AUcl29zwAkghWYm505JETURfFgIWImjr5OlC4y5zFkJWfAwwnxXzRb6IOyfGXGh7UiG7vN13TdDyewr2ijopbADDu46YVZC0zLIV7RFACiH042BDchEwCRr1tfk7YVCDpjXYeKBF1FwxYiEhNkoCshqKYwj3q4CN7nXrdX28TgYh7MHDVZsDVT3S6lrtJvV5OipiGX2W9F1g5YDGkAhe+Uz92/ksxjfkdEH4lMOBx0Tnb5K85OB7RJYQBCxGplRw29z5rrBT/y7LWimmfe0SWpCZf/N9/ARA6BQi/Wvxfeky9zZyNYhp+lfXX9AxvGNdHEgMXAuaeY2XRDV3Sj/wPcMUaka0hoksGAxYiUpMrusryd4qpsbahGTGAfg8DMQ19prj4iv8BQD9ITEuPm59fVw4UNGyjuYAFMGdZ6krFdOBCcwAUMEL0q0JElywGLESkJgcs3nFiWrBDTPO3i1ZAHmGi6/ohLwL6RGDEy6JpMgD4WQlY8rYCUr3Yntx82ZqAkeZ5336iqfLwRYB+MDD4WUccGRF1Y20KWBYvXoy4uDh4eHggKSkJ27Zta3bdu+66CxqNpsnf4MGDlXWWL19udZ3q6uq27B4R2aLOAOy8Ezj9gXlZdT5QuFvMD/m7mMoBi1x/JWKaKA7SDwCuOwIkPGB+vmWGRZLEvGX9lZYEjjDPRyQ3LEsSHcXZ22U/EfU4dgcsK1aswIIFC/Dss8/iwIEDmDx5MqZPn46MjAyr67/xxhvIzs5W/jIzMxEYGIibb75ZtZ6fn59qvezsbHh4eLTtqIguVUX7gW03mYtxWrL/CSD9E2DvQ6InWaChjookMigxvxOBScV50Toos6EvlJZGI/brJ55TVwJU54hlcv2ViKtb3h/LDEt4K+sS0SXH7oDltddew7x58zB//nwMHDgQr7/+OmJiYrBkyRKr6+v1eoSHhyt/v/32G4qLi3H33epBvDQajWq98PDwth0R0aXst0dEN/e/XAlcWA1U5QKnFouO1owWGcvsDcCZhsyKVA8celo8nvaOWBY5A3D1BfyHiv+3/V6MGuweBERe0/zr6zwAn75ivvQ4UJUDlB4FoAFCr2h5371jRV0Vr15AWCvrEtElx66xhGpra7Fv3z48/bR6LI7k5GTs2LHDpm0sXboUV111FWJj1V1pl5eXIzY2FkajEcOHD8c//vEPjBgxopmtADU1NaipqVH+NxgMdhwJUQ+U/6u5cquxGtj2OwAaMVoxABz5u2jN4x4IHP6bWBZ1g2jCnPktsPEK0YzZVQ/EN9xQBE8Q3eXLPdCOed9cX6U5+kFA2SkRsFScF8sCRqgHLbRGowGSd4n9dfG069CJqOezK8NSUFAAo9GIsDD1KKdhYWHIyclp9fnZ2dlYu3Yt5s+fr1o+YMAALF++HKtXr8YXX3wBDw8PTJw4EWlpac1ua9GiRdDr9cpfTEyMPYdC1PMcf1lM+9wlmh1LJnHxDxoDeEUDlZnAgceBXXeLeZ8+wMT/AfENv8fCXWIU4ynfmyvHBk8wb7/P3bb1KqvUYzkGnF8h5m3tjVbnxmCFiKxq02jNGo1G9b8kSU2WWbN8+XL4+/tj1qxZquXjxo3DuHHjlP8nTpyIkSNH4q233sKbb75pdVvPPPMMFi5cqPxvMBgYtFDPVpkFXPweiL3F3AdJ6UmgtgjQeQIXVwPQAIOeFq1sYueI0Y31AxuKe94VFWAlkwhMhvxNDA445AXg3BdAfRkw/lMg7DLza4ZfKdbxjLK9V1m5pVDeVpFpAcS+EBG1g10BS3BwMHQ6XZNsSl5eXpOsS2OSJGHZsmWYO3cu3NxaHjlVq9Vi9OjRLWZY3N3d4e7ubvvOE3V3h/8KnP1IdIU/7iNRjHPqbfU60TcAfv3FvNzSBhB1SwYsEH+NeYYDyTtFwBI8rtFjEcD1Z0TQ4upj237KGRbDCTENTAJ8+9r2XCKiZthVJOTm5oakpCSkpKSolqekpGDChAnNPEvYsmULTp8+jXnz5rX6OpIk4eDBg4iIiLBn94i6H8nUdNyd5hT9JqYV54GfrzQHK+4NdUM0OmDQM23bD//BTYMVmWeY7cEK0BAwWWRcezG7QkTtZ3eR0MKFCzF37lyMGjUK48ePx/vvv4+MjAw88IDoi+GZZ57BxYsX8cknn6iet3TpUowdOxaJiYlNtvnCCy9g3LhxSEhIgMFgwJtvvomDBw/inXfeaeNhEXUDkgRsvEzUJ5l+sOXKrKY686CD4VeLoh2vaGDsMtG/SUU6YDICfgmdsectc/ECfOKA8rPi/9jZzt0fIuoR7A5Y5syZg8LCQrz44ovIzs5GYmIi1qxZo7T6yc7ObtInS2lpKVauXIk33rBeBl5SUoL77rsPOTk50Ov1GDFiBLZu3YoxY8a04ZCIuonKC6L3WED0cRJ/d/PrGlJF0OLiC1yxTrTc8e1nzny01IOsM+gHi4AleLxorkxE1E4aSZK7o+zeDAYD9Ho9SktL4efn5+zdIWpdxjfA9oYOFCOmiUAEEJ20eUWpRyI+9yWw41YRACTb1oWAU51+H9hzPzDxS1a4JaIW2Xr95lhCRM4id4EPiN5gq/OB0x8Cq+OAw8+p1y09Iqb6pkWqXVL8vcDNBgYrROQwDFiInKVADlgaOndLWwIceEIsylipXrekIWDxH9Jpu9cuGo3oKZeIyEEYsBB1loyvgW8CgfRPAVM9ULRPLO9zl5geeR6oKxXz5adFvyuy7hawEBE5GAMWos5gSBU9zNYWA4efB0oOA8ZK0Q1+omXxjwbwaBhHK2+LmNaVARXnxDwDFiK6RDFgIepoxmpg+2ygvkL8X5EOHHlBzAeNFk2A5S7w+94LxN4q5uWApeSomHpGiMEHiYguQQxYiDra/sdFRsU9BIi9TSy7uFpMg8aK6Zj3gaH/AEa8au4aXw5YShsCFj2zK0R06WrTWEJE1ILSk4BXJODqJyrPpi0Wy8d/AvjEA+f/Z15XDlj8B4s/AAiZDEAjOoqrymH9FSIiMMNC5DjGamDvw8BPA4HV8cCJV4HdDUNRDHoKiLxG9EQbMc38nOCxTbfjHgj4DxXzeVuB4gNingELEV3CGLAQOULlBWDDBHM2paZANFGuKxWdvQ39h3ndfo+KqW8/wCPU+vZCG4qF9j5o7g03MKlj9p2IqBtgwELUXpIE7LpHZELcg4Apq4ER/wFcfERAMvELda+1UdcBk74CJn3d/Dbleiy1ReK5I14B/LtJp3FERB2AdViI2uviD2IwQq0bcPUOwK+fWN73ftEhnJu+6XN63dzyNsOvArzjRAA09kMgYJjj95uIqBthwELUHsYaYP9CMT/gcXOwApgHJmwLVz/ghjOix1giImLAQqRSXwX8MlUUxUReB8TeIvpKsWSsAX6+EihLExmQ8jOij5TBf3HsvjBYISJSMGAhslSwU/wBonfak/8FZqSK1j2yzJVAQcOIyTX5Yjr8pfZlVIiIqEUMWIgsyU2IA5OA+nIRtORuVAcsp98T04SHgdDJolJs9O86f1+JiC4hbCVEZKn4oJhGzQR63SLm87aZHy89IfpG0eiAwc8AsXOAmBtZfENE1MGYYSGypGRYRgA6TzGfv000XdZozNmVqBmAV5Rz9pGI6BLEgIVIVl8lusMHgIARgJs/oHERncJVZgDuocDZj8Xjfe932m4SEV2KGLBQz1GdD7gFANo2fq1Lj4p+U9xDAM9IkVEJHAkU7hHFQsYqoK4E8O4NhCc7cs+JiKgVrMNCPUPpSWBVBLDj9rZvQy4OChhurpMSMllMc38Bjv1bzPf/I6DVtf11iIjIbgxYqGfI3yqyI3mbrT9eXynqobSkSA5YRpiXhTYELOmfABXnAI8wFgcRETkBAxbqGUqPi2l1HlBbqn7s4o/AV95A2hLzsh1zgbVJQEWmeZncQsgyYAmZJKaSUUwHPgm4eDl014mIqHUMWKhnKD1hni9LUz92foWYnl0uppUXgXOfAcX7Ra+2VTmAyQiUHBaPB1oELO5BgH5Qw3wIkPBAh+w+ERG1jAEL9QyG4+b5slPqxwp3i2nxPpF9yf3FYt004JergJOvAcZKQOcF+PRVPz/qBjEd/BfAxdvx+05ERK1iKyHq/uoMoumxzGARsNQUmTMukkl0+pbzs/i/1xxR96X0GHDwz2JZwLCmFWqH/F2MrmxZVERERJ2KGRbqXk68qq6LAogWQpYsMyyFe9SP5f4C5DYELPH3AFfvEKMsB48H3AKBPnc3fU2du2jezN5siYichhkW6j7K04EDTwDQALG3io7dAHNxkEYnKseqApaG4iC3AKC2GDj3uRiwUOsmKtS6eAEj/9OZR0FERG3ADAt1H0W/NcxIQMlR83K5wm3o5WJqOGVuwlzQELD0e0RM5dGVg8eztQ8RUTfCgIW6j6J95vnSIxbzDRmWqBmARgvUlwHVuSJoKWooEoq6HtAPNj8nbGrH7y8RETkMAxbqPiwDluLD5nlDQ4YlYLjoNh8QxULlZ4CaQkDrDvgPA8KuND8n3GKeiIi6PAYs1D1IkvUMS30VUH5WzPsNBHz7iXnDKXNxUMAIQOdmDlhcfICgMZ2z30RE5BAMWKjrydvatCVQxTlRaVZWckQEMWWnAEiihY9HqDlgKTtlrnAbPFZMI68F+t4HJL0BaF07+iiIiMiB2EqIuhZTPbDtRlGUE5AEBDdkQuTsiv8QwHCyoe+VDHP9Ff1A0ezYryFgyd5gzryETBRTnRsw5r3OOxYiInIYZlioaynYIYIVQF2xVg5YgsaJoh9AZFmK94t5eZmcYSk5JCrfhk4Bomd1+G4TEVHHYsBCXcuF1eZ5Q6p5Xg5YApNElgUACvcCZz8W8xHTxNQ3wfwcnz7ApJUs/iEi6gEYsFDXcvEH87wcsFhWuA1MAvyHivnUN0S/Kl7R5iyKV4wIVNwCgMt+ADyCO23XiYio47AOC3UdhlR1L7VlDQFLZQZQWyQyJf5DzJ2/1ZWKacLDgLbhq6zVAdMPAVK9uSdcIiLq9phhIeeorwKK9quXydkVuT5K2RnAVAcUNvRwq08U4/rIGRYA0HkA8fPV23H1YbBCRNTDMGAh59i/EFiXBJz7wrxMrr+S8ACg8xJZkvJ0IH+7WB48Xkw9I0WRDwD0vp3FPkRElwAGLNT5JBOQuVLMn1kqptV5QMGvYj56JuDXX8wbTgL528R8yCQx1WiAmN8Drv5ipGUiIurxGLBQ5ys5aq6HkrcJqMoFziwTgUzQGMA71hywFO0Dig+I+dDJ5m2M/QD4fZ7of4WIiHo8BizU+XJSzPOSCchYAZx+V/yf8JCY+jYELOkfi3W8e4vWQJbYXJmI6JLBgIU6X85GMfWJF9PDzwMV50X3+r1mi2VyhqXivJiGTAYREV26GLBQ5zLWiLGCACDpTTGtKxHT+HmAi6eYlwMWWSgDFiKiSxkDFupcBbsAY6UYqDDyGnNFWmhE6yCZ3MW+jBkWIqJLGjuOo44nScD5L0Xnbvk7xLKwqwCNFoi7QzRbjrpe9FArc/URdVYqLwDuIU0zLkREdElhwEId7/gi4NCz6mXhV4lp/HzAM8Ii02LBt78IWEImiabMRER0yWKREHWszFXmYMW7t5i6eJsHK9RogKgZ1numDbtCTGN+19F7SUREXRwzLNRxCn8DdvxBzPd7BBj1luhuX+sKeEW2/vxBfwZibgT8BnTsfhIRUZfHgIU6RsEuYNM1ooJt+NXAyP+K5b7xtm9D68qO4YiICAADFuoIBbuBX5KB+jJR/2TySvNoykRERG3AOizkeAefEsFK2BXAFesAV19n7xEREXVzDFjIsUz1QOEeMT/qHVHBloiIqJ0YsJBjlR4DjFWAqx/7TiEiIodhwELtI0lAxkrAcEr8X7hbTANHi47hiIiIHIBXFGqfzG+A7TcBW2eK4EUuDgoa49z9IiKiHoUBC7XPiVfF1HASKD7AgIWIiDpEmwKWxYsXIy4uDh4eHkhKSsK2bduaXfeuu+6CRqNp8jd48GDVeitXrsSgQYPg7u6OQYMGYdWqVW3ZNepM+TvNRUAAcGapqMMCMGAhIiKHsjtgWbFiBRYsWIBnn30WBw4cwOTJkzF9+nRkZGRYXf+NN95Adna28peZmYnAwEDcfPPNyjo7d+7EnDlzMHfuXBw6dAhz587F7NmzsXv3bqvbpC7i5Gti6h0npqffBySTGLTQlp5siYiIbKSRJEmy5wljx47FyJEjsWTJEmXZwIEDMWvWLCxatKjV53/33Xe48cYbkZ6ejtjYWADAnDlzYDAYsHbtWmW9a665BgEBAfjiiy9s2i+DwQC9Xo/S0lL4+fnZc0jUFuXpwA99RYAybQ/w85VAfbl4LOZG0VkcERFRK2y9ftuVYamtrcW+ffuQnJysWp6cnIwdO3bYtI2lS5fiqquuUoIVQGRYGm9z2rRpLW6zpqYGBoNB9Ued6MwyEayEJwNBo4GoG8yPsTiIiIgczK6ApaCgAEajEWFhYarlYWFhyMnJafX52dnZWLt2LebPn69anpOTY/c2Fy1aBL1er/zFxMTYcSTUbjkbxLT37WIae4v5MQYsRETkYG2qdKvRaFT/S5LUZJk1y5cvh7+/P2bNmtXubT7zzDMoLS1V/jIzM23beWq/2hKg6DcxHz5VTCOSAe/egEeY6IOFiIjIgewakS44OBg6na5J5iMvL69JhqQxSZKwbNkyzJ07F25ubqrHwsPD7d6mu7s73N3d7dl9cpS8LaI4yK8/4BUlluncgWv2ieWuPs7dPyIi6nHsyrC4ubkhKSkJKSkpquUpKSmYMGFCi8/dsmULTp8+jXnz5jV5bPz48U22uWHDhla3SU6S87OYhk1VL3cPBDyCO39/iIiox7MrwwIACxcuxNy5czFq1CiMHz8e77//PjIyMvDAAw8AEEU1Fy9exCeffKJ63tKlSzF27FgkJiY22eZjjz2GKVOm4KWXXsLMmTPx/fffY+PGjdi+fXsbD4s6VK4csFzp3P0gIqJLht0By5w5c1BYWIgXX3wR2dnZSExMxJo1a5RWP9nZ2U36ZCktLcXKlSvxxhtvWN3mhAkT8OWXX+Kvf/0rnnvuOcTHx2PFihUYO3ZsGw6JOlRVNlB6HIAGCLvC2XtD1KK8o3nY+dpOXPb8ZfCP9Xf27hBRO9jdD0tXxX5YOpDJCOycC9SVAvpBwIn/AAEjgen7nL1nRC364f4fsP/9/Zj49ERctegqZ+8OEVlh6/Xb7gwLXYIufAucb+jAL2uNmIZPbX59oi6iIqcCAJB/LN/Je0JE7cXBD6llkgQcf0nM+w8DNDoxb9lRHFEXVZHfELAcZ8BC1N0xw0Ity9sMFO0DdB7AlSlAfRlQlQOEsAUXdX2V+ZUAgOKzxairqoOrp6uT94iI2ooBCzWV+R2w71EgYhpgOCWW9bkb8AgBEAL49HHm3hHZTM6wQAIKUwsRPjzcpued23IO9VX16HtN3w7cOyKyB4uEqKnU14HKC8CZpUD+NkCjBQYsdPZeEdnFWGtETWmN8r+txUKmehO+uP4L/G/G/1CaUdpRu0dEdmLAQmr1lUDBTjEfMQ2ABoifD/jyTrO7SvlzCtY/vt7Zu9HpKgsqVf/bGrCU55ajtqwWklHC2Y1nO2LXiKgNGLBcakz1QHl684/nbwdMtYBXDHD5WmBOJTB6SeftXytMRhMkU49oid8pyrLLsOOVHdj12i6U55Y7e3c6lVIc1MDWgKUsq0yZT/+5hd8KEXUqBiyXmkPPAKv7ABd/tP643O1++FRAoxGVbTVd42tirDPi3aHvYumEpegh3Qd1OMuLdPHZYifuSeeTK9zKbM6wZJsDu/Rf0vldI+oiusaViDqHyQic/VjMZ66yvk7uL2LaeJygLqAkvQT5x/NxcfdFVORWtP4EUl2kS9JLnLcjAIrOFHVq0CRnWIL6BYnXP12E+pr6Vp9nmWEpzynvEk2iq0uqcezrYzjyxRGcWHUCdZV1zt6lHufC7guoLq129m5QCxiwXEoK9wA1DSffgh1NH68tFk2YgS45TpBlBcjCU4VO3JPuo6tkWKqKq/DBqA/w4bgPbQoaHKEiTwQsYcPC4K53h2SUUJRW1OrzLAMWAF2iHstPD/6Eb2Z/g29v+xZf3fgVNv99s7N3qUc59dMpLB23FOsXXnp1vboTBiyXkos/mOcNJ4GaQtEx3Mn/ir+sdQAkwG8A4BXptN1sTsn5EmW+MK37BSxVRVX4+dmfYbhgsGn9vUv2Im1tWovrHFh2AMe/Od7s4wXHC5T54nTnBSypq1NRXVKNyvxKlJ7vnJY3cpGQV4gXQgaGAADyT7SeLZEDFo8ADwBdox5L0WkRaPnFiG7LM7ZntLR6t1FfU4/Nf9+M3MO5Tt2P0+tOAwAu7Ljg1P2gljFguZRcXK3+P3+HaLa8f6H42/kHsbwLFgcBUF3obLlT7mp2vrYT2/+9HesWrGt13ZyDOVjz0Bp8fdPXqK2otbpO0ZkirJ63Gt/M+abZYERVJHS2pE377QgnvjmhzHdWpkcuEvIO8UbwoGAAttVjkQOWxFvEyPLnt5yHqd7UQXtpm+oSUVRx2fOXAQByD+fCZHTuPjnCsa+OYcsLW/D1zV87tTL9xV0XAYjvZk94X3sqBiw9mSQBxxYB574Eys8CpcdE1/oxN4rHC34F0j+zWL/hh9pFxwnq7kVC2fuyAQBpa9KaDUJkWfuyAAB1lXU4vVbc/aX+kIr3R72vXHQv7BR3g5JJwp639zTZRkV+happry0ZlmNfHcNHkz9yaP8jNYYanNlwxq79cARVhmVQQ4bFYkyhvUv24pOrPkFVcZXqeXKl24RrE+AR4IEaQw2yfsvqlH1ujhywRI2JgounC+oq6lB8pvtXoi5MFb/jwlOFSpajs9VV1SHnYA4A0XcP+97puhiwdHG15bVtL/Mv3Asc+guw41Zgy0yxLGQyEDlDzOf8AmR8LeanfA8kPgckPAREzWj/jneA7p5hkU+K9VX1rZ6c5XUB4Pg3x2GqN2Hto2uRvS8bexfvBQBc2GVOXx/48ABqympU25ADGzdfNwCAIdMAY62xxdfd8/YeZGzPwO63dtt4VK079eMp1et2VoZFDli8Q7wRNiQMAJBzQLyvkiRh64tbkf5zOk6sPKF6npxh8YvxQ/TYaABA3rE8216zsNLhmQJJkpSAxSvIy3wsFt+R7sqyIviu13c1u15VUZXd76tkMr9vLcnen63KoNlyM1Rb0Y7zMrUZA5YurDynHG8PeBtLEpe0LSVdesxi/qiYRt8AhEwU80V7gboSwCtaBClDXwRGvwNou+Z4K5Z3PkWni7pVfyzlOeUozzE3l7UsIrEm95C5TP/Uj6dwdMVRJWCTK4HKAYtGp0GNoQYHlx9UbUMOWGInx8LFwwWSSWr17lF+jRPfnHBYc165jo13qDeAzmutJBcJeYV4IXK0qJNVdLoIlQWVMGQalM/DMvAz1hmVyrq+Eb7wifAR28prvVVa2to0/Cf0P9jw5AaHHkddRZ3y+/fw90DY8J4TsFgGr2dTzloNDPOO5eGV0Ffw/d3f27XtjU9vxCshr7Raadry8wdavxmqLa/Fu0Pfbft5mdqMAUsnMxlNOPjxQez8707s/O/OFlPNax5Zg7KLZSg63cbmoIaGi2LQGMDFWwQi0bMA3wTAPdi8XuxtXaavleZIJgmlmeaLbX11vc2VV7uCnEPi4uLiKYbvOvXjKdRV1SH7QDYyd2Sq1pVMknIxktP/ax5eozxemFqIwrRCJaiZ8KQYiHL3G7tVQZwcsIQMDkFAnwAATYtjzqScQdEZcYI21ZtguCje05JzJcjen93u464tr1WKtMb8cYzYBydkWDwDPBE8QHznL+y+oLpIWc7LQYzWRQuvYC8lyLKlGf22f26DZJKw9529TTqtqyqqwt4le7Hzvzux+83dqu9ya+QsgdZVCxdPF2U8pB4RsDR8H+Wm57vfaJrZO5tyFpJRQvov9lV+ljOT2/61rcX15Porrl7iRq1xhf4aQw2OfHEE9dUio3Lok0MoPluMotNFSmVoZzm/7XyPqYBti659leqBDi4/iO/v+h4bFm7AhoUb8PEVHzcpQwfEj80yVd2mviBKG54fdycw4yQw7TfAJ050CBdsMdpy3Fz7t93JynPKYaozQaPTICBeXHy7U0sh+eLS/4b+8IvxQ215Lb6++Wu8n/Q+Ppr8EXKPmDMqJedKUFtWC527DiPnjwQA1JTWQOuiVY59xys7YKo3wTfSF1OenQIPfw8UnynGqZ9OKduRWwgFDwyGf5w/AHWwkLY2DZ8lf4aVt6wEIIpCJKM54Gmp9ZGtzm48i/rqegTEB2DArAFN9qGjmOpNqCoSvyuvEC8AQPQ4UbxzYZc6YMk/nq/0vyEXB/lE+ECj1ZgDllYyLBf3XFQCT2ONEfve26d6fNPzm7DmoTXYsHAD1j22Dj/M/8HaZqySAxYPfw9oNBpEjIgA0P0DltqKWiUQTH41GQBw+NPDTYZUkI/TcMFgc/8z5bnlSibv3OZzLb5X8ndhwO/E97PolDoI2fS3Tfj2tm+xet5qSCZJFVQ5s4+e2vJafJb8GT6a/BHW/Wldq8W9PQEDlk527EtRTBMzIQZ+0eLCdWDpAdU6VcVVyh21fEfeph+G4aSY+g0QxT4BQ82PhUwSU/9hgH+i/dvuBLlHcvH9Pd+jLLtMadLsF+WnNFF1ZMXbirwKfH/P903Sw+2x5509+PWVXyFJEnIPioAkfHg4Bt00CACQ9lMaIKHJSVA+uYYmhiLxVvNnM3jOYAy6WTxXLv6JHhcNNx83jLxXBDa7X7c4mTY04Q0ZZM6wqOoMvCbqDOQcyoGp3qRqNg40LRY69MkhbHhig11FRZk7xUU8bmocAuLEPtSU1qiC9LLsMqy+dzU+n/45Pp/+OX557pd2F0dVFpovel5BImCJGhcFQNxRqz5nCcjaKzKdcsDiG+kLAPAOsy1gkT8/udnx3nf2qi4g8l189HgRNGX8mmFzcYJlwAIAoUNCAY2oHFyeW469S/aq3rPqkmr8+OCPSsVta+oq67Dm0TVOvTuXv4se/h5IuC4BEUkRqK+ux7731cGeZbAhZwMbK88tx48P/IijK0TR98XdF1WPW8vcACIIMlwwQKPVKL+1xjdCaWtE1wJH/ncEax5dozrvODNgKTpTpGR9dr++G+8OfxefT/8cX930FQpOFrTybPt9f/f3+Or3Xzk1UGbA0okqCyqRvkmkNX/36e9w+YuXAwD2vLVHdfJKXZ2KirwKBCYEYuKfRX0Tu38YxmqgoqHsVj+w6eMJ9wN9HwDGvGf3cXSWH+79AQc/OohfX/5VqXuhj9UjMCEQgGMr3h769BAOfnQQ2xdtd8j2qkursfbRtdj4543I3JGp/MjDh4crJ0ZXb1dMfEp8voc/O6wUI1iuGz02GgHxAdBoNRj3p3HoM7UPAMBUJ74v8kV4zCNjoNFpkP5LOnIP56KquEpp7RIyMKRJhiXvaJ5Stm+qM6E0o1R5jyNHRcLFwwVFp4tU/WOsW7AOO1/dqapf0xrlQj0uGq5ervAJ91Htx7nN5/DeiPdw4MMDOL3uNE6vO41t/9yGvCO2VXJtjlwc5BnoCa2LVtkHQBQJycVdkaNE3RY5gJHfMyVgsaFIyHDRgGNfiRuRm7++Gb6RvijPKVeWmepNSgZt1vJZcPdzR11Fnc0VeRsHLG7ebkoRyo7/7MCah9Zg2z+3KRfpXW/swr5392Hz3zY3u82DHx/E3rf3IuXJFJv2oSPIxUEBfQKg0WgwbsE4AOpgz1hrVJ37rN2knN96Hu+NeA/73tuHH+//EcZao/J5ho8QxWdH/nfE6lhaF3aL9cKGhimZq5JzJcrrl2aWqs4zvy3+DYD5s3BmwCL/hnzCfeCud0fBiQKcXncaJ1aewJ53mrYabK+0NWk48e0Jp9bbYcDSwX64/we8N+I9lOeW4+R3JyEZJUSMjEBAnwAMuXUIvEK8UJpRihOrzMU/8gUr4doE5Qdnzw/jwq4LeC3mDRzePhhw1QMe4U1XcvUDxiwBgse2ur28o3l4f9T7+Hz6583e+UqShP9d9z+8qHsRL+pexKuRrzZ7N2TrMcgn4PSf05XKoPpebQ9Yqkuq8c7Ad7D2j2ubPCY3r2zPPlsqPlsMNLxV2/+9XTnRhg8PR9ToKNy15S48dOwhTF00FZGjI1XFCJYBi0arwZ2/3In5e+YjMikSMRNjoHPXKa8jt2LR99Jj4I0iMN31xi6c+FZ8n/yi/eDu526uw9Jwktv1hrpFRmFaofIehwwOQd/pYnRuuViopqwG1cXiwmlr3SFTvQkX95oDFgBK4FSSXoK0NWn4ZOonqMitQGhiKG5YeoNSOfbsz81XlFw1dxXeHvB2k7o/liwr3MpCB4fC1dsVtWW1MNYY4RnkiSG3DwFgDlgaZ1h8wlqvdLvvvX0w1ZsQOyUW0WOjMfrh0QDMd/UFqQUw1hjh5uOGwL6BiBobpXrN1jQOWAAo9Vh2/menskx+z+SO7loKiOQm8TmHctrU70ja2jT8N+a/OL6y7cWG8ndR/k4Mnj0YPuE+KMsqU753+SfyleAcUP/mJZOE7S9tx8dXfqwEmjWlNTj781nlvR390GhEj4+GsbZpMR1g/gyixkXBJ8IHrt6ukIySEkzJ72X48HAlSNRoNcrNZlcIWGIvi8XDxx/GrE9mYfjdwwEAZRfKWnim/SoLKpXfgFwXzBkYsHQgY60RB5cdRM7BHKx9dK3yIxx4k7iwuHi4YNSDowCoU/mWxQdy/xEFJwpsPrGc+ukUyrKrcXTHEFEcpNG0+RgOfXIIH4z5ANn7snF63ekmA8rJClMLkbYmDZJJgmSSUJ5d3q4uzS2bOOYdyVNS9vpYvXLiaK5IqCKvAmXZTX+w57edR8HJAhxYdqBJ4CVvqyS9xCGtYyyLXuT3xTvMW8kwxE6JhX+sv9U7S8uABRDBSGSSuJC7eroiZkIMANE6KCIpQnkdeTuHPj6k1JFImJEAAKoiocqCShz57Iiybfn4lSxWLz3ik+MBmJsBl100v5/W3ltZjaFGCfpyj+Sivqoe7np3BPcPVu1H8dli7H5TVBIe+PuBmL97PkbcMwKDZw8GAKRvtF7BsuRcCQ5/dhiFqYVYftlybFu0DWd/PotzW84p6XFAXeFWpnXRImp0lPJ/9LhopYjmwq4LkCSpaZFQQ4alsqBS+f0VpxejxmBuQi5/NxNvE5mzkfeOBDRA1m9ZMFw0KJ9n2LAwaLQaJWCRs0+taSlgAQA0/LzTN6ajtrxWCUZKz5eittx6fz/yhbq+qr5J4C+ZJOQdy2v2d1BZUInv7/oehgsGHPn8SIv7XphW2GzdCvk3In8ndG46jHpInA93/XcXJElqUvwgF9dUFVfhy5lf4uenf4ZklDB07lDlYn1sxTFc3GMOlOUA8sjnR5ock2UGUKPRILCv+mZIDlj6XtsXMz+aCVdvV4yYNwIJ14rfVWFqoRhBXpJQeKqw1exD4alCGOscU9fE8v3zjfTFsLnD0H9mfwDq32hFfoVSmb41xlqj1YrEcvGyPlYPNx+3du552zFg6UBFp4uUL/Dxr48rnWcN+v0gZZ3RD46G1lWLzB2ZyD2cq/qRhg8PR0BcAHTuOtRX19vcpbl80i3KDbReHGSj9F/S8d2d36G+ynwhaK7CpHx3FzslFsPvGg7A9jvxxgwXDEpwJ98hp65OBQD4x/ojKCFI2ZfGJwjJJOG9ke9hSeIS1FWpK+jJd0N1FXUwZKr3TT5B1VXW2dSEtTXW3ifVRcbCoJsGKcUIX8/+Wtm3sKFhVtePmxqnPO7mbT55RI+PRuToSKXi7PgnxmP6m9MBQKk/UlVUhRU3rkB9dT0ikiIweI4IEIrSisxZrFg9/Hv7AzA3c7b8LBuPtSOrr67HsonL8M6Ad5C1L0u5cEaPjYZGK66q8sUpe1+2cjGY+u+pSgsN+djObz1v9cQu39G7ernCVG/CL3/5BZ9e9Sk+vvxj/PTwT8p61jIs8nukzI+LRvjwcOjcdKgqrELxmWJVpVsA8Ar2AjTie1VVWIXSzFK83e9tfD79c/NrNXxf/KJF/RXvEG9EjBSBZPov6U0CUMvKv7ZoLWC59u1rAQCZOzJxZsMZ1W/CWl2GysJKVZDSOCjY8eoOLElcgt1vWq/3sW7BOuWYW8ownPzuJN7u9zZ+/svPVh9vnGEBgFH3j4LOXYes38T3R943+fOQK8T+MP8HnPrxFHTuOsx4fwZmfTwLw+4YBkAEJnUVdXDzdUPwwGD0v6E/dO46FJ4qVHUcaKwzKq005c9EPrcUphVCkiTlvNZnah/ETIjBU0VPYcZ7M+Df2x8uHi6or65HybkS/Pbub3i7/9stFsUc+eII3u7/Nra8uKXZdexh7f3zjRCBtvw9liQJH4z6AIsHL1YF2c355blf8FbCW0qGVqa0OGy4gXYWBiwdSPkxywkOSVxk5AwBIMof+04T6ffT60+jNKMU1SXV0LpqETIoBFoXrXJ3amv6UU6PFucFwOTVv837f/jTwwCAxFsT0WtSL7HNZnoplS8+8dPiEdhP3KW0lJYsTi/Grjd2We3xdc87eyAZJfS+vDeGzhUVheW7NH0vPfyi/eDi4SIqip4rUT23srASZRfLUFVU1SQDYzmujuV7WVteq7oIN+4nxHDBgE3Pb0LKUyn4+S8/Wx2PprqkGrvf3K2cFOT3yfLC0lzAonPTIfnVZGhdtEj9XgRmAfEBcPdzt7p+0n1J6D+zPy5/4XLVco1Gg+sWX4eEaxMw57s5SH4lGTpXUXzk5uOmXLwztmXA1csVV79ytap4zTLDoo8VmRd5mS0By5YXtyDvaB5M9Sbsem2XKt0uk0+ux1eKJqeNfw/hw8LhGeSJ2vJa5S7Zktx/zVUvX4Xr3r0OESMjlBT18a+OK0GqZS+3luQLkzzv4u6iBBcXdl1okmHRumiVSrvlueXIOSAqKMvN1AFzwCJnYwBz4JX+c7oqYwqYi/EKThZYbSHYmLWAJe7KOAyeMxhXv3I1Rj04Cn7RfjDWGrH1H1tVz7V2zmhcIdUyYJEkCQc+FI0AGjcGAERz/COfH1HOaS2NgH1shajDc+yrY1azNfIFVw5iAfEeysV0u17fpbx3A38vbrwK0wpRX12vVIT9w/o/IOneJGg0GvSa3AteIV5KwBY1JgpanRbuvu7oe426iBMQwxvUV9fDI8BDCVTkc1fhqUIUnCxAeXY5XDxclKymzk0HjUYDrU6rfO/yj+dj/wf7rb63luT39dQPp5pdx1Rvwt4le23qbdfa+yd/b8uzyyGZJFTkVaA0oxQ1pTVKfZ2WyOfxxp89A5ZLgPwhD7l1CIIHii+33MrDkuXJTT55hAwKgc5Np8xbbq818knXZNShpDS+TfturDXi5HeildGoB0YpqVJrmQOT0YRzm84pxyLfabaUYVn32DqsX7BeOalZkpeNeXSMUslUpo/VQ6M1p24b30FaZkcaByyW75/lfOMUaONj3PKPLdj64lbseHkHti/ajtX3NBqTCeLOZN1j67D9JVFpVx63Z9RDo5SgQK6fYU3iLYm4c/Od8I0SJxy5Mqg13iHeuOW7W9D/+qbBaOSoSNz2020YMHNAk8fkE2zwwGDcu/dexF0Rpypek1sJ+cf6K0VFNYYaVJdWqz7L8qymlRez9mXh15d/Vf4/9tUxnE0Rd6eWQYJ8cpWzQHLxqEyj1SDuSvPvwVJpZqkIgjTAwBsHYtT9o3Dfvvvw0PGHlKbichbTchwhS9HjoqF10ULnplM+j+gJYv/2f7C/ScACQNW0Wf5u1FXUoba8FpIkKd85ub4LAPS5qo9yDI0zLF7BXsr311pQ1pi1gEXnpsNNX96ECU9MgEajUc4h8mvJrZusnTPkQNLFw0X1HEAMXSD/bvKO5KEgVf372vVfUVQ7fuF4uPs1PwK2JJn7TTFkGpr8xiRJalIkJBv3mCjaPLHyhPL+DL5ZZAIrcitwev1p1FfXwyfCB7FTYpXnaXVapR4XoP7eyS3zLAMW+X2wzADKgUtRWpHy/es1qZfyXlmSz8unfjhlLjptJpivyK/Auc3nAIj3tbmiuqMrjmLNQ2vw3V3fWX1cJpkk5WZNFfCFeQMaEfhUFlSqAp/WMnqSSULBCfF5n0k5o+olWL7ZY8DSg8kni/AR4bh97e246uWrMH7h+CbrySebjG0Zyg/U8m5cHrit4EQBsvZl4dPkT1XR8qbnN2H9wvWQjLVARYbqR1OYF9qmfU/flI7qkmp4h3kjZmKMqrJkYzkHclBdUg13P3dEJkW2GrAY64zmk1mjdSSTpCyLHB2J2CmxSisPwFznQi4uaTzKq2VrjsYV9CwzI6qWB42aMTYOWOQKuQnXJQAa8cNvfAd0Zp24UGbtyVJtIzA+ELeuvhXT356Ogb9ruXiu18ReuP/A/bjqpaswdZHjx3O69u1rkfxqMu7dc69y4rEsXqurENkJvxg/uHm7wTPIE4AoFrKWYSnPKcenyZ/ivZHv4fNrPodklDB4zmDETomFqd6kdMIWNcacYZGLpmTyhcSSZQBvSU5T95rUS0l9AyKzpFyQvhYXpOYyLN6h3rjl+1tw6w+3wkMvAoAxj4yBq5crzm89j6pCkfFQBSwWTZstM4zlOeWoKa1Rsn+Wr9VrYi/o3HQwXDCgsqASGp0GIYPNJ3t7ioXkC4e73nrGDTC/Z7Kk+5MAmL/nu9/cjW9v/xY1ZTXKa8qDOzYeBsJS42EL5O9B/xv6t3gjlXc0T3XzIH+WO17dgW9v/1Z83yrrAI35Ny0LGxqGuCvjIJkk1FXWQeuqRfS4aOVz2PeuqDzbZ2ofaBrVz7P8PlkGLP2u7wetqxb5x/KV84Bcf8UyAyjfXGRsz8CWF0TRTeP3Viafly17mG4uYDn53UmlU0fJJDXbYagcMJzbfM5qqybldbLLYKwxQqPTQB9jfv90rjolwC7LKlNVI2itzlRpRqnSz42pzoTUH1KVx5hhuQRYfsj+sf6Y+OREpazeUmhiKLxDvVFXWYeDyw4CUAcs8pck67csfDP7G5xNOYv1f1oPALi49yK2vrgVu/67C8WrHoVxZR9VxdjCzLZ1s69UEL5xILQ6bZNWJpbkyrW9L+8NrYtWCVhKM0utpoIv7r6oXBwbdxJVWViptArwCfeBm4+bUknRM8hTqbPRXPfklidJy4ClNLNUeU2gUYal0R1i42IvOTiZ9PQkpWjMsoy35HyJcgeZczCnyd1P8IBgjHl4jHIX1xLvEG9M/PPEJhd2RwgbGobxC8erKs3JLSOU1w/1hqun+N8/1h+AOH5rAYucRck5kIPKgkp4h3pj+lvTMXaBueVZUL8gpUgFAHyjfKF1FaedkEEhSp86luSsWubOTFWRoVwcZC3IkZelrk5FfU291Uq3soRrE5RKxYAIoqb+nzlA1Lnp4BnoqXpPABEMW454XZ5Trnzf3HzdlPcNEHVs5GIEQGS3LB+XL5KZ2zNRWVDZ4rg01jIsjVlmIkMTQxF3hbjI5h/PR42hBilPpuDI/45g41MblWKLkfeOhEarQUVuhRJcyr/72MtiVf/LLIu/WhoBu3Gwmf5zOkrOl2DjnzfiyP+O4Ls7vgMg6v24uDfNXlh+h+Rssxxcn14vek62FkjEXhaLgD4B8PD3UL3/HnoP5TOXgzAlw2IR2IQmhsLNxw3GGqMINLUa9Lu+X5PXAaB8dy1bMcnF8Y3JgbRGp1G9dmPKjZAEnFx1ssnj8vlUvnHU99KrbugAc7BdllXWJMPSUoOCxp+j/HurLqlWfvNySYGzMGDpIKZ6k3Jn3lpUapnSlU8cqoCl4YeRdzRPCRgu7LyAC7svqDpEKj60E+Ul5rQ0ABSdLmnTvss/FvlCYK2nVJl8cpKPwS9KBCx1FXVWK3pZNlmV72hl8g/DO9RbqX8hb9fyTqy57smbKxKSf4yWHfHJP155PbnIxPKiZDKalEqw+li91dSy5cm5sqASF/dehLFW3P3IwVtXpdFolAsBoH6P5fmS8yXqIqHccvH9bnjfEm9JxO3rbsf9B+6Hd4g3+t/QX6m0a3kxAETaXg6EGhcHyQLiA6DvpYepzqQMEll0pggZv4pOzizT/rLocdHwjfRFbVkt9r23T7mLtqxX0pIxD49BzERxgfON9FXduauKhBplWOS7YMviIJnlBbVx/SX5fTm78SxeCXkF/wn9T5OO+2S2BCy+kb7K9zduapxyzik+W4xjXx9TskC/LfkNNYYauHq5ImpMlFIkmHMwB/kn8pF/LB9aVy1u+PAGaHQa5BzIUVp91dfUK/viHeqN0MEie9tSwCL3Hpu+KV1pFQZAaZLeXGDe77p+Sq/O8nsnZz/k7gKsBSw6Vx3m7ZqHB48+qAo6AfO57MjnR1CeU67cZFhmAD30Hnjw6IO4fd3tuH3d7XjwyIPKcTamOq83fF1qy2ubDEJaWVipZJTlDh6bDVgsMiKNg8XDnx3Gvzz+hbQ1aeb6K1beP8uKt5bfqaqiqhaHErAcygMQgWGNoUb5LflG+SpZSWdhwNJBis8Ww1hrhKuXa5OUpzWNf3xhw8wtRAL7BqqiaLk45Jf7/4VjKw4ry0uy3VBmUEfAbelc7dyWc6gqrIJXsJdSRixnWAyZBlXrjfrqeqW3TLnc3tXLVTlZWCsWanyBt9S44y4AGH7ncATEByitAABRORMQ9U8sTxCWaVTLoh75xxifHC8GCyytUV5Lfo/ip4k7sMYXJVO9GBLAN8JXuVhm/pqpNBVsfDcpB3v+sf5N7n66IuVCACiVbS3nG2dYIImLt/y+xV0Vh77T+porquq0uPqVq+Ed5q00NbU07K5hCOoXhJHzRlrdH41Go7Re2vD4BtQYavDDvT8AkviMrAWBGq1GqZi57rF1qMitgE+4T4v1hho/f+aymQhMCFT1LgyYi4TKc8pVRaKWGRZrgVFLAUv4sHBVMFdjqMH5Leet7pstAQsgshJ+0X4YMW8EvEK8RJGeBPz6f6JukUeA+fmRoyOhddGqAn858xB/dTwC+wai9+W9AZgzEnLWSuuihUeAR5MiodLMUuQdy4OxzohzW84BEFlJNx83VBVWYc+bogWN5bmtcf0VmUarwdWvXA2fcB+l4r1l5ezAhEBVUYgl7xBv5abJ0oBZA+AR4IGCkwX49vZvAYibFM8AdWDjH+uPvtP6ou+0vi3ebAbEByjZwtgpsUol+cbFQqnfp0IySggbFoZhc8U5rLlsh2VG5Nzmc6oxqY5/fRzGWiN2vb7L3EKoj3+TbfhEiuC5LLsMhgz1+belIkj5cxx00yAE9Q+CscaIUz+dUoqpnF0cBDBg6TDyhx88MNimogDLlK4+Vq/6EencdMqPdfhdwzHzo5kAgPRD4TDVm7ddnBeAMs0UAIDWpSF70Ibxdk79KGqx95/VX7ng+oT7WB3x9+zPYqwYn3AfVbpQrjzaOGCprahV/WgaByyNm5UC4qT2x9N/VPoZAcQFwjfSF5Cg6hXVMsNSmV+pnOwt6xPJFR7lZfJ7JKeMDZkGc0+XDXc8ftF+SnGX3DT25KqTqsqFcmAqFxc1dzLualQBi5UMS+HJQiUTJhcnlWWVKe+bZYZGNuimQXgi5wnlomdpyrNT8EjqIy0G8pf97TL49/ZH6flSLJ2wFOc2nYOrlyuufefaZp+jqr8wPhrz98y3644wqF8QHj31KKb+W11/SA5G8o7mqcayUQUsYU0DlqjRUcpFrHHAonXR4p4d9+Bvpr8p9U2a+63aGrCMun8U/pT5J4QNCYNGo1EuMPJd9S3f3aL8LuViVrlo9fBnh7HjPzsAmDNf8vspt2qRbwa8Q71V2y88VYjq0mp8OOZDLElcgm9mf4Paslp4BnoiclSk8h0w1ZsQmBCIO3+5U/l9y1kUawb+biAez35cOTdafk/lmyN7ePh74JrXrwEA5TfbOANoD52rTmnBKXdNAKgDFlO9CXsX7xXr3DwI4SPCoXXVoiK3okk3FSajSTlf+kb6QjJKSqtBwHy+Sv8lXakDYzXDEtk0wyJfP2wJWEIGhyif/d639yLvqDi/MmDpweytpOTf21+5wFlr/nrVy1ch6YEkTPvvNESMjEDsMHMmoVeSKDopyfNHWZ1oEhg1RtS1KD1fivqaeqT+kGpzd81yUZblj1mj0ViteLvnLbHNwbcMVqXRm6t4m7EtQ5T5NqzaXMBimWFpjrViocZdqMsXActa7pZ3htUl1cqdY69JveDiqQ7KLJv6yuQf89EvjiLvaB7Kc8rh4umipHvlzINl/whdmeWdq7UMizwekKuXq1LsUJxerJxwLZ/vKG4+brj+w+sBQOk748p/X4nA+MBmn9NrUi+Mf2I8Ln/hcty15a5m78DtJQcsjYsfy3PKle+btQyL1kWL6z+4HhOenKC0fLKk0WhEZ2Ut9NwsSZLNAUtjljcQgX0D0WtyL8xZNQdD/zAUY/8o6ojIv6H8Y/moKa1Br0m9lMq4cqeEcsaxcXDmF+MHNx83mOpM2Pz8ZqU4W25dGHdlnGj1ZZFpGvvYWHgGeuKW727BsDuGYcQ9I2w+HsvAuLmKsK0ZOneo0oszoK5w2xZX/+dqjH54NEbcM8JqwLLr9V3I3pcNd707Rtw9Aq6ersp73jh4KMsqg6neBK2LFkkPiCBWLhaqq6pTsiqSUVKadVu7KVKaNmeVK+cvOWPZXMVbSZJUmZSk+5Lg5uOGzB2ZSg/BDFh6sLbUqk64TvSeaFlZTNbvun6YsWSGOGlJJoybthkA4B9SjLFXi/ni/ACUVYginPAR4XDzcYNkknBh1wV8fdPXWPvIWmQfyG51P5orH21c8Tb/eD7OrD8DjVajnABlzQUslh3MAepB6gD7AhZrFW8bd/pWlFYESZJUn4dlwCIHND7hPqIL+4Zjlk/Slk19ZQN/PxAanQaZOzLx+TWiA7Fek3o1uVvrLhmW1uqwyAGdX7SfcoeesT0DkkmCm4+b1eyCI/SZ2gcj5osLWvT4aIx5ZEyL62u0GiS/kozL/naZUv/JEeT6KZaVK4HWi4QA0d381S9f3WKWVemszErPzXUVdUoTcHsDFstzz8CbBkKj0SBqdBR+9+nvlCKTiJERShZ1/OPjcccvdyiVg+XfcHm2KBZtfKwajUYJipQblzmDlaxS/DUiY9n3mr6ARhRJDb9zOABRb2TWx7Ns+p3LAvsGws3XDS6eLlYzd7bQaDSY8d4MuPmKTGHs5NhWntGyvtP64tq3rxXjZDVkjeRzWOGpQmx6bhMAYNpr05Rjtexd2ZIcXPhF+5l7fP4lHXVVdSg8VajU/wGg1ONpKWApTDNnRuXt5RzKsTridVlWGWoMNdDoRJ02fS+9UhFdXr8rBCxNq2eTQ7QlYJn676mIHhetlMU3q/ggBgz7Dbc8WYvgiFzU1wDAZBTnB6O8QJyofSN9EdQvCNn7s7Hh8Q1KEUfmjkxlkC9rJFPz/SMoFW8bLuZyT5j9Z/ZvEtw0F7DI9T0Sb0nE+S3nUVdRh7qqOuUk6agMS8igECUgafxjtAxY5Lta+S43oE8A8o/nK0GZchLpZS4T94/1x8yPZuLH+35U9jduapyqbN7y/erqLDMkloGZ5TzQELA0fC5yfYvAhMAmTUsd6dq3rkXMhBj0v74/tDrn3F81Dka8Q71RkSda1sgtXNoTtMnvvxxcW76fcnZF66K12sKwJZbnHmstqwBR3+P2dbdD56pT9WkCiOPUumiVJurWskkhg0KQtTcLkkmCu587rv/gelQVVuH81vNKXaDgAcGYmzIX3qHe7erW3dXLFXf8fAckk6RqeWYvfYwe9/x6D8ouljn0ImzZaRsA/PjAj6ivrkefq/uo6nJFj4vGnjf3KD1Byyx7mg7qFwTvMG9U5FYg50COcuPkG+mryuBYO8fIlW7ljIm73h0hg0OU577e+3VV3bpBNw1SWkIFJQQp/X+NfnA0jq04hoxtoo6is1sIAcywdAiT0aR8Wez5kN183DDktiFWm/mp5Iiurvtf2wtBoybBP0RcXKvK3JWO1HwjfZWLcPY+c1altbb4ZdllzbZwUcajOVuCysJKHPrkEACo6pbI5Oda9nYrSZJSHho/LV750Vi2FGpLwJJ3JE/p3VK+C5RbfBSdKlKCR/nHqAQsx/KV4g75vZIrsclBm3wSaXzxHjZ3GObtmofABFEhesDMAfAO8VYyEED3ybB4BnkiNDEUnkGeCOpvDl68QrxUHWb5Rfspd5Fy/zcdURxkycXDBSPuHiG6yHeSxsGIfIdcnt16hsUWAX3EaNy15bVKUFBjqIGp3qQqDrI3MIwcFQmvYC9EjYlSevO1ps/UPk2CFUBUnpZ/h4YLBqv1dSwv+CPmj4C7rzv8e/tj2B3DVFmuPlP7IGyI9aEm7BE1OkrpKbg9woaEKb3fOoplkVBVUZXSmeaM92aoPjv5hrHxeE2Wxc8ajUbVV498Dku4LkFpxePm42b1d9H43CmPWdbvBhGUVOZXojy7XPnb89YebP+36PDS8vPUaDW4YekN8AjwQERSRLuCREdhhqUDlJ4vRX11PXTuug7pTwO5DWNzhE8FfOLhnrkSXn4VqDR4I2ufqIxlGbAAov2/ZJRa7ahKqX1upYWLZXHJrtd3ob6qHuEjwtFrcq8m27GWYakurlbS6r6RvvAM8kRFbgUqCypV6Wf58dYExgfC1dsVdRUiZaqP1Svpy16TemH/B/tRmFaIzF9FUKJ0ltY/CNCIZn573xYV4uQLr3KMjTIs1iqIhg8Lx0PHHhId7DX09xE+PFwZKLBDPvsOoNFoMH/PfBhrjaqxiTQaDfS99EpRhW+0r/lzaTjPWn7Heio3bze4erkq362YCTFI/T4V5bnlStajPQGLzk0HfaweJeklSgXW90e+j4G/H4ik+0RdBnuLgwDAM8ATj6U/Bo1O0+YsmF+0n9JKrLkMCyAubq0V2fV0lgGLPHRDQJ+AJucBuW5YXUUdqourlRaVchZFfjx6XDRSv08VLYoaigVDBolMyZZjW+Af52/1c/UO9YZGq1GKkORz13XvXIcxj4xRjTV16JND2PXaLqUXXrlvHVlQQhAeS3/Mak+/zsAMSwcozWy4yMU07dSn3Yy1QN42MR8+FYi8BvDrj4AwcaG3DAgs734nPDEBgGgx0LiiqyU5s2At1ShnDPKO5ilNJSf/ZbLVH421gEWulOcZ6AkXdxfl7kCuxyKZJGWUUVsCFo1WozRvzjmYo5xQXTzN48PkH8tXuoyX7zBcPV0x+uHR8A71Fh1gDQxWeqFtXE/HMk1rjc5Vp+qcTM76uPmae4rtDlw9Xa22qLE8bssiIZm1FkI9keVFWr7zlYyS8j2x1g+LPZQhEtIKcWLlCdRV1uHkdydRVSSyj20JWABxF27ZYZ29LH/H1rJJva/ojbipcZjy3JRuE6B3FFXA0mg4Bkuunq7Ke2jZT4rcBFkOMOTv2cXdF1UteJLuT0KvSb2a1BuUaV20qiyY/BvWaDUIHRyK8GHhyt/Uf09VZVWsFZF56D1az/p3EgYsHUC+MFs2zXWYwl2AsRLwCAX0iYDWFZi2FwEjJqtW8430VTqcc/F0wcQ/T1RaeLQ0CJa1AbVkchBTX1UPU70JA2YNaLa+jXyiqy6pVsbNUN6XcIuRcGFuKVSRXyHuJDS237FaVry1PKEGxAcAGlFhrL6qHnFT41T9uFz71rV4IvcJPJH7BB4+/rA5w9LQzLIwtRAV+RVKx3e29KUDQAmUghKCOrRuR2exPG6rAUsHFwl1FZYXgKB+Qcp3V76LbU+GBYCqpZBcz6u2rFa58LU1YGkv3+imRUKWwZmbtxvu2HgHLv/75c7YvS7FstKtPLaQfH5qTP5dWXYR0biCf+SoSGi0GpRmlCpjOoUMCoFvhC/u3nY3Rs633o8RoL7ha+nc5eLughuW3aBUCndEsV1HYsDSARpfmO1WWwJsvg5Ie6/pY7mi1jnCrgTkC6KrL/zjzb0x6tx1Srlj8qvJmP3NbHgGeqrKRMuyyvDNnG/w2TWf4bNrPlP6CrA2ZLnM3dddOVF7+Hvg2sXXNntRdvdzV2riyx2stRawKH2whPnYnJmS72ByD+Wqeh119XRVmrW6erni+g+utymACB4QDJ9wH9SW1+LgRwcBqIcEaE2/6/th0l8mYdp/p9m0flfXWoblUigSAswBiYunC7zDvFW/bbkjtfaQM1W5h3KVHn0BKKl6ZwUslhkWy35YqCm5smt9Vb1SKV3OADemdMrYkMGVJMmczW0IMNx83BA6pOG8Lolzqq2tqizH2mouOyyLHhuNm1bchGn/nYbQxLaNPddZGLB0gHYHLFlrgaw1wN4HgLMfqx8rFIEFgieqFquGGI/wVfp4GL9wPBKuFc2l5T4HLuy8gNXzVuPYV8dwZv0ZnFl/Bmv/uBY1hppmWwjJ5L4Zpr0+TfWjsKZxsVDj90UuMmkcsNjT1FEOWLIPZDcpY5e73L7qpatsTldrdVoMuFF0Jy63gmpc4bYlOlcdpv5rqtVKjN1R4wyLV5CX0runR4BHl6iI1xnkDEtAnwBoNBrVb1vuSK095EzVmZQzMNaYe5KWW5K4+zc/8GFHUsYFyyg1j8/UQc3YuztXL1clsJQzJ9aKhICmGRbLTLTlb86yq4SQQSE2f8/k3m4bb685g24aZLXxRFfDgKUDVOQ0pE7bWiRUZdFXyu75QJYY6BCSBBSJTnwQNEr1FMuMSHMXfPnLn/5zOk6vOw2duw4z3psBfS89JKOEc1vOtVgkBAC/++R3mLdzntKfQkuaC1i8w8UJT86wyK2E2hKwhCaGQqPVoDK/Umm54hUqtnvdu9fh7m13Y/TDo23eHmBuAipXnrW1OKgnkoM1nZsOXsFe0Gg1SqB6qRQHAeYgWA58Gwcs7SVnquTKlTp30cKmvloMiujsDEv+sXylsqa1ASVJsDx3eQR4wC/G+lhijTMscuDiFeylar5uGbA0rhBr637Yc8PV1TFg6QDtzrBUN/QrovMCpHrg11uA+iqgKks8ptEB/sNUT1FlWJq54IcODlWNzHv5C5cj6b4kpYOntJ/SlKChuYyEd6i3zd1Zt5ZhaVwkJLcQsrw7aI2rp7n31TPrz4jnN5SxewV5odekXnbf/cZOjoVXiDlz0FpKtScLHxEO7zBv9L2mr/I+yt+vS6XCLSBGIte56ZReUuWgG3BMxqFxqzx5/ByZswMWuS6Xh7+H0k8HNWV57g0fHt7suadxhqW5yv2NMyz27ofWVdv261AXxIClA7Q7YKlqCFgGPwN4RgF1JUD+NqDoN7FcPxhwUbdA0cfolaHLm7vga120iBotikkikiIw4XHRckgel+PoF0cBOK6FixKwZNpXh8WeDAtgTrvKY6a0945X66JVRpkFLu0Mi4feA3/K/BPmfDdHWSZ/PpdK/RVADAj4TNkzGP2QyNZZVjx1RIZF66JVjasz/k/jVY87rdJthK+ql14WB7XMMqveXHEQYM56yBVtlSbNjc41Qf2ClM9ebkRhC/k3qo/R2zSWXXfBgKUDOCzD4tULiGiovJm93lwcFDiqyVO0Llrly97SBX/SM5PQd3pf3Pj5jcodXdwVYlwO+S4qIC7AIS1clJ5xz4hipiYBS1AzAUsrdWMaa1wT3xEnVcueQS/lDAsg6uVYfh9GPTQK8cnxGPqHoS08q+exzCyoioQcdBGXM1bBA4IRMihElTV1VsCiddGqLsKscNuyxhmW5sjn6orcCtRX15v7e2p0rpFHrR46d6hdAz7GXRGH/jP7Y9JfJtmz+11e12hc3YOYjOYxN9odsHiGi4Dl7DIRsHg1dNAWmGT1acEDglGSXtJiBdP45HhlVGKZV7AXwoeHK00oHdVDq2XfEkDzGZb21GEBmp4YHHFS7X15b3iFeKEyv/KSqqthiz5T+6hGF78UOboOCwCEDg3FqR9PIX6a+H2GDw9X6pQ5K2ABRKZUrs/V3v5mejpbAxbPIE+lM8LSzFJlwFlr9U1Gzh/ZYhNma1y9XHHLd7fY9ZzugAGLg1UWVIq+GTTtqJwmFwl5hItsikYLlB4DKs6J5VYyLIAYYCs+OR79Z/a3+yXjpsYpAYujxsCR7xhLM0pRW16rZFIcXiQ0zPEBi85Vh9t+ug1Fp4u6fN8E1Pk6ImCZ+OREeId4K+POhA0Pw4lvTwBwfsBycbcY0kOu0E7WyecunZtOqVtnjdyLdMHJAhSfLVaaQctDipB1LBJyMKUlTIh323q5NdUBNaKTIHiGA+6BQGBDK5f6CkDjAgRYT8UHDwjGuAXj2tQroeVw7Y7KsHiFeImRWyWIMXskMUSAXBQkByx1lXWqcVTsDVi8Q71Vz3HUXWDU6CgMuXWIQ7ZFPYtlwOKo75uHvwfGLRin9DhseYfu7IBFxiKhloUNDQM0YmiQ1iony8U/J1edRI2hBh7+Hi2O+UQMWByu/fVX8tFwZQfcGooiIiw6IfNPBHSOP3nFTo5V+tdwVIZFo9EoxSnyiJ8+YT5KJTA3XzclqMs7mgfJJImAJsT+uzj55K7RarpVl/jUPXkGeirf3Y66iHfFgIVFQi0L6heER1IfwZxVc1pdV67HIjd26H1Fb6eNSN5d8N1xMIdVuPUIBbQNEbplwNJMcVB7ufm4YcwjYxA+PByxkx3X6ZnckkQJWCzeF41Go2RZjq4QP9rQwaFt+tHKFW+9gr34o6cOp9FqkHhrIsKGhtnV3NQeftF+SLg2ATETY+yuiO7o/ZAxw9K6oIQgkVluhZxhkRs7WGa5yTrWYXEwhzVp9rColxE0BnDVA3WlzVa4dYRprzm+O3k5YJFHiW78vngFe6E8pxyHlh8CAAy8yfrYRK2R70Z7Up8D1LX97pPfdej2NRoNbvvptg59DVswYOkYjZswX+oV2W3BgMXBGvfmardqKwGL1gUY9BRw/gsgelb7drCTyUVCco+djd8XOcNSXVINQN2c2B79r++PEfNHoN+Mfm3dVSKyQhWwsB8Wh7FsEeQb6Yug/myN2BoGLA6mdMvviCbNlgY/I/66mca9oTZ+Xyzrm4QMCrGrcyRLLh4uuOGDG9r0XCJqnm+kL9x83GCqNzm1aKqnscywxE2N6xGju3c0BiwO1iFFQt1Y495QrRUJydpaHEREHUfnpsMfNvwBpjoT3HxsG7WcWucbJXoRlkwS66/YiAGLgzms0m3jDEs35RngCa9gryZ9sMgsA5ZBv29bcRARdayY8ewfxNF0rjr0vrw3co/kImF6grN3p1tgwOJgzLA0FZgQ2GrAEpgQiNAhoZ2+b0REznL72ttRX1MPd9/WWxURmzU7VH11vVJ5lBkWM8t6LI3fl4E3DkTk6EhMXTSVZbhEdEnRuekYrNiBGRYHKs8V2RWdm67tHT31xAxLP3M9lsYBi76XHvfuubezd4mIiLoZBiwOcG7zORxcfhBxV4qKUz7hPm3LFtRXAPViPJ2emGFx9XJlpT0iImqTNhUJLV68GHFxcfDw8EBSUhK2bdvW4vo1NTV49tlnERsbC3d3d8THx2PZsmXK48uXL4dGo2nyV11d3Zbd63TbF23HoY8P4bs7vwPQhuKg8rNAzs9Ada74X+cJuPSc5oMRIyMAjRjriMU+RETUFnZnWFasWIEFCxZg8eLFmDhxIt577z1Mnz4dx48fR69evaw+Z/bs2cjNzcXSpUvRt29f5OXlob6+XrWOn58fUlNTVcs8PJw3foY95Iq2MrsCFkkCNs8ADCeA/o+JZR7hQA+6sAf2DcS8nfPgF+XX+spERERW2B2wvPbaa5g3bx7mz58PAHj99dexfv16LFmyBIsWLWqy/rp167BlyxacPXsWgYGiLkPv3r2brKfRaBAe3j2LQSryRWdxg24ahJPfn0TcVXa0qS89JoIVAEh9Q0x7UHGQLHpstLN3gYiIujG7ioRqa2uxb98+JCcnq5YnJydjx44dVp+zevVqjBo1Ci+//DKioqLQr18/PPHEE6iqqlKtV15ejtjYWERHR2PGjBk4cOBAi/tSU1MDg8Gg+nMGSZKUJrvJrybjL+V/wdhHx9q+gQvfN13WgyrcEhEROYJdAUtBQQGMRiPCwsJUy8PCwpCTk2P1OWfPnsX27dtx9OhRrFq1Cq+//jq++eYbPPzww8o6AwYMwPLly7F69Wp88cUX8PDwwMSJE5GWltbsvixatAh6vV75i4lxTsdGNaU1MNWZAABeIV7Quemsr1h8ENj7sLkVkEwOWGJuMi/rgRkWIiKi9mhTpdvGFSclSWq2MqXJZIJGo8Hnn3+OMWPG4Nprr8Vrr72G5cuXK1mWcePG4Q9/+AOGDRuGyZMn46uvvkK/fv3w1ltvNbsPzzzzDEpLS5W/zMzMthxKu8nFQa7ernD1dG1+xT0PAmmLgV13i3orAFB5ESjaC0ADjHobiLlRLPdjF/VERESW7KrDEhwcDJ1O1ySbkpeX1yTrIouIiEBUVBT0evNATwMHDoQkSbhw4QISEpp2SazVajF69OgWMyzu7u5wd3d+hzuV+aI4yDukhVFMS44ChbvEfPY64PyXQO9bgYurxbLg8YBnGDDhcyA7BYi4uoP3moiIqHuxK8Pi5uaGpKQkpKSkqJanpKRgwoQJVp8zceJEZGVlobzc3JLm1KlT0Gq1iI62XhFTkiQcPHgQERER9uyeU8gZFq8Qr+ZXOvOhmLo2tJLZ9xhgSAUyvhH/R88UU50HEH29mBIREZHC7iKhhQsX4sMPP8SyZctw4sQJ/OlPf0JGRgYeeOABAKKo5o477lDWv+222xAUFIS7774bx48fx9atW/Hkk0/innvugaenJwDghRdewPr163H27FkcPHgQ8+bNw8GDB5VtdmWtZliM1UD6p2J+/CeAfjBQkw/8OADI/UUslwMWIiIissruZs1z5sxBYWEhXnzxRWRnZyMxMRFr1qxBbGwsACA7OxsZGRnK+j4+PkhJScGjjz6KUaNGISgoCLNnz8Y///lPZZ2SkhLcd999yMnJgV6vx4gRI7B161aMGTPGAYfoGMZaI85vO4/YKbHQuZor1raaYclcBdQWAV4xQOQMwDMS2HwdUFciHo+6AfDr38F7T0RE1L1pJEmuAdq9GQwG6PV6lJaWws/P8R2U7X5rN9b9cR0GzxmMm740t+hZv3A9dv13F8Y/MR7JryQ3feLPVwG5PwOJzwND/+7w/SIiIurObL1+c7RmG+UcEBWNj604hhPfnlCWt1gkJJmA/O1iPvaWDt9HIiKinooBi41KM0qV+Z8e+glVRaJJdotFQlU5gKkG0GgB3/hO2U8iIqKeiAGLjUrPi4DF1dsVFbkVSHlKtJRqMcNScU5MvWIAbQt9tBAREVGLGLDYQDJJKM0UAcs1r18DAEj9XgzU2GKGpTxdTL3tGFuIiIiImmDAYoOKvAoYa4zQaDUYdPMgQCMyKxX5Fa1kWBoCFp/enbezREREPRADFhvI9Vd8I33hofeAf29/AEDW3izUV9cDALxDrQQszLAQERE5BAMWG5ScLwEA6HuJ4QVCBoUAAM5vPQ8AcPFwgau3lToqch0WHwYsRERE7cGAxQZyhVt9bKOAZYsIWLxCvKwP/sgMCxERkUMwYLGBXCTUOMOS9VsWgGbqr5jqgcqGHn+ZYSEiImoXBiw2aC7DYqo3AWimhVDlBUAyAlo3wLPrD+JIRETUlTFgsUHjDEvwwGDV4y32weIdKzqOIyIiojbjldQGcqVb/1h/AIC7rzv8YszjHbAPFiIioo7FgKUVNWU1qC6uBmDOsADmYiGgmYBF6YOFAQsREVF7MWBphVwc5OHvAXc/d2W5ZcBitUionAELERGRozBgaUXj+iuy1jMs58TUu3cH7RkREdGlgwFLKxq3EJLZnGFhHRYiIqJ2Y8DSiuYyLJYthZpkWIw1QJXoo4VFQkRERO3n4uwd6Oqay7B4Bnhi4O8HwnDBgIC4APWTKs4DkAAXb8Bd3QSaiIiI7MeApRWNmzRbmv3NbOtPKj8jpj7xgLUu+4mIiMguDFhaETU2CloXLQITAm1/UtlpMfXt2zE7RUREdIlhwNKKaa9Os/9JZWli6sOAhYiIyBFY6bYjMMNCRETkUAxYOkK5HLAkOHc/iIiIeggGLI5mqjf3wcIMCxERkUMwYHG0ivOAVA/oPADPSGfvDRERUY/AgMXR5PorPvGAhm8vERGRI/CK6mhyCyHWXyEiInIYBiyOVs4WQkRERI7GgMXRlCIhBixERESOwoDF0ZhhISIicjgGLI5kqgfKz4p51mEhIiJyGAYsjlSZCZjqAK074BXt7L0hIiLqMRiwOJJSf6UPmzQTERE5EK+qjqQ0aWb9FSIiIkdiwOJIZafE1Lefc/eDiIioh2HA4kiGVDH16+/c/SAiIuphGLA4khywMMNCRETkUAxYHMVYDVScE/PMsBARETkUAxZHKTsDQAJc/QCPMGfvDRERUY/CgMVRyuTioP6ARuPcfSEiIuphGLA4ilLhlvVXiIiIHI0Bi6MYLDIsRERE5FAMWBxF7oOFFW6JiIgcjgGLo7APFiIiog7DgMURqguA2iIxz275iYiIHI4BiyPILYS8YgAXb+fuCxERUQ/EgMURDKy/QkRE1JEYsDhCGVsIERERdSQGLI5gOCmm7IOFiIioQzBgcYTiQ2LqP8S5+0FERNRDMWBpr5oi86CHASOcuitEREQ9FQOW9io+KKbecYCbvzP3hIiIqMdiwNJexfvFNHCkc/eDiIioB2PA0l5FB8SUxUFEREQdhgFLezHDQkRE1OEYsLRHfYV5DCFmWIiIiDoMA5b2KD4MQAI8IwDPcGfvDRERUY/VpoBl8eLFiIuLg4eHB5KSkrBt27YW16+pqcGzzz6L2NhYuLu7Iz4+HsuWLVOts3LlSgwaNAju7u4YNGgQVq1a1ZZd61xycRCzK0RERB3K7oBlxYoVWLBgAZ599lkcOHAAkydPxvTp05GRkdHsc2bPno2ff/4ZS5cuRWpqKr744gsMGDBAeXznzp2YM2cO5s6di0OHDmHu3LmYPXs2du/e3baj6izFcoVb1l8hIiLqSBpJkiR7njB27FiMHDkSS5YsUZYNHDgQs2bNwqJFi5qsv27dOtxyyy04e/YsAgMDrW5zzpw5MBgMWLt2rbLsmmuuQUBAAL744gub9stgMECv16O0tBR+fn72HFLbrR0pgpbJK4GYGzvnNYmIiHoQW6/fdmVYamtrsW/fPiQnJ6uWJycnY8eOHVafs3r1aowaNQovv/wyoqKi0K9fPzzxxBOoqqpS1tm5c2eTbU6bNq3ZbQKimMlgMKj+OpXJCJQeE/MBwzv3tYmIiC4xLvasXFBQAKPRiLCwMNXysLAw5OTkWH3O2bNnsX37dnh4eGDVqlUoKCjAQw89hKKiIqUeS05Ojl3bBIBFixbhhRdesGf3Has6GzDVAhoXwCvWeftBRER0CWhTpVuNRqP6X5KkJstkJpMJGo0Gn3/+OcaMGYNrr70Wr732GpYvX67KstizTQB45plnUFpaqvxlZma25VDaruK8mHpFA1pd5742ERHRJcauDEtwcDB0Ol2TzEdeXl6TDIksIiICUVFR0Ov1yrKBAwdCkiRcuHABCQkJCA8Pt2ubAODu7g53d3d7dt+x5IDFm9kVIiKijmZXhsXNzQ1JSUlISUlRLU9JScGECROsPmfixInIyspCeXm5suzUqVPQarWIjo4GAIwfP77JNjds2NDsNrsEBixERESdxu4ioYULF+LDDz/EsmXLcOLECfzpT39CRkYGHnjgAQCiqOaOO+5Q1r/tttsQFBSEu+++G8ePH8fWrVvx5JNP4p577oGnpycA4LHHHsOGDRvw0ksv4eTJk3jppZewceNGLFiwwDFH2REYsBAREXUau4qEANEEubCwEC+++CKys7ORmJiINWvWIDZWXLizs7NVfbL4+PggJSUFjz76KEaNGoWgoCDMnj0b//znP5V1JkyYgC+//BJ//etf8dxzzyE+Ph4rVqzA2LFjHXCIHYQBCxERUaexux+WrqrT+2H5cRBgOAFcmQKEX9Xxr0dERNQDdUg/LNRAkixaCTHDQkRE1NEYsLRFTSFgrBTz3jHO3RciIqJLAAOWtqhsyK54hAM6D+fuCxER0SWAAUtbsMItERFRp2LA0hYMWIiIiDoVA5a2YMBCRETUqRiwtAUDFiIiok7FgKUtGLAQERF1KgYsbcGAhYiIqFMxYLFXXTlQWyTmGbAQERF1CgYs9pKzK67+gGsnDAFAREREDFjsVp0jpl5Rzt0PIiKiSwgDFnvVlYqpq965+0FERHQJYcBirzqDmDJgISIi6jQMWOxVK2dYWH+FiIioszBgsZecYXFjhoWIiKizMGCxV71cJMQMCxERUWdhwGKvWla6JSIi6mwMWOxVxwwLERFRZ2PAYi82ayYiIup0DFjsxQwLERFRp2PAYq86NmsmIiLqbAxY7MWO44iIiDodAxZ7McNCRETU6Riw2EMyAXVlYp4ZFiIiok7DgMUe9eUAJDHPDAsREVGnYcBiD7n+itYV0Hk4d1+IiIguIQxY7GE58KFG49x9ISIiuoQwYLEHWwgRERE5BQMWe7DTOCIiIqdgwGIPNmkmIiJyCgYs9mCREBERkVMwYLEHMyxEREROwYDFHsywEBEROQUDFnsww0JEROQUDFjswQwLERGRUzBgsQczLERERE7BgMUezLAQERE5BQMWe9Qyw0JEROQMDFjsUc+ebomIiJyBAYs95CIhNxYJERERdSYGLPZgkRAREZFTMGCxlakeMFaKeVa6JSIi6lQMWGwlFwcBzLAQERF1MgYstpIDFp0noHV17r4QERFdYhiw2IqdxhERETkNAxZb1bFJMxERkbMwYLGVkmFhhVsiIqLOxoDFVsywEBEROQ0DFlsxw0JEROQ0DFhsxQwLERGR0zBgsVV1npi6BTp3P4iIiC5BDFhsVZYmpn4Jzt0PIiKiSxADFluVnRJT337O3Q8iIqJLEAMWW5jqgLIzYp4BCxERUadjwGKL8nOAVA/ovACvKGfvDRER0SWHAYstlOKgBEDDt4yIiKiztenqu3jxYsTFxcHDwwNJSUnYtm1bs+tu3rwZGo2myd/JkyeVdZYvX251nerq6rbsnuMZUsXUj8VBREREzuBi7xNWrFiBBQsWYPHixZg4cSLee+89TJ8+HcePH0evXr2afV5qair8/Mx9mISEhKge9/PzQ2pqqmqZh4eHvbvXMVjhloiIyKnsDlhee+01zJs3D/PnzwcAvP7661i/fj2WLFmCRYsWNfu80NBQ+Pv7N/u4RqNBeHi4vbvTOeSAxa+/c/eDiIjoEmVXkVBtbS327duH5ORk1fLk5GTs2LGjxeeOGDECERERmDp1KjZt2tTk8fLycsTGxiI6OhozZszAgQMHWtxeTU0NDAaD6q/DyEVCzLAQERE5hV0BS0FBAYxGI8LCwlTLw8LCkJOTY/U5EREReP/997Fy5Up8++236N+/P6ZOnYqtW7cq6wwYMADLly/H6tWr8cUXX8DDwwMTJ05EWlpas/uyaNEi6PV65S8mJsaeQ7FdXTlQlSXmWYeFiIjIKTSSJEm2rpyVlYWoqCjs2LED48ePV5b/61//wqeffqqqSNuS66+/HhqNBqtXr7b6uMlkwsiRIzFlyhS8+eabVtepqalBTU2N8r/BYEBMTAxKS0tVdWXaregAsG4k4B4C/D7PcdslIiIiGAwG6PX6Vq/fdmVYgoODodPpmmRT8vLymmRdWjJu3LgWsydarRajR49ucR13d3f4+fmp/joEWwgRERE5nV0Bi5ubG5KSkpCSkqJanpKSggkTJti8nQMHDiAiIqLZxyVJwsGDB1tcp9OwhRAREZHT2d1KaOHChZg7dy5GjRqF8ePH4/3330dGRgYeeOABAMAzzzyDixcv4pNPPgEgWhH17t0bgwcPRm1tLT777DOsXLkSK1euVLb5wgsvYNy4cUhISIDBYMCbb76JgwcP4p133nHQYbYDK9wSERE5nd0By5w5c1BYWIgXX3wR2dnZSExMxJo1axAbGwsAyM7ORkZGhrJ+bW0tnnjiCVy8eBGenp4YPHgwfvrpJ1x77bXKOiUlJbjvvvuQk5MDvV6PESNGYOvWrRgzZowDDrGd2KSZiIjI6eyqdNuV2Vppx25nlgHFh4ABCwCfOMdtl4iIiGy+ftudYbnkxN/j7D0gIiK65HEkPyIiIuryGLAQERFRl8eAhYiIiLo8BixERETU5TFgISIioi6PAQsRERF1eQxYiIiIqMtjwEJERERdHgMWIiIi6vIYsBAREVGXx4CFiIiIujwGLERERNTlMWAhIiKiLq/HjNYsSRIAMUw1ERERdQ/ydVu+jjenxwQsZWVlAICYmBgn7wkRERHZq6ysDHq9vtnHNVJrIU03YTKZkJWVBV9fX2g0Godt12AwICYmBpmZmfDz83PYdrsSHmP319OPD+Ax9gQ9/fiAnn+MHXF8kiShrKwMkZGR0Gqbr6nSYzIsWq0W0dHRHbZ9Pz+/Hvnls8Rj7P56+vEBPMaeoKcfH9Dzj9HRx9dSZkXGSrdERETU5TFgISIioi6PAUsr3N3d8fzzz8Pd3d3Zu9JheIzdX08/PoDH2BP09OMDev4xOvP4ekylWyIiIuq5mGEhIiKiLo8BCxEREXV5DFiIiIioy2PAQkRERF0eA5ZWLF68GHFxcfDw8EBSUhK2bdvm7F1qk0WLFmH06NHw9fVFaGgoZs2ahdTUVNU6d911FzQajepv3LhxTtpj+/39739vsv/h4eHK45Ik4e9//zsiIyPh6emJyy+/HMeOHXPiHtund+/eTY5Po9Hg4YcfBtA9P7+tW7fi+uuvR2RkJDQaDb777jvV47Z8ZjU1NXj00UcRHBwMb29v3HDDDbhw4UInHkXLWjrGuro6PPXUUxgyZAi8vb0RGRmJO+64A1lZWaptXH755U0+21tuuaWTj8S61j5DW76X3fkzBGD1d6nRaPDKK68o63Tlz9CW60NX+C0yYGnBihUrsGDBAjz77LM4cOAAJk+ejOnTpyMjI8PZu2a3LVu24OGHH8auXbuQkpKC+vp6JCcno6KiQrXeNddcg+zsbOVvzZo1Ttrjthk8eLBq/48cOaI89vLLL+O1117D22+/jb179yI8PBxXX321Mg5VV7d3717VsaWkpAAAbr75ZmWd7vb5VVRUYNiwYXj77betPm7LZ7ZgwQKsWrUKX375JbZv347y8nLMmDEDRqOxsw6jRS0dY2VlJfbv34/nnnsO+/fvx7fffotTp07hhhtuaLLuvffeq/ps33vvvc7Y/Va19hkCrX8vu/NnCEB1bNnZ2Vi2bBk0Gg1+//vfq9brqp+hLdeHLvFblKhZY8aMkR544AHVsgEDBkhPP/20k/bIcfLy8iQA0pYtW5Rld955pzRz5kzn7VQ7Pf/889KwYcOsPmYymaTw8HDp//7v/5Rl1dXVkl6vl959991O2kPHeuyxx6T4+HjJZDJJktT9Pz8A0qpVq5T/bfnMSkpKJFdXV+nLL79U1rl48aKk1WqldevWddq+26rxMVqzZ88eCYB0/vx5Zdlll10mPfbYYx27cw5g7fha+172xM9w5syZ0pVXXqla1l0+Q0lqen3oKr9FZliaUVtbi3379iE5OVm1PDk5GTt27HDSXjlOaWkpACAwMFC1fPPmzQgNDUW/fv1w7733Ii8vzxm712ZpaWmIjIxEXFwcbrnlFpw9exYAkJ6ejpycHNXn6e7ujssuu6xbfp61tbX47LPPcM8996gG++zun58lWz6zffv2oa6uTrVOZGQkEhMTu+XnCojfpkajgb+/v2r5559/juDgYAwePBhPPPFEt8kMAi1/L3vaZ5ibm4uffvoJ8+bNa/JYd/kMG18fuspvsccMfuhoBQUFMBqNCAsLUy0PCwtDTk6Ok/bKMSRJwsKFCzFp0iQkJiYqy6dPn46bb74ZsbGxSE9Px3PPPYcrr7wS+/bt6xa9No4dOxaffPIJ+vXrh9zcXPzzn//EhAkTcOzYMeUzs/Z5nj9/3hm72y7fffcdSkpKcNdddynLuvvn15gtn1lOTg7c3NwQEBDQZJ3u+Dutrq7G008/jdtuu001sNztt9+OuLg4hIeH4+jRo3jmmWdw6NAhpViwK2vte9nTPsOPP/4Yvr6+uPHGG1XLu8tnaO360FV+iwxYWmF59wqID7Pxsu7mkUceweHDh7F9+3bV8jlz5ijziYmJGDVqFGJjY/HTTz81+fF1RdOnT1fmhwwZgvHjxyM+Ph4ff/yxUsmvp3yeS5cuxfTp0xEZGaks6+6fX3Pa8pl1x8+1rq4Ot9xyC0wmExYvXqx67N5771XmExMTkZCQgFGjRmH//v0YOXJkZ++qXdr6veyOnyEALFu2DLfffjs8PDxUy7vLZ9jc9QFw/m+RRULNCA4Ohk6naxIZ5uXlNYkyu5NHH30Uq1evxqZNmxAdHd3iuhEREYiNjUVaWlon7Z1jeXt7Y8iQIUhLS1NaC/WEz/P8+fPYuHEj5s+f3+J63f3zs+UzCw8PR21tLYqLi5tdpzuoq6vD7NmzkZ6ejpSUFFV2xZqRI0fC1dW1W362jb+XPeUzBIBt27YhNTW11d8m0DU/w+auD13lt8iApRlubm5ISkpqkq5LSUnBhAkTnLRXbSdJEh555BF8++23+OWXXxAXF9fqcwoLC5GZmYmIiIhO2EPHq6mpwYkTJxAREaGkYi0/z9raWmzZsqXbfZ4fffQRQkNDcd1117W4Xnf//Gz5zJKSkuDq6qpaJzs7G0ePHu02n6scrKSlpWHjxo0ICgpq9TnHjh1DXV1dt/xsG38ve8JnKFu6dCmSkpIwbNiwVtftSp9ha9eHLvNbdEjV3R7qyy+/lFxdXaWlS5dKx48flxYsWCB5e3tL586dc/au2e3BBx+U9Hq9tHnzZik7O1v5q6yslCRJksrKyqTHH39c2rFjh5Seni5t2rRJGj9+vBQVFSUZDAYn771tHn/8cWnz5s3S2bNnpV27dkkzZsyQfH19lc/r//7v/yS9Xi99++230pEjR6Rbb71VioiI6DbHJ0mSZDQapV69eklPPfWUanl3/fzKysqkAwcOSAcOHJAASK+99pp04MABpYWMLZ/ZAw88IEVHR0sbN26U9u/fL1155ZXSsGHDpPr6emcdlkpLx1hXVyfdcMMNUnR0tHTw4EHVb7OmpkaSJEk6ffq09MILL0h79+6V0tPTpZ9++kkaMGCANGLEiC5xjC0dn63fy+78GcpKS0slLy8vacmSJU2e39U/w9auD5LUNX6LDFha8c4770ixsbGSm5ubNHLkSFUz4O4EgNW/jz76SJIkSaqsrJSSk5OlkJAQydXVVerVq5d05513ShkZGc7dcTvMmTNHioiIkFxdXaXIyEjpxhtvlI4dO6Y8bjKZpOeff14KDw+X3N3dpSlTpkhHjhxx4h7bb/369RIAKTU1VbW8u35+mzZtsvq9vPPOOyVJsu0zq6qqkh555BEpMDBQ8vT0lGbMmNGljrulY0xPT2/2t7lp0yZJkiQpIyNDmjJlihQYGCi5ublJ8fHx0h//+EepsLDQuQfWoKXjs/V72Z0/Q9l7770neXp6SiUlJU2e39U/w9auD5LUNX6LmoadJSIiIuqyWIeFiIiIujwGLERERNTlMWAhIiKiLo8BCxEREXV5DFiIiIioy2PAQkRERF0eAxYiIiLq8hiwEBERUZfHgIWIiIi6PAYsRERE1OUxYCEiIqIujwELERERdXn/D2Zi2l9nrsCaAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='Training Accuracy', c='orange')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', c='purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS BLOCK WILL TAKE A LONG TIME, DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for: 700 100 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_132 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8133 - accuracy: 0.5534 - val_loss: 0.6403 - val_accuracy: 0.6543\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7064 - accuracy: 0.5697 - val_loss: 0.6339 - val_accuracy: 0.6684\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6776 - accuracy: 0.5836 - val_loss: 0.6391 - val_accuracy: 0.6633\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5942 - val_loss: 0.6358 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6112 - val_loss: 0.6298 - val_accuracy: 0.6786\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6150 - val_loss: 0.6304 - val_accuracy: 0.6760\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6011 - val_loss: 0.6292 - val_accuracy: 0.6824\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6169 - val_loss: 0.6274 - val_accuracy: 0.6747\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6215 - val_loss: 0.6301 - val_accuracy: 0.6786\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6226 - val_loss: 0.6292 - val_accuracy: 0.6824\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6189 - val_loss: 0.6249 - val_accuracy: 0.6786\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6255 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6315 - val_loss: 0.6278 - val_accuracy: 0.6849\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6304 - val_loss: 0.6239 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6299 - val_loss: 0.6263 - val_accuracy: 0.6798\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6339 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6301 - val_loss: 0.6270 - val_accuracy: 0.6824\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6394 - val_loss: 0.6206 - val_accuracy: 0.6875\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6392 - val_loss: 0.6259 - val_accuracy: 0.6875\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6353 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6400 - val_loss: 0.6205 - val_accuracy: 0.6952\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6385 - val_loss: 0.6239 - val_accuracy: 0.6837\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6403 - val_loss: 0.6216 - val_accuracy: 0.6875\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6497 - val_loss: 0.6212 - val_accuracy: 0.6862\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6451 - val_loss: 0.6190 - val_accuracy: 0.6964\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6559 - val_loss: 0.6184 - val_accuracy: 0.6901\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6500 - val_loss: 0.6198 - val_accuracy: 0.6952\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6482 - val_loss: 0.6178 - val_accuracy: 0.6952\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6491 - val_loss: 0.6213 - val_accuracy: 0.6939\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6583 - val_loss: 0.6170 - val_accuracy: 0.6952\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6574 - val_loss: 0.6163 - val_accuracy: 0.6990\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6565 - val_loss: 0.6172 - val_accuracy: 0.7028\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6505 - val_loss: 0.6158 - val_accuracy: 0.6952\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6507 - val_loss: 0.6168 - val_accuracy: 0.7003\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6555 - val_loss: 0.6171 - val_accuracy: 0.6977\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6629 - val_loss: 0.6175 - val_accuracy: 0.6913\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6583 - val_loss: 0.6211 - val_accuracy: 0.6926\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6198 - accuracy: 0.6576 - val_loss: 0.6176 - val_accuracy: 0.6939\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6567 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6613 - val_loss: 0.6138 - val_accuracy: 0.6952\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6662 - val_loss: 0.6171 - val_accuracy: 0.6926\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6650 - val_loss: 0.6169 - val_accuracy: 0.7003\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6738 - val_loss: 0.6160 - val_accuracy: 0.6901\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6073 - accuracy: 0.6750 - val_loss: 0.6144 - val_accuracy: 0.6901\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6137 - accuracy: 0.6620 - val_loss: 0.6212 - val_accuracy: 0.6926\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.6670 - val_loss: 0.6150 - val_accuracy: 0.6939\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6067 - accuracy: 0.6713 - val_loss: 0.6174 - val_accuracy: 0.6939\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6652 - val_loss: 0.6139 - val_accuracy: 0.6964\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6098 - accuracy: 0.6713 - val_loss: 0.6128 - val_accuracy: 0.6977\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6060 - accuracy: 0.6728 - val_loss: 0.6158 - val_accuracy: 0.6977\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6073 - accuracy: 0.6722 - val_loss: 0.6221 - val_accuracy: 0.6888\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6086 - accuracy: 0.6677 - val_loss: 0.6197 - val_accuracy: 0.6952\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6006 - accuracy: 0.6751 - val_loss: 0.6191 - val_accuracy: 0.6901\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6011 - accuracy: 0.6801 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6048 - accuracy: 0.6747 - val_loss: 0.6217 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6033 - accuracy: 0.6786 - val_loss: 0.6172 - val_accuracy: 0.6939\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5990 - accuracy: 0.6814 - val_loss: 0.6210 - val_accuracy: 0.6888\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6020 - accuracy: 0.6778 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6024 - accuracy: 0.6786 - val_loss: 0.6204 - val_accuracy: 0.6964\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5988 - accuracy: 0.6776 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5999 - accuracy: 0.6806 - val_loss: 0.6274 - val_accuracy: 0.6798\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5971 - accuracy: 0.6856 - val_loss: 0.6221 - val_accuracy: 0.6862\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5901 - accuracy: 0.6876 - val_loss: 0.6235 - val_accuracy: 0.6824\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5951 - accuracy: 0.6876 - val_loss: 0.6233 - val_accuracy: 0.6901\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5964 - accuracy: 0.6800 - val_loss: 0.6239 - val_accuracy: 0.6952\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5966 - accuracy: 0.6859 - val_loss: 0.6279 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5920 - accuracy: 0.6925 - val_loss: 0.6262 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5942 - accuracy: 0.6822 - val_loss: 0.6276 - val_accuracy: 0.6888\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5882 - accuracy: 0.6938 - val_loss: 0.6268 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5860 - accuracy: 0.6907 - val_loss: 0.6240 - val_accuracy: 0.6875\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5859 - accuracy: 0.6966 - val_loss: 0.6261 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5838 - accuracy: 0.6959 - val_loss: 0.6260 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5850 - accuracy: 0.6938 - val_loss: 0.6276 - val_accuracy: 0.6837\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5842 - accuracy: 0.6953 - val_loss: 0.6315 - val_accuracy: 0.6837\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5803 - accuracy: 0.6971 - val_loss: 0.6269 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5854 - accuracy: 0.6886 - val_loss: 0.6335 - val_accuracy: 0.6798\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5825 - accuracy: 0.6952 - val_loss: 0.6270 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5776 - accuracy: 0.6991 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5797 - accuracy: 0.6924 - val_loss: 0.6302 - val_accuracy: 0.6849\n",
      "Calculating for: 700 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_136 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8537 - accuracy: 0.5237 - val_loss: 0.6479 - val_accuracy: 0.6390\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.7240 - accuracy: 0.5501 - val_loss: 0.6423 - val_accuracy: 0.6467\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6984 - accuracy: 0.5558 - val_loss: 0.6462 - val_accuracy: 0.6505\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6860 - accuracy: 0.5569 - val_loss: 0.6497 - val_accuracy: 0.6531\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6782 - accuracy: 0.5698 - val_loss: 0.6446 - val_accuracy: 0.6518\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5727 - val_loss: 0.6492 - val_accuracy: 0.6645\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6758 - accuracy: 0.5786 - val_loss: 0.6472 - val_accuracy: 0.6645\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6721 - accuracy: 0.5874 - val_loss: 0.6437 - val_accuracy: 0.6531\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5909 - val_loss: 0.6458 - val_accuracy: 0.6594\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6720 - accuracy: 0.5878 - val_loss: 0.6458 - val_accuracy: 0.6582\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6680 - accuracy: 0.5993 - val_loss: 0.6367 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6684 - accuracy: 0.5961 - val_loss: 0.6463 - val_accuracy: 0.6518\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6040 - val_loss: 0.6413 - val_accuracy: 0.6582\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6031 - val_loss: 0.6377 - val_accuracy: 0.6620\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6004 - val_loss: 0.6376 - val_accuracy: 0.6594\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5975 - val_loss: 0.6435 - val_accuracy: 0.6671\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6045 - val_loss: 0.6346 - val_accuracy: 0.6696\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.6024 - val_loss: 0.6342 - val_accuracy: 0.6709\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6017 - val_loss: 0.6401 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6609 - accuracy: 0.6058 - val_loss: 0.6328 - val_accuracy: 0.6658\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6592 - accuracy: 0.6073 - val_loss: 0.6371 - val_accuracy: 0.6684\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6076 - val_loss: 0.6399 - val_accuracy: 0.6735\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6104 - val_loss: 0.6386 - val_accuracy: 0.6696\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6103 - val_loss: 0.6356 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6569 - accuracy: 0.6154 - val_loss: 0.6402 - val_accuracy: 0.6696\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6560 - accuracy: 0.6216 - val_loss: 0.6389 - val_accuracy: 0.6620\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6566 - accuracy: 0.6199 - val_loss: 0.6298 - val_accuracy: 0.6811\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6587 - accuracy: 0.6122 - val_loss: 0.6402 - val_accuracy: 0.6620\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6559 - accuracy: 0.6154 - val_loss: 0.6330 - val_accuracy: 0.6798\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6171 - val_loss: 0.6278 - val_accuracy: 0.6786\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6548 - accuracy: 0.6129 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6216 - val_loss: 0.6267 - val_accuracy: 0.6849\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6203 - val_loss: 0.6276 - val_accuracy: 0.6747\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6496 - accuracy: 0.6262 - val_loss: 0.6326 - val_accuracy: 0.6735\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6236 - val_loss: 0.6293 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6231 - val_loss: 0.6299 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6489 - accuracy: 0.6289 - val_loss: 0.6312 - val_accuracy: 0.6709\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6498 - accuracy: 0.6241 - val_loss: 0.6317 - val_accuracy: 0.6620\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6281 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6503 - accuracy: 0.6323 - val_loss: 0.6294 - val_accuracy: 0.6747\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6222 - val_loss: 0.6298 - val_accuracy: 0.6684\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6502 - accuracy: 0.6211 - val_loss: 0.6283 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6321 - val_loss: 0.6320 - val_accuracy: 0.6607\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6258 - val_loss: 0.6302 - val_accuracy: 0.6633\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6487 - accuracy: 0.6323 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6445 - accuracy: 0.6329 - val_loss: 0.6214 - val_accuracy: 0.6837\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6345 - val_loss: 0.6292 - val_accuracy: 0.6633\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6472 - accuracy: 0.6311 - val_loss: 0.6257 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6323 - val_loss: 0.6249 - val_accuracy: 0.6811\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6343 - val_loss: 0.6257 - val_accuracy: 0.6760\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6414 - accuracy: 0.6390 - val_loss: 0.6247 - val_accuracy: 0.6786\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6465 - accuracy: 0.6331 - val_loss: 0.6238 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6425 - accuracy: 0.6361 - val_loss: 0.6188 - val_accuracy: 0.6773\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6384 - val_loss: 0.6223 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6341 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6419 - accuracy: 0.6360 - val_loss: 0.6167 - val_accuracy: 0.6811\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6412 - val_loss: 0.6210 - val_accuracy: 0.6811\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6456 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6374 - val_loss: 0.6253 - val_accuracy: 0.6773\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6393 - val_loss: 0.6206 - val_accuracy: 0.6837\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6373 - val_loss: 0.6175 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6410 - val_loss: 0.6201 - val_accuracy: 0.6862\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6400 - accuracy: 0.6442 - val_loss: 0.6168 - val_accuracy: 0.6786\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6385 - accuracy: 0.6427 - val_loss: 0.6227 - val_accuracy: 0.6811\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6436 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6446 - val_loss: 0.6236 - val_accuracy: 0.6786\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6473 - val_loss: 0.6211 - val_accuracy: 0.6811\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6480 - val_loss: 0.6185 - val_accuracy: 0.6849\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6420 - val_loss: 0.6125 - val_accuracy: 0.6888\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6451 - val_loss: 0.6170 - val_accuracy: 0.6875\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6368 - accuracy: 0.6409 - val_loss: 0.6204 - val_accuracy: 0.6849\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6428 - val_loss: 0.6172 - val_accuracy: 0.6862\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6490 - val_loss: 0.6236 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6510 - val_loss: 0.6198 - val_accuracy: 0.6862\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6433 - val_loss: 0.6231 - val_accuracy: 0.6901\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6507 - val_loss: 0.6255 - val_accuracy: 0.6773\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6485 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6512 - val_loss: 0.6153 - val_accuracy: 0.6926\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6500 - val_loss: 0.6202 - val_accuracy: 0.6862\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6532 - val_loss: 0.6185 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6541 - val_loss: 0.6187 - val_accuracy: 0.6875\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6572 - val_loss: 0.6158 - val_accuracy: 0.6926\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6574 - val_loss: 0.6113 - val_accuracy: 0.6952\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6284 - accuracy: 0.6540 - val_loss: 0.6115 - val_accuracy: 0.6964\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6500 - val_loss: 0.6161 - val_accuracy: 0.6926\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6576 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6262 - accuracy: 0.6560 - val_loss: 0.6153 - val_accuracy: 0.6901\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6320 - accuracy: 0.6559 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6259 - accuracy: 0.6570 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6611 - val_loss: 0.6138 - val_accuracy: 0.6939\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6606 - val_loss: 0.6138 - val_accuracy: 0.6862\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6556 - val_loss: 0.6156 - val_accuracy: 0.6837\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6579 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6550 - val_loss: 0.6103 - val_accuracy: 0.6926\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6282 - accuracy: 0.6610 - val_loss: 0.6173 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6614 - val_loss: 0.6130 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6657 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6253 - accuracy: 0.6623 - val_loss: 0.6173 - val_accuracy: 0.6888\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6561 - val_loss: 0.6156 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6628 - val_loss: 0.6122 - val_accuracy: 0.6888\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6692 - val_loss: 0.6126 - val_accuracy: 0.6926\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6611 - val_loss: 0.6141 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6598 - val_loss: 0.6155 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6643 - val_loss: 0.6135 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6616 - val_loss: 0.6190 - val_accuracy: 0.6773\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6691 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6644 - val_loss: 0.6208 - val_accuracy: 0.6786\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6577 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6600 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6625 - val_loss: 0.6173 - val_accuracy: 0.6837\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6670 - val_loss: 0.6188 - val_accuracy: 0.6862\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6171 - accuracy: 0.6670 - val_loss: 0.6207 - val_accuracy: 0.6811\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.6692 - val_loss: 0.6115 - val_accuracy: 0.6913\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6629 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6727 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6663 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6664 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6615 - val_loss: 0.6146 - val_accuracy: 0.6913\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6712 - val_loss: 0.6197 - val_accuracy: 0.6722\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.6712 - val_loss: 0.6206 - val_accuracy: 0.6786\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6728 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.6728 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6134 - accuracy: 0.6707 - val_loss: 0.6168 - val_accuracy: 0.6862\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6142 - accuracy: 0.6682 - val_loss: 0.6184 - val_accuracy: 0.6888\n",
      "Calculating for: 700 100 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_140 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8488 - accuracy: 0.5085 - val_loss: 0.6684 - val_accuracy: 0.6212\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7409 - accuracy: 0.5149 - val_loss: 0.6685 - val_accuracy: 0.6173\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7102 - accuracy: 0.5151 - val_loss: 0.6778 - val_accuracy: 0.6250\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6996 - accuracy: 0.5225 - val_loss: 0.6755 - val_accuracy: 0.6212\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.5180 - val_loss: 0.6765 - val_accuracy: 0.6224\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6929 - accuracy: 0.5232 - val_loss: 0.6674 - val_accuracy: 0.6199\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6905 - accuracy: 0.5310 - val_loss: 0.6712 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5178 - val_loss: 0.6727 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5300 - val_loss: 0.6668 - val_accuracy: 0.6199\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5291 - val_loss: 0.6713 - val_accuracy: 0.6212\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5207 - val_loss: 0.6700 - val_accuracy: 0.6212\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6912 - accuracy: 0.5319 - val_loss: 0.6685 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6879 - accuracy: 0.5416 - val_loss: 0.6634 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5322 - val_loss: 0.6685 - val_accuracy: 0.6237\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5391 - val_loss: 0.6682 - val_accuracy: 0.6250\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6900 - accuracy: 0.5353 - val_loss: 0.6689 - val_accuracy: 0.6237\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6889 - accuracy: 0.5433 - val_loss: 0.6648 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5403 - val_loss: 0.6664 - val_accuracy: 0.6224\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.5339 - val_loss: 0.6617 - val_accuracy: 0.6237\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5328 - val_loss: 0.6650 - val_accuracy: 0.6263\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6863 - accuracy: 0.5528 - val_loss: 0.6612 - val_accuracy: 0.6237\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.5515 - val_loss: 0.6614 - val_accuracy: 0.6263\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6879 - accuracy: 0.5394 - val_loss: 0.6632 - val_accuracy: 0.6263\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6873 - accuracy: 0.5445 - val_loss: 0.6632 - val_accuracy: 0.6263\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5526 - val_loss: 0.6592 - val_accuracy: 0.6263\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6882 - accuracy: 0.5418 - val_loss: 0.6646 - val_accuracy: 0.6263\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6875 - accuracy: 0.5406 - val_loss: 0.6621 - val_accuracy: 0.6263\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6875 - accuracy: 0.5471 - val_loss: 0.6618 - val_accuracy: 0.6301\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6844 - accuracy: 0.5504 - val_loss: 0.6618 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6846 - accuracy: 0.5539 - val_loss: 0.6608 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5463 - val_loss: 0.6588 - val_accuracy: 0.6390\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5538 - val_loss: 0.6595 - val_accuracy: 0.6403\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6873 - accuracy: 0.5441 - val_loss: 0.6606 - val_accuracy: 0.6327\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6835 - accuracy: 0.5531 - val_loss: 0.6569 - val_accuracy: 0.6378\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5509 - val_loss: 0.6612 - val_accuracy: 0.6416\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5452 - val_loss: 0.6596 - val_accuracy: 0.6416\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6864 - accuracy: 0.5453 - val_loss: 0.6591 - val_accuracy: 0.6416\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6835 - accuracy: 0.5580 - val_loss: 0.6587 - val_accuracy: 0.6429\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5569 - val_loss: 0.6565 - val_accuracy: 0.6429\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6818 - accuracy: 0.5620 - val_loss: 0.6525 - val_accuracy: 0.6403\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6820 - accuracy: 0.5602 - val_loss: 0.6562 - val_accuracy: 0.6403\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6851 - accuracy: 0.5520 - val_loss: 0.6556 - val_accuracy: 0.6429\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6806 - accuracy: 0.5649 - val_loss: 0.6530 - val_accuracy: 0.6441\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.5569 - val_loss: 0.6548 - val_accuracy: 0.6429\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5592 - val_loss: 0.6533 - val_accuracy: 0.6416\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6832 - accuracy: 0.5605 - val_loss: 0.6550 - val_accuracy: 0.6441\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.5622 - val_loss: 0.6520 - val_accuracy: 0.6429\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6819 - accuracy: 0.5659 - val_loss: 0.6511 - val_accuracy: 0.6441\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6805 - accuracy: 0.5662 - val_loss: 0.6521 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.5653 - val_loss: 0.6528 - val_accuracy: 0.6429\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5556 - val_loss: 0.6572 - val_accuracy: 0.6429\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6811 - accuracy: 0.5653 - val_loss: 0.6536 - val_accuracy: 0.6416\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6793 - accuracy: 0.5602 - val_loss: 0.6523 - val_accuracy: 0.6403\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6773 - accuracy: 0.5721 - val_loss: 0.6549 - val_accuracy: 0.6403\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6803 - accuracy: 0.5678 - val_loss: 0.6520 - val_accuracy: 0.6429\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6812 - accuracy: 0.5602 - val_loss: 0.6557 - val_accuracy: 0.6416\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6791 - accuracy: 0.5654 - val_loss: 0.6535 - val_accuracy: 0.6403\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6784 - accuracy: 0.5691 - val_loss: 0.6517 - val_accuracy: 0.6429\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5735 - val_loss: 0.6528 - val_accuracy: 0.6416\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.5679 - val_loss: 0.6493 - val_accuracy: 0.6429\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6780 - accuracy: 0.5706 - val_loss: 0.6509 - val_accuracy: 0.6416\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6762 - accuracy: 0.5776 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6776 - accuracy: 0.5770 - val_loss: 0.6524 - val_accuracy: 0.6429\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6785 - accuracy: 0.5653 - val_loss: 0.6505 - val_accuracy: 0.6390\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.5759 - val_loss: 0.6470 - val_accuracy: 0.6403\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6764 - accuracy: 0.5749 - val_loss: 0.6488 - val_accuracy: 0.6390\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5733 - val_loss: 0.6485 - val_accuracy: 0.6441\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6756 - accuracy: 0.5756 - val_loss: 0.6452 - val_accuracy: 0.6429\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6763 - accuracy: 0.5809 - val_loss: 0.6472 - val_accuracy: 0.6416\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6765 - accuracy: 0.5759 - val_loss: 0.6446 - val_accuracy: 0.6429\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5813 - val_loss: 0.6495 - val_accuracy: 0.6467\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.5808 - val_loss: 0.6465 - val_accuracy: 0.6454\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5808 - val_loss: 0.6451 - val_accuracy: 0.6441\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5823 - val_loss: 0.6476 - val_accuracy: 0.6492\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6751 - accuracy: 0.5774 - val_loss: 0.6477 - val_accuracy: 0.6441\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6759 - accuracy: 0.5801 - val_loss: 0.6454 - val_accuracy: 0.6403\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5740 - val_loss: 0.6445 - val_accuracy: 0.6454\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5847 - val_loss: 0.6430 - val_accuracy: 0.6429\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5826 - val_loss: 0.6421 - val_accuracy: 0.6467\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6742 - accuracy: 0.5785 - val_loss: 0.6473 - val_accuracy: 0.6492\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5819 - val_loss: 0.6432 - val_accuracy: 0.6441\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.5781 - val_loss: 0.6439 - val_accuracy: 0.6429\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6726 - accuracy: 0.5836 - val_loss: 0.6444 - val_accuracy: 0.6518\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6727 - accuracy: 0.5873 - val_loss: 0.6427 - val_accuracy: 0.6441\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.5854 - val_loss: 0.6460 - val_accuracy: 0.6531\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6738 - accuracy: 0.5831 - val_loss: 0.6460 - val_accuracy: 0.6518\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6690 - accuracy: 0.5922 - val_loss: 0.6439 - val_accuracy: 0.6518\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6728 - accuracy: 0.5889 - val_loss: 0.6422 - val_accuracy: 0.6492\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5899 - val_loss: 0.6405 - val_accuracy: 0.6480\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.5761 - val_loss: 0.6440 - val_accuracy: 0.6492\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5884 - val_loss: 0.6411 - val_accuracy: 0.6505\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6738 - accuracy: 0.5843 - val_loss: 0.6433 - val_accuracy: 0.6467\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5835 - val_loss: 0.6442 - val_accuracy: 0.6467\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5838 - val_loss: 0.6434 - val_accuracy: 0.6543\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5934 - val_loss: 0.6409 - val_accuracy: 0.6582\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6718 - accuracy: 0.5854 - val_loss: 0.6431 - val_accuracy: 0.6531\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6727 - accuracy: 0.5872 - val_loss: 0.6422 - val_accuracy: 0.6492\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5978 - val_loss: 0.6414 - val_accuracy: 0.6684\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6021 - val_loss: 0.6382 - val_accuracy: 0.6518\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5885 - val_loss: 0.6398 - val_accuracy: 0.6684\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5926 - val_loss: 0.6392 - val_accuracy: 0.6531\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5859 - val_loss: 0.6426 - val_accuracy: 0.6556\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6671 - accuracy: 0.5908 - val_loss: 0.6386 - val_accuracy: 0.6645\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5968 - val_loss: 0.6381 - val_accuracy: 0.6531\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.5997 - val_loss: 0.6387 - val_accuracy: 0.6505\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6675 - accuracy: 0.5918 - val_loss: 0.6371 - val_accuracy: 0.6633\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5878 - val_loss: 0.6416 - val_accuracy: 0.6684\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5932 - val_loss: 0.6370 - val_accuracy: 0.6594\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6034 - val_loss: 0.6342 - val_accuracy: 0.6696\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5892 - val_loss: 0.6365 - val_accuracy: 0.6722\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5947 - val_loss: 0.6358 - val_accuracy: 0.6696\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.6002 - val_loss: 0.6362 - val_accuracy: 0.6671\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6639 - accuracy: 0.6021 - val_loss: 0.6347 - val_accuracy: 0.6645\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6698 - accuracy: 0.5934 - val_loss: 0.6393 - val_accuracy: 0.6582\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5987 - val_loss: 0.6339 - val_accuracy: 0.6633\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6689 - accuracy: 0.5962 - val_loss: 0.6360 - val_accuracy: 0.6696\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6039 - val_loss: 0.6353 - val_accuracy: 0.6722\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.5968 - val_loss: 0.6331 - val_accuracy: 0.6633\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6671 - accuracy: 0.6060 - val_loss: 0.6360 - val_accuracy: 0.6735\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6654 - accuracy: 0.6035 - val_loss: 0.6361 - val_accuracy: 0.6735\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6019 - val_loss: 0.6319 - val_accuracy: 0.6645\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.6007 - val_loss: 0.6353 - val_accuracy: 0.6684\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6661 - accuracy: 0.6015 - val_loss: 0.6369 - val_accuracy: 0.6722\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6127 - val_loss: 0.6330 - val_accuracy: 0.6658\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5950 - val_loss: 0.6325 - val_accuracy: 0.6620\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6083 - val_loss: 0.6300 - val_accuracy: 0.6684\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5978 - val_loss: 0.6352 - val_accuracy: 0.6709\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6040 - val_loss: 0.6309 - val_accuracy: 0.6722\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6019 - val_loss: 0.6360 - val_accuracy: 0.6773\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6030 - val_loss: 0.6335 - val_accuracy: 0.6773\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6084 - val_loss: 0.6287 - val_accuracy: 0.6684\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6079 - val_loss: 0.6347 - val_accuracy: 0.6760\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6660 - accuracy: 0.5985 - val_loss: 0.6330 - val_accuracy: 0.6696\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6091 - val_loss: 0.6317 - val_accuracy: 0.6633\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.6010 - val_loss: 0.6357 - val_accuracy: 0.6786\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6088 - val_loss: 0.6286 - val_accuracy: 0.6620\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6017 - val_loss: 0.6318 - val_accuracy: 0.6671\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6014 - val_loss: 0.6359 - val_accuracy: 0.6773\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6118 - val_loss: 0.6293 - val_accuracy: 0.6735\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6041 - val_loss: 0.6296 - val_accuracy: 0.6735\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6643 - accuracy: 0.6041 - val_loss: 0.6382 - val_accuracy: 0.6824\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6081 - val_loss: 0.6295 - val_accuracy: 0.6684\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6132 - val_loss: 0.6278 - val_accuracy: 0.6696\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6061 - val_loss: 0.6308 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6099 - val_loss: 0.6299 - val_accuracy: 0.6709\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6080 - val_loss: 0.6302 - val_accuracy: 0.6696\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6107 - val_loss: 0.6332 - val_accuracy: 0.6735\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6604 - accuracy: 0.6075 - val_loss: 0.6307 - val_accuracy: 0.6786\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6100 - val_loss: 0.6292 - val_accuracy: 0.6811\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6079 - val_loss: 0.6342 - val_accuracy: 0.6786\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6604 - accuracy: 0.6075 - val_loss: 0.6312 - val_accuracy: 0.6773\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6078 - val_loss: 0.6296 - val_accuracy: 0.6722\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6079 - val_loss: 0.6369 - val_accuracy: 0.6811\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6139 - val_loss: 0.6263 - val_accuracy: 0.6722\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6103 - val_loss: 0.6273 - val_accuracy: 0.6773\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6144 - val_loss: 0.6295 - val_accuracy: 0.6773\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6134 - val_loss: 0.6243 - val_accuracy: 0.6798\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6093 - val_loss: 0.6260 - val_accuracy: 0.6760\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6149 - val_loss: 0.6283 - val_accuracy: 0.6760\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6181 - val_loss: 0.6320 - val_accuracy: 0.6798\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6155 - val_loss: 0.6285 - val_accuracy: 0.6798\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6122 - val_loss: 0.6322 - val_accuracy: 0.6811\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6099 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6181 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6177 - val_loss: 0.6314 - val_accuracy: 0.6773\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6143 - val_loss: 0.6296 - val_accuracy: 0.6849\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6148 - val_loss: 0.6269 - val_accuracy: 0.6760\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6132 - val_loss: 0.6279 - val_accuracy: 0.6811\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6177 - val_loss: 0.6304 - val_accuracy: 0.6811\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6118 - val_loss: 0.6279 - val_accuracy: 0.6786\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6186 - val_loss: 0.6291 - val_accuracy: 0.6798\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6114 - val_loss: 0.6305 - val_accuracy: 0.6798\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6599 - accuracy: 0.6167 - val_loss: 0.6294 - val_accuracy: 0.6837\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6125 - val_loss: 0.6306 - val_accuracy: 0.6862\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6149 - val_loss: 0.6277 - val_accuracy: 0.6824\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6599 - accuracy: 0.6115 - val_loss: 0.6275 - val_accuracy: 0.6786\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6240 - val_loss: 0.6308 - val_accuracy: 0.6811\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6183 - val_loss: 0.6263 - val_accuracy: 0.6798\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6561 - accuracy: 0.6142 - val_loss: 0.6246 - val_accuracy: 0.6798\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6119 - val_loss: 0.6245 - val_accuracy: 0.6837\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6584 - accuracy: 0.6169 - val_loss: 0.6285 - val_accuracy: 0.6837\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6184 - val_loss: 0.6320 - val_accuracy: 0.6798\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6215 - val_loss: 0.6267 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6215 - val_loss: 0.6312 - val_accuracy: 0.6798\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6174 - val_loss: 0.6267 - val_accuracy: 0.6849\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6158 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6184 - val_loss: 0.6282 - val_accuracy: 0.6798\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6177 - val_loss: 0.6260 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6198 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6280 - val_loss: 0.6251 - val_accuracy: 0.6824\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6264 - val_loss: 0.6283 - val_accuracy: 0.6824\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6529 - accuracy: 0.6218 - val_loss: 0.6263 - val_accuracy: 0.6786\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6193 - val_loss: 0.6253 - val_accuracy: 0.6837\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6194 - val_loss: 0.6277 - val_accuracy: 0.6849\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6153 - val_loss: 0.6273 - val_accuracy: 0.6811\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6206 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6243 - val_loss: 0.6226 - val_accuracy: 0.6849\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6250 - val_loss: 0.6230 - val_accuracy: 0.6798\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6177 - val_loss: 0.6280 - val_accuracy: 0.6862\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6159 - val_loss: 0.6286 - val_accuracy: 0.6811\n",
      "Calculating for: 700 250 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_144 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8208 - accuracy: 0.5554 - val_loss: 0.6750 - val_accuracy: 0.6008\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7154 - accuracy: 0.5799 - val_loss: 0.6592 - val_accuracy: 0.6314\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6837 - accuracy: 0.5878 - val_loss: 0.6449 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6069 - val_loss: 0.6492 - val_accuracy: 0.6467\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6044 - val_loss: 0.6485 - val_accuracy: 0.6352\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6198 - val_loss: 0.6378 - val_accuracy: 0.6505\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6187 - val_loss: 0.6344 - val_accuracy: 0.6518\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6241 - val_loss: 0.6346 - val_accuracy: 0.6505\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6269 - val_loss: 0.6381 - val_accuracy: 0.6543\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6270 - val_loss: 0.6320 - val_accuracy: 0.6620\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6343 - val_loss: 0.6328 - val_accuracy: 0.6556\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6343 - val_loss: 0.6305 - val_accuracy: 0.6620\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6310 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6456 - val_loss: 0.6263 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6387 - val_loss: 0.6289 - val_accuracy: 0.6658\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6384 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6379 - accuracy: 0.6433 - val_loss: 0.6280 - val_accuracy: 0.6594\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6469 - val_loss: 0.6244 - val_accuracy: 0.6709\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6395 - val_loss: 0.6279 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6444 - val_loss: 0.6264 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6529 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6508 - val_loss: 0.6220 - val_accuracy: 0.6773\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6459 - val_loss: 0.6244 - val_accuracy: 0.6760\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6506 - val_loss: 0.6236 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6454 - val_loss: 0.6246 - val_accuracy: 0.6645\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6570 - val_loss: 0.6264 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6567 - val_loss: 0.6234 - val_accuracy: 0.6735\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6591 - val_loss: 0.6258 - val_accuracy: 0.6671\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6539 - val_loss: 0.6299 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6206 - accuracy: 0.6601 - val_loss: 0.6296 - val_accuracy: 0.6696\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6588 - val_loss: 0.6311 - val_accuracy: 0.6645\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6549 - val_loss: 0.6263 - val_accuracy: 0.6696\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6625 - val_loss: 0.6298 - val_accuracy: 0.6709\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6620 - val_loss: 0.6298 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6649 - val_loss: 0.6262 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6688 - val_loss: 0.6264 - val_accuracy: 0.6709\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6642 - val_loss: 0.6286 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6086 - accuracy: 0.6704 - val_loss: 0.6276 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6706 - val_loss: 0.6300 - val_accuracy: 0.6709\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6670 - val_loss: 0.6295 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6056 - accuracy: 0.6770 - val_loss: 0.6291 - val_accuracy: 0.6671\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6688 - val_loss: 0.6269 - val_accuracy: 0.6709\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6036 - accuracy: 0.6703 - val_loss: 0.6313 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6796 - val_loss: 0.6339 - val_accuracy: 0.6671\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6080 - accuracy: 0.6743 - val_loss: 0.6303 - val_accuracy: 0.6671\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6009 - accuracy: 0.6786 - val_loss: 0.6366 - val_accuracy: 0.6645\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6008 - accuracy: 0.6795 - val_loss: 0.6351 - val_accuracy: 0.6645\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6057 - accuracy: 0.6761 - val_loss: 0.6374 - val_accuracy: 0.6696\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6021 - accuracy: 0.6824 - val_loss: 0.6330 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6018 - accuracy: 0.6711 - val_loss: 0.6349 - val_accuracy: 0.6696\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5986 - accuracy: 0.6843 - val_loss: 0.6337 - val_accuracy: 0.6671\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5929 - accuracy: 0.6871 - val_loss: 0.6407 - val_accuracy: 0.6671\n",
      "Calculating for: 700 250 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_148 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8294 - accuracy: 0.5313 - val_loss: 0.6579 - val_accuracy: 0.6658\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7179 - accuracy: 0.5553 - val_loss: 0.6482 - val_accuracy: 0.6582\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5715 - val_loss: 0.6403 - val_accuracy: 0.6671\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6805 - accuracy: 0.5741 - val_loss: 0.6427 - val_accuracy: 0.6671\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6710 - accuracy: 0.5887 - val_loss: 0.6410 - val_accuracy: 0.6696\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6736 - accuracy: 0.5816 - val_loss: 0.6376 - val_accuracy: 0.6709\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6698 - accuracy: 0.5873 - val_loss: 0.6386 - val_accuracy: 0.6684\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5890 - val_loss: 0.6418 - val_accuracy: 0.6658\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6024 - val_loss: 0.6374 - val_accuracy: 0.6722\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.5997 - val_loss: 0.6356 - val_accuracy: 0.6671\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6090 - val_loss: 0.6320 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6007 - val_loss: 0.6374 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6074 - val_loss: 0.6333 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.5978 - val_loss: 0.6334 - val_accuracy: 0.6760\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6098 - val_loss: 0.6321 - val_accuracy: 0.6735\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6059 - val_loss: 0.6307 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6113 - val_loss: 0.6347 - val_accuracy: 0.6811\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6148 - val_loss: 0.6275 - val_accuracy: 0.6773\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6169 - val_loss: 0.6287 - val_accuracy: 0.6760\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6137 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6192 - val_loss: 0.6284 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6122 - val_loss: 0.6283 - val_accuracy: 0.6824\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6203 - val_loss: 0.6272 - val_accuracy: 0.6798\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6232 - val_loss: 0.6299 - val_accuracy: 0.6849\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6154 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6181 - val_loss: 0.6237 - val_accuracy: 0.6913\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6232 - val_loss: 0.6226 - val_accuracy: 0.6939\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6253 - val_loss: 0.6278 - val_accuracy: 0.6939\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6240 - val_loss: 0.6274 - val_accuracy: 0.6913\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6247 - val_loss: 0.6230 - val_accuracy: 0.6913\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6479 - accuracy: 0.6325 - val_loss: 0.6214 - val_accuracy: 0.6862\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6243 - val_loss: 0.6229 - val_accuracy: 0.6888\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6225 - val_loss: 0.6232 - val_accuracy: 0.6901\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6220 - val_loss: 0.6230 - val_accuracy: 0.6913\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6323 - val_loss: 0.6230 - val_accuracy: 0.6888\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6506 - accuracy: 0.6295 - val_loss: 0.6308 - val_accuracy: 0.6862\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6284 - val_loss: 0.6199 - val_accuracy: 0.6913\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6316 - val_loss: 0.6240 - val_accuracy: 0.6849\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6230 - val_loss: 0.6222 - val_accuracy: 0.6888\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6331 - val_loss: 0.6201 - val_accuracy: 0.6849\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6345 - val_loss: 0.6243 - val_accuracy: 0.6837\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6319 - val_loss: 0.6251 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6329 - val_loss: 0.6245 - val_accuracy: 0.6837\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6341 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6348 - val_loss: 0.6210 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6416 - accuracy: 0.6397 - val_loss: 0.6284 - val_accuracy: 0.6671\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6361 - val_loss: 0.6205 - val_accuracy: 0.6888\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6387 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6361 - val_loss: 0.6228 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6372 - accuracy: 0.6378 - val_loss: 0.6179 - val_accuracy: 0.6888\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6373 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6384 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6410 - val_loss: 0.6186 - val_accuracy: 0.6773\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6458 - val_loss: 0.6165 - val_accuracy: 0.6773\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6352 - accuracy: 0.6415 - val_loss: 0.6149 - val_accuracy: 0.6786\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6436 - val_loss: 0.6149 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6431 - val_loss: 0.6170 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6431 - val_loss: 0.6228 - val_accuracy: 0.6722\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6443 - val_loss: 0.6163 - val_accuracy: 0.6837\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6454 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6480 - val_loss: 0.6144 - val_accuracy: 0.6862\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6517 - val_loss: 0.6131 - val_accuracy: 0.6849\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6526 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6495 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6508 - val_loss: 0.6168 - val_accuracy: 0.6875\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6290 - accuracy: 0.6496 - val_loss: 0.6144 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6482 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6473 - val_loss: 0.6126 - val_accuracy: 0.6862\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6526 - val_loss: 0.6132 - val_accuracy: 0.6952\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6541 - val_loss: 0.6158 - val_accuracy: 0.6849\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6493 - val_loss: 0.6152 - val_accuracy: 0.6888\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6557 - val_loss: 0.6159 - val_accuracy: 0.6888\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6572 - val_loss: 0.6107 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6550 - val_loss: 0.6112 - val_accuracy: 0.6901\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6492 - val_loss: 0.6123 - val_accuracy: 0.6939\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6562 - val_loss: 0.6177 - val_accuracy: 0.6888\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6567 - val_loss: 0.6185 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6577 - val_loss: 0.6107 - val_accuracy: 0.6952\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6511 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6583 - val_loss: 0.6095 - val_accuracy: 0.6913\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6647 - val_loss: 0.6138 - val_accuracy: 0.6913\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6536 - val_loss: 0.6169 - val_accuracy: 0.6952\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6559 - val_loss: 0.6131 - val_accuracy: 0.6939\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6581 - val_loss: 0.6110 - val_accuracy: 0.6926\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6600 - val_loss: 0.6112 - val_accuracy: 0.6939\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6199 - accuracy: 0.6625 - val_loss: 0.6093 - val_accuracy: 0.6990\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6184 - accuracy: 0.6652 - val_loss: 0.6123 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6654 - val_loss: 0.6100 - val_accuracy: 0.7003\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6590 - val_loss: 0.6109 - val_accuracy: 0.6977\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6605 - val_loss: 0.6142 - val_accuracy: 0.6952\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6610 - val_loss: 0.6134 - val_accuracy: 0.6901\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6634 - val_loss: 0.6150 - val_accuracy: 0.6939\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6599 - val_loss: 0.6134 - val_accuracy: 0.6926\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6618 - val_loss: 0.6123 - val_accuracy: 0.6964\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6639 - val_loss: 0.6143 - val_accuracy: 0.7003\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6162 - accuracy: 0.6643 - val_loss: 0.6158 - val_accuracy: 0.6939\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6616 - val_loss: 0.6136 - val_accuracy: 0.6990\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6154 - accuracy: 0.6630 - val_loss: 0.6088 - val_accuracy: 0.7015\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6647 - val_loss: 0.6096 - val_accuracy: 0.7015\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6691 - val_loss: 0.6090 - val_accuracy: 0.6952\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6691 - val_loss: 0.6088 - val_accuracy: 0.6990\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6150 - accuracy: 0.6675 - val_loss: 0.6115 - val_accuracy: 0.6926\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6644 - val_loss: 0.6142 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6634 - val_loss: 0.6131 - val_accuracy: 0.6926\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6128 - accuracy: 0.6729 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6713 - val_loss: 0.6158 - val_accuracy: 0.6862\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6075 - accuracy: 0.6751 - val_loss: 0.6205 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6071 - accuracy: 0.6712 - val_loss: 0.6139 - val_accuracy: 0.6926\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6694 - val_loss: 0.6148 - val_accuracy: 0.6888\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6093 - accuracy: 0.6693 - val_loss: 0.6176 - val_accuracy: 0.6875\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6760 - val_loss: 0.6145 - val_accuracy: 0.6837\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6668 - val_loss: 0.6181 - val_accuracy: 0.6888\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6098 - accuracy: 0.6704 - val_loss: 0.6128 - val_accuracy: 0.6926\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6714 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6082 - accuracy: 0.6785 - val_loss: 0.6187 - val_accuracy: 0.6811\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6073 - accuracy: 0.6729 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6145 - accuracy: 0.6644 - val_loss: 0.6154 - val_accuracy: 0.6811\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6088 - accuracy: 0.6760 - val_loss: 0.6168 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6101 - accuracy: 0.6740 - val_loss: 0.6148 - val_accuracy: 0.6913\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6069 - accuracy: 0.6727 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6084 - accuracy: 0.6689 - val_loss: 0.6154 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6088 - accuracy: 0.6751 - val_loss: 0.6183 - val_accuracy: 0.6786\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6036 - accuracy: 0.6812 - val_loss: 0.6166 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6042 - accuracy: 0.6788 - val_loss: 0.6184 - val_accuracy: 0.6824\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6029 - accuracy: 0.6794 - val_loss: 0.6183 - val_accuracy: 0.6849\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6037 - accuracy: 0.6829 - val_loss: 0.6138 - val_accuracy: 0.6888\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6045 - accuracy: 0.6822 - val_loss: 0.6154 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6822 - val_loss: 0.6182 - val_accuracy: 0.6786\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5996 - accuracy: 0.6871 - val_loss: 0.6192 - val_accuracy: 0.6798\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6009 - accuracy: 0.6805 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6027 - accuracy: 0.6761 - val_loss: 0.6164 - val_accuracy: 0.6901\n",
      "Calculating for: 700 250 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_152 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8582 - accuracy: 0.4956 - val_loss: 0.6875 - val_accuracy: 0.5651\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7378 - accuracy: 0.5108 - val_loss: 0.6657 - val_accuracy: 0.6352\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7056 - accuracy: 0.5235 - val_loss: 0.6614 - val_accuracy: 0.6186\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6971 - accuracy: 0.5220 - val_loss: 0.6694 - val_accuracy: 0.6212\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5274 - val_loss: 0.6691 - val_accuracy: 0.6224\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.5383 - val_loss: 0.6640 - val_accuracy: 0.6199\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6908 - accuracy: 0.5357 - val_loss: 0.6665 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5345 - val_loss: 0.6660 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6895 - accuracy: 0.5365 - val_loss: 0.6656 - val_accuracy: 0.6186\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5301 - val_loss: 0.6701 - val_accuracy: 0.6199\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6882 - accuracy: 0.5435 - val_loss: 0.6647 - val_accuracy: 0.6212\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6893 - accuracy: 0.5345 - val_loss: 0.6636 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5289 - val_loss: 0.6661 - val_accuracy: 0.6199\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6905 - accuracy: 0.5239 - val_loss: 0.6672 - val_accuracy: 0.6199\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5383 - val_loss: 0.6657 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6895 - accuracy: 0.5358 - val_loss: 0.6650 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5379 - val_loss: 0.6646 - val_accuracy: 0.6224\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6887 - accuracy: 0.5505 - val_loss: 0.6598 - val_accuracy: 0.6224\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6858 - accuracy: 0.5491 - val_loss: 0.6602 - val_accuracy: 0.6212\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.5484 - val_loss: 0.6618 - val_accuracy: 0.6276\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6847 - accuracy: 0.5502 - val_loss: 0.6555 - val_accuracy: 0.6263\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5463 - val_loss: 0.6561 - val_accuracy: 0.6301\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5416 - val_loss: 0.6624 - val_accuracy: 0.6301\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6870 - accuracy: 0.5504 - val_loss: 0.6582 - val_accuracy: 0.6288\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5473 - val_loss: 0.6578 - val_accuracy: 0.6288\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5558 - val_loss: 0.6590 - val_accuracy: 0.6314\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5521 - val_loss: 0.6567 - val_accuracy: 0.6250\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6830 - accuracy: 0.5574 - val_loss: 0.6542 - val_accuracy: 0.6314\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5543 - val_loss: 0.6542 - val_accuracy: 0.6339\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6871 - accuracy: 0.5534 - val_loss: 0.6591 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5526 - val_loss: 0.6555 - val_accuracy: 0.6339\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6847 - accuracy: 0.5490 - val_loss: 0.6527 - val_accuracy: 0.6352\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5569 - val_loss: 0.6531 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5533 - val_loss: 0.6514 - val_accuracy: 0.6339\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6798 - accuracy: 0.5663 - val_loss: 0.6486 - val_accuracy: 0.6390\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6830 - accuracy: 0.5599 - val_loss: 0.6509 - val_accuracy: 0.6365\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6843 - accuracy: 0.5568 - val_loss: 0.6542 - val_accuracy: 0.6390\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.5730 - val_loss: 0.6473 - val_accuracy: 0.6352\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5744 - val_loss: 0.6454 - val_accuracy: 0.6416\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5664 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6815 - accuracy: 0.5627 - val_loss: 0.6494 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6805 - accuracy: 0.5697 - val_loss: 0.6460 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.5639 - val_loss: 0.6470 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6791 - accuracy: 0.5683 - val_loss: 0.6448 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6834 - accuracy: 0.5590 - val_loss: 0.6475 - val_accuracy: 0.6403\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6814 - accuracy: 0.5698 - val_loss: 0.6465 - val_accuracy: 0.6416\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6797 - accuracy: 0.5691 - val_loss: 0.6456 - val_accuracy: 0.6416\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5760 - val_loss: 0.6431 - val_accuracy: 0.6403\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5732 - val_loss: 0.6430 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6777 - accuracy: 0.5663 - val_loss: 0.6426 - val_accuracy: 0.6441\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6809 - accuracy: 0.5664 - val_loss: 0.6473 - val_accuracy: 0.6416\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5687 - val_loss: 0.6474 - val_accuracy: 0.6429\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6784 - accuracy: 0.5746 - val_loss: 0.6448 - val_accuracy: 0.6429\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5703 - val_loss: 0.6435 - val_accuracy: 0.6441\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5717 - val_loss: 0.6425 - val_accuracy: 0.6429\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.5710 - val_loss: 0.6413 - val_accuracy: 0.6429\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5731 - val_loss: 0.6419 - val_accuracy: 0.6429\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6736 - accuracy: 0.5830 - val_loss: 0.6402 - val_accuracy: 0.6441\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6744 - accuracy: 0.5828 - val_loss: 0.6428 - val_accuracy: 0.6531\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5820 - val_loss: 0.6423 - val_accuracy: 0.6480\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6766 - accuracy: 0.5735 - val_loss: 0.6391 - val_accuracy: 0.6429\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6771 - accuracy: 0.5717 - val_loss: 0.6417 - val_accuracy: 0.6467\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6719 - accuracy: 0.5828 - val_loss: 0.6377 - val_accuracy: 0.6441\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5795 - val_loss: 0.6393 - val_accuracy: 0.6454\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6742 - accuracy: 0.5823 - val_loss: 0.6391 - val_accuracy: 0.6429\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6719 - accuracy: 0.5882 - val_loss: 0.6364 - val_accuracy: 0.6518\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6717 - accuracy: 0.5840 - val_loss: 0.6396 - val_accuracy: 0.6582\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5785 - val_loss: 0.6406 - val_accuracy: 0.6492\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6753 - accuracy: 0.5849 - val_loss: 0.6392 - val_accuracy: 0.6429\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6702 - accuracy: 0.5918 - val_loss: 0.6363 - val_accuracy: 0.6467\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6710 - accuracy: 0.5887 - val_loss: 0.6359 - val_accuracy: 0.6467\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5936 - val_loss: 0.6383 - val_accuracy: 0.6480\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5865 - val_loss: 0.6367 - val_accuracy: 0.6556\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5921 - val_loss: 0.6344 - val_accuracy: 0.6531\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5888 - val_loss: 0.6330 - val_accuracy: 0.6556\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5922 - val_loss: 0.6340 - val_accuracy: 0.6492\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5848 - val_loss: 0.6341 - val_accuracy: 0.6492\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5872 - val_loss: 0.6347 - val_accuracy: 0.6492\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5878 - val_loss: 0.6371 - val_accuracy: 0.6582\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5927 - val_loss: 0.6365 - val_accuracy: 0.6658\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6722 - accuracy: 0.5813 - val_loss: 0.6350 - val_accuracy: 0.6607\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5950 - val_loss: 0.6310 - val_accuracy: 0.6531\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5909 - val_loss: 0.6336 - val_accuracy: 0.6556\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5970 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5934 - val_loss: 0.6308 - val_accuracy: 0.6543\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6000 - val_loss: 0.6322 - val_accuracy: 0.6569\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6653 - accuracy: 0.6017 - val_loss: 0.6322 - val_accuracy: 0.6556\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6680 - accuracy: 0.5896 - val_loss: 0.6341 - val_accuracy: 0.6582\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5986 - val_loss: 0.6313 - val_accuracy: 0.6607\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.6006 - val_loss: 0.6307 - val_accuracy: 0.6531\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5962 - val_loss: 0.6329 - val_accuracy: 0.6633\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5978 - val_loss: 0.6313 - val_accuracy: 0.6786\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5981 - val_loss: 0.6299 - val_accuracy: 0.6747\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5955 - val_loss: 0.6308 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6037 - val_loss: 0.6319 - val_accuracy: 0.6709\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6674 - accuracy: 0.5960 - val_loss: 0.6306 - val_accuracy: 0.6735\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.5988 - val_loss: 0.6296 - val_accuracy: 0.6747\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5971 - val_loss: 0.6313 - val_accuracy: 0.6671\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6016 - val_loss: 0.6306 - val_accuracy: 0.6671\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6044 - val_loss: 0.6295 - val_accuracy: 0.6620\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6172 - val_loss: 0.6275 - val_accuracy: 0.6594\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6088 - val_loss: 0.6312 - val_accuracy: 0.6747\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.6014 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6006 - val_loss: 0.6281 - val_accuracy: 0.6735\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6046 - val_loss: 0.6293 - val_accuracy: 0.6709\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6004 - val_loss: 0.6287 - val_accuracy: 0.6735\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6042 - val_loss: 0.6303 - val_accuracy: 0.6722\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.6024 - val_loss: 0.6266 - val_accuracy: 0.6735\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5955 - val_loss: 0.6279 - val_accuracy: 0.6747\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.6039 - val_loss: 0.6280 - val_accuracy: 0.6684\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6117 - val_loss: 0.6244 - val_accuracy: 0.6747\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6101 - val_loss: 0.6248 - val_accuracy: 0.6696\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6052 - val_loss: 0.6284 - val_accuracy: 0.6696\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6019 - val_loss: 0.6270 - val_accuracy: 0.6747\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6114 - val_loss: 0.6249 - val_accuracy: 0.6722\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6095 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6166 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5993 - val_loss: 0.6285 - val_accuracy: 0.6684\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6027 - val_loss: 0.6260 - val_accuracy: 0.6645\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6117 - val_loss: 0.6251 - val_accuracy: 0.6645\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6076 - val_loss: 0.6252 - val_accuracy: 0.6709\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6095 - val_loss: 0.6278 - val_accuracy: 0.6709\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6085 - val_loss: 0.6258 - val_accuracy: 0.6658\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5992 - val_loss: 0.6288 - val_accuracy: 0.6671\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6044 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6088 - val_loss: 0.6261 - val_accuracy: 0.6747\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6127 - val_loss: 0.6293 - val_accuracy: 0.6722\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6101 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6145 - val_loss: 0.6219 - val_accuracy: 0.6696\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6061 - val_loss: 0.6283 - val_accuracy: 0.6760\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6089 - val_loss: 0.6244 - val_accuracy: 0.6735\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6173 - val_loss: 0.6217 - val_accuracy: 0.6645\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6154 - val_loss: 0.6214 - val_accuracy: 0.6671\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6052 - val_loss: 0.6220 - val_accuracy: 0.6684\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6071 - val_loss: 0.6254 - val_accuracy: 0.6760\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6133 - val_loss: 0.6217 - val_accuracy: 0.6696\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6152 - val_loss: 0.6241 - val_accuracy: 0.6722\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6215 - val_loss: 0.6222 - val_accuracy: 0.6773\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6154 - val_loss: 0.6230 - val_accuracy: 0.6811\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6114 - val_loss: 0.6245 - val_accuracy: 0.6786\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6154 - val_loss: 0.6198 - val_accuracy: 0.6747\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6127 - val_loss: 0.6250 - val_accuracy: 0.6747\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6192 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6089 - val_loss: 0.6252 - val_accuracy: 0.6798\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6144 - val_loss: 0.6209 - val_accuracy: 0.6747\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6101 - val_loss: 0.6264 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6182 - val_loss: 0.6238 - val_accuracy: 0.6735\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6198 - val_loss: 0.6219 - val_accuracy: 0.6671\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6117 - val_loss: 0.6222 - val_accuracy: 0.6696\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6197 - val_loss: 0.6233 - val_accuracy: 0.6786\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6220 - val_loss: 0.6218 - val_accuracy: 0.6773\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6109 - val_loss: 0.6254 - val_accuracy: 0.6837\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6587 - accuracy: 0.6157 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6227 - val_loss: 0.6179 - val_accuracy: 0.6760\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6172 - val_loss: 0.6215 - val_accuracy: 0.6824\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6167 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6119 - val_loss: 0.6224 - val_accuracy: 0.6709\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6149 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6163 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6161 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6181 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6209 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6188 - val_loss: 0.6176 - val_accuracy: 0.6811\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6148 - val_loss: 0.6180 - val_accuracy: 0.6722\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6208 - val_loss: 0.6178 - val_accuracy: 0.6786\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6198 - val_loss: 0.6236 - val_accuracy: 0.6709\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6285 - val_loss: 0.6213 - val_accuracy: 0.6811\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6192 - val_loss: 0.6185 - val_accuracy: 0.6798\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6172 - val_loss: 0.6233 - val_accuracy: 0.6849\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6240 - val_loss: 0.6185 - val_accuracy: 0.6837\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6230 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6193 - val_loss: 0.6179 - val_accuracy: 0.6773\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6223 - val_loss: 0.6179 - val_accuracy: 0.6786\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6248 - val_loss: 0.6184 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6208 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6223 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6177 - val_loss: 0.6182 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6206 - val_loss: 0.6181 - val_accuracy: 0.6837\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6197 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6191 - val_loss: 0.6179 - val_accuracy: 0.6760\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6225 - val_loss: 0.6155 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6243 - val_loss: 0.6223 - val_accuracy: 0.6798\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6261 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6264 - val_loss: 0.6180 - val_accuracy: 0.6773\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6251 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6326 - val_loss: 0.6146 - val_accuracy: 0.6811\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6262 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6256 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6233 - val_loss: 0.6196 - val_accuracy: 0.6773\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6286 - val_loss: 0.6156 - val_accuracy: 0.6798\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6314 - val_loss: 0.6166 - val_accuracy: 0.6824\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6218 - val_loss: 0.6196 - val_accuracy: 0.6824\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6217 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6274 - val_loss: 0.6169 - val_accuracy: 0.6798\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6321 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6269 - val_loss: 0.6165 - val_accuracy: 0.6773\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6310 - val_loss: 0.6166 - val_accuracy: 0.6798\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6290 - val_loss: 0.6170 - val_accuracy: 0.6824\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6228 - val_loss: 0.6191 - val_accuracy: 0.6837\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6496 - accuracy: 0.6299 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Calculating for: 700 400 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_156 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7884 - accuracy: 0.5536 - val_loss: 0.6536 - val_accuracy: 0.6798\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6941 - accuracy: 0.5882 - val_loss: 0.6481 - val_accuracy: 0.6620\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6720 - accuracy: 0.6004 - val_loss: 0.6423 - val_accuracy: 0.6569\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6061 - val_loss: 0.6378 - val_accuracy: 0.6645\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6603 - accuracy: 0.6086 - val_loss: 0.6355 - val_accuracy: 0.6786\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6590 - accuracy: 0.6113 - val_loss: 0.6313 - val_accuracy: 0.6709\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6525 - accuracy: 0.6256 - val_loss: 0.6330 - val_accuracy: 0.6760\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6529 - accuracy: 0.6155 - val_loss: 0.6335 - val_accuracy: 0.6735\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6501 - accuracy: 0.6228 - val_loss: 0.6281 - val_accuracy: 0.6798\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6261 - val_loss: 0.6275 - val_accuracy: 0.6747\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6436 - accuracy: 0.6361 - val_loss: 0.6242 - val_accuracy: 0.6773\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6424 - accuracy: 0.6329 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6446 - accuracy: 0.6286 - val_loss: 0.6257 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6434 - accuracy: 0.6345 - val_loss: 0.6228 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6403 - accuracy: 0.6380 - val_loss: 0.6242 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6412 - val_loss: 0.6229 - val_accuracy: 0.6837\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6336 - val_loss: 0.6256 - val_accuracy: 0.6671\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6352 - accuracy: 0.6410 - val_loss: 0.6197 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6439 - val_loss: 0.6224 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6318 - accuracy: 0.6466 - val_loss: 0.6218 - val_accuracy: 0.6658\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6315 - accuracy: 0.6462 - val_loss: 0.6201 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6329 - accuracy: 0.6476 - val_loss: 0.6200 - val_accuracy: 0.6747\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6302 - accuracy: 0.6518 - val_loss: 0.6208 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6284 - accuracy: 0.6510 - val_loss: 0.6196 - val_accuracy: 0.6773\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6278 - accuracy: 0.6535 - val_loss: 0.6213 - val_accuracy: 0.6811\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6240 - accuracy: 0.6555 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6203 - accuracy: 0.6588 - val_loss: 0.6172 - val_accuracy: 0.6926\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6259 - accuracy: 0.6495 - val_loss: 0.6211 - val_accuracy: 0.6811\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6216 - accuracy: 0.6618 - val_loss: 0.6148 - val_accuracy: 0.6990\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6166 - accuracy: 0.6629 - val_loss: 0.6149 - val_accuracy: 0.6888\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6206 - accuracy: 0.6629 - val_loss: 0.6167 - val_accuracy: 0.6901\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6183 - accuracy: 0.6663 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6167 - accuracy: 0.6616 - val_loss: 0.6166 - val_accuracy: 0.6913\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6159 - accuracy: 0.6608 - val_loss: 0.6194 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6113 - accuracy: 0.6701 - val_loss: 0.6247 - val_accuracy: 0.6798\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6146 - accuracy: 0.6631 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6065 - accuracy: 0.6721 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6104 - accuracy: 0.6752 - val_loss: 0.6188 - val_accuracy: 0.6875\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6090 - accuracy: 0.6662 - val_loss: 0.6211 - val_accuracy: 0.6862\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6085 - accuracy: 0.6727 - val_loss: 0.6200 - val_accuracy: 0.6964\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6040 - accuracy: 0.6765 - val_loss: 0.6230 - val_accuracy: 0.6888\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6057 - accuracy: 0.6723 - val_loss: 0.6187 - val_accuracy: 0.6837\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6088 - accuracy: 0.6750 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6028 - accuracy: 0.6765 - val_loss: 0.6236 - val_accuracy: 0.6939\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6032 - accuracy: 0.6760 - val_loss: 0.6222 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5970 - accuracy: 0.6830 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5954 - accuracy: 0.6830 - val_loss: 0.6243 - val_accuracy: 0.6786\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5966 - accuracy: 0.6802 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5999 - accuracy: 0.6807 - val_loss: 0.6278 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6017 - accuracy: 0.6768 - val_loss: 0.6228 - val_accuracy: 0.6913\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5956 - accuracy: 0.6797 - val_loss: 0.6236 - val_accuracy: 0.6913\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5958 - accuracy: 0.6829 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5944 - accuracy: 0.6858 - val_loss: 0.6289 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5927 - accuracy: 0.6894 - val_loss: 0.6343 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5962 - accuracy: 0.6832 - val_loss: 0.6283 - val_accuracy: 0.6862\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5949 - accuracy: 0.6831 - val_loss: 0.6237 - val_accuracy: 0.6824\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5908 - accuracy: 0.6858 - val_loss: 0.6330 - val_accuracy: 0.6786\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5903 - accuracy: 0.6874 - val_loss: 0.6305 - val_accuracy: 0.6837\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5887 - accuracy: 0.6937 - val_loss: 0.6299 - val_accuracy: 0.6824\n",
      "Calculating for: 700 400 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_160 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8120 - accuracy: 0.5345 - val_loss: 0.6454 - val_accuracy: 0.6352\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7088 - accuracy: 0.5634 - val_loss: 0.6418 - val_accuracy: 0.6633\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6824 - accuracy: 0.5836 - val_loss: 0.6400 - val_accuracy: 0.6658\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6791 - accuracy: 0.5781 - val_loss: 0.6379 - val_accuracy: 0.6582\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6755 - accuracy: 0.5808 - val_loss: 0.6389 - val_accuracy: 0.6620\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6708 - accuracy: 0.5882 - val_loss: 0.6385 - val_accuracy: 0.6671\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.5929 - val_loss: 0.6380 - val_accuracy: 0.6671\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6680 - accuracy: 0.5975 - val_loss: 0.6369 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6660 - accuracy: 0.6007 - val_loss: 0.6316 - val_accuracy: 0.6684\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6685 - accuracy: 0.5941 - val_loss: 0.6346 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6637 - accuracy: 0.5968 - val_loss: 0.6384 - val_accuracy: 0.6747\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6634 - accuracy: 0.6010 - val_loss: 0.6331 - val_accuracy: 0.6709\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6615 - accuracy: 0.6148 - val_loss: 0.6314 - val_accuracy: 0.6709\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6592 - accuracy: 0.6060 - val_loss: 0.6315 - val_accuracy: 0.6709\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6032 - val_loss: 0.6370 - val_accuracy: 0.6773\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6587 - accuracy: 0.6112 - val_loss: 0.6335 - val_accuracy: 0.6824\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6573 - accuracy: 0.6115 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6563 - accuracy: 0.6130 - val_loss: 0.6257 - val_accuracy: 0.6875\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6560 - accuracy: 0.6105 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6528 - accuracy: 0.6152 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6562 - accuracy: 0.6186 - val_loss: 0.6238 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6525 - accuracy: 0.6246 - val_loss: 0.6236 - val_accuracy: 0.6773\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6542 - accuracy: 0.6215 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6554 - accuracy: 0.6148 - val_loss: 0.6255 - val_accuracy: 0.6811\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6522 - accuracy: 0.6265 - val_loss: 0.6216 - val_accuracy: 0.6875\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6497 - accuracy: 0.6272 - val_loss: 0.6226 - val_accuracy: 0.6824\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6159 - val_loss: 0.6191 - val_accuracy: 0.6862\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6517 - accuracy: 0.6238 - val_loss: 0.6238 - val_accuracy: 0.6760\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6499 - accuracy: 0.6247 - val_loss: 0.6219 - val_accuracy: 0.6786\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6498 - accuracy: 0.6222 - val_loss: 0.6250 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6491 - accuracy: 0.6255 - val_loss: 0.6242 - val_accuracy: 0.6798\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6485 - accuracy: 0.6240 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6493 - accuracy: 0.6206 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6476 - accuracy: 0.6217 - val_loss: 0.6198 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6296 - val_loss: 0.6180 - val_accuracy: 0.6811\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6466 - accuracy: 0.6252 - val_loss: 0.6210 - val_accuracy: 0.6837\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6429 - accuracy: 0.6320 - val_loss: 0.6204 - val_accuracy: 0.6849\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6260 - val_loss: 0.6198 - val_accuracy: 0.6837\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6453 - accuracy: 0.6270 - val_loss: 0.6192 - val_accuracy: 0.6849\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6440 - accuracy: 0.6360 - val_loss: 0.6195 - val_accuracy: 0.6849\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6463 - accuracy: 0.6269 - val_loss: 0.6169 - val_accuracy: 0.6875\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6227 - val_loss: 0.6212 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6453 - accuracy: 0.6318 - val_loss: 0.6169 - val_accuracy: 0.6913\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6424 - accuracy: 0.6333 - val_loss: 0.6204 - val_accuracy: 0.6773\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6386 - accuracy: 0.6400 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6387 - accuracy: 0.6404 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6391 - accuracy: 0.6409 - val_loss: 0.6170 - val_accuracy: 0.6786\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6406 - accuracy: 0.6370 - val_loss: 0.6176 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6418 - accuracy: 0.6388 - val_loss: 0.6190 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6393 - accuracy: 0.6374 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6381 - accuracy: 0.6336 - val_loss: 0.6151 - val_accuracy: 0.6824\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6363 - accuracy: 0.6417 - val_loss: 0.6137 - val_accuracy: 0.6811\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6395 - accuracy: 0.6405 - val_loss: 0.6139 - val_accuracy: 0.6875\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6405 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6413 - val_loss: 0.6132 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6323 - accuracy: 0.6513 - val_loss: 0.6152 - val_accuracy: 0.6926\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6365 - accuracy: 0.6414 - val_loss: 0.6191 - val_accuracy: 0.6875\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6463 - val_loss: 0.6126 - val_accuracy: 0.6990\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6365 - accuracy: 0.6446 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6476 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6368 - accuracy: 0.6462 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6462 - val_loss: 0.6125 - val_accuracy: 0.6952\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6327 - accuracy: 0.6469 - val_loss: 0.6147 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6338 - accuracy: 0.6463 - val_loss: 0.6164 - val_accuracy: 0.6862\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6299 - accuracy: 0.6551 - val_loss: 0.6138 - val_accuracy: 0.6862\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6354 - accuracy: 0.6491 - val_loss: 0.6146 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6301 - accuracy: 0.6488 - val_loss: 0.6158 - val_accuracy: 0.6849\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6451 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6540 - val_loss: 0.6095 - val_accuracy: 0.6926\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6239 - accuracy: 0.6594 - val_loss: 0.6179 - val_accuracy: 0.6811\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6276 - accuracy: 0.6506 - val_loss: 0.6127 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6258 - accuracy: 0.6591 - val_loss: 0.6123 - val_accuracy: 0.6888\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6283 - accuracy: 0.6513 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6279 - accuracy: 0.6537 - val_loss: 0.6135 - val_accuracy: 0.6901\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6266 - accuracy: 0.6572 - val_loss: 0.6094 - val_accuracy: 0.6977\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6240 - accuracy: 0.6530 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6269 - accuracy: 0.6541 - val_loss: 0.6154 - val_accuracy: 0.6926\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6207 - accuracy: 0.6564 - val_loss: 0.6135 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6223 - accuracy: 0.6541 - val_loss: 0.6140 - val_accuracy: 0.6888\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6245 - accuracy: 0.6556 - val_loss: 0.6139 - val_accuracy: 0.6875\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6198 - accuracy: 0.6611 - val_loss: 0.6114 - val_accuracy: 0.6926\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6249 - accuracy: 0.6579 - val_loss: 0.6106 - val_accuracy: 0.6913\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6222 - accuracy: 0.6647 - val_loss: 0.6113 - val_accuracy: 0.6939\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6202 - accuracy: 0.6662 - val_loss: 0.6112 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 3s 10ms/step - loss: 0.6173 - accuracy: 0.6668 - val_loss: 0.6125 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6195 - accuracy: 0.6614 - val_loss: 0.6113 - val_accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6191 - accuracy: 0.6650 - val_loss: 0.6097 - val_accuracy: 0.6926\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6203 - accuracy: 0.6589 - val_loss: 0.6129 - val_accuracy: 0.6811\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6193 - accuracy: 0.6609 - val_loss: 0.6109 - val_accuracy: 0.6901\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6194 - accuracy: 0.6635 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6196 - accuracy: 0.6707 - val_loss: 0.6127 - val_accuracy: 0.6939\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6183 - accuracy: 0.6615 - val_loss: 0.6132 - val_accuracy: 0.6913\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6163 - accuracy: 0.6634 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6123 - accuracy: 0.6677 - val_loss: 0.6123 - val_accuracy: 0.6862\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6179 - accuracy: 0.6647 - val_loss: 0.6123 - val_accuracy: 0.6888\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6143 - accuracy: 0.6692 - val_loss: 0.6148 - val_accuracy: 0.6824\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6185 - accuracy: 0.6701 - val_loss: 0.6151 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6146 - accuracy: 0.6648 - val_loss: 0.6138 - val_accuracy: 0.6901\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6100 - accuracy: 0.6756 - val_loss: 0.6129 - val_accuracy: 0.6926\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6119 - accuracy: 0.6708 - val_loss: 0.6174 - val_accuracy: 0.6849\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6113 - accuracy: 0.6687 - val_loss: 0.6126 - val_accuracy: 0.6875\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6128 - accuracy: 0.6625 - val_loss: 0.6161 - val_accuracy: 0.6824\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6107 - accuracy: 0.6753 - val_loss: 0.6133 - val_accuracy: 0.6875\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6123 - accuracy: 0.6716 - val_loss: 0.6139 - val_accuracy: 0.6824\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6100 - accuracy: 0.6718 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Calculating for: 700 400 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_164 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.8457 - accuracy: 0.5013 - val_loss: 0.6613 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7340 - accuracy: 0.5152 - val_loss: 0.6604 - val_accuracy: 0.6212\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7008 - accuracy: 0.5335 - val_loss: 0.6623 - val_accuracy: 0.6224\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6970 - accuracy: 0.5226 - val_loss: 0.6661 - val_accuracy: 0.6224\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6924 - accuracy: 0.5276 - val_loss: 0.6621 - val_accuracy: 0.6199\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6913 - accuracy: 0.5315 - val_loss: 0.6630 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6887 - accuracy: 0.5391 - val_loss: 0.6624 - val_accuracy: 0.6199\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6892 - accuracy: 0.5473 - val_loss: 0.6637 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6906 - accuracy: 0.5308 - val_loss: 0.6645 - val_accuracy: 0.6224\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 3s 11ms/step - loss: 0.6901 - accuracy: 0.5359 - val_loss: 0.6640 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6908 - accuracy: 0.5325 - val_loss: 0.6636 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6899 - accuracy: 0.5413 - val_loss: 0.6647 - val_accuracy: 0.6237\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6891 - accuracy: 0.5387 - val_loss: 0.6616 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6890 - accuracy: 0.5453 - val_loss: 0.6614 - val_accuracy: 0.6224\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6859 - accuracy: 0.5511 - val_loss: 0.6589 - val_accuracy: 0.6237\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6870 - accuracy: 0.5378 - val_loss: 0.6581 - val_accuracy: 0.6237\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6882 - accuracy: 0.5470 - val_loss: 0.6571 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6870 - accuracy: 0.5457 - val_loss: 0.6584 - val_accuracy: 0.6250\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6878 - accuracy: 0.5465 - val_loss: 0.6577 - val_accuracy: 0.6250\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 3s 12ms/step - loss: 0.6862 - accuracy: 0.5528 - val_loss: 0.6566 - val_accuracy: 0.6263\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6862 - accuracy: 0.5529 - val_loss: 0.6544 - val_accuracy: 0.6250\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6868 - accuracy: 0.5489 - val_loss: 0.6554 - val_accuracy: 0.6250\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6854 - accuracy: 0.5558 - val_loss: 0.6571 - val_accuracy: 0.6263\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6831 - accuracy: 0.5608 - val_loss: 0.6535 - val_accuracy: 0.6263\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6846 - accuracy: 0.5551 - val_loss: 0.6539 - val_accuracy: 0.6276\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6855 - accuracy: 0.5502 - val_loss: 0.6548 - val_accuracy: 0.6276\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6824 - accuracy: 0.5566 - val_loss: 0.6517 - val_accuracy: 0.6352\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6844 - accuracy: 0.5539 - val_loss: 0.6509 - val_accuracy: 0.6352\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6823 - accuracy: 0.5599 - val_loss: 0.6546 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6814 - accuracy: 0.5674 - val_loss: 0.6525 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6825 - accuracy: 0.5647 - val_loss: 0.6511 - val_accuracy: 0.6390\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6815 - accuracy: 0.5676 - val_loss: 0.6510 - val_accuracy: 0.6365\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6831 - accuracy: 0.5570 - val_loss: 0.6507 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6794 - accuracy: 0.5703 - val_loss: 0.6485 - val_accuracy: 0.6390\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6801 - accuracy: 0.5727 - val_loss: 0.6485 - val_accuracy: 0.6403\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6814 - accuracy: 0.5672 - val_loss: 0.6492 - val_accuracy: 0.6429\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6805 - accuracy: 0.5604 - val_loss: 0.6488 - val_accuracy: 0.6429\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6804 - accuracy: 0.5661 - val_loss: 0.6482 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6782 - accuracy: 0.5757 - val_loss: 0.6492 - val_accuracy: 0.6441\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6797 - accuracy: 0.5648 - val_loss: 0.6476 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6780 - accuracy: 0.5642 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6753 - accuracy: 0.5791 - val_loss: 0.6453 - val_accuracy: 0.6441\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6777 - accuracy: 0.5750 - val_loss: 0.6455 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6776 - accuracy: 0.5716 - val_loss: 0.6438 - val_accuracy: 0.6454\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6790 - accuracy: 0.5751 - val_loss: 0.6473 - val_accuracy: 0.6441\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6770 - accuracy: 0.5716 - val_loss: 0.6458 - val_accuracy: 0.6429\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6756 - accuracy: 0.5764 - val_loss: 0.6461 - val_accuracy: 0.6403\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6747 - accuracy: 0.5803 - val_loss: 0.6435 - val_accuracy: 0.6416\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6741 - accuracy: 0.5835 - val_loss: 0.6430 - val_accuracy: 0.6403\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6742 - accuracy: 0.5841 - val_loss: 0.6440 - val_accuracy: 0.6403\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6753 - accuracy: 0.5764 - val_loss: 0.6449 - val_accuracy: 0.6416\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6748 - accuracy: 0.5863 - val_loss: 0.6432 - val_accuracy: 0.6403\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6737 - accuracy: 0.5821 - val_loss: 0.6421 - val_accuracy: 0.6403\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6753 - accuracy: 0.5805 - val_loss: 0.6434 - val_accuracy: 0.6403\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.5775 - val_loss: 0.6417 - val_accuracy: 0.6403\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6738 - accuracy: 0.5836 - val_loss: 0.6415 - val_accuracy: 0.6403\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6720 - accuracy: 0.5841 - val_loss: 0.6409 - val_accuracy: 0.6416\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6726 - accuracy: 0.5927 - val_loss: 0.6405 - val_accuracy: 0.6403\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6715 - accuracy: 0.5924 - val_loss: 0.6400 - val_accuracy: 0.6403\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6769 - accuracy: 0.5739 - val_loss: 0.6424 - val_accuracy: 0.6480\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6724 - accuracy: 0.5918 - val_loss: 0.6394 - val_accuracy: 0.6467\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6737 - accuracy: 0.5869 - val_loss: 0.6404 - val_accuracy: 0.6467\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5858 - val_loss: 0.6400 - val_accuracy: 0.6480\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6697 - accuracy: 0.5834 - val_loss: 0.6383 - val_accuracy: 0.6403\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6698 - accuracy: 0.5916 - val_loss: 0.6367 - val_accuracy: 0.6467\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6704 - accuracy: 0.5903 - val_loss: 0.6377 - val_accuracy: 0.6492\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6737 - accuracy: 0.5848 - val_loss: 0.6372 - val_accuracy: 0.6505\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6726 - accuracy: 0.5864 - val_loss: 0.6376 - val_accuracy: 0.6480\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6701 - accuracy: 0.5887 - val_loss: 0.6366 - val_accuracy: 0.6492\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6656 - accuracy: 0.6002 - val_loss: 0.6324 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6665 - accuracy: 0.5973 - val_loss: 0.6330 - val_accuracy: 0.6518\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6678 - accuracy: 0.6001 - val_loss: 0.6387 - val_accuracy: 0.6492\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6687 - accuracy: 0.5921 - val_loss: 0.6358 - val_accuracy: 0.6467\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6689 - accuracy: 0.5975 - val_loss: 0.6352 - val_accuracy: 0.6505\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6664 - accuracy: 0.5953 - val_loss: 0.6337 - val_accuracy: 0.6505\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6673 - accuracy: 0.5960 - val_loss: 0.6347 - val_accuracy: 0.6518\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6671 - accuracy: 0.5965 - val_loss: 0.6326 - val_accuracy: 0.6518\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6671 - accuracy: 0.6054 - val_loss: 0.6314 - val_accuracy: 0.6531\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6672 - accuracy: 0.5934 - val_loss: 0.6329 - val_accuracy: 0.6518\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6687 - accuracy: 0.5908 - val_loss: 0.6334 - val_accuracy: 0.6543\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6694 - accuracy: 0.5982 - val_loss: 0.6333 - val_accuracy: 0.6518\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6006 - val_loss: 0.6319 - val_accuracy: 0.6556\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6668 - accuracy: 0.6040 - val_loss: 0.6307 - val_accuracy: 0.6518\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.5995 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6665 - accuracy: 0.5967 - val_loss: 0.6330 - val_accuracy: 0.6492\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6063 - val_loss: 0.6309 - val_accuracy: 0.6543\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6652 - accuracy: 0.6031 - val_loss: 0.6318 - val_accuracy: 0.6518\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6024 - val_loss: 0.6303 - val_accuracy: 0.6569\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6655 - accuracy: 0.5978 - val_loss: 0.6314 - val_accuracy: 0.6658\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6035 - val_loss: 0.6288 - val_accuracy: 0.6531\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6071 - val_loss: 0.6288 - val_accuracy: 0.6582\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6638 - accuracy: 0.6054 - val_loss: 0.6284 - val_accuracy: 0.6569\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6089 - val_loss: 0.6287 - val_accuracy: 0.6569\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6066 - val_loss: 0.6298 - val_accuracy: 0.6569\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6645 - accuracy: 0.6015 - val_loss: 0.6300 - val_accuracy: 0.6492\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6665 - accuracy: 0.6022 - val_loss: 0.6282 - val_accuracy: 0.6582\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6619 - accuracy: 0.6075 - val_loss: 0.6272 - val_accuracy: 0.6658\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.6004 - val_loss: 0.6278 - val_accuracy: 0.6658\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6632 - accuracy: 0.6052 - val_loss: 0.6256 - val_accuracy: 0.6645\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6036 - val_loss: 0.6283 - val_accuracy: 0.6607\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6618 - accuracy: 0.6104 - val_loss: 0.6285 - val_accuracy: 0.6582\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6025 - val_loss: 0.6277 - val_accuracy: 0.6594\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6629 - accuracy: 0.6069 - val_loss: 0.6285 - val_accuracy: 0.6531\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6607 - accuracy: 0.6144 - val_loss: 0.6257 - val_accuracy: 0.6735\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6085 - val_loss: 0.6238 - val_accuracy: 0.6684\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6641 - accuracy: 0.5983 - val_loss: 0.6292 - val_accuracy: 0.6735\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6597 - accuracy: 0.6108 - val_loss: 0.6248 - val_accuracy: 0.6696\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6633 - accuracy: 0.6060 - val_loss: 0.6252 - val_accuracy: 0.6722\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6636 - accuracy: 0.6095 - val_loss: 0.6249 - val_accuracy: 0.6709\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6627 - accuracy: 0.6100 - val_loss: 0.6237 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6608 - accuracy: 0.6081 - val_loss: 0.6257 - val_accuracy: 0.6696\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6605 - accuracy: 0.6107 - val_loss: 0.6220 - val_accuracy: 0.6709\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.6122 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6642 - accuracy: 0.6075 - val_loss: 0.6271 - val_accuracy: 0.6709\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6617 - accuracy: 0.6118 - val_loss: 0.6272 - val_accuracy: 0.6633\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6108 - val_loss: 0.6248 - val_accuracy: 0.6645\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6138 - val_loss: 0.6247 - val_accuracy: 0.6684\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6657 - accuracy: 0.6031 - val_loss: 0.6267 - val_accuracy: 0.6722\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6605 - accuracy: 0.6060 - val_loss: 0.6208 - val_accuracy: 0.6760\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6576 - accuracy: 0.6088 - val_loss: 0.6235 - val_accuracy: 0.6747\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6604 - accuracy: 0.6065 - val_loss: 0.6225 - val_accuracy: 0.6735\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6589 - accuracy: 0.6164 - val_loss: 0.6215 - val_accuracy: 0.6696\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6588 - accuracy: 0.6104 - val_loss: 0.6285 - val_accuracy: 0.6773\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6585 - accuracy: 0.6129 - val_loss: 0.6223 - val_accuracy: 0.6747\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6572 - accuracy: 0.6157 - val_loss: 0.6233 - val_accuracy: 0.6722\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6132 - val_loss: 0.6240 - val_accuracy: 0.6747\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6105 - val_loss: 0.6224 - val_accuracy: 0.6747\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6166 - val_loss: 0.6212 - val_accuracy: 0.6735\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6169 - val_loss: 0.6187 - val_accuracy: 0.6722\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6600 - accuracy: 0.6088 - val_loss: 0.6204 - val_accuracy: 0.6747\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6591 - accuracy: 0.6117 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6569 - accuracy: 0.6177 - val_loss: 0.6209 - val_accuracy: 0.6747\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6570 - accuracy: 0.6222 - val_loss: 0.6197 - val_accuracy: 0.6735\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6557 - accuracy: 0.6189 - val_loss: 0.6185 - val_accuracy: 0.6747\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6557 - accuracy: 0.6159 - val_loss: 0.6212 - val_accuracy: 0.6760\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6567 - accuracy: 0.6172 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6564 - accuracy: 0.6206 - val_loss: 0.6219 - val_accuracy: 0.6760\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6528 - accuracy: 0.6236 - val_loss: 0.6183 - val_accuracy: 0.6735\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6554 - accuracy: 0.6275 - val_loss: 0.6201 - val_accuracy: 0.6735\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6562 - accuracy: 0.6181 - val_loss: 0.6212 - val_accuracy: 0.6773\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6549 - accuracy: 0.6267 - val_loss: 0.6197 - val_accuracy: 0.6760\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6568 - accuracy: 0.6203 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6549 - accuracy: 0.6194 - val_loss: 0.6176 - val_accuracy: 0.6747\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6541 - accuracy: 0.6202 - val_loss: 0.6159 - val_accuracy: 0.6760\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6569 - accuracy: 0.6144 - val_loss: 0.6181 - val_accuracy: 0.6760\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6572 - accuracy: 0.6173 - val_loss: 0.6210 - val_accuracy: 0.6747\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6575 - accuracy: 0.6117 - val_loss: 0.6211 - val_accuracy: 0.6824\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6555 - accuracy: 0.6207 - val_loss: 0.6176 - val_accuracy: 0.6798\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6520 - accuracy: 0.6182 - val_loss: 0.6171 - val_accuracy: 0.6786\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6566 - accuracy: 0.6173 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6531 - accuracy: 0.6251 - val_loss: 0.6202 - val_accuracy: 0.6747\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6523 - accuracy: 0.6204 - val_loss: 0.6156 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6521 - accuracy: 0.6281 - val_loss: 0.6162 - val_accuracy: 0.6786\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6548 - accuracy: 0.6217 - val_loss: 0.6170 - val_accuracy: 0.6747\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6484 - accuracy: 0.6301 - val_loss: 0.6149 - val_accuracy: 0.6811\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6528 - accuracy: 0.6260 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 3s 13ms/step - loss: 0.6496 - accuracy: 0.6267 - val_loss: 0.6172 - val_accuracy: 0.6811\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 8s 34ms/step - loss: 0.6499 - accuracy: 0.6262 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 5s 20ms/step - loss: 0.6554 - accuracy: 0.6203 - val_loss: 0.6157 - val_accuracy: 0.6811\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6511 - accuracy: 0.6267 - val_loss: 0.6154 - val_accuracy: 0.6786\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6534 - accuracy: 0.6274 - val_loss: 0.6182 - val_accuracy: 0.6760\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6535 - accuracy: 0.6270 - val_loss: 0.6178 - val_accuracy: 0.6760\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6498 - accuracy: 0.6245 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6509 - accuracy: 0.6260 - val_loss: 0.6201 - val_accuracy: 0.6786\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6510 - accuracy: 0.6235 - val_loss: 0.6143 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6496 - accuracy: 0.6248 - val_loss: 0.6163 - val_accuracy: 0.6786\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6481 - accuracy: 0.6276 - val_loss: 0.6138 - val_accuracy: 0.6811\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6518 - accuracy: 0.6209 - val_loss: 0.6141 - val_accuracy: 0.6760\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6540 - accuracy: 0.6212 - val_loss: 0.6206 - val_accuracy: 0.6798\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6519 - accuracy: 0.6235 - val_loss: 0.6161 - val_accuracy: 0.6773\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6542 - accuracy: 0.6245 - val_loss: 0.6187 - val_accuracy: 0.6798\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6513 - accuracy: 0.6242 - val_loss: 0.6152 - val_accuracy: 0.6760\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6518 - accuracy: 0.6213 - val_loss: 0.6147 - val_accuracy: 0.6786\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6524 - accuracy: 0.6220 - val_loss: 0.6154 - val_accuracy: 0.6773\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6470 - accuracy: 0.6277 - val_loss: 0.6155 - val_accuracy: 0.6786\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6507 - accuracy: 0.6238 - val_loss: 0.6142 - val_accuracy: 0.6786\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6475 - accuracy: 0.6307 - val_loss: 0.6149 - val_accuracy: 0.6811\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6479 - accuracy: 0.6262 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6252 - val_loss: 0.6166 - val_accuracy: 0.6798\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6498 - accuracy: 0.6241 - val_loss: 0.6125 - val_accuracy: 0.6798\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6488 - accuracy: 0.6267 - val_loss: 0.6166 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.6255 - val_loss: 0.6140 - val_accuracy: 0.6811\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6262 - val_loss: 0.6171 - val_accuracy: 0.6824\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6532 - accuracy: 0.6267 - val_loss: 0.6157 - val_accuracy: 0.6773\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6497 - accuracy: 0.6256 - val_loss: 0.6162 - val_accuracy: 0.6849\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6295 - val_loss: 0.6156 - val_accuracy: 0.6849\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6167 - val_loss: 0.6143 - val_accuracy: 0.6798\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6493 - accuracy: 0.6326 - val_loss: 0.6177 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6466 - accuracy: 0.6365 - val_loss: 0.6124 - val_accuracy: 0.6824\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6483 - accuracy: 0.6364 - val_loss: 0.6141 - val_accuracy: 0.6811\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6455 - accuracy: 0.6392 - val_loss: 0.6122 - val_accuracy: 0.6837\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6295 - val_loss: 0.6151 - val_accuracy: 0.6798\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6289 - val_loss: 0.6127 - val_accuracy: 0.6773\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6478 - accuracy: 0.6325 - val_loss: 0.6133 - val_accuracy: 0.6786\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6299 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6318 - val_loss: 0.6158 - val_accuracy: 0.6798\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6474 - accuracy: 0.6374 - val_loss: 0.6116 - val_accuracy: 0.6798\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6251 - val_loss: 0.6117 - val_accuracy: 0.6824\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6467 - accuracy: 0.6380 - val_loss: 0.6120 - val_accuracy: 0.6798\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6468 - accuracy: 0.6264 - val_loss: 0.6143 - val_accuracy: 0.6824\n",
      "Calculating for: 850 100 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_168 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7770 - accuracy: 0.5499 - val_loss: 0.6650 - val_accuracy: 0.6148\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6926 - accuracy: 0.5795 - val_loss: 0.6375 - val_accuracy: 0.6607\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6776 - accuracy: 0.5927 - val_loss: 0.6403 - val_accuracy: 0.6543\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6096 - val_loss: 0.6330 - val_accuracy: 0.6735\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6603 - accuracy: 0.6100 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6154 - val_loss: 0.6319 - val_accuracy: 0.6658\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.6202 - val_loss: 0.6323 - val_accuracy: 0.6620\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6208 - val_loss: 0.6280 - val_accuracy: 0.6760\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6209 - val_loss: 0.6340 - val_accuracy: 0.6658\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6225 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6450 - accuracy: 0.6301 - val_loss: 0.6246 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6301 - val_loss: 0.6251 - val_accuracy: 0.6684\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6309 - val_loss: 0.6291 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6402 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6368 - val_loss: 0.6260 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6399 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6462 - val_loss: 0.6252 - val_accuracy: 0.6735\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6437 - val_loss: 0.6202 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6436 - val_loss: 0.6241 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6410 - val_loss: 0.6245 - val_accuracy: 0.6722\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6481 - val_loss: 0.6242 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6483 - val_loss: 0.6288 - val_accuracy: 0.6645\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6491 - val_loss: 0.6232 - val_accuracy: 0.6696\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6496 - val_loss: 0.6280 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6564 - val_loss: 0.6213 - val_accuracy: 0.6696\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6277 - accuracy: 0.6498 - val_loss: 0.6225 - val_accuracy: 0.6735\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6510 - val_loss: 0.6273 - val_accuracy: 0.6735\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6601 - val_loss: 0.6262 - val_accuracy: 0.6645\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6209 - accuracy: 0.6655 - val_loss: 0.6272 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6564 - val_loss: 0.6272 - val_accuracy: 0.6620\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6654 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6601 - val_loss: 0.6282 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6600 - val_loss: 0.6309 - val_accuracy: 0.6709\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6624 - val_loss: 0.6272 - val_accuracy: 0.6709\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6112 - accuracy: 0.6658 - val_loss: 0.6296 - val_accuracy: 0.6747\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6165 - accuracy: 0.6689 - val_loss: 0.6304 - val_accuracy: 0.6709\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6687 - val_loss: 0.6294 - val_accuracy: 0.6722\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.6679 - val_loss: 0.6351 - val_accuracy: 0.6709\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6723 - val_loss: 0.6306 - val_accuracy: 0.6709\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6049 - accuracy: 0.6772 - val_loss: 0.6303 - val_accuracy: 0.6747\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6096 - accuracy: 0.6740 - val_loss: 0.6297 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6061 - accuracy: 0.6680 - val_loss: 0.6321 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6026 - accuracy: 0.6777 - val_loss: 0.6265 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6005 - accuracy: 0.6788 - val_loss: 0.6262 - val_accuracy: 0.6773\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6014 - accuracy: 0.6763 - val_loss: 0.6341 - val_accuracy: 0.6645\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6004 - accuracy: 0.6795 - val_loss: 0.6389 - val_accuracy: 0.6607\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6737 - val_loss: 0.6348 - val_accuracy: 0.6773\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6001 - accuracy: 0.6865 - val_loss: 0.6320 - val_accuracy: 0.6709\n",
      "Calculating for: 850 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_172 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7902 - accuracy: 0.5344 - val_loss: 0.6562 - val_accuracy: 0.6467\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7106 - accuracy: 0.5484 - val_loss: 0.6501 - val_accuracy: 0.6416\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5556 - val_loss: 0.6472 - val_accuracy: 0.6556\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5757 - val_loss: 0.6449 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5770 - val_loss: 0.6465 - val_accuracy: 0.6709\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6750 - accuracy: 0.5769 - val_loss: 0.6406 - val_accuracy: 0.6671\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5838 - val_loss: 0.6395 - val_accuracy: 0.6735\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5965 - val_loss: 0.6402 - val_accuracy: 0.6760\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5912 - val_loss: 0.6364 - val_accuracy: 0.6786\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6021 - val_loss: 0.6368 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6650 - accuracy: 0.6005 - val_loss: 0.6368 - val_accuracy: 0.6735\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.5995 - val_loss: 0.6392 - val_accuracy: 0.6658\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6084 - val_loss: 0.6359 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6075 - val_loss: 0.6340 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6597 - accuracy: 0.6073 - val_loss: 0.6388 - val_accuracy: 0.6620\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6624 - accuracy: 0.6090 - val_loss: 0.6351 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6565 - accuracy: 0.6163 - val_loss: 0.6282 - val_accuracy: 0.6735\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6152 - val_loss: 0.6343 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6169 - val_loss: 0.6288 - val_accuracy: 0.6709\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6124 - val_loss: 0.6359 - val_accuracy: 0.6684\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6566 - accuracy: 0.6142 - val_loss: 0.6337 - val_accuracy: 0.6735\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6189 - val_loss: 0.6326 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6250 - val_loss: 0.6316 - val_accuracy: 0.6671\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6238 - val_loss: 0.6298 - val_accuracy: 0.6786\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6208 - val_loss: 0.6264 - val_accuracy: 0.6824\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6281 - val_loss: 0.6268 - val_accuracy: 0.6798\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6255 - val_loss: 0.6258 - val_accuracy: 0.6837\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6258 - val_loss: 0.6250 - val_accuracy: 0.6849\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6309 - val_loss: 0.6314 - val_accuracy: 0.6747\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6228 - val_loss: 0.6261 - val_accuracy: 0.6773\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6465 - accuracy: 0.6255 - val_loss: 0.6209 - val_accuracy: 0.6862\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6233 - val_loss: 0.6291 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6277 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6280 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6450 - accuracy: 0.6307 - val_loss: 0.6220 - val_accuracy: 0.6824\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6286 - val_loss: 0.6264 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6335 - val_loss: 0.6225 - val_accuracy: 0.6824\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6274 - val_loss: 0.6248 - val_accuracy: 0.6722\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6319 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6435 - accuracy: 0.6349 - val_loss: 0.6206 - val_accuracy: 0.6811\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6454 - accuracy: 0.6306 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6309 - val_loss: 0.6215 - val_accuracy: 0.6811\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6382 - val_loss: 0.6168 - val_accuracy: 0.6849\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.6346 - val_loss: 0.6208 - val_accuracy: 0.6811\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6406 - accuracy: 0.6354 - val_loss: 0.6214 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6449 - val_loss: 0.6219 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6468 - val_loss: 0.6277 - val_accuracy: 0.6722\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6420 - val_loss: 0.6198 - val_accuracy: 0.6786\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6417 - val_loss: 0.6200 - val_accuracy: 0.6735\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6402 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6380 - accuracy: 0.6408 - val_loss: 0.6185 - val_accuracy: 0.6798\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6317 - accuracy: 0.6500 - val_loss: 0.6120 - val_accuracy: 0.6901\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6436 - val_loss: 0.6135 - val_accuracy: 0.6849\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6335 - accuracy: 0.6442 - val_loss: 0.6168 - val_accuracy: 0.6837\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6495 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6341 - accuracy: 0.6497 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6466 - val_loss: 0.6201 - val_accuracy: 0.6798\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6347 - accuracy: 0.6495 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6467 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6554 - val_loss: 0.6216 - val_accuracy: 0.6811\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6459 - val_loss: 0.6204 - val_accuracy: 0.6875\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6304 - accuracy: 0.6459 - val_loss: 0.6192 - val_accuracy: 0.6888\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6473 - val_loss: 0.6159 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6315 - accuracy: 0.6559 - val_loss: 0.6211 - val_accuracy: 0.6849\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6577 - val_loss: 0.6231 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6603 - val_loss: 0.6144 - val_accuracy: 0.6913\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6493 - val_loss: 0.6186 - val_accuracy: 0.6888\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6557 - val_loss: 0.6116 - val_accuracy: 0.6926\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6511 - val_loss: 0.6119 - val_accuracy: 0.6939\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6288 - accuracy: 0.6540 - val_loss: 0.6207 - val_accuracy: 0.6862\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6505 - val_loss: 0.6181 - val_accuracy: 0.6849\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6608 - val_loss: 0.6165 - val_accuracy: 0.6849\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6276 - accuracy: 0.6555 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6234 - accuracy: 0.6556 - val_loss: 0.6191 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6245 - accuracy: 0.6579 - val_loss: 0.6145 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6626 - val_loss: 0.6158 - val_accuracy: 0.6901\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6223 - accuracy: 0.6591 - val_loss: 0.6149 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6255 - accuracy: 0.6576 - val_loss: 0.6149 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6236 - accuracy: 0.6572 - val_loss: 0.6165 - val_accuracy: 0.6849\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6616 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6246 - accuracy: 0.6601 - val_loss: 0.6215 - val_accuracy: 0.6862\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6222 - accuracy: 0.6608 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6169 - accuracy: 0.6686 - val_loss: 0.6168 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6593 - val_loss: 0.6157 - val_accuracy: 0.6811\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6181 - accuracy: 0.6663 - val_loss: 0.6133 - val_accuracy: 0.6798\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6649 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6213 - accuracy: 0.6687 - val_loss: 0.6165 - val_accuracy: 0.6773\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6673 - val_loss: 0.6196 - val_accuracy: 0.6735\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6154 - accuracy: 0.6687 - val_loss: 0.6188 - val_accuracy: 0.6786\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6161 - accuracy: 0.6643 - val_loss: 0.6180 - val_accuracy: 0.6875\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6170 - accuracy: 0.6713 - val_loss: 0.6218 - val_accuracy: 0.6760\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6643 - val_loss: 0.6220 - val_accuracy: 0.6760\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6615 - val_loss: 0.6210 - val_accuracy: 0.6760\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6572 - val_loss: 0.6163 - val_accuracy: 0.6811\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6155 - accuracy: 0.6683 - val_loss: 0.6172 - val_accuracy: 0.6811\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6628 - val_loss: 0.6208 - val_accuracy: 0.6786\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6140 - accuracy: 0.6645 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6133 - accuracy: 0.6639 - val_loss: 0.6249 - val_accuracy: 0.6747\n",
      "Calculating for: 850 100 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_176 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "227/249 [==========================>...] - ETA: 0s - loss: 0.8466 - accuracy: 0.4985WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8391 - accuracy: 0.4976 - val_loss: 0.6624 - val_accuracy: 0.6250\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7290 - accuracy: 0.5133 - val_loss: 0.6630 - val_accuracy: 0.6327\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.7061 - accuracy: 0.5177 - val_loss: 0.6621 - val_accuracy: 0.6250\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6977 - accuracy: 0.5187 - val_loss: 0.6671 - val_accuracy: 0.6263\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5306 - val_loss: 0.6660 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6937 - accuracy: 0.5256 - val_loss: 0.6760 - val_accuracy: 0.6288\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6913 - accuracy: 0.5251 - val_loss: 0.6706 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6897 - accuracy: 0.5360 - val_loss: 0.6705 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5431 - val_loss: 0.6654 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6893 - accuracy: 0.5407 - val_loss: 0.6686 - val_accuracy: 0.6250\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5285 - val_loss: 0.6636 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6877 - accuracy: 0.5430 - val_loss: 0.6658 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6914 - accuracy: 0.5369 - val_loss: 0.6663 - val_accuracy: 0.6212\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6903 - accuracy: 0.5354 - val_loss: 0.6710 - val_accuracy: 0.6301\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6883 - accuracy: 0.5430 - val_loss: 0.6630 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6902 - accuracy: 0.5340 - val_loss: 0.6679 - val_accuracy: 0.6301\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5360 - val_loss: 0.6665 - val_accuracy: 0.6378\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6887 - accuracy: 0.5357 - val_loss: 0.6642 - val_accuracy: 0.6339\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5443 - val_loss: 0.6660 - val_accuracy: 0.6390\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5421 - val_loss: 0.6608 - val_accuracy: 0.6224\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6883 - accuracy: 0.5421 - val_loss: 0.6637 - val_accuracy: 0.6250\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5472 - val_loss: 0.6644 - val_accuracy: 0.6390\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5510 - val_loss: 0.6614 - val_accuracy: 0.6339\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6856 - accuracy: 0.5517 - val_loss: 0.6615 - val_accuracy: 0.6390\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.5477 - val_loss: 0.6591 - val_accuracy: 0.6365\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6857 - accuracy: 0.5433 - val_loss: 0.6565 - val_accuracy: 0.6352\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6878 - accuracy: 0.5367 - val_loss: 0.6610 - val_accuracy: 0.6390\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6850 - accuracy: 0.5544 - val_loss: 0.6606 - val_accuracy: 0.6390\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6844 - accuracy: 0.5504 - val_loss: 0.6599 - val_accuracy: 0.6365\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6814 - accuracy: 0.5656 - val_loss: 0.6509 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6835 - accuracy: 0.5582 - val_loss: 0.6580 - val_accuracy: 0.6390\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6870 - accuracy: 0.5481 - val_loss: 0.6566 - val_accuracy: 0.6416\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6857 - accuracy: 0.5501 - val_loss: 0.6616 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5501 - val_loss: 0.6634 - val_accuracy: 0.6429\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6833 - accuracy: 0.5576 - val_loss: 0.6543 - val_accuracy: 0.6416\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5637 - val_loss: 0.6559 - val_accuracy: 0.6416\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6823 - accuracy: 0.5620 - val_loss: 0.6549 - val_accuracy: 0.6403\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6819 - accuracy: 0.5587 - val_loss: 0.6531 - val_accuracy: 0.6390\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6816 - accuracy: 0.5600 - val_loss: 0.6541 - val_accuracy: 0.6378\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6830 - accuracy: 0.5574 - val_loss: 0.6538 - val_accuracy: 0.6403\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5657 - val_loss: 0.6524 - val_accuracy: 0.6416\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6801 - accuracy: 0.5736 - val_loss: 0.6584 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5672 - val_loss: 0.6500 - val_accuracy: 0.6429\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5641 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6834 - accuracy: 0.5619 - val_loss: 0.6546 - val_accuracy: 0.6378\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5597 - val_loss: 0.6518 - val_accuracy: 0.6390\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5663 - val_loss: 0.6522 - val_accuracy: 0.6378\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6768 - accuracy: 0.5674 - val_loss: 0.6483 - val_accuracy: 0.6454\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5643 - val_loss: 0.6547 - val_accuracy: 0.6378\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6803 - accuracy: 0.5643 - val_loss: 0.6528 - val_accuracy: 0.6416\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.5705 - val_loss: 0.6467 - val_accuracy: 0.6390\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6785 - accuracy: 0.5746 - val_loss: 0.6519 - val_accuracy: 0.6416\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6782 - accuracy: 0.5705 - val_loss: 0.6485 - val_accuracy: 0.6416\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6782 - accuracy: 0.5676 - val_loss: 0.6484 - val_accuracy: 0.6416\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6778 - accuracy: 0.5703 - val_loss: 0.6500 - val_accuracy: 0.6403\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5715 - val_loss: 0.6504 - val_accuracy: 0.6416\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5754 - val_loss: 0.6441 - val_accuracy: 0.6416\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5751 - val_loss: 0.6513 - val_accuracy: 0.6429\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6731 - accuracy: 0.5847 - val_loss: 0.6459 - val_accuracy: 0.6416\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6757 - accuracy: 0.5855 - val_loss: 0.6470 - val_accuracy: 0.6416\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6768 - accuracy: 0.5781 - val_loss: 0.6456 - val_accuracy: 0.6403\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5839 - val_loss: 0.6454 - val_accuracy: 0.6505\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6716 - accuracy: 0.5898 - val_loss: 0.6407 - val_accuracy: 0.6454\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.5841 - val_loss: 0.6430 - val_accuracy: 0.6480\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6757 - accuracy: 0.5765 - val_loss: 0.6415 - val_accuracy: 0.6429\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6753 - accuracy: 0.5761 - val_loss: 0.6465 - val_accuracy: 0.6441\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5819 - val_loss: 0.6416 - val_accuracy: 0.6416\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5800 - val_loss: 0.6445 - val_accuracy: 0.6429\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5809 - val_loss: 0.6468 - val_accuracy: 0.6556\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6700 - accuracy: 0.5877 - val_loss: 0.6394 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5793 - val_loss: 0.6460 - val_accuracy: 0.6569\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5813 - val_loss: 0.6432 - val_accuracy: 0.6492\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5885 - val_loss: 0.6418 - val_accuracy: 0.6480\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5889 - val_loss: 0.6431 - val_accuracy: 0.6467\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5830 - val_loss: 0.6428 - val_accuracy: 0.6480\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5847 - val_loss: 0.6422 - val_accuracy: 0.6480\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.5841 - val_loss: 0.6462 - val_accuracy: 0.6594\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5970 - val_loss: 0.6407 - val_accuracy: 0.6518\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5877 - val_loss: 0.6377 - val_accuracy: 0.6518\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6699 - accuracy: 0.5904 - val_loss: 0.6430 - val_accuracy: 0.6671\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.5847 - val_loss: 0.6430 - val_accuracy: 0.6607\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5953 - val_loss: 0.6446 - val_accuracy: 0.6671\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6699 - accuracy: 0.5831 - val_loss: 0.6379 - val_accuracy: 0.6480\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6708 - accuracy: 0.5907 - val_loss: 0.6398 - val_accuracy: 0.6543\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5893 - val_loss: 0.6414 - val_accuracy: 0.6556\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6681 - accuracy: 0.5899 - val_loss: 0.6397 - val_accuracy: 0.6671\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5928 - val_loss: 0.6416 - val_accuracy: 0.6582\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6718 - accuracy: 0.5870 - val_loss: 0.6408 - val_accuracy: 0.6582\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5995 - val_loss: 0.6439 - val_accuracy: 0.6747\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5830 - val_loss: 0.6434 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5903 - val_loss: 0.6457 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5943 - val_loss: 0.6374 - val_accuracy: 0.6671\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5903 - val_loss: 0.6369 - val_accuracy: 0.6594\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.5961 - val_loss: 0.6359 - val_accuracy: 0.6594\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6676 - accuracy: 0.5947 - val_loss: 0.6374 - val_accuracy: 0.6696\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5917 - val_loss: 0.6389 - val_accuracy: 0.6747\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5884 - val_loss: 0.6379 - val_accuracy: 0.6696\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5955 - val_loss: 0.6368 - val_accuracy: 0.6696\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.6017 - val_loss: 0.6341 - val_accuracy: 0.6735\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.5957 - val_loss: 0.6362 - val_accuracy: 0.6696\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5997 - val_loss: 0.6356 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6630 - accuracy: 0.6015 - val_loss: 0.6339 - val_accuracy: 0.6658\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5972 - val_loss: 0.6410 - val_accuracy: 0.6722\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.6020 - val_loss: 0.6322 - val_accuracy: 0.6773\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.6041 - val_loss: 0.6386 - val_accuracy: 0.6722\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6634 - accuracy: 0.6000 - val_loss: 0.6331 - val_accuracy: 0.6747\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6652 - accuracy: 0.6068 - val_loss: 0.6313 - val_accuracy: 0.6760\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6660 - accuracy: 0.6030 - val_loss: 0.6327 - val_accuracy: 0.6773\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.5981 - val_loss: 0.6374 - val_accuracy: 0.6760\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6662 - accuracy: 0.6014 - val_loss: 0.6321 - val_accuracy: 0.6747\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6025 - val_loss: 0.6334 - val_accuracy: 0.6722\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5921 - val_loss: 0.6347 - val_accuracy: 0.6709\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6641 - accuracy: 0.6042 - val_loss: 0.6343 - val_accuracy: 0.6709\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6623 - accuracy: 0.6042 - val_loss: 0.6329 - val_accuracy: 0.6760\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6642 - accuracy: 0.5991 - val_loss: 0.6332 - val_accuracy: 0.6747\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5977 - val_loss: 0.6352 - val_accuracy: 0.6747\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6632 - accuracy: 0.6050 - val_loss: 0.6330 - val_accuracy: 0.6722\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6059 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6046 - val_loss: 0.6309 - val_accuracy: 0.6786\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6614 - accuracy: 0.6081 - val_loss: 0.6282 - val_accuracy: 0.6773\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6662 - accuracy: 0.5985 - val_loss: 0.6294 - val_accuracy: 0.6735\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6645 - accuracy: 0.6046 - val_loss: 0.6367 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6644 - accuracy: 0.6044 - val_loss: 0.6321 - val_accuracy: 0.6786\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6640 - accuracy: 0.6005 - val_loss: 0.6302 - val_accuracy: 0.6760\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6000 - val_loss: 0.6273 - val_accuracy: 0.6786\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6618 - accuracy: 0.6036 - val_loss: 0.6281 - val_accuracy: 0.6786\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6022 - val_loss: 0.6313 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6060 - val_loss: 0.6279 - val_accuracy: 0.6773\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6632 - accuracy: 0.6058 - val_loss: 0.6305 - val_accuracy: 0.6760\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6579 - accuracy: 0.6064 - val_loss: 0.6274 - val_accuracy: 0.6735\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6108 - val_loss: 0.6306 - val_accuracy: 0.6786\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6630 - accuracy: 0.6071 - val_loss: 0.6306 - val_accuracy: 0.6824\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6583 - accuracy: 0.6159 - val_loss: 0.6263 - val_accuracy: 0.6786\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6603 - accuracy: 0.6101 - val_loss: 0.6278 - val_accuracy: 0.6811\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6143 - val_loss: 0.6326 - val_accuracy: 0.6824\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.6103 - val_loss: 0.6312 - val_accuracy: 0.6811\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.6109 - val_loss: 0.6293 - val_accuracy: 0.6837\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6593 - accuracy: 0.6112 - val_loss: 0.6283 - val_accuracy: 0.6849\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6142 - val_loss: 0.6250 - val_accuracy: 0.6837\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6611 - accuracy: 0.6125 - val_loss: 0.6284 - val_accuracy: 0.6824\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.6103 - val_loss: 0.6272 - val_accuracy: 0.6773\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6123 - val_loss: 0.6246 - val_accuracy: 0.6811\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6155 - val_loss: 0.6282 - val_accuracy: 0.6811\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6143 - val_loss: 0.6261 - val_accuracy: 0.6849\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6133 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6132 - val_loss: 0.6274 - val_accuracy: 0.6824\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6084 - val_loss: 0.6294 - val_accuracy: 0.6811\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6575 - accuracy: 0.6118 - val_loss: 0.6271 - val_accuracy: 0.6837\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6252 - val_loss: 0.6254 - val_accuracy: 0.6798\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6587 - accuracy: 0.6133 - val_loss: 0.6259 - val_accuracy: 0.6862\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6158 - val_loss: 0.6268 - val_accuracy: 0.6798\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6081 - val_loss: 0.6242 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6591 - accuracy: 0.6150 - val_loss: 0.6261 - val_accuracy: 0.6837\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6127 - val_loss: 0.6283 - val_accuracy: 0.6849\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6573 - accuracy: 0.6178 - val_loss: 0.6241 - val_accuracy: 0.6811\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6209 - val_loss: 0.6264 - val_accuracy: 0.6862\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6582 - accuracy: 0.6184 - val_loss: 0.6267 - val_accuracy: 0.6798\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6167 - val_loss: 0.6244 - val_accuracy: 0.6824\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6584 - accuracy: 0.6245 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6208 - val_loss: 0.6225 - val_accuracy: 0.6824\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6191 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6216 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6569 - accuracy: 0.6178 - val_loss: 0.6214 - val_accuracy: 0.6786\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6201 - val_loss: 0.6195 - val_accuracy: 0.6862\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6153 - val_loss: 0.6272 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6246 - val_loss: 0.6232 - val_accuracy: 0.6824\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6122 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6238 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6208 - val_loss: 0.6248 - val_accuracy: 0.6837\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6206 - val_loss: 0.6241 - val_accuracy: 0.6837\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6558 - accuracy: 0.6233 - val_loss: 0.6235 - val_accuracy: 0.6837\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6578 - accuracy: 0.6172 - val_loss: 0.6280 - val_accuracy: 0.6849\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6161 - val_loss: 0.6240 - val_accuracy: 0.6811\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6177 - val_loss: 0.6230 - val_accuracy: 0.6849\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6516 - accuracy: 0.6270 - val_loss: 0.6203 - val_accuracy: 0.6875\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6257 - val_loss: 0.6255 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6534 - accuracy: 0.6207 - val_loss: 0.6243 - val_accuracy: 0.6862\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6260 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6262 - val_loss: 0.6245 - val_accuracy: 0.6786\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6556 - accuracy: 0.6230 - val_loss: 0.6255 - val_accuracy: 0.6811\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6242 - val_loss: 0.6234 - val_accuracy: 0.6837\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6265 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6275 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6237 - val_loss: 0.6244 - val_accuracy: 0.6837\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6286 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6336 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6544 - accuracy: 0.6231 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6204 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6526 - accuracy: 0.6236 - val_loss: 0.6227 - val_accuracy: 0.6837\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6289 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6533 - accuracy: 0.6179 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6211 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6329 - val_loss: 0.6205 - val_accuracy: 0.6849\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6211 - val_loss: 0.6226 - val_accuracy: 0.6849\n",
      "Calculating for: 850 250 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_180 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7977 - accuracy: 0.5529 - val_loss: 0.7656 - val_accuracy: 0.3750\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6968 - accuracy: 0.5901 - val_loss: 0.6800 - val_accuracy: 0.5804\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5987 - val_loss: 0.6595 - val_accuracy: 0.6237\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6035 - val_loss: 0.6506 - val_accuracy: 0.6301\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6140 - val_loss: 0.6521 - val_accuracy: 0.6276\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6297 - val_loss: 0.6568 - val_accuracy: 0.6173\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6167 - val_loss: 0.6469 - val_accuracy: 0.6314\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6266 - val_loss: 0.6445 - val_accuracy: 0.6327\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6247 - val_loss: 0.6429 - val_accuracy: 0.6378\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6316 - val_loss: 0.6456 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6331 - val_loss: 0.6407 - val_accuracy: 0.6390\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6410 - val_loss: 0.6446 - val_accuracy: 0.6365\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6368 - val_loss: 0.6420 - val_accuracy: 0.6403\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6419 - val_loss: 0.6438 - val_accuracy: 0.6339\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6371 - accuracy: 0.6399 - val_loss: 0.6383 - val_accuracy: 0.6454\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6405 - val_loss: 0.6372 - val_accuracy: 0.6467\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6437 - val_loss: 0.6358 - val_accuracy: 0.6467\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6505 - val_loss: 0.6322 - val_accuracy: 0.6543\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6487 - val_loss: 0.6343 - val_accuracy: 0.6582\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6463 - val_loss: 0.6392 - val_accuracy: 0.6480\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6458 - val_loss: 0.6349 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6604 - val_loss: 0.6376 - val_accuracy: 0.6594\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6525 - val_loss: 0.6421 - val_accuracy: 0.6569\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6245 - accuracy: 0.6595 - val_loss: 0.6339 - val_accuracy: 0.6620\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6236 - accuracy: 0.6560 - val_loss: 0.6369 - val_accuracy: 0.6633\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6193 - accuracy: 0.6541 - val_loss: 0.6369 - val_accuracy: 0.6658\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6662 - val_loss: 0.6358 - val_accuracy: 0.6582\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6197 - accuracy: 0.6590 - val_loss: 0.6347 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6611 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6595 - val_loss: 0.6370 - val_accuracy: 0.6582\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6682 - val_loss: 0.6405 - val_accuracy: 0.6607\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6780 - val_loss: 0.6411 - val_accuracy: 0.6543\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6062 - accuracy: 0.6724 - val_loss: 0.6469 - val_accuracy: 0.6403\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6097 - accuracy: 0.6704 - val_loss: 0.6421 - val_accuracy: 0.6569\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6082 - accuracy: 0.6736 - val_loss: 0.6355 - val_accuracy: 0.6645\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6085 - accuracy: 0.6628 - val_loss: 0.6384 - val_accuracy: 0.6556\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6087 - accuracy: 0.6716 - val_loss: 0.6396 - val_accuracy: 0.6633\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6067 - accuracy: 0.6687 - val_loss: 0.6442 - val_accuracy: 0.6454\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6051 - accuracy: 0.6745 - val_loss: 0.6400 - val_accuracy: 0.6582\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6011 - accuracy: 0.6753 - val_loss: 0.6427 - val_accuracy: 0.6607\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5981 - accuracy: 0.6829 - val_loss: 0.6342 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5979 - accuracy: 0.6807 - val_loss: 0.6469 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5963 - accuracy: 0.6848 - val_loss: 0.6422 - val_accuracy: 0.6543\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5989 - accuracy: 0.6797 - val_loss: 0.6467 - val_accuracy: 0.6556\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5984 - accuracy: 0.6848 - val_loss: 0.6489 - val_accuracy: 0.6518\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5930 - accuracy: 0.6850 - val_loss: 0.6454 - val_accuracy: 0.6569\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5920 - accuracy: 0.6897 - val_loss: 0.6549 - val_accuracy: 0.6454\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5954 - accuracy: 0.6849 - val_loss: 0.6482 - val_accuracy: 0.6467\n",
      "Calculating for: 850 250 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_184 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8119 - accuracy: 0.5398 - val_loss: 0.6475 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7113 - accuracy: 0.5630 - val_loss: 0.6427 - val_accuracy: 0.6607\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6849 - accuracy: 0.5669 - val_loss: 0.6406 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5764 - val_loss: 0.6374 - val_accuracy: 0.6556\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5843 - val_loss: 0.6392 - val_accuracy: 0.6645\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.5967 - val_loss: 0.6350 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6675 - accuracy: 0.5941 - val_loss: 0.6352 - val_accuracy: 0.6671\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.6010 - val_loss: 0.6360 - val_accuracy: 0.6671\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.5970 - val_loss: 0.6357 - val_accuracy: 0.6620\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6653 - accuracy: 0.5962 - val_loss: 0.6346 - val_accuracy: 0.6620\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6120 - val_loss: 0.6304 - val_accuracy: 0.6671\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6095 - val_loss: 0.6308 - val_accuracy: 0.6722\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6059 - val_loss: 0.6276 - val_accuracy: 0.6747\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6059 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6118 - val_loss: 0.6255 - val_accuracy: 0.6722\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6164 - val_loss: 0.6284 - val_accuracy: 0.6735\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6140 - val_loss: 0.6317 - val_accuracy: 0.6735\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6128 - val_loss: 0.6265 - val_accuracy: 0.6747\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6582 - accuracy: 0.6150 - val_loss: 0.6240 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6163 - val_loss: 0.6266 - val_accuracy: 0.6798\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6154 - val_loss: 0.6207 - val_accuracy: 0.6786\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6130 - val_loss: 0.6272 - val_accuracy: 0.6875\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6548 - accuracy: 0.6153 - val_loss: 0.6255 - val_accuracy: 0.6773\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.6202 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6176 - val_loss: 0.6281 - val_accuracy: 0.6824\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6182 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6287 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6223 - val_loss: 0.6274 - val_accuracy: 0.6633\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6281 - val_loss: 0.6243 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6494 - accuracy: 0.6246 - val_loss: 0.6219 - val_accuracy: 0.6824\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6328 - val_loss: 0.6197 - val_accuracy: 0.6798\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6290 - val_loss: 0.6253 - val_accuracy: 0.6645\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6350 - val_loss: 0.6229 - val_accuracy: 0.6747\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6356 - val_loss: 0.6195 - val_accuracy: 0.6798\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6481 - accuracy: 0.6264 - val_loss: 0.6231 - val_accuracy: 0.6786\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6297 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6321 - val_loss: 0.6219 - val_accuracy: 0.6696\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6370 - val_loss: 0.6225 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6365 - val_loss: 0.6168 - val_accuracy: 0.6722\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6326 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6407 - val_loss: 0.6224 - val_accuracy: 0.6620\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6403 - accuracy: 0.6361 - val_loss: 0.6181 - val_accuracy: 0.6735\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6390 - val_loss: 0.6227 - val_accuracy: 0.6620\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6390 - accuracy: 0.6402 - val_loss: 0.6187 - val_accuracy: 0.6760\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6355 - accuracy: 0.6447 - val_loss: 0.6172 - val_accuracy: 0.6671\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6379 - accuracy: 0.6405 - val_loss: 0.6192 - val_accuracy: 0.6709\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6413 - val_loss: 0.6149 - val_accuracy: 0.6849\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6408 - val_loss: 0.6150 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6408 - val_loss: 0.6153 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6367 - accuracy: 0.6395 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6409 - val_loss: 0.6184 - val_accuracy: 0.6811\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6415 - val_loss: 0.6156 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6366 - val_loss: 0.6144 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6492 - val_loss: 0.6171 - val_accuracy: 0.6786\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6512 - val_loss: 0.6136 - val_accuracy: 0.6798\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6488 - val_loss: 0.6139 - val_accuracy: 0.6798\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6555 - val_loss: 0.6135 - val_accuracy: 0.6786\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6483 - val_loss: 0.6169 - val_accuracy: 0.6798\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6451 - val_loss: 0.6158 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6511 - val_loss: 0.6152 - val_accuracy: 0.6798\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6427 - val_loss: 0.6169 - val_accuracy: 0.6824\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6541 - val_loss: 0.6146 - val_accuracy: 0.6811\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6520 - val_loss: 0.6170 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6500 - val_loss: 0.6171 - val_accuracy: 0.6747\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6557 - val_loss: 0.6173 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6544 - val_loss: 0.6194 - val_accuracy: 0.6747\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6487 - val_loss: 0.6154 - val_accuracy: 0.6837\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6505 - val_loss: 0.6170 - val_accuracy: 0.6798\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6464 - val_loss: 0.6122 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6554 - val_loss: 0.6144 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6250 - accuracy: 0.6594 - val_loss: 0.6182 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6251 - accuracy: 0.6560 - val_loss: 0.6135 - val_accuracy: 0.6811\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6211 - accuracy: 0.6605 - val_loss: 0.6167 - val_accuracy: 0.6773\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6562 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6521 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6193 - accuracy: 0.6629 - val_loss: 0.6125 - val_accuracy: 0.6901\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6575 - val_loss: 0.6118 - val_accuracy: 0.6875\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6659 - val_loss: 0.6132 - val_accuracy: 0.6824\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6594 - val_loss: 0.6161 - val_accuracy: 0.6773\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6668 - val_loss: 0.6124 - val_accuracy: 0.6824\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6180 - accuracy: 0.6643 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6199 - accuracy: 0.6574 - val_loss: 0.6116 - val_accuracy: 0.6837\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6229 - accuracy: 0.6638 - val_loss: 0.6109 - val_accuracy: 0.6913\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6652 - val_loss: 0.6144 - val_accuracy: 0.6837\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6644 - val_loss: 0.6118 - val_accuracy: 0.6849\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6654 - val_loss: 0.6176 - val_accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6169 - accuracy: 0.6631 - val_loss: 0.6136 - val_accuracy: 0.6849\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6166 - accuracy: 0.6647 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6153 - accuracy: 0.6659 - val_loss: 0.6125 - val_accuracy: 0.6849\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6677 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6663 - val_loss: 0.6143 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6634 - val_loss: 0.6149 - val_accuracy: 0.6849\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6683 - val_loss: 0.6144 - val_accuracy: 0.6901\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6648 - val_loss: 0.6152 - val_accuracy: 0.6849\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6703 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6158 - accuracy: 0.6654 - val_loss: 0.6187 - val_accuracy: 0.6824\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6125 - accuracy: 0.6628 - val_loss: 0.6165 - val_accuracy: 0.6824\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6135 - accuracy: 0.6682 - val_loss: 0.6185 - val_accuracy: 0.6849\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6680 - val_loss: 0.6168 - val_accuracy: 0.6811\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6688 - val_loss: 0.6179 - val_accuracy: 0.6773\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6691 - val_loss: 0.6161 - val_accuracy: 0.6862\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6130 - accuracy: 0.6718 - val_loss: 0.6173 - val_accuracy: 0.6811\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6071 - accuracy: 0.6763 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6085 - accuracy: 0.6708 - val_loss: 0.6191 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6074 - accuracy: 0.6704 - val_loss: 0.6176 - val_accuracy: 0.6773\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6040 - accuracy: 0.6807 - val_loss: 0.6157 - val_accuracy: 0.6773\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6043 - accuracy: 0.6797 - val_loss: 0.6149 - val_accuracy: 0.6760\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6060 - accuracy: 0.6752 - val_loss: 0.6211 - val_accuracy: 0.6747\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6688 - val_loss: 0.6199 - val_accuracy: 0.6709\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6704 - val_loss: 0.6194 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6064 - accuracy: 0.6756 - val_loss: 0.6162 - val_accuracy: 0.6798\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6051 - accuracy: 0.6771 - val_loss: 0.6161 - val_accuracy: 0.6811\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6031 - accuracy: 0.6737 - val_loss: 0.6149 - val_accuracy: 0.6824\n",
      "Calculating for: 850 250 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_188 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8272 - accuracy: 0.5028 - val_loss: 0.6611 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7332 - accuracy: 0.5162 - val_loss: 0.6675 - val_accuracy: 0.6327\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6992 - accuracy: 0.5324 - val_loss: 0.6636 - val_accuracy: 0.6212\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6964 - accuracy: 0.5231 - val_loss: 0.6668 - val_accuracy: 0.6224\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.5250 - val_loss: 0.6718 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5262 - val_loss: 0.6755 - val_accuracy: 0.6199\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5274 - val_loss: 0.6666 - val_accuracy: 0.6199\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6916 - accuracy: 0.5269 - val_loss: 0.6711 - val_accuracy: 0.6186\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6905 - accuracy: 0.5319 - val_loss: 0.6685 - val_accuracy: 0.6199\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.5377 - val_loss: 0.6626 - val_accuracy: 0.6186\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6887 - accuracy: 0.5423 - val_loss: 0.6619 - val_accuracy: 0.6186\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6901 - accuracy: 0.5309 - val_loss: 0.6623 - val_accuracy: 0.6199\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5355 - val_loss: 0.6647 - val_accuracy: 0.6212\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.5339 - val_loss: 0.6620 - val_accuracy: 0.6199\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6897 - accuracy: 0.5402 - val_loss: 0.6654 - val_accuracy: 0.6212\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5406 - val_loss: 0.6635 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5371 - val_loss: 0.6671 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6878 - accuracy: 0.5484 - val_loss: 0.6627 - val_accuracy: 0.6237\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6869 - accuracy: 0.5465 - val_loss: 0.6619 - val_accuracy: 0.6224\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6884 - accuracy: 0.5481 - val_loss: 0.6620 - val_accuracy: 0.6237\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5394 - val_loss: 0.6600 - val_accuracy: 0.6301\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6874 - accuracy: 0.5430 - val_loss: 0.6578 - val_accuracy: 0.6250\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.5525 - val_loss: 0.6573 - val_accuracy: 0.6352\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6881 - accuracy: 0.5448 - val_loss: 0.6610 - val_accuracy: 0.6352\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5559 - val_loss: 0.6568 - val_accuracy: 0.6339\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6842 - accuracy: 0.5618 - val_loss: 0.6562 - val_accuracy: 0.6301\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5599 - val_loss: 0.6551 - val_accuracy: 0.6352\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5533 - val_loss: 0.6584 - val_accuracy: 0.6378\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.5536 - val_loss: 0.6541 - val_accuracy: 0.6352\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6812 - accuracy: 0.5657 - val_loss: 0.6533 - val_accuracy: 0.6352\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5602 - val_loss: 0.6521 - val_accuracy: 0.6352\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6843 - accuracy: 0.5584 - val_loss: 0.6537 - val_accuracy: 0.6365\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5584 - val_loss: 0.6553 - val_accuracy: 0.6378\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.5585 - val_loss: 0.6527 - val_accuracy: 0.6365\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6811 - accuracy: 0.5686 - val_loss: 0.6523 - val_accuracy: 0.6390\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6812 - accuracy: 0.5593 - val_loss: 0.6516 - val_accuracy: 0.6390\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6808 - accuracy: 0.5614 - val_loss: 0.6514 - val_accuracy: 0.6467\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6826 - accuracy: 0.5544 - val_loss: 0.6495 - val_accuracy: 0.6467\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6777 - accuracy: 0.5686 - val_loss: 0.6475 - val_accuracy: 0.6403\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6806 - accuracy: 0.5657 - val_loss: 0.6500 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5687 - val_loss: 0.6492 - val_accuracy: 0.6480\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6768 - accuracy: 0.5737 - val_loss: 0.6474 - val_accuracy: 0.6467\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5721 - val_loss: 0.6478 - val_accuracy: 0.6441\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6795 - accuracy: 0.5708 - val_loss: 0.6497 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6781 - accuracy: 0.5749 - val_loss: 0.6481 - val_accuracy: 0.6429\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6769 - accuracy: 0.5785 - val_loss: 0.6469 - val_accuracy: 0.6441\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6769 - accuracy: 0.5799 - val_loss: 0.6459 - val_accuracy: 0.6441\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5770 - val_loss: 0.6461 - val_accuracy: 0.6429\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5749 - val_loss: 0.6470 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6760 - accuracy: 0.5804 - val_loss: 0.6447 - val_accuracy: 0.6416\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6765 - accuracy: 0.5782 - val_loss: 0.6455 - val_accuracy: 0.6403\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5838 - val_loss: 0.6423 - val_accuracy: 0.6416\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5784 - val_loss: 0.6435 - val_accuracy: 0.6429\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5894 - val_loss: 0.6410 - val_accuracy: 0.6441\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5826 - val_loss: 0.6425 - val_accuracy: 0.6429\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6730 - accuracy: 0.5898 - val_loss: 0.6428 - val_accuracy: 0.6467\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5862 - val_loss: 0.6435 - val_accuracy: 0.6467\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6723 - accuracy: 0.5839 - val_loss: 0.6436 - val_accuracy: 0.6467\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5818 - val_loss: 0.6428 - val_accuracy: 0.6480\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5862 - val_loss: 0.6424 - val_accuracy: 0.6416\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5902 - val_loss: 0.6420 - val_accuracy: 0.6403\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.5823 - val_loss: 0.6441 - val_accuracy: 0.6403\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5874 - val_loss: 0.6421 - val_accuracy: 0.6403\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5869 - val_loss: 0.6405 - val_accuracy: 0.6480\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5916 - val_loss: 0.6392 - val_accuracy: 0.6505\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5931 - val_loss: 0.6389 - val_accuracy: 0.6492\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.5919 - val_loss: 0.6394 - val_accuracy: 0.6505\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5898 - val_loss: 0.6398 - val_accuracy: 0.6480\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5936 - val_loss: 0.6400 - val_accuracy: 0.6505\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5906 - val_loss: 0.6403 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5922 - val_loss: 0.6387 - val_accuracy: 0.6492\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6672 - accuracy: 0.5968 - val_loss: 0.6377 - val_accuracy: 0.6531\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5981 - val_loss: 0.6397 - val_accuracy: 0.6492\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5932 - val_loss: 0.6405 - val_accuracy: 0.6518\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5897 - val_loss: 0.6413 - val_accuracy: 0.6518\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5972 - val_loss: 0.6373 - val_accuracy: 0.6505\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5963 - val_loss: 0.6391 - val_accuracy: 0.6543\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.6002 - val_loss: 0.6353 - val_accuracy: 0.6518\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5951 - val_loss: 0.6383 - val_accuracy: 0.6492\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.6027 - val_loss: 0.6378 - val_accuracy: 0.6505\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5934 - val_loss: 0.6381 - val_accuracy: 0.6543\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6680 - accuracy: 0.5981 - val_loss: 0.6359 - val_accuracy: 0.6505\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6650 - accuracy: 0.5986 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6020 - val_loss: 0.6335 - val_accuracy: 0.6594\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5933 - val_loss: 0.6345 - val_accuracy: 0.6594\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.6004 - val_loss: 0.6386 - val_accuracy: 0.6633\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5998 - val_loss: 0.6350 - val_accuracy: 0.6607\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.5980 - val_loss: 0.6338 - val_accuracy: 0.6633\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6656 - accuracy: 0.6004 - val_loss: 0.6350 - val_accuracy: 0.6696\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5982 - val_loss: 0.6344 - val_accuracy: 0.6582\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.5858 - val_loss: 0.6338 - val_accuracy: 0.6645\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6012 - val_loss: 0.6338 - val_accuracy: 0.6645\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.5995 - val_loss: 0.6346 - val_accuracy: 0.6645\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.5991 - val_loss: 0.6317 - val_accuracy: 0.6722\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.5992 - val_loss: 0.6335 - val_accuracy: 0.6607\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.6042 - val_loss: 0.6331 - val_accuracy: 0.6722\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6030 - val_loss: 0.6360 - val_accuracy: 0.6709\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5963 - val_loss: 0.6310 - val_accuracy: 0.6696\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6037 - val_loss: 0.6297 - val_accuracy: 0.6709\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6040 - val_loss: 0.6306 - val_accuracy: 0.6735\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6056 - val_loss: 0.6270 - val_accuracy: 0.6633\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6034 - val_loss: 0.6293 - val_accuracy: 0.6735\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6041 - val_loss: 0.6290 - val_accuracy: 0.6722\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6093 - val_loss: 0.6288 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6127 - val_loss: 0.6280 - val_accuracy: 0.6760\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6046 - val_loss: 0.6313 - val_accuracy: 0.6709\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6075 - val_loss: 0.6292 - val_accuracy: 0.6760\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6010 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6107 - val_loss: 0.6294 - val_accuracy: 0.6671\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6094 - val_loss: 0.6311 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6035 - val_loss: 0.6289 - val_accuracy: 0.6735\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6110 - val_loss: 0.6312 - val_accuracy: 0.6760\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6049 - val_loss: 0.6298 - val_accuracy: 0.6760\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6168 - val_loss: 0.6253 - val_accuracy: 0.6760\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6120 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6119 - val_loss: 0.6289 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6081 - val_loss: 0.6248 - val_accuracy: 0.6747\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6014 - val_loss: 0.6259 - val_accuracy: 0.6747\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6133 - val_loss: 0.6260 - val_accuracy: 0.6735\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6127 - val_loss: 0.6281 - val_accuracy: 0.6798\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6155 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6059 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6083 - val_loss: 0.6259 - val_accuracy: 0.6760\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6089 - val_loss: 0.6253 - val_accuracy: 0.6722\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6088 - val_loss: 0.6310 - val_accuracy: 0.6811\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6058 - val_loss: 0.6258 - val_accuracy: 0.6786\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6085 - val_loss: 0.6273 - val_accuracy: 0.6824\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6078 - val_loss: 0.6299 - val_accuracy: 0.6837\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6158 - val_loss: 0.6221 - val_accuracy: 0.6747\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6197 - val_loss: 0.6233 - val_accuracy: 0.6798\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6173 - val_loss: 0.6257 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6101 - val_loss: 0.6241 - val_accuracy: 0.6798\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6125 - val_loss: 0.6261 - val_accuracy: 0.6824\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6128 - val_loss: 0.6283 - val_accuracy: 0.6798\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6196 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6130 - val_loss: 0.6239 - val_accuracy: 0.6760\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6163 - val_loss: 0.6237 - val_accuracy: 0.6837\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6178 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6193 - val_loss: 0.6225 - val_accuracy: 0.6760\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6167 - val_loss: 0.6262 - val_accuracy: 0.6811\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6171 - val_loss: 0.6237 - val_accuracy: 0.6811\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6154 - val_loss: 0.6223 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6217 - val_loss: 0.6246 - val_accuracy: 0.6798\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6144 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6245 - val_loss: 0.6200 - val_accuracy: 0.6773\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6145 - val_loss: 0.6223 - val_accuracy: 0.6786\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6118 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6162 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6178 - val_loss: 0.6169 - val_accuracy: 0.6824\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6186 - val_loss: 0.6190 - val_accuracy: 0.6837\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6222 - val_loss: 0.6251 - val_accuracy: 0.6773\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6563 - accuracy: 0.6155 - val_loss: 0.6213 - val_accuracy: 0.6798\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6531 - accuracy: 0.6186 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6213 - val_loss: 0.6193 - val_accuracy: 0.6811\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6250 - val_loss: 0.6212 - val_accuracy: 0.6786\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6140 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6174 - val_loss: 0.6218 - val_accuracy: 0.6811\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6198 - val_accuracy: 0.6837\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6187 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6196 - val_loss: 0.6205 - val_accuracy: 0.6824\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6267 - val_loss: 0.6203 - val_accuracy: 0.6786\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6192 - val_loss: 0.6187 - val_accuracy: 0.6824\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6150 - val_loss: 0.6214 - val_accuracy: 0.6786\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6264 - val_loss: 0.6178 - val_accuracy: 0.6811\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.6201 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6236 - val_loss: 0.6194 - val_accuracy: 0.6798\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6521 - accuracy: 0.6212 - val_loss: 0.6191 - val_accuracy: 0.6824\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6233 - val_loss: 0.6198 - val_accuracy: 0.6798\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6198 - val_loss: 0.6199 - val_accuracy: 0.6849\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6243 - val_loss: 0.6182 - val_accuracy: 0.6760\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6230 - val_loss: 0.6217 - val_accuracy: 0.6760\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6208 - val_loss: 0.6175 - val_accuracy: 0.6747\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6204 - val_loss: 0.6191 - val_accuracy: 0.6760\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6140 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6275 - val_loss: 0.6186 - val_accuracy: 0.6849\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6222 - val_loss: 0.6191 - val_accuracy: 0.6773\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6230 - val_loss: 0.6185 - val_accuracy: 0.6773\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6202 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6225 - val_loss: 0.6194 - val_accuracy: 0.6824\n",
      "Calculating for: 850 400 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_192 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7962 - accuracy: 0.5462 - val_loss: 0.6430 - val_accuracy: 0.6556\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6953 - accuracy: 0.5883 - val_loss: 0.6276 - val_accuracy: 0.6773\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6698 - accuracy: 0.5990 - val_loss: 0.6278 - val_accuracy: 0.6837\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6566 - accuracy: 0.6140 - val_loss: 0.6249 - val_accuracy: 0.6875\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6559 - accuracy: 0.6177 - val_loss: 0.6239 - val_accuracy: 0.6824\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6477 - accuracy: 0.6306 - val_loss: 0.6205 - val_accuracy: 0.6786\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6211 - val_loss: 0.6250 - val_accuracy: 0.6875\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6458 - accuracy: 0.6253 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6460 - accuracy: 0.6296 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6432 - accuracy: 0.6314 - val_loss: 0.6159 - val_accuracy: 0.6913\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6434 - accuracy: 0.6334 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6428 - accuracy: 0.6374 - val_loss: 0.6208 - val_accuracy: 0.6888\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6356 - val_loss: 0.6154 - val_accuracy: 0.6862\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6358 - accuracy: 0.6451 - val_loss: 0.6184 - val_accuracy: 0.6862\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6339 - accuracy: 0.6468 - val_loss: 0.6205 - val_accuracy: 0.6926\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.6472 - val_loss: 0.6154 - val_accuracy: 0.6875\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6319 - accuracy: 0.6441 - val_loss: 0.6162 - val_accuracy: 0.6875\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6311 - accuracy: 0.6443 - val_loss: 0.6149 - val_accuracy: 0.6811\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6306 - accuracy: 0.6506 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6269 - accuracy: 0.6565 - val_loss: 0.6167 - val_accuracy: 0.6798\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6291 - accuracy: 0.6475 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6286 - accuracy: 0.6539 - val_loss: 0.6160 - val_accuracy: 0.6849\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6272 - accuracy: 0.6525 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6203 - accuracy: 0.6623 - val_loss: 0.6162 - val_accuracy: 0.6773\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6235 - accuracy: 0.6531 - val_loss: 0.6137 - val_accuracy: 0.6862\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6191 - accuracy: 0.6536 - val_loss: 0.6154 - val_accuracy: 0.6849\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6205 - accuracy: 0.6614 - val_loss: 0.6201 - val_accuracy: 0.6786\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6143 - accuracy: 0.6716 - val_loss: 0.6128 - val_accuracy: 0.6849\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6143 - accuracy: 0.6664 - val_loss: 0.6172 - val_accuracy: 0.6875\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6112 - accuracy: 0.6706 - val_loss: 0.6180 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6137 - accuracy: 0.6637 - val_loss: 0.6134 - val_accuracy: 0.6862\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6047 - accuracy: 0.6746 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6134 - accuracy: 0.6697 - val_loss: 0.6143 - val_accuracy: 0.6837\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6113 - accuracy: 0.6721 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6104 - accuracy: 0.6718 - val_loss: 0.6214 - val_accuracy: 0.6811\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6030 - accuracy: 0.6709 - val_loss: 0.6195 - val_accuracy: 0.6849\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6041 - accuracy: 0.6757 - val_loss: 0.6238 - val_accuracy: 0.6773\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6048 - accuracy: 0.6755 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6043 - accuracy: 0.6737 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6023 - accuracy: 0.6777 - val_loss: 0.6244 - val_accuracy: 0.6773\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6002 - accuracy: 0.6817 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6019 - accuracy: 0.6801 - val_loss: 0.6318 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6004 - accuracy: 0.6799 - val_loss: 0.6283 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5966 - accuracy: 0.6820 - val_loss: 0.6235 - val_accuracy: 0.6811\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6004 - accuracy: 0.6801 - val_loss: 0.6246 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5962 - accuracy: 0.6767 - val_loss: 0.6284 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5919 - accuracy: 0.6873 - val_loss: 0.6281 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5927 - accuracy: 0.6881 - val_loss: 0.6267 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5914 - accuracy: 0.6938 - val_loss: 0.6274 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5949 - accuracy: 0.6849 - val_loss: 0.6320 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5921 - accuracy: 0.6791 - val_loss: 0.6273 - val_accuracy: 0.6735\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5821 - accuracy: 0.6922 - val_loss: 0.6289 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5891 - accuracy: 0.6894 - val_loss: 0.6279 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5834 - accuracy: 0.6976 - val_loss: 0.6359 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5866 - accuracy: 0.6919 - val_loss: 0.6287 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5847 - accuracy: 0.6937 - val_loss: 0.6304 - val_accuracy: 0.6760\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5837 - accuracy: 0.6930 - val_loss: 0.6300 - val_accuracy: 0.6709\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.5814 - accuracy: 0.6996 - val_loss: 0.6327 - val_accuracy: 0.6760\n",
      "Calculating for: 850 400 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_196 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.8353 - accuracy: 0.5412 - val_loss: 0.6612 - val_accuracy: 0.6684\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.7284 - accuracy: 0.5497 - val_loss: 0.6513 - val_accuracy: 0.6696\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6854 - accuracy: 0.5764 - val_loss: 0.6548 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6804 - accuracy: 0.5777 - val_loss: 0.6493 - val_accuracy: 0.6722\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6723 - accuracy: 0.5873 - val_loss: 0.6527 - val_accuracy: 0.6543\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6679 - accuracy: 0.5987 - val_loss: 0.6496 - val_accuracy: 0.6569\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6672 - accuracy: 0.5936 - val_loss: 0.6474 - val_accuracy: 0.6671\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6678 - accuracy: 0.5952 - val_loss: 0.6414 - val_accuracy: 0.6671\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6663 - accuracy: 0.5971 - val_loss: 0.6386 - val_accuracy: 0.6696\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6624 - accuracy: 0.6054 - val_loss: 0.6420 - val_accuracy: 0.6722\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6635 - accuracy: 0.6058 - val_loss: 0.6439 - val_accuracy: 0.6454\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6583 - accuracy: 0.6112 - val_loss: 0.6366 - val_accuracy: 0.6620\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6572 - accuracy: 0.6119 - val_loss: 0.6358 - val_accuracy: 0.6735\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6578 - accuracy: 0.6171 - val_loss: 0.6392 - val_accuracy: 0.6518\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6529 - accuracy: 0.6242 - val_loss: 0.6354 - val_accuracy: 0.6607\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6558 - accuracy: 0.6115 - val_loss: 0.6302 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6540 - accuracy: 0.6179 - val_loss: 0.6311 - val_accuracy: 0.6658\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6553 - accuracy: 0.6118 - val_loss: 0.6380 - val_accuracy: 0.6543\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6519 - accuracy: 0.6222 - val_loss: 0.6307 - val_accuracy: 0.6633\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6533 - accuracy: 0.6174 - val_loss: 0.6323 - val_accuracy: 0.6607\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6517 - accuracy: 0.6188 - val_loss: 0.6275 - val_accuracy: 0.6582\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6542 - accuracy: 0.6223 - val_loss: 0.6319 - val_accuracy: 0.6671\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6524 - accuracy: 0.6207 - val_loss: 0.6326 - val_accuracy: 0.6633\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6496 - accuracy: 0.6237 - val_loss: 0.6343 - val_accuracy: 0.6582\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6483 - accuracy: 0.6282 - val_loss: 0.6299 - val_accuracy: 0.6607\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6460 - accuracy: 0.6319 - val_loss: 0.6274 - val_accuracy: 0.6633\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6483 - accuracy: 0.6306 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6493 - accuracy: 0.6279 - val_loss: 0.6293 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6481 - accuracy: 0.6305 - val_loss: 0.6249 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6445 - accuracy: 0.6296 - val_loss: 0.6247 - val_accuracy: 0.6645\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6459 - accuracy: 0.6341 - val_loss: 0.6225 - val_accuracy: 0.6773\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6461 - accuracy: 0.6309 - val_loss: 0.6272 - val_accuracy: 0.6607\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6448 - accuracy: 0.6341 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6434 - accuracy: 0.6343 - val_loss: 0.6233 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6406 - accuracy: 0.6374 - val_loss: 0.6212 - val_accuracy: 0.6722\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6375 - accuracy: 0.6414 - val_loss: 0.6182 - val_accuracy: 0.6798\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6446 - accuracy: 0.6304 - val_loss: 0.6218 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6402 - accuracy: 0.6372 - val_loss: 0.6230 - val_accuracy: 0.6722\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6392 - accuracy: 0.6355 - val_loss: 0.6238 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6402 - accuracy: 0.6358 - val_loss: 0.6237 - val_accuracy: 0.6671\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6375 - accuracy: 0.6370 - val_loss: 0.6261 - val_accuracy: 0.6543\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6406 - accuracy: 0.6385 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6380 - accuracy: 0.6402 - val_loss: 0.6206 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6348 - accuracy: 0.6476 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6342 - accuracy: 0.6417 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6355 - accuracy: 0.6458 - val_loss: 0.6201 - val_accuracy: 0.6760\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6375 - accuracy: 0.6380 - val_loss: 0.6226 - val_accuracy: 0.6773\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6330 - accuracy: 0.6444 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6348 - accuracy: 0.6467 - val_loss: 0.6289 - val_accuracy: 0.6569\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6350 - accuracy: 0.6402 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6314 - accuracy: 0.6516 - val_loss: 0.6197 - val_accuracy: 0.6773\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6307 - accuracy: 0.6415 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6312 - accuracy: 0.6503 - val_loss: 0.6283 - val_accuracy: 0.6620\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6300 - accuracy: 0.6478 - val_loss: 0.6174 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6293 - accuracy: 0.6520 - val_loss: 0.6241 - val_accuracy: 0.6735\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6300 - accuracy: 0.6481 - val_loss: 0.6253 - val_accuracy: 0.6722\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6249 - accuracy: 0.6555 - val_loss: 0.6200 - val_accuracy: 0.6760\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6310 - accuracy: 0.6490 - val_loss: 0.6212 - val_accuracy: 0.6747\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6260 - accuracy: 0.6496 - val_loss: 0.6235 - val_accuracy: 0.6798\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6276 - accuracy: 0.6532 - val_loss: 0.6196 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6278 - accuracy: 0.6518 - val_loss: 0.6187 - val_accuracy: 0.6773\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6249 - accuracy: 0.6487 - val_loss: 0.6194 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6266 - accuracy: 0.6506 - val_loss: 0.6184 - val_accuracy: 0.6773\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6252 - accuracy: 0.6575 - val_loss: 0.6181 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6220 - accuracy: 0.6545 - val_loss: 0.6204 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6253 - accuracy: 0.6583 - val_loss: 0.6154 - val_accuracy: 0.6798\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6262 - accuracy: 0.6539 - val_loss: 0.6155 - val_accuracy: 0.6773\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6211 - accuracy: 0.6640 - val_loss: 0.6166 - val_accuracy: 0.6773\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6213 - accuracy: 0.6603 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6224 - accuracy: 0.6581 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6237 - accuracy: 0.6577 - val_loss: 0.6196 - val_accuracy: 0.6786\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6190 - accuracy: 0.6600 - val_loss: 0.6196 - val_accuracy: 0.6773\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6193 - accuracy: 0.6605 - val_loss: 0.6213 - val_accuracy: 0.6747\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6185 - accuracy: 0.6579 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6178 - accuracy: 0.6654 - val_loss: 0.6225 - val_accuracy: 0.6786\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6176 - accuracy: 0.6603 - val_loss: 0.6250 - val_accuracy: 0.6786\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6157 - accuracy: 0.6626 - val_loss: 0.6203 - val_accuracy: 0.6773\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6629 - val_loss: 0.6244 - val_accuracy: 0.6773\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6190 - accuracy: 0.6652 - val_loss: 0.6207 - val_accuracy: 0.6773\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6118 - accuracy: 0.6683 - val_loss: 0.6206 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6174 - accuracy: 0.6640 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6113 - accuracy: 0.6727 - val_loss: 0.6202 - val_accuracy: 0.6747\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6135 - accuracy: 0.6701 - val_loss: 0.6221 - val_accuracy: 0.6773\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6128 - accuracy: 0.6693 - val_loss: 0.6220 - val_accuracy: 0.6798\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6137 - accuracy: 0.6682 - val_loss: 0.6202 - val_accuracy: 0.6786\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6149 - accuracy: 0.6642 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6123 - accuracy: 0.6663 - val_loss: 0.6242 - val_accuracy: 0.6684\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 2s 9ms/step - loss: 0.6141 - accuracy: 0.6659 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.6099 - accuracy: 0.6719 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6102 - accuracy: 0.6709 - val_loss: 0.6207 - val_accuracy: 0.6747\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6068 - accuracy: 0.6741 - val_loss: 0.6226 - val_accuracy: 0.6760\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6121 - accuracy: 0.6673 - val_loss: 0.6236 - val_accuracy: 0.6786\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6047 - accuracy: 0.6734 - val_loss: 0.6223 - val_accuracy: 0.6798\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6105 - accuracy: 0.6659 - val_loss: 0.6221 - val_accuracy: 0.6709\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6054 - accuracy: 0.6775 - val_loss: 0.6231 - val_accuracy: 0.6684\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6051 - accuracy: 0.6717 - val_loss: 0.6236 - val_accuracy: 0.6735\n",
      "Calculating for: 850 400 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_200 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.8557 - accuracy: 0.5087 - val_loss: 0.6652 - val_accuracy: 0.6186\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.7391 - accuracy: 0.5270 - val_loss: 0.6619 - val_accuracy: 0.6224\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.7060 - accuracy: 0.5310 - val_loss: 0.6615 - val_accuracy: 0.6224\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6948 - accuracy: 0.5299 - val_loss: 0.6619 - val_accuracy: 0.6224\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6905 - accuracy: 0.5388 - val_loss: 0.6666 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6903 - accuracy: 0.5343 - val_loss: 0.6663 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6904 - accuracy: 0.5329 - val_loss: 0.6645 - val_accuracy: 0.6224\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6905 - accuracy: 0.5285 - val_loss: 0.6667 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6889 - accuracy: 0.5458 - val_loss: 0.6634 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6887 - accuracy: 0.5430 - val_loss: 0.6614 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6899 - accuracy: 0.5430 - val_loss: 0.6642 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6878 - accuracy: 0.5416 - val_loss: 0.6633 - val_accuracy: 0.6224\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6863 - accuracy: 0.5447 - val_loss: 0.6605 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6870 - accuracy: 0.5496 - val_loss: 0.6616 - val_accuracy: 0.6224\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6872 - accuracy: 0.5482 - val_loss: 0.6621 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6869 - accuracy: 0.5521 - val_loss: 0.6586 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6848 - accuracy: 0.5553 - val_loss: 0.6589 - val_accuracy: 0.6237\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6851 - accuracy: 0.5569 - val_loss: 0.6590 - val_accuracy: 0.6250\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6857 - accuracy: 0.5536 - val_loss: 0.6593 - val_accuracy: 0.6301\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6847 - accuracy: 0.5541 - val_loss: 0.6603 - val_accuracy: 0.6339\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6836 - accuracy: 0.5579 - val_loss: 0.6558 - val_accuracy: 0.6339\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6831 - accuracy: 0.5604 - val_loss: 0.6566 - val_accuracy: 0.6365\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6825 - accuracy: 0.5615 - val_loss: 0.6555 - val_accuracy: 0.6365\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6800 - accuracy: 0.5664 - val_loss: 0.6531 - val_accuracy: 0.6352\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6805 - accuracy: 0.5668 - val_loss: 0.6523 - val_accuracy: 0.6352\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6820 - accuracy: 0.5658 - val_loss: 0.6526 - val_accuracy: 0.6352\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6808 - accuracy: 0.5649 - val_loss: 0.6518 - val_accuracy: 0.6352\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6829 - accuracy: 0.5638 - val_loss: 0.6538 - val_accuracy: 0.6429\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6800 - accuracy: 0.5723 - val_loss: 0.6518 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6820 - accuracy: 0.5674 - val_loss: 0.6532 - val_accuracy: 0.6416\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6805 - accuracy: 0.5728 - val_loss: 0.6511 - val_accuracy: 0.6429\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6804 - accuracy: 0.5646 - val_loss: 0.6522 - val_accuracy: 0.6429\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6786 - accuracy: 0.5749 - val_loss: 0.6495 - val_accuracy: 0.6416\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6793 - accuracy: 0.5744 - val_loss: 0.6504 - val_accuracy: 0.6429\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6813 - accuracy: 0.5668 - val_loss: 0.6516 - val_accuracy: 0.6429\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6787 - accuracy: 0.5685 - val_loss: 0.6501 - val_accuracy: 0.6416\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6762 - accuracy: 0.5767 - val_loss: 0.6501 - val_accuracy: 0.6403\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6775 - accuracy: 0.5745 - val_loss: 0.6487 - val_accuracy: 0.6429\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6774 - accuracy: 0.5757 - val_loss: 0.6474 - val_accuracy: 0.6429\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6765 - accuracy: 0.5762 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6756 - accuracy: 0.5794 - val_loss: 0.6466 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6779 - accuracy: 0.5710 - val_loss: 0.6466 - val_accuracy: 0.6429\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6758 - accuracy: 0.5772 - val_loss: 0.6452 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6769 - accuracy: 0.5775 - val_loss: 0.6467 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6732 - accuracy: 0.5852 - val_loss: 0.6463 - val_accuracy: 0.6416\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6734 - accuracy: 0.5785 - val_loss: 0.6456 - val_accuracy: 0.6429\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6761 - accuracy: 0.5804 - val_loss: 0.6446 - val_accuracy: 0.6429\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6723 - accuracy: 0.5806 - val_loss: 0.6431 - val_accuracy: 0.6441\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6722 - accuracy: 0.5844 - val_loss: 0.6441 - val_accuracy: 0.6441\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6712 - accuracy: 0.5903 - val_loss: 0.6442 - val_accuracy: 0.6429\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6756 - accuracy: 0.5823 - val_loss: 0.6450 - val_accuracy: 0.6467\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6726 - accuracy: 0.5834 - val_loss: 0.6439 - val_accuracy: 0.6454\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6717 - accuracy: 0.5875 - val_loss: 0.6443 - val_accuracy: 0.6429\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6724 - accuracy: 0.5860 - val_loss: 0.6445 - val_accuracy: 0.6492\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6691 - accuracy: 0.5877 - val_loss: 0.6425 - val_accuracy: 0.6518\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6681 - accuracy: 0.5904 - val_loss: 0.6390 - val_accuracy: 0.6467\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6706 - accuracy: 0.5875 - val_loss: 0.6414 - val_accuracy: 0.6492\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6704 - accuracy: 0.5870 - val_loss: 0.6417 - val_accuracy: 0.6531\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6708 - accuracy: 0.5908 - val_loss: 0.6405 - val_accuracy: 0.6492\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6737 - accuracy: 0.5855 - val_loss: 0.6424 - val_accuracy: 0.6480\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6695 - accuracy: 0.5879 - val_loss: 0.6411 - val_accuracy: 0.6505\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6726 - accuracy: 0.5884 - val_loss: 0.6422 - val_accuracy: 0.6480\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6699 - accuracy: 0.5904 - val_loss: 0.6395 - val_accuracy: 0.6480\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6668 - accuracy: 0.5941 - val_loss: 0.6405 - val_accuracy: 0.6518\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6700 - accuracy: 0.5942 - val_loss: 0.6405 - val_accuracy: 0.6505\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6691 - accuracy: 0.5950 - val_loss: 0.6408 - val_accuracy: 0.6531\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6663 - accuracy: 0.5971 - val_loss: 0.6386 - val_accuracy: 0.6543\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6687 - accuracy: 0.5975 - val_loss: 0.6416 - val_accuracy: 0.6492\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6660 - accuracy: 0.6002 - val_loss: 0.6390 - val_accuracy: 0.6492\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5975 - val_loss: 0.6395 - val_accuracy: 0.6505\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6683 - accuracy: 0.5970 - val_loss: 0.6386 - val_accuracy: 0.6518\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6669 - accuracy: 0.5995 - val_loss: 0.6379 - val_accuracy: 0.6518\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6644 - accuracy: 0.6046 - val_loss: 0.6370 - val_accuracy: 0.6531\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6684 - accuracy: 0.5938 - val_loss: 0.6370 - val_accuracy: 0.6531\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6685 - accuracy: 0.6004 - val_loss: 0.6358 - val_accuracy: 0.6531\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6650 - accuracy: 0.6064 - val_loss: 0.6342 - val_accuracy: 0.6556\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6669 - accuracy: 0.5976 - val_loss: 0.6354 - val_accuracy: 0.6556\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6661 - accuracy: 0.5951 - val_loss: 0.6352 - val_accuracy: 0.6543\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6663 - accuracy: 0.5956 - val_loss: 0.6344 - val_accuracy: 0.6505\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6633 - accuracy: 0.6022 - val_loss: 0.6338 - val_accuracy: 0.6607\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6677 - accuracy: 0.6004 - val_loss: 0.6321 - val_accuracy: 0.6633\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6622 - accuracy: 0.6074 - val_loss: 0.6320 - val_accuracy: 0.6620\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6662 - accuracy: 0.5941 - val_loss: 0.6335 - val_accuracy: 0.6582\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6646 - accuracy: 0.6064 - val_loss: 0.6332 - val_accuracy: 0.6543\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6620 - accuracy: 0.6093 - val_loss: 0.6317 - val_accuracy: 0.6543\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6675 - accuracy: 0.5961 - val_loss: 0.6374 - val_accuracy: 0.6633\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6647 - accuracy: 0.6068 - val_loss: 0.6317 - val_accuracy: 0.6505\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6619 - accuracy: 0.6110 - val_loss: 0.6318 - val_accuracy: 0.6620\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6615 - accuracy: 0.6069 - val_loss: 0.6316 - val_accuracy: 0.6620\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6643 - accuracy: 0.6046 - val_loss: 0.6328 - val_accuracy: 0.6607\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6614 - accuracy: 0.6031 - val_loss: 0.6300 - val_accuracy: 0.6556\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6647 - accuracy: 0.6011 - val_loss: 0.6325 - val_accuracy: 0.6633\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.6118 - val_loss: 0.6294 - val_accuracy: 0.6722\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6636 - accuracy: 0.6045 - val_loss: 0.6299 - val_accuracy: 0.6620\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6641 - accuracy: 0.6084 - val_loss: 0.6291 - val_accuracy: 0.6696\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6629 - accuracy: 0.6009 - val_loss: 0.6304 - val_accuracy: 0.6696\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6627 - accuracy: 0.6109 - val_loss: 0.6299 - val_accuracy: 0.6722\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6596 - accuracy: 0.6133 - val_loss: 0.6290 - val_accuracy: 0.6709\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6610 - accuracy: 0.6103 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6581 - accuracy: 0.6155 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6564 - accuracy: 0.6101 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6612 - accuracy: 0.6049 - val_loss: 0.6295 - val_accuracy: 0.6709\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6631 - accuracy: 0.6081 - val_loss: 0.6303 - val_accuracy: 0.6735\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6554 - accuracy: 0.6178 - val_loss: 0.6254 - val_accuracy: 0.6684\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6577 - accuracy: 0.6178 - val_loss: 0.6258 - val_accuracy: 0.6696\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6582 - accuracy: 0.6133 - val_loss: 0.6271 - val_accuracy: 0.6709\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6624 - accuracy: 0.6109 - val_loss: 0.6268 - val_accuracy: 0.6722\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6592 - accuracy: 0.6142 - val_loss: 0.6288 - val_accuracy: 0.6709\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6581 - accuracy: 0.6150 - val_loss: 0.6257 - val_accuracy: 0.6684\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6598 - accuracy: 0.6169 - val_loss: 0.6264 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6579 - accuracy: 0.6153 - val_loss: 0.6270 - val_accuracy: 0.6722\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6593 - accuracy: 0.6124 - val_loss: 0.6300 - val_accuracy: 0.6709\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6596 - accuracy: 0.6093 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.6119 - val_loss: 0.6233 - val_accuracy: 0.6696\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6582 - accuracy: 0.6084 - val_loss: 0.6235 - val_accuracy: 0.6696\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6589 - accuracy: 0.6081 - val_loss: 0.6263 - val_accuracy: 0.6735\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6570 - accuracy: 0.6164 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6588 - accuracy: 0.6139 - val_loss: 0.6241 - val_accuracy: 0.6709\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6560 - accuracy: 0.6189 - val_loss: 0.6214 - val_accuracy: 0.6735\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6536 - accuracy: 0.6183 - val_loss: 0.6194 - val_accuracy: 0.6722\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6512 - accuracy: 0.6211 - val_loss: 0.6231 - val_accuracy: 0.6722\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6551 - accuracy: 0.6169 - val_loss: 0.6225 - val_accuracy: 0.6709\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6580 - accuracy: 0.6178 - val_loss: 0.6225 - val_accuracy: 0.6722\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6596 - accuracy: 0.6154 - val_loss: 0.6258 - val_accuracy: 0.6760\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6571 - accuracy: 0.6164 - val_loss: 0.6209 - val_accuracy: 0.6722\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6575 - accuracy: 0.6211 - val_loss: 0.6228 - val_accuracy: 0.6760\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6594 - accuracy: 0.6186 - val_loss: 0.6251 - val_accuracy: 0.6747\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6533 - accuracy: 0.6187 - val_loss: 0.6217 - val_accuracy: 0.6735\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6531 - accuracy: 0.6139 - val_loss: 0.6188 - val_accuracy: 0.6735\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6584 - accuracy: 0.6132 - val_loss: 0.6225 - val_accuracy: 0.6722\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6545 - accuracy: 0.6218 - val_loss: 0.6218 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6558 - accuracy: 0.6179 - val_loss: 0.6219 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6573 - accuracy: 0.6240 - val_loss: 0.6255 - val_accuracy: 0.6798\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6568 - accuracy: 0.6188 - val_loss: 0.6215 - val_accuracy: 0.6735\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6544 - accuracy: 0.6209 - val_loss: 0.6225 - val_accuracy: 0.6824\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6554 - accuracy: 0.6179 - val_loss: 0.6208 - val_accuracy: 0.6760\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6528 - accuracy: 0.6218 - val_loss: 0.6204 - val_accuracy: 0.6773\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6546 - accuracy: 0.6237 - val_loss: 0.6201 - val_accuracy: 0.6773\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6521 - accuracy: 0.6238 - val_loss: 0.6212 - val_accuracy: 0.6773\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.6232 - val_loss: 0.6191 - val_accuracy: 0.6798\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6506 - accuracy: 0.6261 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6546 - accuracy: 0.6216 - val_loss: 0.6210 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6544 - accuracy: 0.6274 - val_loss: 0.6164 - val_accuracy: 0.6824\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6560 - accuracy: 0.6183 - val_loss: 0.6207 - val_accuracy: 0.6798\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6521 - accuracy: 0.6232 - val_loss: 0.6187 - val_accuracy: 0.6760\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6556 - accuracy: 0.6203 - val_loss: 0.6247 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6514 - accuracy: 0.6218 - val_loss: 0.6183 - val_accuracy: 0.6798\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6514 - accuracy: 0.6213 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6534 - accuracy: 0.6216 - val_loss: 0.6235 - val_accuracy: 0.6811\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6528 - accuracy: 0.6236 - val_loss: 0.6175 - val_accuracy: 0.6786\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6483 - accuracy: 0.6275 - val_loss: 0.6163 - val_accuracy: 0.6837\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6508 - accuracy: 0.6216 - val_loss: 0.6184 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6513 - accuracy: 0.6289 - val_loss: 0.6192 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6522 - accuracy: 0.6240 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6511 - accuracy: 0.6295 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6507 - accuracy: 0.6238 - val_loss: 0.6143 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.6275 - val_loss: 0.6145 - val_accuracy: 0.6849\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6521 - accuracy: 0.6247 - val_loss: 0.6186 - val_accuracy: 0.6837\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6517 - accuracy: 0.6236 - val_loss: 0.6163 - val_accuracy: 0.6811\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6504 - accuracy: 0.6250 - val_loss: 0.6164 - val_accuracy: 0.6837\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6477 - accuracy: 0.6295 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6499 - accuracy: 0.6274 - val_loss: 0.6157 - val_accuracy: 0.6837\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6537 - accuracy: 0.6238 - val_loss: 0.6182 - val_accuracy: 0.6798\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6491 - accuracy: 0.6258 - val_loss: 0.6201 - val_accuracy: 0.6824\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6514 - accuracy: 0.6256 - val_loss: 0.6171 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6507 - accuracy: 0.6297 - val_loss: 0.6168 - val_accuracy: 0.6824\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6478 - accuracy: 0.6291 - val_loss: 0.6146 - val_accuracy: 0.6837\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6507 - accuracy: 0.6226 - val_loss: 0.6157 - val_accuracy: 0.6824\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6476 - accuracy: 0.6300 - val_loss: 0.6138 - val_accuracy: 0.6837\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6486 - accuracy: 0.6341 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6458 - accuracy: 0.6271 - val_loss: 0.6138 - val_accuracy: 0.6811\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6480 - accuracy: 0.6358 - val_loss: 0.6150 - val_accuracy: 0.6837\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6488 - accuracy: 0.6321 - val_loss: 0.6135 - val_accuracy: 0.6837\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6508 - accuracy: 0.6257 - val_loss: 0.6176 - val_accuracy: 0.6849\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6463 - accuracy: 0.6330 - val_loss: 0.6154 - val_accuracy: 0.6811\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6484 - accuracy: 0.6304 - val_loss: 0.6127 - val_accuracy: 0.6849\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6458 - accuracy: 0.6318 - val_loss: 0.6123 - val_accuracy: 0.6849\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6445 - accuracy: 0.6341 - val_loss: 0.6137 - val_accuracy: 0.6824\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6444 - accuracy: 0.6311 - val_loss: 0.6105 - val_accuracy: 0.6837\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6456 - accuracy: 0.6351 - val_loss: 0.6124 - val_accuracy: 0.6811\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6440 - accuracy: 0.6345 - val_loss: 0.6117 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6433 - accuracy: 0.6338 - val_loss: 0.6125 - val_accuracy: 0.6837\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6466 - accuracy: 0.6292 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6476 - accuracy: 0.6280 - val_loss: 0.6161 - val_accuracy: 0.6849\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6482 - accuracy: 0.6320 - val_loss: 0.6122 - val_accuracy: 0.6862\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6487 - accuracy: 0.6325 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6470 - accuracy: 0.6296 - val_loss: 0.6133 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6472 - accuracy: 0.6333 - val_loss: 0.6140 - val_accuracy: 0.6824\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6482 - accuracy: 0.6338 - val_loss: 0.6168 - val_accuracy: 0.6837\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6486 - accuracy: 0.6311 - val_loss: 0.6167 - val_accuracy: 0.6824\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6476 - accuracy: 0.6312 - val_loss: 0.6161 - val_accuracy: 0.6786\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6495 - accuracy: 0.6282 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6497 - accuracy: 0.6248 - val_loss: 0.6182 - val_accuracy: 0.6862\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6467 - accuracy: 0.6372 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6425 - accuracy: 0.6372 - val_loss: 0.6121 - val_accuracy: 0.6849\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6451 - accuracy: 0.6319 - val_loss: 0.6148 - val_accuracy: 0.6798\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6464 - accuracy: 0.6302 - val_loss: 0.6113 - val_accuracy: 0.6811\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6458 - accuracy: 0.6351 - val_loss: 0.6134 - val_accuracy: 0.6798\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6474 - accuracy: 0.6353 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6478 - accuracy: 0.6296 - val_loss: 0.6144 - val_accuracy: 0.6824\n",
      "Calculating for: 1000 100 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_204 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7640 - accuracy: 0.5437 - val_loss: 0.6817 - val_accuracy: 0.5574\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6961 - accuracy: 0.5775 - val_loss: 0.6628 - val_accuracy: 0.6148\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.5878 - val_loss: 0.6581 - val_accuracy: 0.6250\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5986 - val_loss: 0.6518 - val_accuracy: 0.6480\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6014 - val_loss: 0.6514 - val_accuracy: 0.6250\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6073 - val_loss: 0.6476 - val_accuracy: 0.6441\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6197 - val_loss: 0.6399 - val_accuracy: 0.6543\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6220 - val_loss: 0.6447 - val_accuracy: 0.6480\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6168 - val_loss: 0.6450 - val_accuracy: 0.6480\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6281 - val_loss: 0.6447 - val_accuracy: 0.6454\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6295 - val_loss: 0.6413 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6481 - accuracy: 0.6267 - val_loss: 0.6423 - val_accuracy: 0.6480\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6338 - val_loss: 0.6364 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6316 - val_loss: 0.6394 - val_accuracy: 0.6543\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6366 - val_loss: 0.6370 - val_accuracy: 0.6505\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6407 - val_loss: 0.6357 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6333 - val_loss: 0.6382 - val_accuracy: 0.6492\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6384 - val_loss: 0.6338 - val_accuracy: 0.6658\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6355 - val_loss: 0.6331 - val_accuracy: 0.6658\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6457 - val_loss: 0.6301 - val_accuracy: 0.6735\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6476 - val_loss: 0.6343 - val_accuracy: 0.6518\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6527 - val_loss: 0.6267 - val_accuracy: 0.6735\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6436 - val_loss: 0.6313 - val_accuracy: 0.6658\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6282 - accuracy: 0.6521 - val_loss: 0.6317 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6507 - val_loss: 0.6323 - val_accuracy: 0.6607\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6268 - accuracy: 0.6497 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6555 - val_loss: 0.6260 - val_accuracy: 0.6773\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6556 - val_loss: 0.6269 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6629 - val_loss: 0.6279 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6615 - val_loss: 0.6271 - val_accuracy: 0.6747\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6598 - val_loss: 0.6248 - val_accuracy: 0.6849\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6639 - val_loss: 0.6217 - val_accuracy: 0.6862\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6680 - val_loss: 0.6262 - val_accuracy: 0.6798\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6649 - val_loss: 0.6274 - val_accuracy: 0.6786\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6709 - val_loss: 0.6261 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6692 - val_loss: 0.6258 - val_accuracy: 0.6747\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6626 - val_loss: 0.6240 - val_accuracy: 0.6862\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6623 - val_loss: 0.6275 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6703 - val_loss: 0.6246 - val_accuracy: 0.6824\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6069 - accuracy: 0.6760 - val_loss: 0.6246 - val_accuracy: 0.6811\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6126 - accuracy: 0.6630 - val_loss: 0.6233 - val_accuracy: 0.6798\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6747 - val_loss: 0.6260 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6724 - val_loss: 0.6274 - val_accuracy: 0.6811\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6721 - val_loss: 0.6312 - val_accuracy: 0.6684\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6027 - accuracy: 0.6747 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6781 - val_loss: 0.6243 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6056 - accuracy: 0.6763 - val_loss: 0.6198 - val_accuracy: 0.6849\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5993 - accuracy: 0.6825 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5998 - accuracy: 0.6827 - val_loss: 0.6264 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5972 - accuracy: 0.6836 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5984 - accuracy: 0.6792 - val_loss: 0.6227 - val_accuracy: 0.6901\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5959 - accuracy: 0.6879 - val_loss: 0.6288 - val_accuracy: 0.6735\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5959 - accuracy: 0.6840 - val_loss: 0.6297 - val_accuracy: 0.6645\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5972 - accuracy: 0.6797 - val_loss: 0.6316 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5893 - accuracy: 0.6863 - val_loss: 0.6275 - val_accuracy: 0.6735\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5899 - accuracy: 0.6883 - val_loss: 0.6288 - val_accuracy: 0.6735\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5926 - accuracy: 0.6895 - val_loss: 0.6315 - val_accuracy: 0.6760\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5927 - accuracy: 0.6879 - val_loss: 0.6330 - val_accuracy: 0.6735\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5934 - accuracy: 0.6865 - val_loss: 0.6287 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5886 - accuracy: 0.6939 - val_loss: 0.6292 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5865 - accuracy: 0.6937 - val_loss: 0.6332 - val_accuracy: 0.6709\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5828 - accuracy: 0.6902 - val_loss: 0.6307 - val_accuracy: 0.6811\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5836 - accuracy: 0.6940 - val_loss: 0.6316 - val_accuracy: 0.6747\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5819 - accuracy: 0.6943 - val_loss: 0.6372 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5855 - accuracy: 0.6874 - val_loss: 0.6318 - val_accuracy: 0.6760\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5799 - accuracy: 0.6994 - val_loss: 0.6325 - val_accuracy: 0.6696\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5780 - accuracy: 0.6979 - val_loss: 0.6360 - val_accuracy: 0.6696\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5763 - accuracy: 0.6942 - val_loss: 0.6415 - val_accuracy: 0.6735\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5751 - accuracy: 0.7021 - val_loss: 0.6336 - val_accuracy: 0.6709\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5784 - accuracy: 0.6962 - val_loss: 0.6338 - val_accuracy: 0.6747\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5738 - accuracy: 0.6976 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5768 - accuracy: 0.6981 - val_loss: 0.6373 - val_accuracy: 0.6747\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5746 - accuracy: 0.7047 - val_loss: 0.6372 - val_accuracy: 0.6747\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5659 - accuracy: 0.7126 - val_loss: 0.6448 - val_accuracy: 0.6607\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5675 - accuracy: 0.7048 - val_loss: 0.6442 - val_accuracy: 0.6607\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5671 - accuracy: 0.7143 - val_loss: 0.6385 - val_accuracy: 0.6645\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5707 - accuracy: 0.7066 - val_loss: 0.6439 - val_accuracy: 0.6658\n",
      "Calculating for: 1000 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_208 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8118 - accuracy: 0.5320 - val_loss: 0.6590 - val_accuracy: 0.6403\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7199 - accuracy: 0.5573 - val_loss: 0.6486 - val_accuracy: 0.6416\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6927 - accuracy: 0.5610 - val_loss: 0.6466 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6765 - accuracy: 0.5803 - val_loss: 0.6427 - val_accuracy: 0.6645\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5789 - val_loss: 0.6477 - val_accuracy: 0.6492\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5787 - val_loss: 0.6522 - val_accuracy: 0.6531\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5924 - val_loss: 0.6493 - val_accuracy: 0.6531\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.6015 - val_loss: 0.6458 - val_accuracy: 0.6569\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5991 - val_loss: 0.6427 - val_accuracy: 0.6620\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6064 - val_loss: 0.6349 - val_accuracy: 0.6696\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6638 - accuracy: 0.6056 - val_loss: 0.6372 - val_accuracy: 0.6684\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6035 - val_loss: 0.6427 - val_accuracy: 0.6582\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6099 - val_loss: 0.6435 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6044 - val_loss: 0.6386 - val_accuracy: 0.6671\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6045 - val_loss: 0.6412 - val_accuracy: 0.6543\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6075 - val_loss: 0.6334 - val_accuracy: 0.6735\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6596 - accuracy: 0.6101 - val_loss: 0.6426 - val_accuracy: 0.6505\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6149 - val_loss: 0.6355 - val_accuracy: 0.6696\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6091 - val_loss: 0.6334 - val_accuracy: 0.6760\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6218 - val_loss: 0.6391 - val_accuracy: 0.6505\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6204 - val_loss: 0.6364 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6182 - val_loss: 0.6356 - val_accuracy: 0.6556\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6216 - val_loss: 0.6365 - val_accuracy: 0.6569\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6257 - val_loss: 0.6420 - val_accuracy: 0.6441\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6213 - val_loss: 0.6340 - val_accuracy: 0.6645\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6280 - val_loss: 0.6332 - val_accuracy: 0.6671\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6252 - val_loss: 0.6297 - val_accuracy: 0.6798\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6206 - val_loss: 0.6293 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6312 - val_loss: 0.6295 - val_accuracy: 0.6735\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6286 - val_loss: 0.6298 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6228 - val_loss: 0.6300 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6282 - val_loss: 0.6342 - val_accuracy: 0.6607\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6480 - accuracy: 0.6304 - val_loss: 0.6330 - val_accuracy: 0.6696\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6311 - val_loss: 0.6298 - val_accuracy: 0.6735\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6445 - accuracy: 0.6287 - val_loss: 0.6318 - val_accuracy: 0.6722\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6329 - val_loss: 0.6313 - val_accuracy: 0.6594\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6324 - val_loss: 0.6287 - val_accuracy: 0.6735\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6445 - accuracy: 0.6318 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6333 - val_loss: 0.6234 - val_accuracy: 0.6798\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6336 - val_loss: 0.6309 - val_accuracy: 0.6620\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6297 - val_loss: 0.6264 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6349 - val_loss: 0.6205 - val_accuracy: 0.6888\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6406 - accuracy: 0.6389 - val_loss: 0.6270 - val_accuracy: 0.6735\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6274 - val_loss: 0.6303 - val_accuracy: 0.6582\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6453 - val_loss: 0.6247 - val_accuracy: 0.6786\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6385 - val_loss: 0.6261 - val_accuracy: 0.6620\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6348 - val_loss: 0.6307 - val_accuracy: 0.6594\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6436 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6384 - accuracy: 0.6414 - val_loss: 0.6251 - val_accuracy: 0.6633\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6409 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6498 - val_loss: 0.6281 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6433 - val_loss: 0.6262 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6444 - val_loss: 0.6280 - val_accuracy: 0.6786\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6400 - val_loss: 0.6291 - val_accuracy: 0.6645\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6414 - val_loss: 0.6267 - val_accuracy: 0.6747\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6431 - val_loss: 0.6266 - val_accuracy: 0.6747\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6426 - val_loss: 0.6275 - val_accuracy: 0.6645\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6507 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6436 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6438 - val_loss: 0.6184 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6517 - val_loss: 0.6235 - val_accuracy: 0.6760\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6487 - val_loss: 0.6231 - val_accuracy: 0.6798\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6539 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6531 - val_loss: 0.6202 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6593 - val_loss: 0.6253 - val_accuracy: 0.6696\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6457 - val_loss: 0.6172 - val_accuracy: 0.6862\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6600 - val_loss: 0.6253 - val_accuracy: 0.6735\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6270 - accuracy: 0.6596 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6260 - accuracy: 0.6575 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6272 - accuracy: 0.6562 - val_loss: 0.6288 - val_accuracy: 0.6722\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6273 - accuracy: 0.6574 - val_loss: 0.6269 - val_accuracy: 0.6671\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6545 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6529 - val_loss: 0.6237 - val_accuracy: 0.6709\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6583 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6545 - val_loss: 0.6218 - val_accuracy: 0.6747\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6540 - val_loss: 0.6224 - val_accuracy: 0.6760\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6615 - val_loss: 0.6214 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6194 - accuracy: 0.6618 - val_loss: 0.6202 - val_accuracy: 0.6735\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6530 - val_loss: 0.6249 - val_accuracy: 0.6684\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6664 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6238 - accuracy: 0.6647 - val_loss: 0.6230 - val_accuracy: 0.6747\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6606 - val_loss: 0.6221 - val_accuracy: 0.6773\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6684 - val_loss: 0.6245 - val_accuracy: 0.6709\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6642 - val_loss: 0.6242 - val_accuracy: 0.6658\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6644 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6139 - accuracy: 0.6654 - val_loss: 0.6207 - val_accuracy: 0.6786\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6168 - accuracy: 0.6673 - val_loss: 0.6265 - val_accuracy: 0.6696\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6603 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6649 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6194 - accuracy: 0.6672 - val_loss: 0.6277 - val_accuracy: 0.6645\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6595 - val_loss: 0.6223 - val_accuracy: 0.6760\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6642 - val_loss: 0.6255 - val_accuracy: 0.6684\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6637 - val_loss: 0.6213 - val_accuracy: 0.6760\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6655 - val_loss: 0.6234 - val_accuracy: 0.6747\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6638 - val_loss: 0.6224 - val_accuracy: 0.6722\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6117 - accuracy: 0.6706 - val_loss: 0.6225 - val_accuracy: 0.6786\n",
      "Calculating for: 1000 100 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_212 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "236/249 [===========================>..] - ETA: 0s - loss: 0.8126 - accuracy: 0.5097WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8087 - accuracy: 0.5113 - val_loss: 0.6791 - val_accuracy: 0.6237\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7319 - accuracy: 0.5098 - val_loss: 0.6776 - val_accuracy: 0.6224\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7053 - accuracy: 0.5190 - val_loss: 0.6766 - val_accuracy: 0.6212\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6961 - accuracy: 0.5231 - val_loss: 0.6685 - val_accuracy: 0.6199\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5264 - val_loss: 0.6703 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6925 - accuracy: 0.5291 - val_loss: 0.6701 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5285 - val_loss: 0.6743 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6909 - accuracy: 0.5279 - val_loss: 0.6673 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6910 - accuracy: 0.5337 - val_loss: 0.6700 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6911 - accuracy: 0.5322 - val_loss: 0.6685 - val_accuracy: 0.6199\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5480 - val_loss: 0.6736 - val_accuracy: 0.6199\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6904 - accuracy: 0.5383 - val_loss: 0.6664 - val_accuracy: 0.6199\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6888 - accuracy: 0.5409 - val_loss: 0.6657 - val_accuracy: 0.6199\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.5352 - val_loss: 0.6699 - val_accuracy: 0.6224\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5333 - val_loss: 0.6703 - val_accuracy: 0.6237\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6893 - accuracy: 0.5377 - val_loss: 0.6653 - val_accuracy: 0.6212\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6894 - accuracy: 0.5330 - val_loss: 0.6670 - val_accuracy: 0.6224\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6883 - accuracy: 0.5399 - val_loss: 0.6698 - val_accuracy: 0.6237\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6873 - accuracy: 0.5423 - val_loss: 0.6636 - val_accuracy: 0.6237\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6864 - accuracy: 0.5445 - val_loss: 0.6633 - val_accuracy: 0.6237\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5404 - val_loss: 0.6652 - val_accuracy: 0.6276\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6860 - accuracy: 0.5467 - val_loss: 0.6620 - val_accuracy: 0.6263\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6854 - accuracy: 0.5455 - val_loss: 0.6613 - val_accuracy: 0.6263\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6868 - accuracy: 0.5472 - val_loss: 0.6654 - val_accuracy: 0.6352\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6870 - accuracy: 0.5487 - val_loss: 0.6677 - val_accuracy: 0.6378\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5461 - val_loss: 0.6584 - val_accuracy: 0.6288\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6849 - accuracy: 0.5556 - val_loss: 0.6581 - val_accuracy: 0.6339\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5482 - val_loss: 0.6610 - val_accuracy: 0.6352\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6845 - accuracy: 0.5570 - val_loss: 0.6593 - val_accuracy: 0.6390\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6863 - accuracy: 0.5524 - val_loss: 0.6666 - val_accuracy: 0.6403\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6830 - accuracy: 0.5593 - val_loss: 0.6568 - val_accuracy: 0.6314\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.5622 - val_loss: 0.6595 - val_accuracy: 0.6378\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5555 - val_loss: 0.6612 - val_accuracy: 0.6403\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6801 - accuracy: 0.5588 - val_loss: 0.6572 - val_accuracy: 0.6403\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6810 - accuracy: 0.5624 - val_loss: 0.6593 - val_accuracy: 0.6416\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6821 - accuracy: 0.5633 - val_loss: 0.6566 - val_accuracy: 0.6441\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5535 - val_loss: 0.6551 - val_accuracy: 0.6365\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6814 - accuracy: 0.5593 - val_loss: 0.6543 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5608 - val_loss: 0.6593 - val_accuracy: 0.6429\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6802 - accuracy: 0.5662 - val_loss: 0.6578 - val_accuracy: 0.6429\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6818 - accuracy: 0.5583 - val_loss: 0.6591 - val_accuracy: 0.6429\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6787 - accuracy: 0.5750 - val_loss: 0.6533 - val_accuracy: 0.6429\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6823 - accuracy: 0.5683 - val_loss: 0.6555 - val_accuracy: 0.6441\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6780 - accuracy: 0.5732 - val_loss: 0.6524 - val_accuracy: 0.6429\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5683 - val_loss: 0.6514 - val_accuracy: 0.6429\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6779 - accuracy: 0.5659 - val_loss: 0.6501 - val_accuracy: 0.6416\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5791 - val_loss: 0.6512 - val_accuracy: 0.6403\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6804 - accuracy: 0.5624 - val_loss: 0.6531 - val_accuracy: 0.6429\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6799 - accuracy: 0.5623 - val_loss: 0.6527 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6770 - accuracy: 0.5756 - val_loss: 0.6500 - val_accuracy: 0.6429\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5647 - val_loss: 0.6507 - val_accuracy: 0.6429\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5742 - val_loss: 0.6496 - val_accuracy: 0.6429\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5774 - val_loss: 0.6493 - val_accuracy: 0.6441\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6771 - accuracy: 0.5745 - val_loss: 0.6500 - val_accuracy: 0.6416\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5662 - val_loss: 0.6529 - val_accuracy: 0.6416\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6778 - accuracy: 0.5735 - val_loss: 0.6531 - val_accuracy: 0.6429\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.5730 - val_loss: 0.6479 - val_accuracy: 0.6416\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5824 - val_loss: 0.6478 - val_accuracy: 0.6390\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6774 - accuracy: 0.5813 - val_loss: 0.6504 - val_accuracy: 0.6429\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.5789 - val_loss: 0.6468 - val_accuracy: 0.6416\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5800 - val_loss: 0.6499 - val_accuracy: 0.6505\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6757 - accuracy: 0.5803 - val_loss: 0.6489 - val_accuracy: 0.6429\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6774 - accuracy: 0.5730 - val_loss: 0.6466 - val_accuracy: 0.6429\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5843 - val_loss: 0.6464 - val_accuracy: 0.6492\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6747 - accuracy: 0.5785 - val_loss: 0.6446 - val_accuracy: 0.6492\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5901 - val_loss: 0.6440 - val_accuracy: 0.6480\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5838 - val_loss: 0.6457 - val_accuracy: 0.6480\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.5870 - val_loss: 0.6469 - val_accuracy: 0.6441\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5867 - val_loss: 0.6442 - val_accuracy: 0.6467\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5926 - val_loss: 0.6448 - val_accuracy: 0.6441\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.5776 - val_loss: 0.6476 - val_accuracy: 0.6492\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.5834 - val_loss: 0.6433 - val_accuracy: 0.6492\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5916 - val_loss: 0.6431 - val_accuracy: 0.6505\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5882 - val_loss: 0.6460 - val_accuracy: 0.6480\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6700 - accuracy: 0.5883 - val_loss: 0.6440 - val_accuracy: 0.6505\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6694 - accuracy: 0.5896 - val_loss: 0.6434 - val_accuracy: 0.6480\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5922 - val_loss: 0.6413 - val_accuracy: 0.6480\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5957 - val_loss: 0.6426 - val_accuracy: 0.6543\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6719 - accuracy: 0.5867 - val_loss: 0.6444 - val_accuracy: 0.6480\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6700 - accuracy: 0.5887 - val_loss: 0.6420 - val_accuracy: 0.6518\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5919 - val_loss: 0.6410 - val_accuracy: 0.6505\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5944 - val_loss: 0.6398 - val_accuracy: 0.6492\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5868 - val_loss: 0.6406 - val_accuracy: 0.6518\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5929 - val_loss: 0.6419 - val_accuracy: 0.6518\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5972 - val_loss: 0.6449 - val_accuracy: 0.6531\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5887 - val_loss: 0.6408 - val_accuracy: 0.6556\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5916 - val_loss: 0.6448 - val_accuracy: 0.6658\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5931 - val_loss: 0.6369 - val_accuracy: 0.6518\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6684 - accuracy: 0.5917 - val_loss: 0.6409 - val_accuracy: 0.6569\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5966 - val_loss: 0.6375 - val_accuracy: 0.6505\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5874 - val_loss: 0.6403 - val_accuracy: 0.6518\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6678 - accuracy: 0.5971 - val_loss: 0.6408 - val_accuracy: 0.6556\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6675 - accuracy: 0.5995 - val_loss: 0.6389 - val_accuracy: 0.6607\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.6021 - val_loss: 0.6356 - val_accuracy: 0.6582\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6007 - val_loss: 0.6403 - val_accuracy: 0.6658\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6678 - accuracy: 0.5919 - val_loss: 0.6385 - val_accuracy: 0.6620\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6004 - val_loss: 0.6351 - val_accuracy: 0.6582\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.6080 - val_loss: 0.6360 - val_accuracy: 0.6620\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.6001 - val_loss: 0.6376 - val_accuracy: 0.6594\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.5966 - val_loss: 0.6352 - val_accuracy: 0.6747\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6071 - val_loss: 0.6349 - val_accuracy: 0.6658\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6673 - accuracy: 0.6014 - val_loss: 0.6412 - val_accuracy: 0.6747\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.5981 - val_loss: 0.6382 - val_accuracy: 0.6684\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6005 - val_loss: 0.6411 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6051 - val_loss: 0.6373 - val_accuracy: 0.6684\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6656 - accuracy: 0.6024 - val_loss: 0.6379 - val_accuracy: 0.6671\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.5988 - val_loss: 0.6355 - val_accuracy: 0.6722\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6639 - accuracy: 0.6065 - val_loss: 0.6343 - val_accuracy: 0.6645\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6096 - val_loss: 0.6339 - val_accuracy: 0.6645\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6660 - accuracy: 0.5996 - val_loss: 0.6348 - val_accuracy: 0.6735\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.5950 - val_loss: 0.6370 - val_accuracy: 0.6747\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6638 - accuracy: 0.6090 - val_loss: 0.6337 - val_accuracy: 0.6696\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6014 - val_loss: 0.6338 - val_accuracy: 0.6709\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6042 - val_loss: 0.6338 - val_accuracy: 0.6735\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6021 - val_loss: 0.6341 - val_accuracy: 0.6722\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6644 - accuracy: 0.6061 - val_loss: 0.6383 - val_accuracy: 0.6696\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6086 - val_loss: 0.6331 - val_accuracy: 0.6735\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6066 - val_loss: 0.6351 - val_accuracy: 0.6798\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6022 - val_loss: 0.6333 - val_accuracy: 0.6722\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6060 - val_loss: 0.6323 - val_accuracy: 0.6722\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6130 - val_loss: 0.6300 - val_accuracy: 0.6722\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6065 - val_loss: 0.6302 - val_accuracy: 0.6684\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6036 - val_loss: 0.6345 - val_accuracy: 0.6875\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6162 - val_loss: 0.6284 - val_accuracy: 0.6722\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6623 - accuracy: 0.6050 - val_loss: 0.6312 - val_accuracy: 0.6735\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6632 - accuracy: 0.6113 - val_loss: 0.6322 - val_accuracy: 0.6773\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6176 - val_loss: 0.6335 - val_accuracy: 0.6747\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6100 - val_loss: 0.6301 - val_accuracy: 0.6709\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6162 - val_loss: 0.6305 - val_accuracy: 0.6786\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6125 - val_loss: 0.6274 - val_accuracy: 0.6773\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6158 - val_loss: 0.6279 - val_accuracy: 0.6722\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.6120 - val_loss: 0.6291 - val_accuracy: 0.6709\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6578 - accuracy: 0.6187 - val_loss: 0.6331 - val_accuracy: 0.6849\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6130 - val_loss: 0.6330 - val_accuracy: 0.6837\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6566 - accuracy: 0.6196 - val_loss: 0.6272 - val_accuracy: 0.6811\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6109 - val_loss: 0.6312 - val_accuracy: 0.6786\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6114 - val_loss: 0.6252 - val_accuracy: 0.6837\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6118 - val_loss: 0.6270 - val_accuracy: 0.6824\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6584 - accuracy: 0.6094 - val_loss: 0.6277 - val_accuracy: 0.6747\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6117 - val_loss: 0.6284 - val_accuracy: 0.6773\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6063 - val_loss: 0.6277 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6162 - val_loss: 0.6297 - val_accuracy: 0.6760\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6125 - val_loss: 0.6270 - val_accuracy: 0.6760\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6208 - val_loss: 0.6300 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6571 - accuracy: 0.6152 - val_loss: 0.6297 - val_accuracy: 0.6798\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6129 - val_loss: 0.6299 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6145 - val_loss: 0.6284 - val_accuracy: 0.6773\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6101 - val_loss: 0.6266 - val_accuracy: 0.6786\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6213 - val_loss: 0.6274 - val_accuracy: 0.6773\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6178 - val_loss: 0.6227 - val_accuracy: 0.6811\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6207 - val_loss: 0.6271 - val_accuracy: 0.6798\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6189 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6242 - val_loss: 0.6237 - val_accuracy: 0.6773\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6199 - val_loss: 0.6275 - val_accuracy: 0.6837\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6262 - val_loss: 0.6235 - val_accuracy: 0.6824\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6179 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6196 - val_loss: 0.6228 - val_accuracy: 0.6786\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6237 - val_loss: 0.6224 - val_accuracy: 0.6798\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6227 - val_loss: 0.6278 - val_accuracy: 0.6798\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6241 - val_loss: 0.6217 - val_accuracy: 0.6811\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6562 - accuracy: 0.6271 - val_loss: 0.6225 - val_accuracy: 0.6811\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6215 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6167 - val_loss: 0.6281 - val_accuracy: 0.6824\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6133 - val_loss: 0.6284 - val_accuracy: 0.6811\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6554 - accuracy: 0.6215 - val_loss: 0.6282 - val_accuracy: 0.6837\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6173 - val_loss: 0.6285 - val_accuracy: 0.6849\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6169 - val_loss: 0.6241 - val_accuracy: 0.6849\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6544 - accuracy: 0.6255 - val_loss: 0.6274 - val_accuracy: 0.6849\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6166 - val_loss: 0.6305 - val_accuracy: 0.6811\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6226 - val_loss: 0.6231 - val_accuracy: 0.6811\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6242 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6246 - val_loss: 0.6261 - val_accuracy: 0.6824\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6262 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6245 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6221 - val_loss: 0.6276 - val_accuracy: 0.6824\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.6182 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6521 - accuracy: 0.6243 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6546 - accuracy: 0.6147 - val_loss: 0.6246 - val_accuracy: 0.6811\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6242 - val_loss: 0.6228 - val_accuracy: 0.6824\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6302 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6232 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6206 - val_loss: 0.6195 - val_accuracy: 0.6824\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6211 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6241 - val_loss: 0.6245 - val_accuracy: 0.6824\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6325 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6274 - val_loss: 0.6239 - val_accuracy: 0.6811\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6255 - val_loss: 0.6211 - val_accuracy: 0.6786\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6186 - val_loss: 0.6222 - val_accuracy: 0.6798\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6284 - val_loss: 0.6214 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6282 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6262 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6227 - val_loss: 0.6196 - val_accuracy: 0.6849\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6237 - val_loss: 0.6186 - val_accuracy: 0.6862\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6204 - val_loss: 0.6212 - val_accuracy: 0.6824\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6265 - val_loss: 0.6237 - val_accuracy: 0.6824\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6262 - val_loss: 0.6217 - val_accuracy: 0.6837\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6516 - accuracy: 0.6211 - val_loss: 0.6222 - val_accuracy: 0.6849\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6292 - val_loss: 0.6219 - val_accuracy: 0.6811\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6247 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6188 - val_loss: 0.6247 - val_accuracy: 0.6811\n",
      "Calculating for: 1000 250 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_216 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7943 - accuracy: 0.5633 - val_loss: 0.6410 - val_accuracy: 0.6722\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7021 - accuracy: 0.5840 - val_loss: 0.6257 - val_accuracy: 0.6786\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6703 - accuracy: 0.6049 - val_loss: 0.6279 - val_accuracy: 0.6849\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6610 - accuracy: 0.6090 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6140 - val_loss: 0.6236 - val_accuracy: 0.6862\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6557 - accuracy: 0.6204 - val_loss: 0.6241 - val_accuracy: 0.6811\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6237 - val_loss: 0.6193 - val_accuracy: 0.6888\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6505 - accuracy: 0.6197 - val_loss: 0.6188 - val_accuracy: 0.6849\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6477 - accuracy: 0.6270 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6437 - accuracy: 0.6256 - val_loss: 0.6192 - val_accuracy: 0.6901\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6411 - accuracy: 0.6364 - val_loss: 0.6155 - val_accuracy: 0.6811\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6395 - accuracy: 0.6392 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6382 - val_loss: 0.6182 - val_accuracy: 0.6964\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6320 - accuracy: 0.6481 - val_loss: 0.6145 - val_accuracy: 0.6901\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6434 - val_loss: 0.6175 - val_accuracy: 0.6926\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6353 - accuracy: 0.6483 - val_loss: 0.6190 - val_accuracy: 0.6888\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6426 - val_loss: 0.6183 - val_accuracy: 0.6913\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6366 - accuracy: 0.6446 - val_loss: 0.6151 - val_accuracy: 0.6926\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6551 - val_loss: 0.6163 - val_accuracy: 0.6888\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6299 - accuracy: 0.6461 - val_loss: 0.6138 - val_accuracy: 0.6901\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6498 - val_loss: 0.6164 - val_accuracy: 0.6977\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6525 - val_loss: 0.6158 - val_accuracy: 0.6977\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6550 - val_loss: 0.6210 - val_accuracy: 0.6811\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6246 - accuracy: 0.6569 - val_loss: 0.6186 - val_accuracy: 0.6913\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6611 - val_loss: 0.6199 - val_accuracy: 0.6837\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6265 - accuracy: 0.6518 - val_loss: 0.6150 - val_accuracy: 0.6964\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6655 - val_loss: 0.6186 - val_accuracy: 0.6977\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6198 - accuracy: 0.6615 - val_loss: 0.6191 - val_accuracy: 0.6888\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6133 - accuracy: 0.6680 - val_loss: 0.6220 - val_accuracy: 0.6901\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6196 - accuracy: 0.6613 - val_loss: 0.6195 - val_accuracy: 0.6964\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6142 - accuracy: 0.6694 - val_loss: 0.6193 - val_accuracy: 0.6964\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6098 - accuracy: 0.6733 - val_loss: 0.6199 - val_accuracy: 0.6849\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6110 - accuracy: 0.6714 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6094 - accuracy: 0.6788 - val_loss: 0.6230 - val_accuracy: 0.6824\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6102 - accuracy: 0.6662 - val_loss: 0.6198 - val_accuracy: 0.6901\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6077 - accuracy: 0.6707 - val_loss: 0.6202 - val_accuracy: 0.6875\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6081 - accuracy: 0.6732 - val_loss: 0.6156 - val_accuracy: 0.6926\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6042 - accuracy: 0.6729 - val_loss: 0.6246 - val_accuracy: 0.6888\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6057 - accuracy: 0.6694 - val_loss: 0.6215 - val_accuracy: 0.6888\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6025 - accuracy: 0.6800 - val_loss: 0.6299 - val_accuracy: 0.6824\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6001 - accuracy: 0.6860 - val_loss: 0.6305 - val_accuracy: 0.6888\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5971 - accuracy: 0.6851 - val_loss: 0.6273 - val_accuracy: 0.6837\n",
      "Calculating for: 1000 250 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_220 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8247 - accuracy: 0.5345 - val_loss: 0.6424 - val_accuracy: 0.6531\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7178 - accuracy: 0.5599 - val_loss: 0.6452 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6887 - accuracy: 0.5745 - val_loss: 0.6401 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6773 - accuracy: 0.5752 - val_loss: 0.6407 - val_accuracy: 0.6671\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6712 - accuracy: 0.5855 - val_loss: 0.6392 - val_accuracy: 0.6696\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6705 - accuracy: 0.5902 - val_loss: 0.6387 - val_accuracy: 0.6658\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6666 - accuracy: 0.5976 - val_loss: 0.6398 - val_accuracy: 0.6582\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6668 - accuracy: 0.5962 - val_loss: 0.6382 - val_accuracy: 0.6645\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6665 - accuracy: 0.5928 - val_loss: 0.6403 - val_accuracy: 0.6556\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6613 - accuracy: 0.6076 - val_loss: 0.6371 - val_accuracy: 0.6505\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6041 - val_loss: 0.6335 - val_accuracy: 0.6671\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6623 - accuracy: 0.6059 - val_loss: 0.6308 - val_accuracy: 0.6658\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6605 - accuracy: 0.6074 - val_loss: 0.6317 - val_accuracy: 0.6671\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6613 - accuracy: 0.6045 - val_loss: 0.6295 - val_accuracy: 0.6658\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6568 - accuracy: 0.6171 - val_loss: 0.6298 - val_accuracy: 0.6722\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6144 - val_loss: 0.6296 - val_accuracy: 0.6722\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6568 - accuracy: 0.6171 - val_loss: 0.6276 - val_accuracy: 0.6633\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6558 - accuracy: 0.6138 - val_loss: 0.6325 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6542 - accuracy: 0.6137 - val_loss: 0.6281 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6540 - accuracy: 0.6191 - val_loss: 0.6305 - val_accuracy: 0.6760\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6290 - val_loss: 0.6254 - val_accuracy: 0.6709\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6183 - val_loss: 0.6299 - val_accuracy: 0.6709\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6530 - accuracy: 0.6194 - val_loss: 0.6260 - val_accuracy: 0.6696\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6240 - val_loss: 0.6236 - val_accuracy: 0.6747\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6290 - val_loss: 0.6224 - val_accuracy: 0.6735\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6193 - val_loss: 0.6234 - val_accuracy: 0.6747\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6264 - val_loss: 0.6239 - val_accuracy: 0.6773\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6228 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6227 - val_loss: 0.6227 - val_accuracy: 0.6773\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6343 - val_loss: 0.6275 - val_accuracy: 0.6722\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6436 - accuracy: 0.6341 - val_loss: 0.6196 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6261 - val_loss: 0.6199 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6442 - accuracy: 0.6289 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6467 - accuracy: 0.6280 - val_loss: 0.6194 - val_accuracy: 0.6875\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6448 - accuracy: 0.6280 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6306 - val_loss: 0.6197 - val_accuracy: 0.6786\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6368 - val_loss: 0.6166 - val_accuracy: 0.6875\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6433 - accuracy: 0.6329 - val_loss: 0.6181 - val_accuracy: 0.6773\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6454 - accuracy: 0.6309 - val_loss: 0.6231 - val_accuracy: 0.6760\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6387 - val_loss: 0.6179 - val_accuracy: 0.6760\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6355 - val_loss: 0.6241 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6409 - accuracy: 0.6408 - val_loss: 0.6170 - val_accuracy: 0.6849\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6389 - accuracy: 0.6429 - val_loss: 0.6189 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6385 - val_loss: 0.6198 - val_accuracy: 0.6696\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6374 - val_loss: 0.6175 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6436 - val_loss: 0.6156 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6409 - val_loss: 0.6178 - val_accuracy: 0.6811\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6336 - accuracy: 0.6434 - val_loss: 0.6158 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6383 - accuracy: 0.6423 - val_loss: 0.6182 - val_accuracy: 0.6747\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6358 - accuracy: 0.6437 - val_loss: 0.6201 - val_accuracy: 0.6773\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6419 - val_loss: 0.6184 - val_accuracy: 0.6747\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6409 - val_loss: 0.6175 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.6438 - val_loss: 0.6196 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6362 - accuracy: 0.6503 - val_loss: 0.6181 - val_accuracy: 0.6760\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6434 - val_loss: 0.6168 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6332 - accuracy: 0.6480 - val_loss: 0.6169 - val_accuracy: 0.6798\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6306 - accuracy: 0.6472 - val_loss: 0.6170 - val_accuracy: 0.6837\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6506 - val_loss: 0.6128 - val_accuracy: 0.6849\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6311 - accuracy: 0.6486 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6292 - accuracy: 0.6501 - val_loss: 0.6131 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6292 - accuracy: 0.6462 - val_loss: 0.6157 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6295 - accuracy: 0.6522 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6540 - val_loss: 0.6148 - val_accuracy: 0.6837\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6529 - val_loss: 0.6164 - val_accuracy: 0.6849\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6556 - val_loss: 0.6135 - val_accuracy: 0.6901\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6492 - val_loss: 0.6141 - val_accuracy: 0.6837\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6237 - accuracy: 0.6572 - val_loss: 0.6125 - val_accuracy: 0.6875\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6659 - val_loss: 0.6131 - val_accuracy: 0.6849\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6551 - val_loss: 0.6156 - val_accuracy: 0.6875\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6546 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6244 - accuracy: 0.6615 - val_loss: 0.6174 - val_accuracy: 0.6964\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6210 - accuracy: 0.6629 - val_loss: 0.6143 - val_accuracy: 0.6964\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6567 - val_loss: 0.6146 - val_accuracy: 0.6990\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6606 - val_loss: 0.6161 - val_accuracy: 0.6939\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6191 - accuracy: 0.6665 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6644 - val_loss: 0.6146 - val_accuracy: 0.6913\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6570 - val_loss: 0.6155 - val_accuracy: 0.6939\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6204 - accuracy: 0.6535 - val_loss: 0.6154 - val_accuracy: 0.6952\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6209 - accuracy: 0.6551 - val_loss: 0.6164 - val_accuracy: 0.6939\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6609 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6212 - accuracy: 0.6605 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6192 - accuracy: 0.6629 - val_loss: 0.6167 - val_accuracy: 0.6926\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6143 - accuracy: 0.6683 - val_loss: 0.6137 - val_accuracy: 0.6939\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6207 - accuracy: 0.6581 - val_loss: 0.6157 - val_accuracy: 0.6939\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6210 - accuracy: 0.6620 - val_loss: 0.6133 - val_accuracy: 0.6977\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6151 - accuracy: 0.6644 - val_loss: 0.6142 - val_accuracy: 0.7003\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6172 - accuracy: 0.6639 - val_loss: 0.6117 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6117 - accuracy: 0.6618 - val_loss: 0.6141 - val_accuracy: 0.6939\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6157 - accuracy: 0.6644 - val_loss: 0.6126 - val_accuracy: 0.6952\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6157 - accuracy: 0.6613 - val_loss: 0.6121 - val_accuracy: 0.6964\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6138 - accuracy: 0.6682 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6140 - accuracy: 0.6619 - val_loss: 0.6173 - val_accuracy: 0.6926\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6124 - accuracy: 0.6667 - val_loss: 0.6141 - val_accuracy: 0.6964\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6090 - accuracy: 0.6747 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6128 - accuracy: 0.6717 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6122 - accuracy: 0.6675 - val_loss: 0.6162 - val_accuracy: 0.6862\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6088 - accuracy: 0.6644 - val_loss: 0.6149 - val_accuracy: 0.6952\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6120 - accuracy: 0.6704 - val_loss: 0.6164 - val_accuracy: 0.6888\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6125 - accuracy: 0.6717 - val_loss: 0.6184 - val_accuracy: 0.6888\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6067 - accuracy: 0.6762 - val_loss: 0.6174 - val_accuracy: 0.6888\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6111 - accuracy: 0.6712 - val_loss: 0.6184 - val_accuracy: 0.6913\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6105 - accuracy: 0.6718 - val_loss: 0.6141 - val_accuracy: 0.6939\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6070 - accuracy: 0.6711 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6077 - accuracy: 0.6713 - val_loss: 0.6160 - val_accuracy: 0.6901\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6043 - accuracy: 0.6777 - val_loss: 0.6158 - val_accuracy: 0.6926\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6074 - accuracy: 0.6765 - val_loss: 0.6138 - val_accuracy: 0.6913\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6035 - accuracy: 0.6760 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6032 - accuracy: 0.6802 - val_loss: 0.6199 - val_accuracy: 0.6811\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6026 - accuracy: 0.6770 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6006 - accuracy: 0.6775 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6046 - accuracy: 0.6791 - val_loss: 0.6166 - val_accuracy: 0.6901\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6089 - accuracy: 0.6722 - val_loss: 0.6207 - val_accuracy: 0.6760\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5976 - accuracy: 0.6777 - val_loss: 0.6193 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6005 - accuracy: 0.6820 - val_loss: 0.6214 - val_accuracy: 0.6849\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5981 - accuracy: 0.6801 - val_loss: 0.6193 - val_accuracy: 0.6837\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6073 - accuracy: 0.6770 - val_loss: 0.6181 - val_accuracy: 0.6901\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6016 - accuracy: 0.6776 - val_loss: 0.6198 - val_accuracy: 0.6875\n",
      "Calculating for: 1000 250 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_224 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8326 - accuracy: 0.5055 - val_loss: 0.6657 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7330 - accuracy: 0.5206 - val_loss: 0.6654 - val_accuracy: 0.6212\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7068 - accuracy: 0.5092 - val_loss: 0.6653 - val_accuracy: 0.6186\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6916 - accuracy: 0.5319 - val_loss: 0.6661 - val_accuracy: 0.6186\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6936 - accuracy: 0.5229 - val_loss: 0.6691 - val_accuracy: 0.6224\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6902 - accuracy: 0.5323 - val_loss: 0.6713 - val_accuracy: 0.6212\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6919 - accuracy: 0.5283 - val_loss: 0.6692 - val_accuracy: 0.6212\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6901 - accuracy: 0.5359 - val_loss: 0.6686 - val_accuracy: 0.6224\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6907 - accuracy: 0.5381 - val_loss: 0.6664 - val_accuracy: 0.6212\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6899 - accuracy: 0.5363 - val_loss: 0.6662 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6909 - accuracy: 0.5335 - val_loss: 0.6703 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6888 - accuracy: 0.5367 - val_loss: 0.6658 - val_accuracy: 0.6212\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6880 - accuracy: 0.5389 - val_loss: 0.6635 - val_accuracy: 0.6224\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6900 - accuracy: 0.5322 - val_loss: 0.6662 - val_accuracy: 0.6199\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6892 - accuracy: 0.5382 - val_loss: 0.6639 - val_accuracy: 0.6224\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6891 - accuracy: 0.5352 - val_loss: 0.6637 - val_accuracy: 0.6224\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6882 - accuracy: 0.5489 - val_loss: 0.6596 - val_accuracy: 0.6224\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6862 - accuracy: 0.5534 - val_loss: 0.6619 - val_accuracy: 0.6339\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6862 - accuracy: 0.5496 - val_loss: 0.6590 - val_accuracy: 0.6288\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6861 - accuracy: 0.5548 - val_loss: 0.6603 - val_accuracy: 0.6365\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6863 - accuracy: 0.5495 - val_loss: 0.6584 - val_accuracy: 0.6339\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6877 - accuracy: 0.5465 - val_loss: 0.6623 - val_accuracy: 0.6365\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6844 - accuracy: 0.5550 - val_loss: 0.6579 - val_accuracy: 0.6365\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6858 - accuracy: 0.5465 - val_loss: 0.6583 - val_accuracy: 0.6378\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6833 - accuracy: 0.5565 - val_loss: 0.6577 - val_accuracy: 0.6416\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6840 - accuracy: 0.5555 - val_loss: 0.6542 - val_accuracy: 0.6390\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6830 - accuracy: 0.5533 - val_loss: 0.6531 - val_accuracy: 0.6403\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6832 - accuracy: 0.5603 - val_loss: 0.6559 - val_accuracy: 0.6378\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6851 - accuracy: 0.5535 - val_loss: 0.6555 - val_accuracy: 0.6403\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6824 - accuracy: 0.5662 - val_loss: 0.6550 - val_accuracy: 0.6390\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6791 - accuracy: 0.5721 - val_loss: 0.6502 - val_accuracy: 0.6378\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.5641 - val_loss: 0.6491 - val_accuracy: 0.6416\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6812 - accuracy: 0.5643 - val_loss: 0.6504 - val_accuracy: 0.6429\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6820 - accuracy: 0.5638 - val_loss: 0.6518 - val_accuracy: 0.6441\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6781 - accuracy: 0.5752 - val_loss: 0.6498 - val_accuracy: 0.6441\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6778 - accuracy: 0.5678 - val_loss: 0.6508 - val_accuracy: 0.6390\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6773 - accuracy: 0.5771 - val_loss: 0.6485 - val_accuracy: 0.6429\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.5700 - val_loss: 0.6492 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6773 - accuracy: 0.5723 - val_loss: 0.6466 - val_accuracy: 0.6416\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6790 - accuracy: 0.5701 - val_loss: 0.6447 - val_accuracy: 0.6454\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6784 - accuracy: 0.5737 - val_loss: 0.6488 - val_accuracy: 0.6390\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6784 - accuracy: 0.5681 - val_loss: 0.6481 - val_accuracy: 0.6403\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6786 - accuracy: 0.5718 - val_loss: 0.6475 - val_accuracy: 0.6403\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6778 - accuracy: 0.5767 - val_loss: 0.6464 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6746 - accuracy: 0.5793 - val_loss: 0.6466 - val_accuracy: 0.6403\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6753 - accuracy: 0.5823 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6768 - accuracy: 0.5795 - val_loss: 0.6458 - val_accuracy: 0.6429\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6744 - accuracy: 0.5730 - val_loss: 0.6434 - val_accuracy: 0.6403\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6767 - accuracy: 0.5742 - val_loss: 0.6449 - val_accuracy: 0.6429\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6728 - accuracy: 0.5859 - val_loss: 0.6423 - val_accuracy: 0.6416\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6758 - accuracy: 0.5760 - val_loss: 0.6446 - val_accuracy: 0.6429\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6747 - accuracy: 0.5774 - val_loss: 0.6459 - val_accuracy: 0.6480\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6718 - accuracy: 0.5824 - val_loss: 0.6398 - val_accuracy: 0.6416\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6700 - accuracy: 0.5879 - val_loss: 0.6372 - val_accuracy: 0.6429\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6750 - accuracy: 0.5860 - val_loss: 0.6420 - val_accuracy: 0.6467\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6730 - accuracy: 0.5870 - val_loss: 0.6418 - val_accuracy: 0.6480\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6747 - accuracy: 0.5809 - val_loss: 0.6396 - val_accuracy: 0.6467\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6708 - accuracy: 0.5909 - val_loss: 0.6401 - val_accuracy: 0.6505\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6687 - accuracy: 0.5864 - val_loss: 0.6355 - val_accuracy: 0.6467\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6713 - accuracy: 0.5883 - val_loss: 0.6387 - val_accuracy: 0.6454\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6710 - accuracy: 0.5873 - val_loss: 0.6420 - val_accuracy: 0.6543\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5927 - val_loss: 0.6380 - val_accuracy: 0.6480\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6703 - accuracy: 0.5873 - val_loss: 0.6368 - val_accuracy: 0.6467\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.5938 - val_loss: 0.6388 - val_accuracy: 0.6492\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6727 - accuracy: 0.5898 - val_loss: 0.6378 - val_accuracy: 0.6480\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5937 - val_loss: 0.6376 - val_accuracy: 0.6518\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6706 - accuracy: 0.5916 - val_loss: 0.6367 - val_accuracy: 0.6454\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6696 - accuracy: 0.5914 - val_loss: 0.6364 - val_accuracy: 0.6480\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.5950 - val_loss: 0.6357 - val_accuracy: 0.6454\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6684 - accuracy: 0.5908 - val_loss: 0.6390 - val_accuracy: 0.6684\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6670 - accuracy: 0.5944 - val_loss: 0.6342 - val_accuracy: 0.6684\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.5943 - val_loss: 0.6369 - val_accuracy: 0.6684\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6685 - accuracy: 0.5917 - val_loss: 0.6359 - val_accuracy: 0.6658\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6653 - accuracy: 0.5923 - val_loss: 0.6332 - val_accuracy: 0.6696\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6688 - accuracy: 0.5946 - val_loss: 0.6345 - val_accuracy: 0.6671\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6673 - accuracy: 0.5913 - val_loss: 0.6359 - val_accuracy: 0.6684\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5946 - val_loss: 0.6364 - val_accuracy: 0.6684\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6046 - val_loss: 0.6320 - val_accuracy: 0.6722\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6655 - accuracy: 0.6012 - val_loss: 0.6350 - val_accuracy: 0.6658\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6675 - accuracy: 0.5967 - val_loss: 0.6321 - val_accuracy: 0.6633\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6689 - accuracy: 0.5941 - val_loss: 0.6360 - val_accuracy: 0.6658\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5962 - val_loss: 0.6331 - val_accuracy: 0.6696\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6622 - accuracy: 0.6010 - val_loss: 0.6303 - val_accuracy: 0.6696\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6606 - accuracy: 0.6032 - val_loss: 0.6314 - val_accuracy: 0.6684\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6633 - accuracy: 0.6066 - val_loss: 0.6300 - val_accuracy: 0.6696\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6648 - accuracy: 0.6014 - val_loss: 0.6310 - val_accuracy: 0.6696\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.6095 - val_loss: 0.6314 - val_accuracy: 0.6671\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6627 - accuracy: 0.6083 - val_loss: 0.6286 - val_accuracy: 0.6709\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6623 - accuracy: 0.6002 - val_loss: 0.6287 - val_accuracy: 0.6684\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6059 - val_loss: 0.6285 - val_accuracy: 0.6735\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6664 - accuracy: 0.6006 - val_loss: 0.6305 - val_accuracy: 0.6722\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6617 - accuracy: 0.6096 - val_loss: 0.6284 - val_accuracy: 0.6696\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6609 - accuracy: 0.6137 - val_loss: 0.6275 - val_accuracy: 0.6696\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6608 - accuracy: 0.6049 - val_loss: 0.6262 - val_accuracy: 0.6658\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6656 - accuracy: 0.6024 - val_loss: 0.6286 - val_accuracy: 0.6735\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6089 - val_loss: 0.6294 - val_accuracy: 0.6658\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6034 - val_loss: 0.6278 - val_accuracy: 0.6735\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6084 - val_loss: 0.6280 - val_accuracy: 0.6684\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6615 - accuracy: 0.6128 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6587 - accuracy: 0.6070 - val_loss: 0.6255 - val_accuracy: 0.6684\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6086 - val_loss: 0.6275 - val_accuracy: 0.6786\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6649 - accuracy: 0.6047 - val_loss: 0.6282 - val_accuracy: 0.6684\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6593 - accuracy: 0.6135 - val_loss: 0.6276 - val_accuracy: 0.6773\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6113 - val_loss: 0.6276 - val_accuracy: 0.6684\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6623 - accuracy: 0.6059 - val_loss: 0.6274 - val_accuracy: 0.6696\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6636 - accuracy: 0.6070 - val_loss: 0.6304 - val_accuracy: 0.6671\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.6113 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.6056 - val_loss: 0.6273 - val_accuracy: 0.6747\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6612 - accuracy: 0.6088 - val_loss: 0.6257 - val_accuracy: 0.6684\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6604 - accuracy: 0.6140 - val_loss: 0.6249 - val_accuracy: 0.6684\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6559 - accuracy: 0.6174 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6573 - accuracy: 0.6211 - val_loss: 0.6256 - val_accuracy: 0.6735\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6573 - accuracy: 0.6167 - val_loss: 0.6211 - val_accuracy: 0.6709\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6169 - val_loss: 0.6268 - val_accuracy: 0.6824\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6600 - accuracy: 0.6093 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6157 - val_loss: 0.6252 - val_accuracy: 0.6760\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6576 - accuracy: 0.6139 - val_loss: 0.6258 - val_accuracy: 0.6722\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6578 - accuracy: 0.6164 - val_loss: 0.6251 - val_accuracy: 0.6722\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6148 - val_loss: 0.6250 - val_accuracy: 0.6798\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6610 - accuracy: 0.6083 - val_loss: 0.6265 - val_accuracy: 0.6786\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6563 - accuracy: 0.6221 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6545 - accuracy: 0.6161 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6194 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6581 - accuracy: 0.6140 - val_loss: 0.6287 - val_accuracy: 0.6798\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6565 - accuracy: 0.6173 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6587 - accuracy: 0.6090 - val_loss: 0.6243 - val_accuracy: 0.6837\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6544 - accuracy: 0.6191 - val_loss: 0.6207 - val_accuracy: 0.6798\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6167 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6549 - accuracy: 0.6179 - val_loss: 0.6227 - val_accuracy: 0.6773\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6591 - accuracy: 0.6132 - val_loss: 0.6240 - val_accuracy: 0.6824\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6570 - accuracy: 0.6130 - val_loss: 0.6200 - val_accuracy: 0.6824\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6573 - accuracy: 0.6177 - val_loss: 0.6221 - val_accuracy: 0.6837\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6557 - accuracy: 0.6192 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6535 - accuracy: 0.6163 - val_loss: 0.6233 - val_accuracy: 0.6798\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6540 - accuracy: 0.6218 - val_loss: 0.6189 - val_accuracy: 0.6798\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6230 - val_loss: 0.6215 - val_accuracy: 0.6837\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6560 - accuracy: 0.6168 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6237 - val_loss: 0.6201 - val_accuracy: 0.6849\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6560 - accuracy: 0.6159 - val_loss: 0.6205 - val_accuracy: 0.6760\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6526 - accuracy: 0.6184 - val_loss: 0.6186 - val_accuracy: 0.6862\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6555 - accuracy: 0.6173 - val_loss: 0.6237 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6235 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6564 - accuracy: 0.6163 - val_loss: 0.6230 - val_accuracy: 0.6811\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6247 - val_loss: 0.6218 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6544 - accuracy: 0.6242 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6585 - accuracy: 0.6193 - val_loss: 0.6219 - val_accuracy: 0.6798\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6237 - val_loss: 0.6205 - val_accuracy: 0.6798\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6271 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6553 - accuracy: 0.6159 - val_loss: 0.6216 - val_accuracy: 0.6798\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6213 - val_loss: 0.6190 - val_accuracy: 0.6811\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6208 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6513 - accuracy: 0.6291 - val_loss: 0.6190 - val_accuracy: 0.6824\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6539 - accuracy: 0.6269 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6547 - accuracy: 0.6192 - val_loss: 0.6182 - val_accuracy: 0.6824\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6209 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6231 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6489 - accuracy: 0.6270 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6242 - val_loss: 0.6161 - val_accuracy: 0.6849\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6253 - val_loss: 0.6205 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6302 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6539 - accuracy: 0.6215 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6319 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6508 - accuracy: 0.6300 - val_loss: 0.6137 - val_accuracy: 0.6824\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6236 - val_loss: 0.6163 - val_accuracy: 0.6837\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6504 - accuracy: 0.6267 - val_loss: 0.6190 - val_accuracy: 0.6824\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6532 - accuracy: 0.6282 - val_loss: 0.6196 - val_accuracy: 0.6798\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6531 - accuracy: 0.6318 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6284 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6501 - accuracy: 0.6316 - val_loss: 0.6158 - val_accuracy: 0.6786\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6282 - val_loss: 0.6157 - val_accuracy: 0.6862\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6295 - val_loss: 0.6195 - val_accuracy: 0.6798\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6265 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6184 - val_loss: 0.6144 - val_accuracy: 0.6811\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6531 - accuracy: 0.6275 - val_loss: 0.6214 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6541 - accuracy: 0.6196 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6505 - accuracy: 0.6232 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6252 - val_loss: 0.6180 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6282 - val_loss: 0.6150 - val_accuracy: 0.6824\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6449 - accuracy: 0.6340 - val_loss: 0.6126 - val_accuracy: 0.6862\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6483 - accuracy: 0.6326 - val_loss: 0.6157 - val_accuracy: 0.6837\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6493 - accuracy: 0.6350 - val_loss: 0.6165 - val_accuracy: 0.6862\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6523 - accuracy: 0.6267 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6484 - accuracy: 0.6297 - val_loss: 0.6158 - val_accuracy: 0.6862\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6496 - accuracy: 0.6297 - val_loss: 0.6133 - val_accuracy: 0.6824\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6366 - val_loss: 0.6160 - val_accuracy: 0.6862\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6487 - accuracy: 0.6247 - val_loss: 0.6171 - val_accuracy: 0.6849\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6508 - accuracy: 0.6262 - val_loss: 0.6141 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6501 - accuracy: 0.6241 - val_loss: 0.6147 - val_accuracy: 0.6901\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6487 - accuracy: 0.6261 - val_loss: 0.6163 - val_accuracy: 0.6888\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6252 - val_loss: 0.6155 - val_accuracy: 0.6875\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6470 - accuracy: 0.6355 - val_loss: 0.6144 - val_accuracy: 0.6875\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6492 - accuracy: 0.6281 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6478 - accuracy: 0.6279 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6478 - accuracy: 0.6304 - val_loss: 0.6149 - val_accuracy: 0.6875\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6458 - accuracy: 0.6316 - val_loss: 0.6130 - val_accuracy: 0.6862\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6465 - accuracy: 0.6331 - val_loss: 0.6121 - val_accuracy: 0.6875\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6464 - accuracy: 0.6302 - val_loss: 0.6145 - val_accuracy: 0.6875\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6440 - accuracy: 0.6377 - val_loss: 0.6115 - val_accuracy: 0.6901\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6325 - val_loss: 0.6172 - val_accuracy: 0.6888\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6454 - accuracy: 0.6389 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Calculating for: 1000 400 0.5 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_228 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.8030 - accuracy: 0.5504 - val_loss: 0.6900 - val_accuracy: 0.5536\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7069 - accuracy: 0.5772 - val_loss: 0.6645 - val_accuracy: 0.6288\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6761 - accuracy: 0.5946 - val_loss: 0.6571 - val_accuracy: 0.6390\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6656 - accuracy: 0.6083 - val_loss: 0.6472 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6576 - accuracy: 0.6204 - val_loss: 0.6441 - val_accuracy: 0.6633\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6509 - accuracy: 0.6238 - val_loss: 0.6493 - val_accuracy: 0.6403\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6527 - accuracy: 0.6233 - val_loss: 0.6443 - val_accuracy: 0.6403\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6493 - accuracy: 0.6286 - val_loss: 0.6378 - val_accuracy: 0.6582\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6494 - accuracy: 0.6292 - val_loss: 0.6433 - val_accuracy: 0.6403\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6420 - accuracy: 0.6417 - val_loss: 0.6347 - val_accuracy: 0.6594\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6408 - accuracy: 0.6368 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6418 - accuracy: 0.6364 - val_loss: 0.6410 - val_accuracy: 0.6480\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6355 - accuracy: 0.6420 - val_loss: 0.6367 - val_accuracy: 0.6569\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6351 - accuracy: 0.6417 - val_loss: 0.6343 - val_accuracy: 0.6543\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6343 - accuracy: 0.6469 - val_loss: 0.6364 - val_accuracy: 0.6429\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6354 - accuracy: 0.6423 - val_loss: 0.6327 - val_accuracy: 0.6594\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6319 - accuracy: 0.6502 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6262 - accuracy: 0.6555 - val_loss: 0.6349 - val_accuracy: 0.6454\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6309 - accuracy: 0.6472 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6289 - accuracy: 0.6513 - val_loss: 0.6352 - val_accuracy: 0.6518\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6194 - accuracy: 0.6669 - val_loss: 0.6323 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6201 - accuracy: 0.6579 - val_loss: 0.6327 - val_accuracy: 0.6582\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6233 - accuracy: 0.6599 - val_loss: 0.6292 - val_accuracy: 0.6582\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6154 - accuracy: 0.6639 - val_loss: 0.6303 - val_accuracy: 0.6620\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6187 - accuracy: 0.6621 - val_loss: 0.6322 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6202 - accuracy: 0.6594 - val_loss: 0.6331 - val_accuracy: 0.6582\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6132 - accuracy: 0.6696 - val_loss: 0.6327 - val_accuracy: 0.6582\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6146 - accuracy: 0.6655 - val_loss: 0.6365 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6137 - accuracy: 0.6625 - val_loss: 0.6318 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6141 - accuracy: 0.6693 - val_loss: 0.6315 - val_accuracy: 0.6658\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6122 - accuracy: 0.6719 - val_loss: 0.6359 - val_accuracy: 0.6582\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6077 - accuracy: 0.6707 - val_loss: 0.6322 - val_accuracy: 0.6658\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6038 - accuracy: 0.6785 - val_loss: 0.6357 - val_accuracy: 0.6620\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6012 - accuracy: 0.6797 - val_loss: 0.6432 - val_accuracy: 0.6556\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6062 - accuracy: 0.6733 - val_loss: 0.6364 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5997 - accuracy: 0.6796 - val_loss: 0.6398 - val_accuracy: 0.6518\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6013 - accuracy: 0.6801 - val_loss: 0.6423 - val_accuracy: 0.6505\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5986 - accuracy: 0.6821 - val_loss: 0.6360 - val_accuracy: 0.6684\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5955 - accuracy: 0.6855 - val_loss: 0.6446 - val_accuracy: 0.6492\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6026 - accuracy: 0.6752 - val_loss: 0.6380 - val_accuracy: 0.6620\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5943 - accuracy: 0.6844 - val_loss: 0.6412 - val_accuracy: 0.6620\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5975 - accuracy: 0.6815 - val_loss: 0.6345 - val_accuracy: 0.6709\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5939 - accuracy: 0.6850 - val_loss: 0.6397 - val_accuracy: 0.6620\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5966 - accuracy: 0.6810 - val_loss: 0.6435 - val_accuracy: 0.6569\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.5901 - accuracy: 0.6870 - val_loss: 0.6451 - val_accuracy: 0.6645\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5961 - accuracy: 0.6854 - val_loss: 0.6442 - val_accuracy: 0.6543\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5904 - accuracy: 0.6866 - val_loss: 0.6353 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5872 - accuracy: 0.6913 - val_loss: 0.6469 - val_accuracy: 0.6480\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.5860 - accuracy: 0.6884 - val_loss: 0.6417 - val_accuracy: 0.6594\n",
      "Calculating for: 1000 400 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_232 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.8244 - accuracy: 0.5402 - val_loss: 0.6595 - val_accuracy: 0.6505\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7160 - accuracy: 0.5681 - val_loss: 0.6510 - val_accuracy: 0.6633\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6868 - accuracy: 0.5762 - val_loss: 0.6418 - val_accuracy: 0.6684\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6750 - accuracy: 0.5844 - val_loss: 0.6523 - val_accuracy: 0.6556\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6739 - accuracy: 0.5870 - val_loss: 0.6486 - val_accuracy: 0.6722\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6685 - accuracy: 0.5951 - val_loss: 0.6467 - val_accuracy: 0.6747\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6668 - accuracy: 0.5966 - val_loss: 0.6431 - val_accuracy: 0.6735\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6608 - accuracy: 0.6056 - val_loss: 0.6399 - val_accuracy: 0.6722\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6634 - accuracy: 0.6039 - val_loss: 0.6471 - val_accuracy: 0.6607\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6628 - accuracy: 0.6049 - val_loss: 0.6428 - val_accuracy: 0.6620\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6590 - accuracy: 0.6135 - val_loss: 0.6434 - val_accuracy: 0.6543\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6602 - accuracy: 0.6094 - val_loss: 0.6396 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6551 - accuracy: 0.6173 - val_loss: 0.6415 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6580 - accuracy: 0.6060 - val_loss: 0.6399 - val_accuracy: 0.6607\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6551 - accuracy: 0.6143 - val_loss: 0.6369 - val_accuracy: 0.6607\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6552 - accuracy: 0.6216 - val_loss: 0.6420 - val_accuracy: 0.6492\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6558 - accuracy: 0.6122 - val_loss: 0.6362 - val_accuracy: 0.6620\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6576 - accuracy: 0.6142 - val_loss: 0.6360 - val_accuracy: 0.6582\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6557 - accuracy: 0.6221 - val_loss: 0.6362 - val_accuracy: 0.6582\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6532 - accuracy: 0.6218 - val_loss: 0.6368 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6489 - accuracy: 0.6242 - val_loss: 0.6377 - val_accuracy: 0.6480\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6503 - accuracy: 0.6267 - val_loss: 0.6409 - val_accuracy: 0.6441\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6521 - accuracy: 0.6253 - val_loss: 0.6355 - val_accuracy: 0.6620\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6506 - accuracy: 0.6242 - val_loss: 0.6365 - val_accuracy: 0.6492\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6484 - accuracy: 0.6247 - val_loss: 0.6330 - val_accuracy: 0.6607\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6466 - accuracy: 0.6368 - val_loss: 0.6369 - val_accuracy: 0.6403\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6468 - accuracy: 0.6240 - val_loss: 0.6359 - val_accuracy: 0.6480\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6518 - accuracy: 0.6240 - val_loss: 0.6403 - val_accuracy: 0.6390\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6454 - accuracy: 0.6300 - val_loss: 0.6287 - val_accuracy: 0.6747\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6447 - accuracy: 0.6323 - val_loss: 0.6321 - val_accuracy: 0.6658\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6449 - accuracy: 0.6312 - val_loss: 0.6263 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6437 - accuracy: 0.6318 - val_loss: 0.6280 - val_accuracy: 0.6658\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6414 - accuracy: 0.6373 - val_loss: 0.6263 - val_accuracy: 0.6671\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6391 - accuracy: 0.6439 - val_loss: 0.6295 - val_accuracy: 0.6569\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6435 - accuracy: 0.6361 - val_loss: 0.6295 - val_accuracy: 0.6633\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6428 - accuracy: 0.6353 - val_loss: 0.6294 - val_accuracy: 0.6594\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6390 - accuracy: 0.6383 - val_loss: 0.6266 - val_accuracy: 0.6633\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6396 - accuracy: 0.6351 - val_loss: 0.6304 - val_accuracy: 0.6531\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6416 - accuracy: 0.6389 - val_loss: 0.6299 - val_accuracy: 0.6620\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6386 - accuracy: 0.6389 - val_loss: 0.6273 - val_accuracy: 0.6747\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6377 - accuracy: 0.6388 - val_loss: 0.6275 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6357 - accuracy: 0.6473 - val_loss: 0.6300 - val_accuracy: 0.6658\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6384 - accuracy: 0.6364 - val_loss: 0.6282 - val_accuracy: 0.6633\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6347 - accuracy: 0.6419 - val_loss: 0.6273 - val_accuracy: 0.6684\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6327 - accuracy: 0.6443 - val_loss: 0.6301 - val_accuracy: 0.6620\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6362 - accuracy: 0.6395 - val_loss: 0.6278 - val_accuracy: 0.6658\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6358 - accuracy: 0.6448 - val_loss: 0.6283 - val_accuracy: 0.6709\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6354 - accuracy: 0.6429 - val_loss: 0.6343 - val_accuracy: 0.6569\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6315 - accuracy: 0.6438 - val_loss: 0.6253 - val_accuracy: 0.6747\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6328 - accuracy: 0.6478 - val_loss: 0.6308 - val_accuracy: 0.6658\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6319 - accuracy: 0.6492 - val_loss: 0.6292 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6334 - accuracy: 0.6530 - val_loss: 0.6292 - val_accuracy: 0.6696\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6323 - accuracy: 0.6534 - val_loss: 0.6244 - val_accuracy: 0.6696\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6300 - accuracy: 0.6511 - val_loss: 0.6275 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6293 - accuracy: 0.6426 - val_loss: 0.6278 - val_accuracy: 0.6658\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6293 - accuracy: 0.6511 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6285 - accuracy: 0.6549 - val_loss: 0.6260 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6266 - accuracy: 0.6574 - val_loss: 0.6243 - val_accuracy: 0.6709\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6275 - accuracy: 0.6500 - val_loss: 0.6289 - val_accuracy: 0.6684\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6267 - accuracy: 0.6531 - val_loss: 0.6309 - val_accuracy: 0.6671\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6256 - accuracy: 0.6513 - val_loss: 0.6276 - val_accuracy: 0.6735\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6224 - accuracy: 0.6562 - val_loss: 0.6277 - val_accuracy: 0.6696\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6263 - accuracy: 0.6523 - val_loss: 0.6272 - val_accuracy: 0.6735\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6271 - accuracy: 0.6574 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6260 - accuracy: 0.6495 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6223 - accuracy: 0.6536 - val_loss: 0.6219 - val_accuracy: 0.6773\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6205 - accuracy: 0.6596 - val_loss: 0.6280 - val_accuracy: 0.6684\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6228 - accuracy: 0.6596 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6218 - accuracy: 0.6574 - val_loss: 0.6177 - val_accuracy: 0.6837\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6232 - accuracy: 0.6603 - val_loss: 0.6244 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6180 - accuracy: 0.6638 - val_loss: 0.6277 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6198 - accuracy: 0.6594 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6186 - accuracy: 0.6585 - val_loss: 0.6256 - val_accuracy: 0.6786\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6210 - accuracy: 0.6625 - val_loss: 0.6271 - val_accuracy: 0.6798\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6167 - accuracy: 0.6669 - val_loss: 0.6229 - val_accuracy: 0.6837\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6177 - accuracy: 0.6620 - val_loss: 0.6226 - val_accuracy: 0.6811\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6638 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6633 - val_loss: 0.6293 - val_accuracy: 0.6786\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6178 - accuracy: 0.6629 - val_loss: 0.6350 - val_accuracy: 0.6696\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6165 - accuracy: 0.6691 - val_loss: 0.6335 - val_accuracy: 0.6684\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6149 - accuracy: 0.6616 - val_loss: 0.6264 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6164 - accuracy: 0.6653 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6144 - accuracy: 0.6708 - val_loss: 0.6309 - val_accuracy: 0.6684\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6147 - accuracy: 0.6694 - val_loss: 0.6314 - val_accuracy: 0.6696\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6102 - accuracy: 0.6734 - val_loss: 0.6304 - val_accuracy: 0.6709\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6114 - accuracy: 0.6743 - val_loss: 0.6285 - val_accuracy: 0.6747\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6093 - accuracy: 0.6728 - val_loss: 0.6302 - val_accuracy: 0.6760\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6125 - accuracy: 0.6689 - val_loss: 0.6274 - val_accuracy: 0.6798\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6105 - accuracy: 0.6673 - val_loss: 0.6303 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6101 - accuracy: 0.6706 - val_loss: 0.6320 - val_accuracy: 0.6735\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6093 - accuracy: 0.6703 - val_loss: 0.6330 - val_accuracy: 0.6607\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6058 - accuracy: 0.6718 - val_loss: 0.6369 - val_accuracy: 0.6556\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6084 - accuracy: 0.6765 - val_loss: 0.6312 - val_accuracy: 0.6633\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6061 - accuracy: 0.6772 - val_loss: 0.6271 - val_accuracy: 0.6747\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6041 - accuracy: 0.6805 - val_loss: 0.6234 - val_accuracy: 0.6811\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6099 - accuracy: 0.6721 - val_loss: 0.6283 - val_accuracy: 0.6722\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6106 - accuracy: 0.6712 - val_loss: 0.6347 - val_accuracy: 0.6671\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6020 - accuracy: 0.6757 - val_loss: 0.6358 - val_accuracy: 0.6658\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6076 - accuracy: 0.6736 - val_loss: 0.6301 - val_accuracy: 0.6709\n",
      "Calculating for: 1000 400 0.9 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_236 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.8560 - accuracy: 0.5046 - val_loss: 0.6628 - val_accuracy: 0.6339\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.7370 - accuracy: 0.5106 - val_loss: 0.6602 - val_accuracy: 0.6365\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.7042 - accuracy: 0.5269 - val_loss: 0.6610 - val_accuracy: 0.6339\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6957 - accuracy: 0.5289 - val_loss: 0.6620 - val_accuracy: 0.6212\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6915 - accuracy: 0.5288 - val_loss: 0.6669 - val_accuracy: 0.6212\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6888 - accuracy: 0.5409 - val_loss: 0.6623 - val_accuracy: 0.6224\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6912 - accuracy: 0.5262 - val_loss: 0.6691 - val_accuracy: 0.6224\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6883 - accuracy: 0.5460 - val_loss: 0.6627 - val_accuracy: 0.6212\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6899 - accuracy: 0.5392 - val_loss: 0.6641 - val_accuracy: 0.6224\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6883 - accuracy: 0.5397 - val_loss: 0.6606 - val_accuracy: 0.6224\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6875 - accuracy: 0.5467 - val_loss: 0.6581 - val_accuracy: 0.6224\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6896 - accuracy: 0.5350 - val_loss: 0.6608 - val_accuracy: 0.6224\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6861 - accuracy: 0.5555 - val_loss: 0.6563 - val_accuracy: 0.6237\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6851 - accuracy: 0.5509 - val_loss: 0.6568 - val_accuracy: 0.6237\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6860 - accuracy: 0.5489 - val_loss: 0.6569 - val_accuracy: 0.6301\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6864 - accuracy: 0.5484 - val_loss: 0.6575 - val_accuracy: 0.6314\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6870 - accuracy: 0.5495 - val_loss: 0.6561 - val_accuracy: 0.6276\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6840 - accuracy: 0.5528 - val_loss: 0.6537 - val_accuracy: 0.6352\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6846 - accuracy: 0.5489 - val_loss: 0.6536 - val_accuracy: 0.6378\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6853 - accuracy: 0.5569 - val_loss: 0.6523 - val_accuracy: 0.6365\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6844 - accuracy: 0.5536 - val_loss: 0.6534 - val_accuracy: 0.6365\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6845 - accuracy: 0.5579 - val_loss: 0.6533 - val_accuracy: 0.6365\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6822 - accuracy: 0.5629 - val_loss: 0.6507 - val_accuracy: 0.6390\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6845 - accuracy: 0.5536 - val_loss: 0.6534 - val_accuracy: 0.6390\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6829 - accuracy: 0.5623 - val_loss: 0.6529 - val_accuracy: 0.6441\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6802 - accuracy: 0.5671 - val_loss: 0.6491 - val_accuracy: 0.6416\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6812 - accuracy: 0.5620 - val_loss: 0.6495 - val_accuracy: 0.6441\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6800 - accuracy: 0.5597 - val_loss: 0.6484 - val_accuracy: 0.6429\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6807 - accuracy: 0.5630 - val_loss: 0.6464 - val_accuracy: 0.6429\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6783 - accuracy: 0.5736 - val_loss: 0.6460 - val_accuracy: 0.6429\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6780 - accuracy: 0.5739 - val_loss: 0.6459 - val_accuracy: 0.6429\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6796 - accuracy: 0.5647 - val_loss: 0.6464 - val_accuracy: 0.6416\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6789 - accuracy: 0.5672 - val_loss: 0.6447 - val_accuracy: 0.6416\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6765 - accuracy: 0.5728 - val_loss: 0.6437 - val_accuracy: 0.6390\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6759 - accuracy: 0.5806 - val_loss: 0.6442 - val_accuracy: 0.6441\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6782 - accuracy: 0.5702 - val_loss: 0.6432 - val_accuracy: 0.6429\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6730 - accuracy: 0.5793 - val_loss: 0.6411 - val_accuracy: 0.6429\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6738 - accuracy: 0.5887 - val_loss: 0.6418 - val_accuracy: 0.6416\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6750 - accuracy: 0.5801 - val_loss: 0.6413 - val_accuracy: 0.6390\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6759 - accuracy: 0.5803 - val_loss: 0.6414 - val_accuracy: 0.6390\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6749 - accuracy: 0.5752 - val_loss: 0.6422 - val_accuracy: 0.6390\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6738 - accuracy: 0.5772 - val_loss: 0.6421 - val_accuracy: 0.6416\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6753 - accuracy: 0.5787 - val_loss: 0.6423 - val_accuracy: 0.6416\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6757 - accuracy: 0.5825 - val_loss: 0.6416 - val_accuracy: 0.6416\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6755 - accuracy: 0.5805 - val_loss: 0.6402 - val_accuracy: 0.6480\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6732 - accuracy: 0.5854 - val_loss: 0.6362 - val_accuracy: 0.6454\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6711 - accuracy: 0.5923 - val_loss: 0.6389 - val_accuracy: 0.6492\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6723 - accuracy: 0.5864 - val_loss: 0.6387 - val_accuracy: 0.6492\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6705 - accuracy: 0.5868 - val_loss: 0.6394 - val_accuracy: 0.6492\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6731 - accuracy: 0.5860 - val_loss: 0.6385 - val_accuracy: 0.6492\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6668 - accuracy: 0.5942 - val_loss: 0.6348 - val_accuracy: 0.6505\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6737 - accuracy: 0.5885 - val_loss: 0.6375 - val_accuracy: 0.6518\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6708 - accuracy: 0.5887 - val_loss: 0.6391 - val_accuracy: 0.6531\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6734 - accuracy: 0.5855 - val_loss: 0.6404 - val_accuracy: 0.6492\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6707 - accuracy: 0.5942 - val_loss: 0.6395 - val_accuracy: 0.6505\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6661 - accuracy: 0.5944 - val_loss: 0.6345 - val_accuracy: 0.6480\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6722 - accuracy: 0.5901 - val_loss: 0.6370 - val_accuracy: 0.6556\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6663 - accuracy: 0.6007 - val_loss: 0.6349 - val_accuracy: 0.6531\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6713 - accuracy: 0.5893 - val_loss: 0.6381 - val_accuracy: 0.6531\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6719 - accuracy: 0.5967 - val_loss: 0.6358 - val_accuracy: 0.6556\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6716 - accuracy: 0.5888 - val_loss: 0.6392 - val_accuracy: 0.6531\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6689 - accuracy: 0.5885 - val_loss: 0.6355 - val_accuracy: 0.6531\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6702 - accuracy: 0.5873 - val_loss: 0.6364 - val_accuracy: 0.6518\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6645 - accuracy: 0.5995 - val_loss: 0.6340 - val_accuracy: 0.6531\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6690 - accuracy: 0.5947 - val_loss: 0.6346 - val_accuracy: 0.6658\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6682 - accuracy: 0.5936 - val_loss: 0.6348 - val_accuracy: 0.6747\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6648 - accuracy: 0.5992 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6626 - accuracy: 0.6093 - val_loss: 0.6319 - val_accuracy: 0.6582\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6653 - accuracy: 0.6001 - val_loss: 0.6345 - val_accuracy: 0.6607\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6644 - accuracy: 0.6035 - val_loss: 0.6322 - val_accuracy: 0.6671\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6665 - accuracy: 0.5963 - val_loss: 0.6332 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6647 - accuracy: 0.5992 - val_loss: 0.6314 - val_accuracy: 0.6709\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6636 - accuracy: 0.6066 - val_loss: 0.6296 - val_accuracy: 0.6658\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6658 - accuracy: 0.6000 - val_loss: 0.6304 - val_accuracy: 0.6671\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6654 - accuracy: 0.6021 - val_loss: 0.6315 - val_accuracy: 0.6645\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6687 - accuracy: 0.6000 - val_loss: 0.6363 - val_accuracy: 0.6658\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6655 - accuracy: 0.6036 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6654 - accuracy: 0.5997 - val_loss: 0.6297 - val_accuracy: 0.6760\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6653 - accuracy: 0.6021 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6623 - accuracy: 0.6064 - val_loss: 0.6285 - val_accuracy: 0.6709\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6602 - accuracy: 0.6083 - val_loss: 0.6275 - val_accuracy: 0.6709\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6629 - accuracy: 0.6063 - val_loss: 0.6303 - val_accuracy: 0.6735\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6649 - accuracy: 0.6019 - val_loss: 0.6291 - val_accuracy: 0.6722\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6604 - accuracy: 0.6083 - val_loss: 0.6257 - val_accuracy: 0.6722\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6620 - accuracy: 0.6040 - val_loss: 0.6273 - val_accuracy: 0.6735\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6614 - accuracy: 0.6084 - val_loss: 0.6274 - val_accuracy: 0.6722\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6624 - accuracy: 0.6084 - val_loss: 0.6281 - val_accuracy: 0.6696\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6628 - accuracy: 0.6055 - val_loss: 0.6265 - val_accuracy: 0.6735\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6617 - accuracy: 0.6114 - val_loss: 0.6256 - val_accuracy: 0.6735\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6628 - accuracy: 0.6005 - val_loss: 0.6276 - val_accuracy: 0.6709\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6572 - accuracy: 0.6177 - val_loss: 0.6258 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6579 - accuracy: 0.6093 - val_loss: 0.6253 - val_accuracy: 0.6760\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6581 - accuracy: 0.6127 - val_loss: 0.6259 - val_accuracy: 0.6709\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6619 - accuracy: 0.6094 - val_loss: 0.6272 - val_accuracy: 0.6722\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6631 - accuracy: 0.6069 - val_loss: 0.6321 - val_accuracy: 0.6722\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6613 - accuracy: 0.6107 - val_loss: 0.6258 - val_accuracy: 0.6722\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6590 - accuracy: 0.6117 - val_loss: 0.6245 - val_accuracy: 0.6773\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6617 - accuracy: 0.6078 - val_loss: 0.6255 - val_accuracy: 0.6735\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6616 - accuracy: 0.6188 - val_loss: 0.6242 - val_accuracy: 0.6747\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6613 - accuracy: 0.6073 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6615 - accuracy: 0.6076 - val_loss: 0.6277 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6571 - accuracy: 0.6191 - val_loss: 0.6198 - val_accuracy: 0.6760\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6592 - accuracy: 0.6228 - val_loss: 0.6223 - val_accuracy: 0.6773\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6605 - accuracy: 0.6119 - val_loss: 0.6277 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6572 - accuracy: 0.6109 - val_loss: 0.6197 - val_accuracy: 0.6760\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6573 - accuracy: 0.6208 - val_loss: 0.6213 - val_accuracy: 0.6747\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6581 - accuracy: 0.6117 - val_loss: 0.6238 - val_accuracy: 0.6786\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6576 - accuracy: 0.6183 - val_loss: 0.6235 - val_accuracy: 0.6798\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6598 - accuracy: 0.6117 - val_loss: 0.6231 - val_accuracy: 0.6760\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6561 - accuracy: 0.6110 - val_loss: 0.6207 - val_accuracy: 0.6773\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6584 - accuracy: 0.6091 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6558 - accuracy: 0.6163 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6584 - accuracy: 0.6132 - val_loss: 0.6187 - val_accuracy: 0.6811\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6600 - accuracy: 0.6094 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6566 - accuracy: 0.6202 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6602 - accuracy: 0.6137 - val_loss: 0.6200 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6587 - accuracy: 0.6128 - val_loss: 0.6263 - val_accuracy: 0.6837\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6534 - accuracy: 0.6217 - val_loss: 0.6229 - val_accuracy: 0.6811\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6568 - accuracy: 0.6217 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6536 - accuracy: 0.6207 - val_loss: 0.6181 - val_accuracy: 0.6798\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6562 - accuracy: 0.6164 - val_loss: 0.6226 - val_accuracy: 0.6786\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6537 - accuracy: 0.6172 - val_loss: 0.6190 - val_accuracy: 0.6811\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6539 - accuracy: 0.6173 - val_loss: 0.6161 - val_accuracy: 0.6798\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6564 - accuracy: 0.6135 - val_loss: 0.6182 - val_accuracy: 0.6811\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6560 - accuracy: 0.6217 - val_loss: 0.6239 - val_accuracy: 0.6837\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6538 - accuracy: 0.6197 - val_loss: 0.6211 - val_accuracy: 0.6798\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6529 - accuracy: 0.6198 - val_loss: 0.6162 - val_accuracy: 0.6773\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6553 - accuracy: 0.6144 - val_loss: 0.6173 - val_accuracy: 0.6811\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6550 - accuracy: 0.6183 - val_loss: 0.6171 - val_accuracy: 0.6773\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6541 - accuracy: 0.6206 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6559 - accuracy: 0.6149 - val_loss: 0.6163 - val_accuracy: 0.6786\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6528 - accuracy: 0.6287 - val_loss: 0.6158 - val_accuracy: 0.6798\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6554 - accuracy: 0.6217 - val_loss: 0.6156 - val_accuracy: 0.6773\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6552 - accuracy: 0.6174 - val_loss: 0.6166 - val_accuracy: 0.6811\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6519 - accuracy: 0.6202 - val_loss: 0.6161 - val_accuracy: 0.6798\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6528 - accuracy: 0.6209 - val_loss: 0.6183 - val_accuracy: 0.6824\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6528 - accuracy: 0.6179 - val_loss: 0.6183 - val_accuracy: 0.6824\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6543 - accuracy: 0.6220 - val_loss: 0.6172 - val_accuracy: 0.6824\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6528 - accuracy: 0.6202 - val_loss: 0.6151 - val_accuracy: 0.6824\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6527 - accuracy: 0.6246 - val_loss: 0.6227 - val_accuracy: 0.6811\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6523 - accuracy: 0.6230 - val_loss: 0.6157 - val_accuracy: 0.6786\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6543 - accuracy: 0.6191 - val_loss: 0.6149 - val_accuracy: 0.6824\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6544 - accuracy: 0.6188 - val_loss: 0.6175 - val_accuracy: 0.6824\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6545 - accuracy: 0.6227 - val_loss: 0.6177 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6533 - accuracy: 0.6215 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6484 - accuracy: 0.6206 - val_loss: 0.6147 - val_accuracy: 0.6824\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6500 - accuracy: 0.6281 - val_loss: 0.6144 - val_accuracy: 0.6786\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6512 - accuracy: 0.6231 - val_loss: 0.6148 - val_accuracy: 0.6811\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6507 - accuracy: 0.6246 - val_loss: 0.6151 - val_accuracy: 0.6837\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6489 - accuracy: 0.6236 - val_loss: 0.6124 - val_accuracy: 0.6824\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6482 - accuracy: 0.6261 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6527 - accuracy: 0.6211 - val_loss: 0.6122 - val_accuracy: 0.6824\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6487 - accuracy: 0.6261 - val_loss: 0.6131 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6521 - accuracy: 0.6279 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6498 - accuracy: 0.6305 - val_loss: 0.6132 - val_accuracy: 0.6798\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6493 - accuracy: 0.6300 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6498 - accuracy: 0.6276 - val_loss: 0.6148 - val_accuracy: 0.6824\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6496 - accuracy: 0.6260 - val_loss: 0.6148 - val_accuracy: 0.6824\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6492 - accuracy: 0.6264 - val_loss: 0.6119 - val_accuracy: 0.6849\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6482 - accuracy: 0.6326 - val_loss: 0.6118 - val_accuracy: 0.6849\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6517 - accuracy: 0.6290 - val_loss: 0.6117 - val_accuracy: 0.6824\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6499 - accuracy: 0.6279 - val_loss: 0.6116 - val_accuracy: 0.6837\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6495 - accuracy: 0.6309 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6508 - accuracy: 0.6250 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6509 - accuracy: 0.6264 - val_loss: 0.6157 - val_accuracy: 0.6798\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6477 - accuracy: 0.6335 - val_loss: 0.6129 - val_accuracy: 0.6811\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6486 - accuracy: 0.6272 - val_loss: 0.6129 - val_accuracy: 0.6773\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6466 - accuracy: 0.6326 - val_loss: 0.6103 - val_accuracy: 0.6811\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6429 - accuracy: 0.6368 - val_loss: 0.6094 - val_accuracy: 0.6811\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6430 - accuracy: 0.6334 - val_loss: 0.6119 - val_accuracy: 0.6786\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6477 - accuracy: 0.6266 - val_loss: 0.6156 - val_accuracy: 0.6824\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6483 - accuracy: 0.6246 - val_loss: 0.6130 - val_accuracy: 0.6811\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6475 - accuracy: 0.6356 - val_loss: 0.6113 - val_accuracy: 0.6798\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6495 - accuracy: 0.6246 - val_loss: 0.6153 - val_accuracy: 0.6824\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6470 - accuracy: 0.6309 - val_loss: 0.6111 - val_accuracy: 0.6837\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6440 - accuracy: 0.6309 - val_loss: 0.6124 - val_accuracy: 0.6811\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6462 - accuracy: 0.6324 - val_loss: 0.6113 - val_accuracy: 0.6824\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6479 - accuracy: 0.6323 - val_loss: 0.6152 - val_accuracy: 0.6849\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6457 - accuracy: 0.6295 - val_loss: 0.6124 - val_accuracy: 0.6862\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6442 - accuracy: 0.6318 - val_loss: 0.6112 - val_accuracy: 0.6849\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6459 - accuracy: 0.6345 - val_loss: 0.6113 - val_accuracy: 0.6837\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6461 - accuracy: 0.6291 - val_loss: 0.6092 - val_accuracy: 0.6875\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6438 - accuracy: 0.6348 - val_loss: 0.6107 - val_accuracy: 0.6837\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6447 - accuracy: 0.6380 - val_loss: 0.6118 - val_accuracy: 0.6837\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6425 - accuracy: 0.6339 - val_loss: 0.6101 - val_accuracy: 0.6824\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6494 - accuracy: 0.6237 - val_loss: 0.6123 - val_accuracy: 0.6837\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6427 - accuracy: 0.6405 - val_loss: 0.6117 - val_accuracy: 0.6837\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6484 - accuracy: 0.6358 - val_loss: 0.6118 - val_accuracy: 0.6811\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6439 - accuracy: 0.6368 - val_loss: 0.6089 - val_accuracy: 0.6811\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6437 - accuracy: 0.6344 - val_loss: 0.6086 - val_accuracy: 0.6862\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6452 - accuracy: 0.6392 - val_loss: 0.6094 - val_accuracy: 0.6888\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6434 - accuracy: 0.6408 - val_loss: 0.6098 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6464 - accuracy: 0.6346 - val_loss: 0.6118 - val_accuracy: 0.6875\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6434 - accuracy: 0.6361 - val_loss: 0.6104 - val_accuracy: 0.6849\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6415 - accuracy: 0.6368 - val_loss: 0.6110 - val_accuracy: 0.6888\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6436 - accuracy: 0.6331 - val_loss: 0.6090 - val_accuracy: 0.6875\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6452 - accuracy: 0.6356 - val_loss: 0.6096 - val_accuracy: 0.6862\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6442 - accuracy: 0.6372 - val_loss: 0.6117 - val_accuracy: 0.6811\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6479 - accuracy: 0.6318 - val_loss: 0.6114 - val_accuracy: 0.6875\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6420 - accuracy: 0.6355 - val_loss: 0.6085 - val_accuracy: 0.6901\n"
     ]
    }
   ],
   "source": [
    "# Lets explore some parameters:\n",
    "scores = {}\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30) # Early stop will stop training when the model has failed to improve\n",
    "\n",
    "dense1 = [700, 850, 1000]\n",
    "dense2 = [100, 250, 400]\n",
    "dropout = [0.5, 0.7, 0.9]\n",
    "optimizer = ['sgd']\n",
    "\n",
    "best_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "for d1 in dense1:\n",
    "    for d2 in dense2:\n",
    "        for dr in dropout:\n",
    "            for opt in optimizer:\n",
    "                print('Calculating for:', d1, d2, dr, opt)\n",
    "                keras_model = init_keras_model(dense1=d1, dense2=d2, dropout=dr, optimizer=opt)\n",
    "                keras_model.fit(X_train_tensor, \n",
    "                                y_train_tensor, \n",
    "                                epochs=n_epochs, \n",
    "                                validation_data=(X_val_tensor, y_val_tensor),\n",
    "                                callbacks=[es] \n",
    "                                )\n",
    "                score = keras_model.evaluate(X_test_tensor, y_test_tensor, verbose=0)[1]\n",
    "                scores[(d1, d2, dr, opt)] = score\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_params = {'dense1':d1, 'dense2':d2, 'dropout':dr, 'optimizer':opt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(700, 100, 0.5, 'sgd'): 0.6743295192718506, (700, 100, 0.7, 'sgd'): 0.6858237385749817, (700, 100, 0.9, 'sgd'): 0.6692209243774414, (700, 250, 0.5, 'sgd'): 0.671775221824646, (700, 250, 0.7, 'sgd'): 0.6615580916404724, (700, 250, 0.9, 'sgd'): 0.6704980731010437, (700, 400, 0.5, 'sgd'): 0.6730523705482483, (700, 400, 0.7, 'sgd'): 0.6628352403640747, (700, 400, 0.9, 'sgd'): 0.6692209243774414, (850, 100, 0.5, 'sgd'): 0.6743295192718506, (850, 100, 0.7, 'sgd'): 0.6602809429168701, (850, 100, 0.9, 'sgd'): 0.6743295192718506, (850, 250, 0.5, 'sgd'): 0.6513410210609436, (850, 250, 0.7, 'sgd'): 0.679438054561615, (850, 250, 0.9, 'sgd'): 0.671775221824646, (850, 400, 0.5, 'sgd'): 0.6602809429168701, (850, 400, 0.7, 'sgd'): 0.6704980731010437, (850, 400, 0.9, 'sgd'): 0.6756066679954529, (1000, 100, 0.5, 'sgd'): 0.6679438352584839, (1000, 100, 0.7, 'sgd'): 0.6666666865348816, (1000, 100, 0.9, 'sgd'): 0.6730523705482483, (1000, 250, 0.5, 'sgd'): 0.679438054561615, (1000, 250, 0.7, 'sgd'): 0.6768837571144104, (1000, 250, 0.9, 'sgd'): 0.679438054561615, (1000, 400, 0.5, 'sgd'): 0.656449556350708, (1000, 400, 0.7, 'sgd'): 0.6832695007324219, (1000, 400, 0.9, 'sgd'): 0.6768837571144104}\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'dense1': 700, 'dense2': 100, 'dropout': 0.7, 'optimizer': 'sgd'}, best_score: 0.6858237385749817\n"
     ]
    }
   ],
   "source": [
    "print(f'best_params: {best_params}, best_score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating for: 650 50 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_240 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8221 - accuracy: 0.5309 - val_loss: 0.6546 - val_accuracy: 0.6467\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7168 - accuracy: 0.5504 - val_loss: 0.6543 - val_accuracy: 0.6454\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6942 - accuracy: 0.5605 - val_loss: 0.6454 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6806 - accuracy: 0.5835 - val_loss: 0.6437 - val_accuracy: 0.6607\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5816 - val_loss: 0.6476 - val_accuracy: 0.6543\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5867 - val_loss: 0.6417 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6681 - accuracy: 0.5956 - val_loss: 0.6451 - val_accuracy: 0.6645\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5944 - val_loss: 0.6425 - val_accuracy: 0.6633\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6081 - val_loss: 0.6371 - val_accuracy: 0.6671\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6084 - val_loss: 0.6369 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6094 - val_loss: 0.6402 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6042 - val_loss: 0.6389 - val_accuracy: 0.6658\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6564 - accuracy: 0.6110 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6076 - val_loss: 0.6358 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6207 - val_loss: 0.6359 - val_accuracy: 0.6620\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6149 - val_loss: 0.6328 - val_accuracy: 0.6760\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6222 - val_loss: 0.6268 - val_accuracy: 0.6747\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6248 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6207 - val_loss: 0.6339 - val_accuracy: 0.6531\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6306 - val_loss: 0.6329 - val_accuracy: 0.6786\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6257 - val_loss: 0.6329 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6483 - accuracy: 0.6284 - val_loss: 0.6305 - val_accuracy: 0.6747\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6266 - val_loss: 0.6280 - val_accuracy: 0.6671\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6277 - val_loss: 0.6280 - val_accuracy: 0.6824\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6307 - val_loss: 0.6270 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6237 - val_loss: 0.6319 - val_accuracy: 0.6760\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.6315 - val_loss: 0.6264 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6442 - accuracy: 0.6324 - val_loss: 0.6304 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6427 - accuracy: 0.6363 - val_loss: 0.6289 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6407 - accuracy: 0.6335 - val_loss: 0.6212 - val_accuracy: 0.6913\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6410 - accuracy: 0.6413 - val_loss: 0.6224 - val_accuracy: 0.6901\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6393 - val_loss: 0.6287 - val_accuracy: 0.6684\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6396 - accuracy: 0.6309 - val_loss: 0.6268 - val_accuracy: 0.6684\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6358 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6427 - val_loss: 0.6207 - val_accuracy: 0.6964\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6423 - val_loss: 0.6150 - val_accuracy: 0.6977\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6394 - accuracy: 0.6382 - val_loss: 0.6176 - val_accuracy: 0.6913\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6367 - accuracy: 0.6402 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6325 - accuracy: 0.6508 - val_loss: 0.6179 - val_accuracy: 0.6849\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6462 - val_loss: 0.6192 - val_accuracy: 0.6773\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6347 - accuracy: 0.6454 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6388 - accuracy: 0.6417 - val_loss: 0.6253 - val_accuracy: 0.6786\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6492 - val_loss: 0.6253 - val_accuracy: 0.6735\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6361 - accuracy: 0.6456 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6344 - accuracy: 0.6472 - val_loss: 0.6220 - val_accuracy: 0.6837\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6306 - accuracy: 0.6483 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6534 - val_loss: 0.6193 - val_accuracy: 0.6875\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6523 - val_loss: 0.6185 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6454 - val_loss: 0.6195 - val_accuracy: 0.6901\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6316 - accuracy: 0.6477 - val_loss: 0.6218 - val_accuracy: 0.6824\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6523 - val_loss: 0.6228 - val_accuracy: 0.6773\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6295 - accuracy: 0.6532 - val_loss: 0.6237 - val_accuracy: 0.6849\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6308 - accuracy: 0.6531 - val_loss: 0.6232 - val_accuracy: 0.6747\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6557 - val_loss: 0.6164 - val_accuracy: 0.6901\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6539 - val_loss: 0.6188 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6294 - accuracy: 0.6466 - val_loss: 0.6211 - val_accuracy: 0.6875\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6576 - val_loss: 0.6256 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6557 - val_loss: 0.6186 - val_accuracy: 0.6811\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6231 - accuracy: 0.6616 - val_loss: 0.6144 - val_accuracy: 0.6926\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6270 - accuracy: 0.6546 - val_loss: 0.6215 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6593 - val_loss: 0.6271 - val_accuracy: 0.6837\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6510 - val_loss: 0.6246 - val_accuracy: 0.6849\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6222 - accuracy: 0.6606 - val_loss: 0.6233 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6185 - accuracy: 0.6643 - val_loss: 0.6236 - val_accuracy: 0.6926\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6166 - accuracy: 0.6658 - val_loss: 0.6212 - val_accuracy: 0.6939\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6559 - val_loss: 0.6234 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6213 - accuracy: 0.6591 - val_loss: 0.6230 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6139 - accuracy: 0.6605 - val_loss: 0.6251 - val_accuracy: 0.6735\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6147 - accuracy: 0.6704 - val_loss: 0.6260 - val_accuracy: 0.6849\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6173 - accuracy: 0.6682 - val_loss: 0.6219 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6608 - val_loss: 0.6206 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6129 - accuracy: 0.6647 - val_loss: 0.6223 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6706 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6118 - accuracy: 0.6694 - val_loss: 0.6240 - val_accuracy: 0.6849\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6709 - val_loss: 0.6213 - val_accuracy: 0.6824\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.6678 - val_loss: 0.6199 - val_accuracy: 0.6939\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6648 - val_loss: 0.6254 - val_accuracy: 0.6786\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6093 - accuracy: 0.6645 - val_loss: 0.6230 - val_accuracy: 0.6837\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6682 - val_loss: 0.6258 - val_accuracy: 0.6735\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6611 - val_loss: 0.6249 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6722 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6068 - accuracy: 0.6723 - val_loss: 0.6197 - val_accuracy: 0.6837\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6709 - val_loss: 0.6241 - val_accuracy: 0.6786\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6109 - accuracy: 0.6702 - val_loss: 0.6255 - val_accuracy: 0.6811\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6017 - accuracy: 0.6792 - val_loss: 0.6249 - val_accuracy: 0.6798\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6037 - accuracy: 0.6746 - val_loss: 0.6249 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6827 - val_loss: 0.6238 - val_accuracy: 0.6888\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6044 - accuracy: 0.6728 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6041 - accuracy: 0.6738 - val_loss: 0.6236 - val_accuracy: 0.6837\n",
      "Calculating for: 650 50 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_244 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8211 - accuracy: 0.5214 - val_loss: 0.6578 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7249 - accuracy: 0.5441 - val_loss: 0.6572 - val_accuracy: 0.6543\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6974 - accuracy: 0.5516 - val_loss: 0.6519 - val_accuracy: 0.6633\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6854 - accuracy: 0.5625 - val_loss: 0.6523 - val_accuracy: 0.6633\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6813 - accuracy: 0.5620 - val_loss: 0.6480 - val_accuracy: 0.6671\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5883 - val_loss: 0.6416 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6763 - accuracy: 0.5823 - val_loss: 0.6462 - val_accuracy: 0.6658\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.5853 - val_loss: 0.6451 - val_accuracy: 0.6709\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5918 - val_loss: 0.6471 - val_accuracy: 0.6760\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5936 - val_loss: 0.6398 - val_accuracy: 0.6722\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5888 - val_loss: 0.6386 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5934 - val_loss: 0.6390 - val_accuracy: 0.6760\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.5987 - val_loss: 0.6347 - val_accuracy: 0.6798\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6652 - accuracy: 0.6035 - val_loss: 0.6358 - val_accuracy: 0.6786\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.5958 - val_loss: 0.6401 - val_accuracy: 0.6760\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6011 - val_loss: 0.6417 - val_accuracy: 0.6722\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6600 - accuracy: 0.6076 - val_loss: 0.6322 - val_accuracy: 0.6862\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6093 - val_loss: 0.6344 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6107 - val_loss: 0.6421 - val_accuracy: 0.6684\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6152 - val_loss: 0.6347 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6203 - val_loss: 0.6291 - val_accuracy: 0.6875\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6591 - accuracy: 0.6078 - val_loss: 0.6325 - val_accuracy: 0.6798\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6194 - val_loss: 0.6284 - val_accuracy: 0.6837\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6619 - accuracy: 0.6145 - val_loss: 0.6336 - val_accuracy: 0.6862\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6554 - accuracy: 0.6168 - val_loss: 0.6338 - val_accuracy: 0.6811\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6553 - accuracy: 0.6144 - val_loss: 0.6302 - val_accuracy: 0.6837\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6155 - val_loss: 0.6322 - val_accuracy: 0.6849\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6568 - accuracy: 0.6138 - val_loss: 0.6318 - val_accuracy: 0.6913\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6124 - val_loss: 0.6334 - val_accuracy: 0.6875\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6559 - accuracy: 0.6095 - val_loss: 0.6311 - val_accuracy: 0.6849\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6230 - val_loss: 0.6275 - val_accuracy: 0.6837\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6220 - val_loss: 0.6250 - val_accuracy: 0.6824\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6518 - accuracy: 0.6264 - val_loss: 0.6290 - val_accuracy: 0.6901\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6252 - val_loss: 0.6283 - val_accuracy: 0.6888\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6526 - accuracy: 0.6238 - val_loss: 0.6263 - val_accuracy: 0.6913\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6276 - val_loss: 0.6295 - val_accuracy: 0.6901\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6508 - accuracy: 0.6265 - val_loss: 0.6233 - val_accuracy: 0.6913\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6236 - val_loss: 0.6315 - val_accuracy: 0.6888\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6515 - accuracy: 0.6296 - val_loss: 0.6221 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6493 - accuracy: 0.6252 - val_loss: 0.6206 - val_accuracy: 0.6913\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6467 - accuracy: 0.6346 - val_loss: 0.6157 - val_accuracy: 0.6952\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6306 - val_loss: 0.6220 - val_accuracy: 0.6926\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6346 - val_loss: 0.6239 - val_accuracy: 0.6926\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6496 - accuracy: 0.6243 - val_loss: 0.6257 - val_accuracy: 0.6939\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.6296 - val_loss: 0.6221 - val_accuracy: 0.6913\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6309 - val_loss: 0.6216 - val_accuracy: 0.6926\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6462 - accuracy: 0.6338 - val_loss: 0.6251 - val_accuracy: 0.6926\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6474 - accuracy: 0.6299 - val_loss: 0.6223 - val_accuracy: 0.6901\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6389 - val_loss: 0.6190 - val_accuracy: 0.7015\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6333 - val_loss: 0.6208 - val_accuracy: 0.6901\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6453 - accuracy: 0.6289 - val_loss: 0.6188 - val_accuracy: 0.7003\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6463 - accuracy: 0.6343 - val_loss: 0.6204 - val_accuracy: 0.7041\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6379 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6370 - val_loss: 0.6175 - val_accuracy: 0.7003\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6419 - accuracy: 0.6442 - val_loss: 0.6178 - val_accuracy: 0.6990\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6377 - val_loss: 0.6251 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6397 - val_loss: 0.6248 - val_accuracy: 0.6862\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6404 - accuracy: 0.6359 - val_loss: 0.6208 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6431 - accuracy: 0.6356 - val_loss: 0.6240 - val_accuracy: 0.6888\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6481 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6412 - accuracy: 0.6456 - val_loss: 0.6179 - val_accuracy: 0.6977\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6422 - val_loss: 0.6206 - val_accuracy: 0.6888\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6373 - accuracy: 0.6423 - val_loss: 0.6218 - val_accuracy: 0.6875\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6439 - val_loss: 0.6190 - val_accuracy: 0.6862\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6330 - accuracy: 0.6433 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6385 - accuracy: 0.6453 - val_loss: 0.6218 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6347 - accuracy: 0.6437 - val_loss: 0.6177 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6451 - val_loss: 0.6166 - val_accuracy: 0.6952\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6360 - accuracy: 0.6488 - val_loss: 0.6186 - val_accuracy: 0.6964\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6527 - val_loss: 0.6148 - val_accuracy: 0.6977\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6320 - accuracy: 0.6501 - val_loss: 0.6159 - val_accuracy: 0.6926\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6491 - val_loss: 0.6164 - val_accuracy: 0.6926\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6300 - accuracy: 0.6544 - val_loss: 0.6177 - val_accuracy: 0.6926\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6518 - val_loss: 0.6164 - val_accuracy: 0.6939\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6529 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6311 - accuracy: 0.6486 - val_loss: 0.6214 - val_accuracy: 0.6875\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6323 - accuracy: 0.6550 - val_loss: 0.6169 - val_accuracy: 0.6926\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6506 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6508 - val_loss: 0.6161 - val_accuracy: 0.6939\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6319 - accuracy: 0.6565 - val_loss: 0.6217 - val_accuracy: 0.6913\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6274 - accuracy: 0.6546 - val_loss: 0.6119 - val_accuracy: 0.6913\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6338 - accuracy: 0.6444 - val_loss: 0.6167 - val_accuracy: 0.6939\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6510 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6333 - accuracy: 0.6498 - val_loss: 0.6180 - val_accuracy: 0.6901\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6540 - val_loss: 0.6192 - val_accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6567 - val_loss: 0.6161 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6251 - accuracy: 0.6588 - val_loss: 0.6178 - val_accuracy: 0.6824\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6240 - accuracy: 0.6545 - val_loss: 0.6178 - val_accuracy: 0.6811\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6303 - accuracy: 0.6589 - val_loss: 0.6166 - val_accuracy: 0.6824\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6525 - val_loss: 0.6143 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6257 - accuracy: 0.6603 - val_loss: 0.6141 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6575 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6560 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6576 - val_loss: 0.6143 - val_accuracy: 0.6837\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6556 - val_loss: 0.6140 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6256 - accuracy: 0.6545 - val_loss: 0.6167 - val_accuracy: 0.6888\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6248 - accuracy: 0.6583 - val_loss: 0.6174 - val_accuracy: 0.6837\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6241 - accuracy: 0.6579 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6674 - val_loss: 0.6131 - val_accuracy: 0.6977\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6561 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6649 - val_loss: 0.6156 - val_accuracy: 0.6926\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6204 - accuracy: 0.6628 - val_loss: 0.6140 - val_accuracy: 0.6913\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6625 - val_loss: 0.6137 - val_accuracy: 0.6901\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6194 - accuracy: 0.6728 - val_loss: 0.6106 - val_accuracy: 0.6926\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6654 - val_loss: 0.6142 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6626 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6536 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6176 - accuracy: 0.6600 - val_loss: 0.6148 - val_accuracy: 0.6875\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6638 - val_loss: 0.6139 - val_accuracy: 0.6926\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6232 - accuracy: 0.6611 - val_loss: 0.6129 - val_accuracy: 0.6990\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6174 - accuracy: 0.6647 - val_loss: 0.6135 - val_accuracy: 0.6977\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6188 - accuracy: 0.6620 - val_loss: 0.6120 - val_accuracy: 0.6913\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6680 - val_loss: 0.6178 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6694 - val_loss: 0.6125 - val_accuracy: 0.6952\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6674 - val_loss: 0.6154 - val_accuracy: 0.6952\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6655 - val_loss: 0.6124 - val_accuracy: 0.6964\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6166 - accuracy: 0.6652 - val_loss: 0.6188 - val_accuracy: 0.6913\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6687 - val_loss: 0.6156 - val_accuracy: 0.6913\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.6704 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6659 - val_loss: 0.6149 - val_accuracy: 0.6939\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6624 - val_loss: 0.6171 - val_accuracy: 0.6926\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6120 - accuracy: 0.6689 - val_loss: 0.6147 - val_accuracy: 0.6901\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6138 - accuracy: 0.6721 - val_loss: 0.6127 - val_accuracy: 0.6862\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6139 - accuracy: 0.6734 - val_loss: 0.6181 - val_accuracy: 0.6901\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6119 - accuracy: 0.6738 - val_loss: 0.6129 - val_accuracy: 0.6888\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6113 - accuracy: 0.6738 - val_loss: 0.6152 - val_accuracy: 0.6837\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6097 - accuracy: 0.6702 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6123 - accuracy: 0.6672 - val_loss: 0.6159 - val_accuracy: 0.6798\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6152 - accuracy: 0.6717 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6101 - accuracy: 0.6742 - val_loss: 0.6184 - val_accuracy: 0.6862\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6111 - accuracy: 0.6731 - val_loss: 0.6147 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6131 - accuracy: 0.6654 - val_loss: 0.6161 - val_accuracy: 0.6837\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6094 - accuracy: 0.6799 - val_loss: 0.6201 - val_accuracy: 0.6837\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6089 - accuracy: 0.6703 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Calculating for: 650 50 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_248 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8191 - accuracy: 0.5112 - val_loss: 0.6613 - val_accuracy: 0.6237\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7258 - accuracy: 0.5280 - val_loss: 0.6625 - val_accuracy: 0.6276\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7045 - accuracy: 0.5295 - val_loss: 0.6638 - val_accuracy: 0.6327\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6928 - accuracy: 0.5383 - val_loss: 0.6589 - val_accuracy: 0.6429\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5497 - val_loss: 0.6620 - val_accuracy: 0.6441\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6865 - accuracy: 0.5475 - val_loss: 0.6624 - val_accuracy: 0.6505\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6851 - accuracy: 0.5481 - val_loss: 0.6601 - val_accuracy: 0.6454\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6833 - accuracy: 0.5582 - val_loss: 0.6605 - val_accuracy: 0.6429\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6829 - accuracy: 0.5540 - val_loss: 0.6644 - val_accuracy: 0.6620\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.5578 - val_loss: 0.6613 - val_accuracy: 0.6582\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5604 - val_loss: 0.6557 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6798 - accuracy: 0.5668 - val_loss: 0.6527 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.5673 - val_loss: 0.6524 - val_accuracy: 0.6569\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5766 - val_loss: 0.6531 - val_accuracy: 0.6607\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5764 - val_loss: 0.6496 - val_accuracy: 0.6492\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6752 - accuracy: 0.5772 - val_loss: 0.6510 - val_accuracy: 0.6582\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6751 - accuracy: 0.5787 - val_loss: 0.6525 - val_accuracy: 0.6594\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6732 - accuracy: 0.5864 - val_loss: 0.6501 - val_accuracy: 0.6594\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6750 - accuracy: 0.5815 - val_loss: 0.6537 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5774 - val_loss: 0.6448 - val_accuracy: 0.6582\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5801 - val_loss: 0.6448 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6710 - accuracy: 0.5863 - val_loss: 0.6500 - val_accuracy: 0.6658\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5880 - val_loss: 0.6377 - val_accuracy: 0.6569\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6702 - accuracy: 0.5907 - val_loss: 0.6423 - val_accuracy: 0.6671\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6708 - accuracy: 0.5934 - val_loss: 0.6440 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6704 - accuracy: 0.5864 - val_loss: 0.6422 - val_accuracy: 0.6709\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5960 - val_loss: 0.6484 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6705 - accuracy: 0.5956 - val_loss: 0.6423 - val_accuracy: 0.6722\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5841 - val_loss: 0.6471 - val_accuracy: 0.6760\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6651 - accuracy: 0.5985 - val_loss: 0.6454 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6679 - accuracy: 0.5892 - val_loss: 0.6425 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6677 - accuracy: 0.5968 - val_loss: 0.6435 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5968 - val_loss: 0.6382 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5918 - val_loss: 0.6369 - val_accuracy: 0.6722\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6675 - accuracy: 0.5957 - val_loss: 0.6418 - val_accuracy: 0.6773\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6629 - accuracy: 0.6032 - val_loss: 0.6403 - val_accuracy: 0.6735\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6658 - accuracy: 0.6065 - val_loss: 0.6438 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.5978 - val_loss: 0.6373 - val_accuracy: 0.6709\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6069 - val_loss: 0.6395 - val_accuracy: 0.6747\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6017 - val_loss: 0.6388 - val_accuracy: 0.6786\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6622 - accuracy: 0.6035 - val_loss: 0.6396 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6649 - accuracy: 0.5990 - val_loss: 0.6393 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6054 - val_loss: 0.6412 - val_accuracy: 0.6722\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6047 - val_loss: 0.6354 - val_accuracy: 0.6837\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6086 - val_loss: 0.6352 - val_accuracy: 0.6786\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6125 - val_loss: 0.6358 - val_accuracy: 0.6747\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6088 - val_loss: 0.6385 - val_accuracy: 0.6824\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6623 - accuracy: 0.6107 - val_loss: 0.6411 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6124 - val_loss: 0.6328 - val_accuracy: 0.6747\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6610 - accuracy: 0.6150 - val_loss: 0.6386 - val_accuracy: 0.6811\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6130 - val_loss: 0.6341 - val_accuracy: 0.6824\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6598 - accuracy: 0.6115 - val_loss: 0.6332 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6061 - val_loss: 0.6363 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6147 - val_loss: 0.6383 - val_accuracy: 0.6722\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6589 - accuracy: 0.6158 - val_loss: 0.6337 - val_accuracy: 0.6786\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6089 - val_loss: 0.6326 - val_accuracy: 0.6735\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6613 - accuracy: 0.6027 - val_loss: 0.6339 - val_accuracy: 0.6773\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6173 - val_loss: 0.6336 - val_accuracy: 0.6837\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6130 - val_loss: 0.6334 - val_accuracy: 0.6837\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6564 - accuracy: 0.6167 - val_loss: 0.6298 - val_accuracy: 0.6747\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6601 - accuracy: 0.6099 - val_loss: 0.6380 - val_accuracy: 0.6862\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6110 - val_loss: 0.6378 - val_accuracy: 0.6875\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6144 - val_loss: 0.6360 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6572 - accuracy: 0.6118 - val_loss: 0.6356 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6113 - val_loss: 0.6304 - val_accuracy: 0.6811\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6294 - val_loss: 0.6328 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6600 - accuracy: 0.6153 - val_loss: 0.6328 - val_accuracy: 0.6875\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6137 - val_loss: 0.6286 - val_accuracy: 0.6798\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6230 - val_loss: 0.6302 - val_accuracy: 0.6849\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6218 - val_loss: 0.6264 - val_accuracy: 0.6862\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6550 - accuracy: 0.6245 - val_loss: 0.6272 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6208 - val_loss: 0.6334 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6184 - val_loss: 0.6408 - val_accuracy: 0.6531\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6558 - accuracy: 0.6243 - val_loss: 0.6341 - val_accuracy: 0.6824\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6212 - val_loss: 0.6350 - val_accuracy: 0.6875\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6532 - accuracy: 0.6201 - val_loss: 0.6293 - val_accuracy: 0.6862\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6232 - val_loss: 0.6301 - val_accuracy: 0.6798\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6188 - val_loss: 0.6270 - val_accuracy: 0.6798\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6196 - val_loss: 0.6305 - val_accuracy: 0.6888\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6226 - val_loss: 0.6258 - val_accuracy: 0.6837\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6546 - accuracy: 0.6230 - val_loss: 0.6348 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6221 - val_loss: 0.6334 - val_accuracy: 0.6875\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6553 - accuracy: 0.6204 - val_loss: 0.6323 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6199 - val_loss: 0.6261 - val_accuracy: 0.6849\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6246 - val_loss: 0.6295 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6503 - accuracy: 0.6264 - val_loss: 0.6298 - val_accuracy: 0.6837\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6510 - accuracy: 0.6251 - val_loss: 0.6272 - val_accuracy: 0.6837\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6504 - accuracy: 0.6262 - val_loss: 0.6230 - val_accuracy: 0.6888\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6255 - val_loss: 0.6251 - val_accuracy: 0.6875\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6513 - accuracy: 0.6246 - val_loss: 0.6317 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6265 - val_loss: 0.6308 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6307 - val_loss: 0.6264 - val_accuracy: 0.6875\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6513 - accuracy: 0.6247 - val_loss: 0.6331 - val_accuracy: 0.6888\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6312 - val_loss: 0.6252 - val_accuracy: 0.6811\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6299 - val_loss: 0.6251 - val_accuracy: 0.6824\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6502 - accuracy: 0.6284 - val_loss: 0.6229 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6487 - accuracy: 0.6363 - val_loss: 0.6251 - val_accuracy: 0.6862\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6487 - accuracy: 0.6295 - val_loss: 0.6268 - val_accuracy: 0.6849\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6272 - val_loss: 0.6256 - val_accuracy: 0.6862\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6492 - accuracy: 0.6335 - val_loss: 0.6284 - val_accuracy: 0.6875\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6479 - accuracy: 0.6311 - val_loss: 0.6270 - val_accuracy: 0.6901\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6499 - accuracy: 0.6315 - val_loss: 0.6273 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6328 - val_loss: 0.6308 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6294 - val_loss: 0.6290 - val_accuracy: 0.6913\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6473 - accuracy: 0.6314 - val_loss: 0.6273 - val_accuracy: 0.6888\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6294 - val_loss: 0.6270 - val_accuracy: 0.6939\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6316 - val_loss: 0.6205 - val_accuracy: 0.6824\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6277 - val_loss: 0.6276 - val_accuracy: 0.6837\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6457 - accuracy: 0.6296 - val_loss: 0.6230 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6349 - val_loss: 0.6302 - val_accuracy: 0.6849\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6393 - val_loss: 0.6264 - val_accuracy: 0.6849\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6339 - val_loss: 0.6269 - val_accuracy: 0.6849\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6305 - val_loss: 0.6201 - val_accuracy: 0.6773\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6282 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6370 - val_loss: 0.6232 - val_accuracy: 0.6913\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6323 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6370 - val_loss: 0.6264 - val_accuracy: 0.6888\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6448 - accuracy: 0.6340 - val_loss: 0.6266 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6349 - val_loss: 0.6184 - val_accuracy: 0.6811\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6389 - val_loss: 0.6233 - val_accuracy: 0.6811\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6420 - accuracy: 0.6382 - val_loss: 0.6244 - val_accuracy: 0.6862\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6330 - val_loss: 0.6304 - val_accuracy: 0.6849\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6331 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6436 - accuracy: 0.6387 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6385 - val_loss: 0.6251 - val_accuracy: 0.6862\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6378 - val_loss: 0.6153 - val_accuracy: 0.6811\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6393 - val_loss: 0.6251 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6335 - val_loss: 0.6340 - val_accuracy: 0.6773\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6379 - val_loss: 0.6206 - val_accuracy: 0.6837\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6354 - val_loss: 0.6249 - val_accuracy: 0.6837\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6393 - val_loss: 0.6274 - val_accuracy: 0.6901\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6414 - val_loss: 0.6165 - val_accuracy: 0.6849\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6270 - val_loss: 0.6193 - val_accuracy: 0.6875\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6408 - val_loss: 0.6205 - val_accuracy: 0.6862\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6339 - val_loss: 0.6220 - val_accuracy: 0.6849\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6446 - val_loss: 0.6234 - val_accuracy: 0.6837\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6418 - val_loss: 0.6191 - val_accuracy: 0.6849\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6407 - val_loss: 0.6271 - val_accuracy: 0.6837\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6375 - val_loss: 0.6235 - val_accuracy: 0.6862\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6392 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6389 - val_loss: 0.6193 - val_accuracy: 0.6901\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6458 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6380 - val_loss: 0.6211 - val_accuracy: 0.6964\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6512 - val_loss: 0.6174 - val_accuracy: 0.6952\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6375 - val_loss: 0.6190 - val_accuracy: 0.6913\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6356 - accuracy: 0.6485 - val_loss: 0.6181 - val_accuracy: 0.6888\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6387 - accuracy: 0.6468 - val_loss: 0.6142 - val_accuracy: 0.6913\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6462 - val_loss: 0.6153 - val_accuracy: 0.6875\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6453 - val_loss: 0.6201 - val_accuracy: 0.6875\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6443 - accuracy: 0.6477 - val_loss: 0.6230 - val_accuracy: 0.6875\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6391 - accuracy: 0.6473 - val_loss: 0.6158 - val_accuracy: 0.6952\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6393 - accuracy: 0.6463 - val_loss: 0.6219 - val_accuracy: 0.6926\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6453 - val_loss: 0.6202 - val_accuracy: 0.6926\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6476 - val_loss: 0.6176 - val_accuracy: 0.6977\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6449 - val_loss: 0.6192 - val_accuracy: 0.6939\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6472 - val_loss: 0.6215 - val_accuracy: 0.6939\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6438 - accuracy: 0.6412 - val_loss: 0.6178 - val_accuracy: 0.6964\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6462 - val_loss: 0.6150 - val_accuracy: 0.7003\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6444 - val_loss: 0.6299 - val_accuracy: 0.6773\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6531 - val_loss: 0.6195 - val_accuracy: 0.6952\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6343 - accuracy: 0.6516 - val_loss: 0.6198 - val_accuracy: 0.6952\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6363 - accuracy: 0.6466 - val_loss: 0.6174 - val_accuracy: 0.7054\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6409 - val_loss: 0.6151 - val_accuracy: 0.7028\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6378 - accuracy: 0.6423 - val_loss: 0.6161 - val_accuracy: 0.6977\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6531 - val_loss: 0.6205 - val_accuracy: 0.6926\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6356 - accuracy: 0.6500 - val_loss: 0.6178 - val_accuracy: 0.6913\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6510 - val_loss: 0.6155 - val_accuracy: 0.7041\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6300 - accuracy: 0.6545 - val_loss: 0.6118 - val_accuracy: 0.7066\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6590 - val_loss: 0.6123 - val_accuracy: 0.7028\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6338 - accuracy: 0.6520 - val_loss: 0.6158 - val_accuracy: 0.7003\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6532 - val_loss: 0.6141 - val_accuracy: 0.7041\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6349 - accuracy: 0.6449 - val_loss: 0.6189 - val_accuracy: 0.6977\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6338 - accuracy: 0.6501 - val_loss: 0.6160 - val_accuracy: 0.7028\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6506 - val_loss: 0.6144 - val_accuracy: 0.6977\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6574 - val_loss: 0.6164 - val_accuracy: 0.6990\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6473 - val_loss: 0.6154 - val_accuracy: 0.6939\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6304 - accuracy: 0.6544 - val_loss: 0.6152 - val_accuracy: 0.6926\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6614 - val_loss: 0.6158 - val_accuracy: 0.6964\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6567 - val_loss: 0.6124 - val_accuracy: 0.6926\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6332 - accuracy: 0.6529 - val_loss: 0.6143 - val_accuracy: 0.6952\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6550 - val_loss: 0.6149 - val_accuracy: 0.6977\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6459 - val_loss: 0.6129 - val_accuracy: 0.7028\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6515 - val_loss: 0.6177 - val_accuracy: 0.6977\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6279 - accuracy: 0.6600 - val_loss: 0.6156 - val_accuracy: 0.6913\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6576 - val_loss: 0.6113 - val_accuracy: 0.6990\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6523 - val_loss: 0.6207 - val_accuracy: 0.6849\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6505 - val_loss: 0.6200 - val_accuracy: 0.6849\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6585 - val_loss: 0.6116 - val_accuracy: 0.7015\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6554 - val_loss: 0.6183 - val_accuracy: 0.7003\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6567 - val_loss: 0.6145 - val_accuracy: 0.6977\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6556 - val_loss: 0.6150 - val_accuracy: 0.7015\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6596 - val_loss: 0.6129 - val_accuracy: 0.6977\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6600 - val_loss: 0.6121 - val_accuracy: 0.6939\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6511 - val_loss: 0.6151 - val_accuracy: 0.6964\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6534 - val_loss: 0.6192 - val_accuracy: 0.6964\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6520 - val_loss: 0.6153 - val_accuracy: 0.7003\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6595 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6255 - accuracy: 0.6601 - val_loss: 0.6130 - val_accuracy: 0.6977\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6585 - val_loss: 0.6150 - val_accuracy: 0.6977\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6610 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Calculating for: 650 100 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_252 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8317 - accuracy: 0.5348 - val_loss: 0.7068 - val_accuracy: 0.4694\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7164 - accuracy: 0.5653 - val_loss: 0.6667 - val_accuracy: 0.6276\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6852 - accuracy: 0.5769 - val_loss: 0.6486 - val_accuracy: 0.6633\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6790 - accuracy: 0.5760 - val_loss: 0.6503 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5855 - val_loss: 0.6494 - val_accuracy: 0.6722\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6685 - accuracy: 0.5933 - val_loss: 0.6470 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6056 - val_loss: 0.6449 - val_accuracy: 0.6696\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6022 - val_loss: 0.6407 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6069 - val_loss: 0.6448 - val_accuracy: 0.6556\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6065 - val_loss: 0.6444 - val_accuracy: 0.6607\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6118 - val_loss: 0.6396 - val_accuracy: 0.6722\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6167 - val_loss: 0.6384 - val_accuracy: 0.6696\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6220 - val_loss: 0.6377 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6201 - val_loss: 0.6392 - val_accuracy: 0.6620\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6197 - val_loss: 0.6358 - val_accuracy: 0.6645\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6183 - val_loss: 0.6341 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6197 - val_loss: 0.6390 - val_accuracy: 0.6480\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6276 - val_loss: 0.6346 - val_accuracy: 0.6696\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6250 - val_loss: 0.6358 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6265 - val_loss: 0.6340 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6326 - val_loss: 0.6365 - val_accuracy: 0.6671\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6282 - val_loss: 0.6344 - val_accuracy: 0.6620\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6276 - val_loss: 0.6275 - val_accuracy: 0.6786\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6449 - accuracy: 0.6380 - val_loss: 0.6275 - val_accuracy: 0.6926\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6320 - val_loss: 0.6298 - val_accuracy: 0.6811\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6340 - val_loss: 0.6311 - val_accuracy: 0.6798\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6316 - val_loss: 0.6354 - val_accuracy: 0.6735\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6370 - val_loss: 0.6319 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6309 - val_loss: 0.6293 - val_accuracy: 0.6760\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6383 - val_loss: 0.6290 - val_accuracy: 0.6837\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6379 - val_loss: 0.6253 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6378 - val_loss: 0.6257 - val_accuracy: 0.6811\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6419 - val_loss: 0.6256 - val_accuracy: 0.6786\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6408 - val_loss: 0.6226 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6459 - val_loss: 0.6256 - val_accuracy: 0.6722\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6375 - val_loss: 0.6312 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6408 - val_loss: 0.6291 - val_accuracy: 0.6735\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6417 - val_loss: 0.6227 - val_accuracy: 0.6786\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6449 - val_loss: 0.6272 - val_accuracy: 0.6658\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6522 - val_loss: 0.6299 - val_accuracy: 0.6543\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6503 - val_loss: 0.6295 - val_accuracy: 0.6633\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6510 - val_loss: 0.6198 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6513 - val_loss: 0.6197 - val_accuracy: 0.6773\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6487 - val_loss: 0.6217 - val_accuracy: 0.6773\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6478 - val_loss: 0.6216 - val_accuracy: 0.6747\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6520 - val_loss: 0.6228 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6487 - val_loss: 0.6251 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6502 - val_loss: 0.6210 - val_accuracy: 0.6824\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6530 - val_loss: 0.6220 - val_accuracy: 0.6849\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6598 - val_loss: 0.6220 - val_accuracy: 0.6888\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6523 - val_loss: 0.6185 - val_accuracy: 0.6926\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6521 - val_loss: 0.6214 - val_accuracy: 0.6747\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6567 - val_loss: 0.6236 - val_accuracy: 0.6722\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6571 - val_loss: 0.6239 - val_accuracy: 0.6696\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6210 - accuracy: 0.6590 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6593 - val_loss: 0.6283 - val_accuracy: 0.6658\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6603 - val_loss: 0.6248 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6628 - val_loss: 0.6233 - val_accuracy: 0.6696\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6642 - val_loss: 0.6310 - val_accuracy: 0.6543\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6580 - val_loss: 0.6226 - val_accuracy: 0.6684\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6614 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6574 - val_loss: 0.6214 - val_accuracy: 0.6735\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6633 - val_loss: 0.6210 - val_accuracy: 0.6760\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6642 - val_loss: 0.6219 - val_accuracy: 0.6722\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6125 - accuracy: 0.6677 - val_loss: 0.6212 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6120 - accuracy: 0.6653 - val_loss: 0.6226 - val_accuracy: 0.6747\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6667 - val_loss: 0.6254 - val_accuracy: 0.6658\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6121 - accuracy: 0.6707 - val_loss: 0.6257 - val_accuracy: 0.6658\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6134 - accuracy: 0.6698 - val_loss: 0.6280 - val_accuracy: 0.6620\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6726 - val_loss: 0.6249 - val_accuracy: 0.6684\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6678 - val_loss: 0.6267 - val_accuracy: 0.6633\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6118 - accuracy: 0.6643 - val_loss: 0.6343 - val_accuracy: 0.6518\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6111 - accuracy: 0.6732 - val_loss: 0.6335 - val_accuracy: 0.6505\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.6698 - val_loss: 0.6309 - val_accuracy: 0.6569\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6104 - accuracy: 0.6745 - val_loss: 0.6246 - val_accuracy: 0.6671\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6076 - accuracy: 0.6780 - val_loss: 0.6280 - val_accuracy: 0.6556\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6747 - val_loss: 0.6270 - val_accuracy: 0.6633\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6691 - val_loss: 0.6273 - val_accuracy: 0.6645\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6727 - val_loss: 0.6293 - val_accuracy: 0.6594\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6076 - accuracy: 0.6736 - val_loss: 0.6305 - val_accuracy: 0.6645\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6087 - accuracy: 0.6702 - val_loss: 0.6323 - val_accuracy: 0.6645\n",
      "Calculating for: 650 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_256 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8219 - accuracy: 0.5234 - val_loss: 0.6943 - val_accuracy: 0.5013\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7247 - accuracy: 0.5485 - val_loss: 0.6612 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6915 - accuracy: 0.5578 - val_loss: 0.6571 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6818 - accuracy: 0.5747 - val_loss: 0.6543 - val_accuracy: 0.6518\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6782 - accuracy: 0.5678 - val_loss: 0.6519 - val_accuracy: 0.6620\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6763 - accuracy: 0.5804 - val_loss: 0.6497 - val_accuracy: 0.6633\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.5863 - val_loss: 0.6521 - val_accuracy: 0.6696\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5849 - val_loss: 0.6532 - val_accuracy: 0.6582\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6712 - accuracy: 0.5874 - val_loss: 0.6500 - val_accuracy: 0.6594\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6699 - accuracy: 0.5960 - val_loss: 0.6468 - val_accuracy: 0.6633\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6015 - val_loss: 0.6432 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6007 - val_loss: 0.6502 - val_accuracy: 0.6594\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6016 - val_loss: 0.6446 - val_accuracy: 0.6671\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6066 - val_loss: 0.6449 - val_accuracy: 0.6607\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.6055 - val_loss: 0.6473 - val_accuracy: 0.6645\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5987 - val_loss: 0.6390 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6626 - accuracy: 0.6034 - val_loss: 0.6431 - val_accuracy: 0.6582\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6068 - val_loss: 0.6418 - val_accuracy: 0.6594\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6112 - val_loss: 0.6416 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6153 - val_loss: 0.6387 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6187 - val_loss: 0.6389 - val_accuracy: 0.6518\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6107 - val_loss: 0.6334 - val_accuracy: 0.6671\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6142 - val_loss: 0.6402 - val_accuracy: 0.6556\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6171 - val_loss: 0.6412 - val_accuracy: 0.6531\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6148 - val_loss: 0.6409 - val_accuracy: 0.6505\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6226 - val_loss: 0.6365 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6525 - accuracy: 0.6252 - val_loss: 0.6326 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6206 - val_loss: 0.6351 - val_accuracy: 0.6556\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6223 - val_loss: 0.6378 - val_accuracy: 0.6594\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6197 - val_loss: 0.6396 - val_accuracy: 0.6569\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6236 - val_loss: 0.6343 - val_accuracy: 0.6620\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6223 - val_loss: 0.6366 - val_accuracy: 0.6492\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6197 - val_loss: 0.6330 - val_accuracy: 0.6518\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6512 - accuracy: 0.6232 - val_loss: 0.6357 - val_accuracy: 0.6620\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6233 - val_loss: 0.6334 - val_accuracy: 0.6607\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6256 - val_loss: 0.6323 - val_accuracy: 0.6633\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6492 - accuracy: 0.6274 - val_loss: 0.6311 - val_accuracy: 0.6594\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6309 - val_loss: 0.6361 - val_accuracy: 0.6569\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6301 - val_loss: 0.6286 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6291 - val_loss: 0.6323 - val_accuracy: 0.6607\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6294 - val_loss: 0.6330 - val_accuracy: 0.6454\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6361 - val_loss: 0.6304 - val_accuracy: 0.6645\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6267 - val_loss: 0.6325 - val_accuracy: 0.6556\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6304 - val_loss: 0.6336 - val_accuracy: 0.6594\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6330 - val_loss: 0.6343 - val_accuracy: 0.6582\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6363 - val_loss: 0.6237 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6360 - val_loss: 0.6249 - val_accuracy: 0.6696\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6349 - val_loss: 0.6234 - val_accuracy: 0.6747\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6380 - val_loss: 0.6287 - val_accuracy: 0.6671\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6382 - val_loss: 0.6266 - val_accuracy: 0.6645\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6349 - val_loss: 0.6321 - val_accuracy: 0.6594\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6414 - accuracy: 0.6373 - val_loss: 0.6270 - val_accuracy: 0.6569\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6387 - val_loss: 0.6273 - val_accuracy: 0.6505\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6375 - val_loss: 0.6290 - val_accuracy: 0.6543\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6407 - val_loss: 0.6250 - val_accuracy: 0.6531\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6408 - val_loss: 0.6250 - val_accuracy: 0.6658\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6355 - val_loss: 0.6226 - val_accuracy: 0.6735\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6393 - val_loss: 0.6332 - val_accuracy: 0.6594\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6449 - val_loss: 0.6286 - val_accuracy: 0.6569\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6453 - val_loss: 0.6306 - val_accuracy: 0.6569\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6485 - val_loss: 0.6231 - val_accuracy: 0.6684\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6420 - val_loss: 0.6200 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6458 - val_loss: 0.6288 - val_accuracy: 0.6594\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6497 - val_loss: 0.6287 - val_accuracy: 0.6543\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6467 - val_loss: 0.6257 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6427 - val_loss: 0.6286 - val_accuracy: 0.6607\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6395 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6447 - val_loss: 0.6269 - val_accuracy: 0.6556\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6335 - accuracy: 0.6508 - val_loss: 0.6186 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6507 - val_loss: 0.6249 - val_accuracy: 0.6709\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6497 - val_loss: 0.6329 - val_accuracy: 0.6582\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6534 - val_loss: 0.6233 - val_accuracy: 0.6760\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6501 - val_loss: 0.6236 - val_accuracy: 0.6684\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6312 - accuracy: 0.6559 - val_loss: 0.6201 - val_accuracy: 0.6722\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6488 - val_loss: 0.6221 - val_accuracy: 0.6747\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6523 - val_loss: 0.6260 - val_accuracy: 0.6671\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6496 - val_loss: 0.6206 - val_accuracy: 0.6696\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6515 - val_loss: 0.6288 - val_accuracy: 0.6543\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6555 - val_loss: 0.6243 - val_accuracy: 0.6658\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6498 - val_loss: 0.6259 - val_accuracy: 0.6620\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6526 - val_loss: 0.6257 - val_accuracy: 0.6684\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6516 - val_loss: 0.6242 - val_accuracy: 0.6722\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6522 - val_loss: 0.6246 - val_accuracy: 0.6735\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6610 - val_loss: 0.6221 - val_accuracy: 0.6760\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6560 - val_loss: 0.6218 - val_accuracy: 0.6735\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6620 - val_loss: 0.6228 - val_accuracy: 0.6709\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6561 - val_loss: 0.6254 - val_accuracy: 0.6671\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6544 - val_loss: 0.6266 - val_accuracy: 0.6645\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6224 - accuracy: 0.6596 - val_loss: 0.6309 - val_accuracy: 0.6569\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6603 - val_loss: 0.6220 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6241 - accuracy: 0.6593 - val_loss: 0.6219 - val_accuracy: 0.6722\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6530 - val_loss: 0.6274 - val_accuracy: 0.6722\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6531 - val_loss: 0.6266 - val_accuracy: 0.6722\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6243 - accuracy: 0.6579 - val_loss: 0.6248 - val_accuracy: 0.6684\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6209 - accuracy: 0.6614 - val_loss: 0.6247 - val_accuracy: 0.6696\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6566 - val_loss: 0.6213 - val_accuracy: 0.6811\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6217 - accuracy: 0.6576 - val_loss: 0.6228 - val_accuracy: 0.6773\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6613 - val_loss: 0.6230 - val_accuracy: 0.6658\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6701 - val_loss: 0.6244 - val_accuracy: 0.6658\n",
      "Calculating for: 650 100 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_260 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8143 - accuracy: 0.5225 - val_loss: 0.6581 - val_accuracy: 0.6339\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7274 - accuracy: 0.5261 - val_loss: 0.6553 - val_accuracy: 0.6390\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7005 - accuracy: 0.5226 - val_loss: 0.6565 - val_accuracy: 0.6352\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6906 - accuracy: 0.5369 - val_loss: 0.6601 - val_accuracy: 0.6429\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6888 - accuracy: 0.5426 - val_loss: 0.6616 - val_accuracy: 0.6467\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6851 - accuracy: 0.5482 - val_loss: 0.6574 - val_accuracy: 0.6480\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.5561 - val_loss: 0.6531 - val_accuracy: 0.6454\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6825 - accuracy: 0.5662 - val_loss: 0.6579 - val_accuracy: 0.6543\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6827 - accuracy: 0.5688 - val_loss: 0.6504 - val_accuracy: 0.6441\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.5653 - val_loss: 0.6542 - val_accuracy: 0.6505\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6807 - accuracy: 0.5587 - val_loss: 0.6509 - val_accuracy: 0.6441\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5643 - val_loss: 0.6508 - val_accuracy: 0.6556\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6790 - accuracy: 0.5703 - val_loss: 0.6453 - val_accuracy: 0.6492\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5787 - val_loss: 0.6455 - val_accuracy: 0.6633\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5826 - val_loss: 0.6439 - val_accuracy: 0.6645\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6764 - accuracy: 0.5764 - val_loss: 0.6419 - val_accuracy: 0.6607\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6738 - accuracy: 0.5804 - val_loss: 0.6431 - val_accuracy: 0.6607\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6723 - accuracy: 0.5809 - val_loss: 0.6398 - val_accuracy: 0.6658\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5833 - val_loss: 0.6363 - val_accuracy: 0.6633\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5947 - val_loss: 0.6373 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5845 - val_loss: 0.6389 - val_accuracy: 0.6684\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5977 - val_loss: 0.6356 - val_accuracy: 0.6696\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6728 - accuracy: 0.5816 - val_loss: 0.6374 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5878 - val_loss: 0.6420 - val_accuracy: 0.6671\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6717 - accuracy: 0.5923 - val_loss: 0.6392 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.6030 - val_loss: 0.6330 - val_accuracy: 0.6747\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.6016 - val_loss: 0.6347 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.5998 - val_loss: 0.6343 - val_accuracy: 0.6696\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5957 - val_loss: 0.6341 - val_accuracy: 0.6658\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6702 - accuracy: 0.5926 - val_loss: 0.6401 - val_accuracy: 0.6684\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5997 - val_loss: 0.6387 - val_accuracy: 0.6684\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6010 - val_loss: 0.6345 - val_accuracy: 0.6735\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.6001 - val_loss: 0.6368 - val_accuracy: 0.6671\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6639 - accuracy: 0.5993 - val_loss: 0.6305 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.6014 - val_loss: 0.6356 - val_accuracy: 0.6709\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.5985 - val_loss: 0.6362 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6009 - val_loss: 0.6315 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6063 - val_loss: 0.6334 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6068 - val_loss: 0.6297 - val_accuracy: 0.6735\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6030 - val_loss: 0.6306 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6085 - val_loss: 0.6274 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6613 - accuracy: 0.6064 - val_loss: 0.6258 - val_accuracy: 0.6696\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.6075 - val_loss: 0.6309 - val_accuracy: 0.6709\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6076 - val_loss: 0.6280 - val_accuracy: 0.6722\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6004 - val_loss: 0.6275 - val_accuracy: 0.6735\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6117 - val_loss: 0.6301 - val_accuracy: 0.6747\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6196 - val_loss: 0.6267 - val_accuracy: 0.6798\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6124 - val_loss: 0.6306 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6594 - accuracy: 0.6091 - val_loss: 0.6330 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6148 - val_loss: 0.6251 - val_accuracy: 0.6798\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6576 - accuracy: 0.6124 - val_loss: 0.6296 - val_accuracy: 0.6824\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6567 - accuracy: 0.6172 - val_loss: 0.6285 - val_accuracy: 0.6837\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6577 - accuracy: 0.6235 - val_loss: 0.6293 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6187 - val_loss: 0.6233 - val_accuracy: 0.6849\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6251 - val_loss: 0.6237 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6110 - val_loss: 0.6272 - val_accuracy: 0.6824\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6182 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6137 - val_loss: 0.6266 - val_accuracy: 0.6773\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6242 - val_loss: 0.6269 - val_accuracy: 0.6849\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6177 - val_loss: 0.6244 - val_accuracy: 0.6888\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6125 - val_loss: 0.6278 - val_accuracy: 0.6811\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6172 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6248 - val_loss: 0.6197 - val_accuracy: 0.6888\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6186 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6306 - val_loss: 0.6252 - val_accuracy: 0.6913\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6264 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6164 - val_loss: 0.6219 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6199 - val_loss: 0.6218 - val_accuracy: 0.6939\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6225 - val_accuracy: 0.6849\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6216 - val_loss: 0.6224 - val_accuracy: 0.6837\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6267 - val_loss: 0.6252 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6242 - val_loss: 0.6211 - val_accuracy: 0.6849\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6252 - val_loss: 0.6258 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6208 - val_loss: 0.6240 - val_accuracy: 0.6786\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6240 - val_loss: 0.6250 - val_accuracy: 0.6798\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6230 - val_loss: 0.6235 - val_accuracy: 0.6875\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6547 - accuracy: 0.6248 - val_loss: 0.6208 - val_accuracy: 0.6862\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6295 - val_loss: 0.6233 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6238 - val_loss: 0.6210 - val_accuracy: 0.6837\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6243 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6305 - val_loss: 0.6182 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6134 - val_loss: 0.6204 - val_accuracy: 0.6913\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6301 - val_loss: 0.6226 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6284 - val_loss: 0.6197 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6455 - accuracy: 0.6311 - val_loss: 0.6229 - val_accuracy: 0.6888\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6296 - val_loss: 0.6208 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6310 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6474 - accuracy: 0.6331 - val_loss: 0.6219 - val_accuracy: 0.6862\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6294 - val_loss: 0.6174 - val_accuracy: 0.6888\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6331 - val_loss: 0.6160 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6226 - val_loss: 0.6169 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6355 - val_loss: 0.6199 - val_accuracy: 0.6811\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6365 - val_loss: 0.6184 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6299 - val_loss: 0.6189 - val_accuracy: 0.6849\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6315 - val_loss: 0.6217 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6315 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6284 - val_loss: 0.6197 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6402 - val_loss: 0.6183 - val_accuracy: 0.6913\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6323 - val_loss: 0.6150 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6329 - val_loss: 0.6204 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6400 - val_loss: 0.6205 - val_accuracy: 0.6849\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6402 - val_loss: 0.6209 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6353 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6413 - val_loss: 0.6185 - val_accuracy: 0.6875\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6355 - val_loss: 0.6213 - val_accuracy: 0.6786\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6438 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6312 - val_loss: 0.6211 - val_accuracy: 0.6786\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6364 - val_loss: 0.6226 - val_accuracy: 0.6837\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6409 - val_loss: 0.6175 - val_accuracy: 0.6875\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6330 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6424 - val_loss: 0.6165 - val_accuracy: 0.6837\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6447 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6385 - val_loss: 0.6188 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6426 - val_loss: 0.6190 - val_accuracy: 0.6849\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6452 - val_loss: 0.6141 - val_accuracy: 0.6849\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6372 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6416 - accuracy: 0.6353 - val_loss: 0.6164 - val_accuracy: 0.6837\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6495 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6382 - val_loss: 0.6139 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6417 - val_loss: 0.6149 - val_accuracy: 0.6824\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6375 - val_loss: 0.6147 - val_accuracy: 0.6837\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6475 - val_loss: 0.6188 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6414 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6388 - accuracy: 0.6429 - val_loss: 0.6211 - val_accuracy: 0.6798\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6436 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6429 - val_loss: 0.6142 - val_accuracy: 0.6811\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6436 - val_loss: 0.6188 - val_accuracy: 0.6837\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6438 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6363 - accuracy: 0.6482 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6467 - val_loss: 0.6107 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6390 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6444 - val_loss: 0.6157 - val_accuracy: 0.6849\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6486 - val_loss: 0.6196 - val_accuracy: 0.6811\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6518 - val_loss: 0.6155 - val_accuracy: 0.6824\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6371 - accuracy: 0.6477 - val_loss: 0.6158 - val_accuracy: 0.6875\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6428 - val_loss: 0.6182 - val_accuracy: 0.6888\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6540 - val_loss: 0.6119 - val_accuracy: 0.6939\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6472 - val_loss: 0.6129 - val_accuracy: 0.6939\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6493 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6449 - val_loss: 0.6135 - val_accuracy: 0.6875\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6495 - val_loss: 0.6167 - val_accuracy: 0.6913\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6459 - val_loss: 0.6166 - val_accuracy: 0.6862\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6517 - val_loss: 0.6125 - val_accuracy: 0.6875\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6471 - val_loss: 0.6122 - val_accuracy: 0.6888\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6336 - accuracy: 0.6461 - val_loss: 0.6120 - val_accuracy: 0.6875\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6516 - val_loss: 0.6139 - val_accuracy: 0.6862\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6588 - val_loss: 0.6120 - val_accuracy: 0.6862\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6475 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6468 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6486 - val_loss: 0.6132 - val_accuracy: 0.6862\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6550 - val_loss: 0.6160 - val_accuracy: 0.6849\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6525 - val_loss: 0.6140 - val_accuracy: 0.6811\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6526 - val_loss: 0.6132 - val_accuracy: 0.6888\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6488 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6495 - val_loss: 0.6157 - val_accuracy: 0.6888\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6472 - val_loss: 0.6136 - val_accuracy: 0.6888\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6579 - val_loss: 0.6108 - val_accuracy: 0.6849\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6510 - val_loss: 0.6128 - val_accuracy: 0.6849\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6536 - val_loss: 0.6135 - val_accuracy: 0.6824\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6541 - val_loss: 0.6112 - val_accuracy: 0.6837\n",
      "Calculating for: 650 150 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_264 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8317 - accuracy: 0.5373 - val_loss: 0.6446 - val_accuracy: 0.6531\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7177 - accuracy: 0.5574 - val_loss: 0.6401 - val_accuracy: 0.6671\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6846 - accuracy: 0.5808 - val_loss: 0.6405 - val_accuracy: 0.6556\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5831 - val_loss: 0.6409 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5911 - val_loss: 0.6389 - val_accuracy: 0.6658\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5933 - val_loss: 0.6390 - val_accuracy: 0.6773\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.5970 - val_loss: 0.6360 - val_accuracy: 0.6709\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6049 - val_loss: 0.6351 - val_accuracy: 0.6773\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6606 - accuracy: 0.6074 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6148 - val_loss: 0.6310 - val_accuracy: 0.6849\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6066 - val_loss: 0.6358 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6169 - val_loss: 0.6314 - val_accuracy: 0.6798\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6147 - val_loss: 0.6295 - val_accuracy: 0.6798\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6153 - val_loss: 0.6289 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6207 - val_loss: 0.6324 - val_accuracy: 0.6684\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6218 - val_loss: 0.6310 - val_accuracy: 0.6671\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6504 - accuracy: 0.6220 - val_loss: 0.6314 - val_accuracy: 0.6722\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6208 - val_loss: 0.6289 - val_accuracy: 0.6645\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6238 - val_loss: 0.6272 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6258 - val_loss: 0.6280 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6454 - accuracy: 0.6329 - val_loss: 0.6237 - val_accuracy: 0.6798\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6270 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6250 - val_loss: 0.6275 - val_accuracy: 0.6875\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6260 - val_loss: 0.6301 - val_accuracy: 0.6722\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6272 - val_loss: 0.6281 - val_accuracy: 0.6709\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6373 - val_loss: 0.6284 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6372 - val_loss: 0.6253 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6356 - val_loss: 0.6216 - val_accuracy: 0.6798\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6315 - val_loss: 0.6265 - val_accuracy: 0.6684\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6331 - val_loss: 0.6237 - val_accuracy: 0.6594\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6400 - val_loss: 0.6221 - val_accuracy: 0.6696\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6355 - val_loss: 0.6259 - val_accuracy: 0.6696\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6356 - val_loss: 0.6191 - val_accuracy: 0.6735\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6374 - val_loss: 0.6291 - val_accuracy: 0.6658\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6379 - val_loss: 0.6272 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6478 - val_loss: 0.6209 - val_accuracy: 0.6684\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6390 - val_loss: 0.6244 - val_accuracy: 0.6645\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6478 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6409 - val_loss: 0.6221 - val_accuracy: 0.6671\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6493 - val_loss: 0.6201 - val_accuracy: 0.6735\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6486 - val_loss: 0.6191 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6478 - val_loss: 0.6152 - val_accuracy: 0.6811\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6443 - val_loss: 0.6179 - val_accuracy: 0.6786\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6418 - val_loss: 0.6252 - val_accuracy: 0.6722\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6426 - val_loss: 0.6198 - val_accuracy: 0.6747\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6546 - val_loss: 0.6162 - val_accuracy: 0.6888\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6478 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6591 - val_loss: 0.6182 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6532 - val_loss: 0.6193 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6213 - accuracy: 0.6539 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6614 - val_loss: 0.6186 - val_accuracy: 0.6849\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6555 - val_loss: 0.6182 - val_accuracy: 0.6862\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6556 - val_loss: 0.6208 - val_accuracy: 0.6849\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6521 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6569 - val_loss: 0.6219 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6603 - val_loss: 0.6154 - val_accuracy: 0.6952\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6653 - val_loss: 0.6151 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6606 - val_loss: 0.6173 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6561 - val_loss: 0.6160 - val_accuracy: 0.6862\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6611 - val_loss: 0.6129 - val_accuracy: 0.6990\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6572 - val_loss: 0.6187 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6616 - val_loss: 0.6158 - val_accuracy: 0.6888\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6611 - val_loss: 0.6189 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6598 - val_loss: 0.6204 - val_accuracy: 0.6773\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6166 - accuracy: 0.6648 - val_loss: 0.6204 - val_accuracy: 0.6798\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6652 - val_loss: 0.6222 - val_accuracy: 0.6735\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6126 - accuracy: 0.6655 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6143 - accuracy: 0.6642 - val_loss: 0.6239 - val_accuracy: 0.6709\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6166 - accuracy: 0.6640 - val_loss: 0.6212 - val_accuracy: 0.6786\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6112 - accuracy: 0.6673 - val_loss: 0.6181 - val_accuracy: 0.6786\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6642 - val_loss: 0.6227 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6649 - val_loss: 0.6204 - val_accuracy: 0.6760\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6075 - accuracy: 0.6733 - val_loss: 0.6199 - val_accuracy: 0.6773\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6724 - val_loss: 0.6223 - val_accuracy: 0.6747\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6084 - accuracy: 0.6686 - val_loss: 0.6211 - val_accuracy: 0.6760\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5995 - accuracy: 0.6834 - val_loss: 0.6227 - val_accuracy: 0.6760\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6062 - accuracy: 0.6763 - val_loss: 0.6210 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6745 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6771 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6712 - val_loss: 0.6209 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6060 - accuracy: 0.6772 - val_loss: 0.6209 - val_accuracy: 0.6709\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6000 - accuracy: 0.6811 - val_loss: 0.6230 - val_accuracy: 0.6709\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6035 - accuracy: 0.6726 - val_loss: 0.6237 - val_accuracy: 0.6696\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6811 - val_loss: 0.6205 - val_accuracy: 0.6786\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6810 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5997 - accuracy: 0.6829 - val_loss: 0.6215 - val_accuracy: 0.6786\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6049 - accuracy: 0.6760 - val_loss: 0.6235 - val_accuracy: 0.6722\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6020 - accuracy: 0.6806 - val_loss: 0.6239 - val_accuracy: 0.6747\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6001 - accuracy: 0.6767 - val_loss: 0.6230 - val_accuracy: 0.6773\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5978 - accuracy: 0.6805 - val_loss: 0.6285 - val_accuracy: 0.6709\n",
      "Calculating for: 650 150 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_268 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8212 - accuracy: 0.5259 - val_loss: 0.6860 - val_accuracy: 0.5446\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7199 - accuracy: 0.5446 - val_loss: 0.6692 - val_accuracy: 0.6148\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6936 - accuracy: 0.5540 - val_loss: 0.6551 - val_accuracy: 0.6543\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6799 - accuracy: 0.5678 - val_loss: 0.6526 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.5761 - val_loss: 0.6495 - val_accuracy: 0.6722\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5844 - val_loss: 0.6504 - val_accuracy: 0.6658\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5896 - val_loss: 0.6476 - val_accuracy: 0.6684\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6716 - accuracy: 0.5814 - val_loss: 0.6504 - val_accuracy: 0.6722\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6700 - accuracy: 0.5960 - val_loss: 0.6432 - val_accuracy: 0.6760\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5967 - val_loss: 0.6450 - val_accuracy: 0.6633\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5853 - val_loss: 0.6439 - val_accuracy: 0.6684\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6672 - accuracy: 0.5987 - val_loss: 0.6481 - val_accuracy: 0.6696\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6029 - val_loss: 0.6464 - val_accuracy: 0.6620\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.6029 - val_loss: 0.6461 - val_accuracy: 0.6633\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6017 - val_loss: 0.6387 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6599 - accuracy: 0.6103 - val_loss: 0.6387 - val_accuracy: 0.6709\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6045 - val_loss: 0.6480 - val_accuracy: 0.6505\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6584 - accuracy: 0.6179 - val_loss: 0.6477 - val_accuracy: 0.6454\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6133 - val_loss: 0.6398 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6108 - val_loss: 0.6443 - val_accuracy: 0.6531\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6184 - val_loss: 0.6363 - val_accuracy: 0.6709\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6085 - val_loss: 0.6360 - val_accuracy: 0.6709\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6187 - val_loss: 0.6352 - val_accuracy: 0.6645\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6216 - val_loss: 0.6368 - val_accuracy: 0.6684\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6208 - val_loss: 0.6317 - val_accuracy: 0.6709\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6133 - val_loss: 0.6422 - val_accuracy: 0.6633\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6191 - val_loss: 0.6393 - val_accuracy: 0.6722\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6240 - val_loss: 0.6355 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6291 - val_loss: 0.6384 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6150 - val_loss: 0.6431 - val_accuracy: 0.6454\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6265 - val_loss: 0.6308 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6189 - val_loss: 0.6357 - val_accuracy: 0.6671\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6235 - val_loss: 0.6410 - val_accuracy: 0.6531\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6237 - val_loss: 0.6340 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6248 - val_loss: 0.6321 - val_accuracy: 0.6747\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6173 - val_loss: 0.6338 - val_accuracy: 0.6722\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6307 - val_loss: 0.6378 - val_accuracy: 0.6492\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6297 - val_loss: 0.6317 - val_accuracy: 0.6722\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6251 - val_loss: 0.6335 - val_accuracy: 0.6773\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6271 - val_loss: 0.6306 - val_accuracy: 0.6709\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6305 - val_loss: 0.6310 - val_accuracy: 0.6709\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6232 - val_loss: 0.6308 - val_accuracy: 0.6773\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6439 - val_loss: 0.6266 - val_accuracy: 0.6798\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6280 - val_loss: 0.6263 - val_accuracy: 0.6760\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6294 - val_loss: 0.6299 - val_accuracy: 0.6696\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6426 - val_loss: 0.6263 - val_accuracy: 0.6747\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6384 - val_loss: 0.6307 - val_accuracy: 0.6709\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6306 - val_loss: 0.6300 - val_accuracy: 0.6633\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6359 - val_loss: 0.6282 - val_accuracy: 0.6735\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6345 - val_loss: 0.6225 - val_accuracy: 0.6747\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6366 - val_loss: 0.6269 - val_accuracy: 0.6747\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6338 - val_loss: 0.6355 - val_accuracy: 0.6454\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6354 - val_loss: 0.6286 - val_accuracy: 0.6620\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6302 - val_loss: 0.6254 - val_accuracy: 0.6735\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6405 - val_loss: 0.6248 - val_accuracy: 0.6696\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6454 - val_loss: 0.6274 - val_accuracy: 0.6633\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6359 - val_loss: 0.6253 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6360 - val_loss: 0.6239 - val_accuracy: 0.6760\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6394 - val_loss: 0.6231 - val_accuracy: 0.6862\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6372 - val_loss: 0.6305 - val_accuracy: 0.6645\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6400 - val_loss: 0.6262 - val_accuracy: 0.6709\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6398 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6384 - val_loss: 0.6265 - val_accuracy: 0.6722\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6473 - val_loss: 0.6287 - val_accuracy: 0.6607\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6419 - val_loss: 0.6292 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6384 - val_loss: 0.6255 - val_accuracy: 0.6798\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6350 - val_loss: 0.6317 - val_accuracy: 0.6709\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6478 - val_loss: 0.6265 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6443 - val_loss: 0.6193 - val_accuracy: 0.6811\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6415 - val_loss: 0.6235 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6487 - val_loss: 0.6189 - val_accuracy: 0.6773\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6486 - val_loss: 0.6190 - val_accuracy: 0.6747\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6483 - val_loss: 0.6248 - val_accuracy: 0.6773\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6547 - val_loss: 0.6221 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6446 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6547 - val_loss: 0.6164 - val_accuracy: 0.6735\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6426 - val_loss: 0.6194 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6501 - val_loss: 0.6250 - val_accuracy: 0.6824\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6518 - val_loss: 0.6224 - val_accuracy: 0.6786\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6517 - val_loss: 0.6198 - val_accuracy: 0.6773\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6549 - val_loss: 0.6263 - val_accuracy: 0.6888\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6522 - val_loss: 0.6254 - val_accuracy: 0.6888\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6574 - val_loss: 0.6227 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6512 - val_loss: 0.6272 - val_accuracy: 0.6824\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6515 - val_loss: 0.6230 - val_accuracy: 0.6837\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6575 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6537 - val_loss: 0.6200 - val_accuracy: 0.6888\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6527 - val_loss: 0.6186 - val_accuracy: 0.6862\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6510 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6541 - val_loss: 0.6231 - val_accuracy: 0.6888\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6551 - val_loss: 0.6188 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6540 - val_loss: 0.6196 - val_accuracy: 0.6952\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6596 - val_loss: 0.6204 - val_accuracy: 0.6862\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6570 - val_loss: 0.6202 - val_accuracy: 0.6862\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6570 - val_loss: 0.6224 - val_accuracy: 0.6862\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6616 - val_loss: 0.6201 - val_accuracy: 0.6862\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6660 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6680 - val_loss: 0.6212 - val_accuracy: 0.6901\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6620 - val_loss: 0.6213 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6658 - val_loss: 0.6214 - val_accuracy: 0.6875\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6598 - val_loss: 0.6215 - val_accuracy: 0.6901\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6556 - val_loss: 0.6185 - val_accuracy: 0.6862\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6593 - val_loss: 0.6145 - val_accuracy: 0.6939\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6560 - val_loss: 0.6174 - val_accuracy: 0.6875\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6613 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6645 - val_loss: 0.6231 - val_accuracy: 0.6798\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6624 - val_loss: 0.6186 - val_accuracy: 0.6888\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6588 - val_loss: 0.6231 - val_accuracy: 0.6709\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6689 - val_loss: 0.6209 - val_accuracy: 0.6862\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6615 - val_loss: 0.6225 - val_accuracy: 0.6786\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6604 - val_loss: 0.6212 - val_accuracy: 0.6837\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6583 - val_loss: 0.6235 - val_accuracy: 0.6722\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6694 - val_loss: 0.6202 - val_accuracy: 0.6837\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6113 - accuracy: 0.6702 - val_loss: 0.6257 - val_accuracy: 0.6709\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6140 - accuracy: 0.6714 - val_loss: 0.6230 - val_accuracy: 0.6747\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6182 - accuracy: 0.6561 - val_loss: 0.6184 - val_accuracy: 0.6786\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6655 - val_loss: 0.6183 - val_accuracy: 0.6888\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6645 - val_loss: 0.6189 - val_accuracy: 0.6837\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6114 - accuracy: 0.6653 - val_loss: 0.6212 - val_accuracy: 0.6824\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6109 - accuracy: 0.6682 - val_loss: 0.6290 - val_accuracy: 0.6671\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6101 - accuracy: 0.6752 - val_loss: 0.6248 - val_accuracy: 0.6722\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6674 - val_loss: 0.6198 - val_accuracy: 0.6862\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6729 - val_loss: 0.6237 - val_accuracy: 0.6773\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6091 - accuracy: 0.6699 - val_loss: 0.6243 - val_accuracy: 0.6735\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6089 - accuracy: 0.6726 - val_loss: 0.6224 - val_accuracy: 0.6760\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6112 - accuracy: 0.6677 - val_loss: 0.6233 - val_accuracy: 0.6747\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6733 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6091 - accuracy: 0.6734 - val_loss: 0.6268 - val_accuracy: 0.6645\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6708 - val_loss: 0.6233 - val_accuracy: 0.6671\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6680 - val_loss: 0.6228 - val_accuracy: 0.6684\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6075 - accuracy: 0.6718 - val_loss: 0.6226 - val_accuracy: 0.6760\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6092 - accuracy: 0.6763 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6734 - val_loss: 0.6261 - val_accuracy: 0.6620\n",
      "Calculating for: 650 150 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_272 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8116 - accuracy: 0.5207 - val_loss: 0.6662 - val_accuracy: 0.6505\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7272 - accuracy: 0.5261 - val_loss: 0.6593 - val_accuracy: 0.6492\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6951 - accuracy: 0.5466 - val_loss: 0.6674 - val_accuracy: 0.6556\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6886 - accuracy: 0.5453 - val_loss: 0.6637 - val_accuracy: 0.6492\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6840 - accuracy: 0.5540 - val_loss: 0.6638 - val_accuracy: 0.6454\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6823 - accuracy: 0.5566 - val_loss: 0.6614 - val_accuracy: 0.6480\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.5544 - val_loss: 0.6611 - val_accuracy: 0.6582\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6802 - accuracy: 0.5668 - val_loss: 0.6624 - val_accuracy: 0.6658\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6776 - accuracy: 0.5713 - val_loss: 0.6503 - val_accuracy: 0.6454\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6789 - accuracy: 0.5692 - val_loss: 0.6530 - val_accuracy: 0.6531\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5757 - val_loss: 0.6532 - val_accuracy: 0.6684\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6797 - accuracy: 0.5686 - val_loss: 0.6583 - val_accuracy: 0.6556\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5808 - val_loss: 0.6532 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5799 - val_loss: 0.6557 - val_accuracy: 0.6441\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6719 - accuracy: 0.5872 - val_loss: 0.6446 - val_accuracy: 0.6696\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6741 - accuracy: 0.5759 - val_loss: 0.6423 - val_accuracy: 0.6658\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5850 - val_loss: 0.6506 - val_accuracy: 0.6531\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6728 - accuracy: 0.5865 - val_loss: 0.6498 - val_accuracy: 0.6543\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6696 - accuracy: 0.5978 - val_loss: 0.6419 - val_accuracy: 0.6709\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5841 - val_loss: 0.6475 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5907 - val_loss: 0.6471 - val_accuracy: 0.6607\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5923 - val_loss: 0.6473 - val_accuracy: 0.6454\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5850 - val_loss: 0.6434 - val_accuracy: 0.6633\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6657 - accuracy: 0.6019 - val_loss: 0.6409 - val_accuracy: 0.6658\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6721 - accuracy: 0.5922 - val_loss: 0.6422 - val_accuracy: 0.6671\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.6009 - val_loss: 0.6398 - val_accuracy: 0.6645\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5952 - val_loss: 0.6399 - val_accuracy: 0.6671\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5960 - val_loss: 0.6432 - val_accuracy: 0.6658\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5981 - val_loss: 0.6438 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6010 - val_loss: 0.6423 - val_accuracy: 0.6620\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.6022 - val_loss: 0.6395 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6026 - val_loss: 0.6395 - val_accuracy: 0.6645\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6104 - val_loss: 0.6403 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6000 - val_loss: 0.6348 - val_accuracy: 0.6696\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6035 - val_loss: 0.6431 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6089 - val_loss: 0.6365 - val_accuracy: 0.6671\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.5996 - val_loss: 0.6359 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6610 - accuracy: 0.6118 - val_loss: 0.6482 - val_accuracy: 0.6339\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6140 - val_loss: 0.6365 - val_accuracy: 0.6645\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6108 - val_loss: 0.6400 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6074 - val_loss: 0.6338 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6100 - val_loss: 0.6392 - val_accuracy: 0.6607\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6066 - val_loss: 0.6397 - val_accuracy: 0.6594\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6172 - val_loss: 0.6382 - val_accuracy: 0.6633\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6060 - val_loss: 0.6346 - val_accuracy: 0.6696\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6153 - val_loss: 0.6361 - val_accuracy: 0.6658\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6182 - val_loss: 0.6322 - val_accuracy: 0.6645\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6140 - val_loss: 0.6341 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6150 - val_loss: 0.6353 - val_accuracy: 0.6658\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6173 - val_loss: 0.6369 - val_accuracy: 0.6709\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6138 - val_loss: 0.6344 - val_accuracy: 0.6684\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6128 - val_loss: 0.6366 - val_accuracy: 0.6696\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6216 - val_loss: 0.6352 - val_accuracy: 0.6709\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6289 - val_loss: 0.6305 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6159 - val_loss: 0.6347 - val_accuracy: 0.6696\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6230 - val_loss: 0.6360 - val_accuracy: 0.6620\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6184 - val_loss: 0.6386 - val_accuracy: 0.6582\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6237 - val_loss: 0.6390 - val_accuracy: 0.6429\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6282 - val_loss: 0.6320 - val_accuracy: 0.6709\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6220 - val_loss: 0.6255 - val_accuracy: 0.6824\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6194 - val_loss: 0.6318 - val_accuracy: 0.6747\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6228 - val_loss: 0.6281 - val_accuracy: 0.6671\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6221 - val_loss: 0.6270 - val_accuracy: 0.6811\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6211 - val_loss: 0.6287 - val_accuracy: 0.6798\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6218 - val_loss: 0.6306 - val_accuracy: 0.6709\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6271 - val_loss: 0.6352 - val_accuracy: 0.6543\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6235 - val_loss: 0.6330 - val_accuracy: 0.6671\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6339 - val_loss: 0.6275 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6218 - val_loss: 0.6328 - val_accuracy: 0.6671\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6276 - val_loss: 0.6315 - val_accuracy: 0.6569\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6324 - val_loss: 0.6288 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6296 - val_loss: 0.6288 - val_accuracy: 0.6696\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6202 - val_loss: 0.6367 - val_accuracy: 0.6518\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6256 - val_loss: 0.6309 - val_accuracy: 0.6696\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6304 - val_loss: 0.6394 - val_accuracy: 0.6390\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6218 - val_loss: 0.6329 - val_accuracy: 0.6607\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6349 - val_loss: 0.6314 - val_accuracy: 0.6671\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6280 - val_loss: 0.6252 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6331 - val_loss: 0.6282 - val_accuracy: 0.6709\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6241 - val_loss: 0.6264 - val_accuracy: 0.6849\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6344 - val_loss: 0.6271 - val_accuracy: 0.6824\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6297 - val_loss: 0.6299 - val_accuracy: 0.6696\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6272 - val_loss: 0.6317 - val_accuracy: 0.6594\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6328 - val_loss: 0.6264 - val_accuracy: 0.6786\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6316 - val_loss: 0.6288 - val_accuracy: 0.6773\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6306 - val_loss: 0.6243 - val_accuracy: 0.6849\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6279 - val_loss: 0.6287 - val_accuracy: 0.6786\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6292 - val_loss: 0.6306 - val_accuracy: 0.6696\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6324 - val_loss: 0.6323 - val_accuracy: 0.6684\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6351 - val_loss: 0.6269 - val_accuracy: 0.6773\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6418 - val_loss: 0.6337 - val_accuracy: 0.6429\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6335 - val_loss: 0.6320 - val_accuracy: 0.6645\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6354 - val_loss: 0.6269 - val_accuracy: 0.6735\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6330 - val_loss: 0.6298 - val_accuracy: 0.6735\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6311 - val_loss: 0.6262 - val_accuracy: 0.6824\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6315 - val_loss: 0.6201 - val_accuracy: 0.6913\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6360 - val_loss: 0.6227 - val_accuracy: 0.6875\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6297 - val_loss: 0.6269 - val_accuracy: 0.6620\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6350 - val_loss: 0.6267 - val_accuracy: 0.6747\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6350 - val_loss: 0.6281 - val_accuracy: 0.6684\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6429 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6452 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6380 - val_loss: 0.6214 - val_accuracy: 0.6862\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6379 - val_loss: 0.6235 - val_accuracy: 0.6760\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6383 - val_loss: 0.6254 - val_accuracy: 0.6824\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6377 - val_loss: 0.6199 - val_accuracy: 0.6926\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6439 - val_loss: 0.6298 - val_accuracy: 0.6518\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6453 - val_loss: 0.6275 - val_accuracy: 0.6684\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6404 - val_loss: 0.6327 - val_accuracy: 0.6403\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6492 - val_loss: 0.6262 - val_accuracy: 0.6633\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6394 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6412 - val_loss: 0.6231 - val_accuracy: 0.6786\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6431 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6426 - val_loss: 0.6238 - val_accuracy: 0.6760\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6400 - val_loss: 0.6276 - val_accuracy: 0.6760\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6369 - accuracy: 0.6472 - val_loss: 0.6265 - val_accuracy: 0.6696\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6404 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6369 - val_loss: 0.6253 - val_accuracy: 0.6837\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6456 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6480 - val_loss: 0.6246 - val_accuracy: 0.6684\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6437 - val_loss: 0.6258 - val_accuracy: 0.6696\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6438 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6420 - accuracy: 0.6392 - val_loss: 0.6262 - val_accuracy: 0.6747\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6409 - val_loss: 0.6229 - val_accuracy: 0.6798\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6459 - val_loss: 0.6265 - val_accuracy: 0.6722\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6545 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6454 - val_loss: 0.6194 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6449 - val_loss: 0.6216 - val_accuracy: 0.6773\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6505 - val_loss: 0.6219 - val_accuracy: 0.6747\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6537 - val_loss: 0.6217 - val_accuracy: 0.6773\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6347 - accuracy: 0.6443 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6555 - val_loss: 0.6237 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6485 - val_loss: 0.6225 - val_accuracy: 0.6849\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6345 - accuracy: 0.6446 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6472 - val_loss: 0.6190 - val_accuracy: 0.6888\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6505 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6507 - val_loss: 0.6157 - val_accuracy: 0.6939\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6503 - val_loss: 0.6190 - val_accuracy: 0.6837\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6518 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6532 - val_loss: 0.6245 - val_accuracy: 0.6696\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6464 - val_loss: 0.6201 - val_accuracy: 0.6747\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6331 - accuracy: 0.6456 - val_loss: 0.6245 - val_accuracy: 0.6709\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6444 - val_loss: 0.6260 - val_accuracy: 0.6671\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6491 - val_loss: 0.6189 - val_accuracy: 0.6824\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6555 - val_loss: 0.6185 - val_accuracy: 0.6862\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6513 - val_loss: 0.6248 - val_accuracy: 0.6760\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6497 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6461 - val_loss: 0.6284 - val_accuracy: 0.6671\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6338 - accuracy: 0.6472 - val_loss: 0.6222 - val_accuracy: 0.6773\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6555 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6534 - val_loss: 0.6243 - val_accuracy: 0.6722\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6463 - val_loss: 0.6205 - val_accuracy: 0.6875\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6525 - val_loss: 0.6174 - val_accuracy: 0.6952\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6542 - val_loss: 0.6202 - val_accuracy: 0.6862\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6576 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6575 - val_loss: 0.6211 - val_accuracy: 0.6824\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6594 - val_loss: 0.6192 - val_accuracy: 0.6837\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6510 - val_loss: 0.6205 - val_accuracy: 0.6875\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6570 - val_loss: 0.6185 - val_accuracy: 0.6862\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6554 - val_loss: 0.6207 - val_accuracy: 0.6849\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6551 - val_loss: 0.6229 - val_accuracy: 0.6786\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6551 - val_loss: 0.6237 - val_accuracy: 0.6798\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6571 - val_loss: 0.6180 - val_accuracy: 0.6837\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6571 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6619 - val_loss: 0.6224 - val_accuracy: 0.6747\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6576 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6248 - accuracy: 0.6596 - val_loss: 0.6239 - val_accuracy: 0.6696\n",
      "Calculating for: 700 50 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_276 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8237 - accuracy: 0.5201 - val_loss: 0.6837 - val_accuracy: 0.5663\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7357 - accuracy: 0.5324 - val_loss: 0.6550 - val_accuracy: 0.6339\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7012 - accuracy: 0.5588 - val_loss: 0.6547 - val_accuracy: 0.6288\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6807 - accuracy: 0.5809 - val_loss: 0.6514 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5806 - val_loss: 0.6553 - val_accuracy: 0.6327\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5801 - val_loss: 0.6486 - val_accuracy: 0.6531\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6695 - accuracy: 0.5868 - val_loss: 0.6472 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6652 - accuracy: 0.5944 - val_loss: 0.6417 - val_accuracy: 0.6569\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6060 - val_loss: 0.6474 - val_accuracy: 0.6429\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.6032 - val_loss: 0.6363 - val_accuracy: 0.6671\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6046 - val_loss: 0.6362 - val_accuracy: 0.6645\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6606 - accuracy: 0.6104 - val_loss: 0.6380 - val_accuracy: 0.6582\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6095 - val_loss: 0.6350 - val_accuracy: 0.6773\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6168 - val_loss: 0.6326 - val_accuracy: 0.6645\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6602 - accuracy: 0.6128 - val_loss: 0.6357 - val_accuracy: 0.6671\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6105 - val_loss: 0.6323 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6155 - val_loss: 0.6320 - val_accuracy: 0.6671\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6186 - val_loss: 0.6327 - val_accuracy: 0.6722\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6560 - accuracy: 0.6153 - val_loss: 0.6330 - val_accuracy: 0.6645\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6191 - val_loss: 0.6268 - val_accuracy: 0.6786\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6197 - val_loss: 0.6344 - val_accuracy: 0.6594\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6515 - accuracy: 0.6217 - val_loss: 0.6304 - val_accuracy: 0.6811\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6505 - accuracy: 0.6237 - val_loss: 0.6294 - val_accuracy: 0.6722\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6285 - val_loss: 0.6322 - val_accuracy: 0.6556\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6449 - accuracy: 0.6255 - val_loss: 0.6209 - val_accuracy: 0.6798\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6281 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6480 - accuracy: 0.6294 - val_loss: 0.6304 - val_accuracy: 0.6773\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6256 - val_loss: 0.6301 - val_accuracy: 0.6760\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6453 - accuracy: 0.6321 - val_loss: 0.6283 - val_accuracy: 0.6773\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6436 - accuracy: 0.6316 - val_loss: 0.6239 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6445 - accuracy: 0.6335 - val_loss: 0.6237 - val_accuracy: 0.6811\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6410 - val_loss: 0.6244 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6385 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6408 - accuracy: 0.6373 - val_loss: 0.6223 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6368 - val_loss: 0.6234 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6370 - accuracy: 0.6389 - val_loss: 0.6173 - val_accuracy: 0.6798\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6431 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6420 - accuracy: 0.6358 - val_loss: 0.6213 - val_accuracy: 0.6786\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6356 - val_loss: 0.6201 - val_accuracy: 0.6798\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6434 - val_loss: 0.6164 - val_accuracy: 0.6824\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6367 - accuracy: 0.6395 - val_loss: 0.6154 - val_accuracy: 0.6849\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6426 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6437 - val_loss: 0.6152 - val_accuracy: 0.6952\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6433 - val_loss: 0.6175 - val_accuracy: 0.6849\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6478 - val_loss: 0.6164 - val_accuracy: 0.6913\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6434 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6500 - val_loss: 0.6159 - val_accuracy: 0.6888\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6490 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6518 - val_loss: 0.6143 - val_accuracy: 0.6901\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6261 - accuracy: 0.6556 - val_loss: 0.6188 - val_accuracy: 0.6773\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6464 - val_loss: 0.6158 - val_accuracy: 0.6939\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6261 - accuracy: 0.6581 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6529 - val_loss: 0.6108 - val_accuracy: 0.6990\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6517 - val_loss: 0.6151 - val_accuracy: 0.6901\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6539 - val_loss: 0.6118 - val_accuracy: 0.6964\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6277 - accuracy: 0.6488 - val_loss: 0.6086 - val_accuracy: 0.6964\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6562 - val_loss: 0.6110 - val_accuracy: 0.6926\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6277 - accuracy: 0.6526 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6583 - val_loss: 0.6120 - val_accuracy: 0.6952\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6222 - accuracy: 0.6534 - val_loss: 0.6090 - val_accuracy: 0.6939\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6577 - val_loss: 0.6131 - val_accuracy: 0.6926\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6536 - val_loss: 0.6129 - val_accuracy: 0.6939\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6249 - accuracy: 0.6614 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6626 - val_loss: 0.6127 - val_accuracy: 0.6913\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6195 - accuracy: 0.6596 - val_loss: 0.6093 - val_accuracy: 0.6990\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6189 - accuracy: 0.6650 - val_loss: 0.6111 - val_accuracy: 0.6952\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6585 - val_loss: 0.6103 - val_accuracy: 0.6939\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6589 - val_loss: 0.6100 - val_accuracy: 0.6977\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6616 - val_loss: 0.6119 - val_accuracy: 0.6913\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6139 - accuracy: 0.6693 - val_loss: 0.6098 - val_accuracy: 0.6952\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6643 - val_loss: 0.6107 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6173 - accuracy: 0.6678 - val_loss: 0.6127 - val_accuracy: 0.6888\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6157 - accuracy: 0.6650 - val_loss: 0.6083 - val_accuracy: 0.6990\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6125 - accuracy: 0.6714 - val_loss: 0.6122 - val_accuracy: 0.6875\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6167 - accuracy: 0.6655 - val_loss: 0.6116 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6082 - accuracy: 0.6755 - val_loss: 0.6120 - val_accuracy: 0.6862\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6102 - accuracy: 0.6756 - val_loss: 0.6106 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6116 - accuracy: 0.6686 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6722 - val_loss: 0.6150 - val_accuracy: 0.6811\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6109 - accuracy: 0.6743 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6083 - accuracy: 0.6718 - val_loss: 0.6141 - val_accuracy: 0.6849\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6126 - accuracy: 0.6663 - val_loss: 0.6112 - val_accuracy: 0.6849\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6084 - accuracy: 0.6753 - val_loss: 0.6113 - val_accuracy: 0.6849\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6047 - accuracy: 0.6755 - val_loss: 0.6131 - val_accuracy: 0.6811\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6046 - accuracy: 0.6837 - val_loss: 0.6155 - val_accuracy: 0.6760\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6751 - val_loss: 0.6134 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6047 - accuracy: 0.6723 - val_loss: 0.6168 - val_accuracy: 0.6773\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6016 - accuracy: 0.6795 - val_loss: 0.6166 - val_accuracy: 0.6798\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6078 - accuracy: 0.6733 - val_loss: 0.6129 - val_accuracy: 0.6862\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6026 - accuracy: 0.6781 - val_loss: 0.6157 - val_accuracy: 0.6760\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6025 - accuracy: 0.6816 - val_loss: 0.6125 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6071 - accuracy: 0.6743 - val_loss: 0.6118 - val_accuracy: 0.6811\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6073 - accuracy: 0.6745 - val_loss: 0.6141 - val_accuracy: 0.6811\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6044 - accuracy: 0.6794 - val_loss: 0.6153 - val_accuracy: 0.6798\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6001 - accuracy: 0.6859 - val_loss: 0.6160 - val_accuracy: 0.6747\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6015 - accuracy: 0.6799 - val_loss: 0.6199 - val_accuracy: 0.6760\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6008 - accuracy: 0.6794 - val_loss: 0.6156 - val_accuracy: 0.6811\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6015 - accuracy: 0.6815 - val_loss: 0.6180 - val_accuracy: 0.6824\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6020 - accuracy: 0.6778 - val_loss: 0.6147 - val_accuracy: 0.6837\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.6850 - val_loss: 0.6172 - val_accuracy: 0.6824\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6002 - accuracy: 0.6819 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.5945 - accuracy: 0.6874 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5943 - accuracy: 0.6855 - val_loss: 0.6177 - val_accuracy: 0.6786\n",
      "Calculating for: 700 50 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_280 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7986 - accuracy: 0.5325 - val_loss: 0.6468 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7241 - accuracy: 0.5383 - val_loss: 0.6496 - val_accuracy: 0.6467\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6976 - accuracy: 0.5534 - val_loss: 0.6521 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6880 - accuracy: 0.5510 - val_loss: 0.6510 - val_accuracy: 0.6594\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6801 - accuracy: 0.5745 - val_loss: 0.6499 - val_accuracy: 0.6620\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6759 - accuracy: 0.5736 - val_loss: 0.6466 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6743 - accuracy: 0.5833 - val_loss: 0.6511 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5872 - val_loss: 0.6426 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6727 - accuracy: 0.5872 - val_loss: 0.6421 - val_accuracy: 0.6633\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5854 - val_loss: 0.6432 - val_accuracy: 0.6633\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5824 - val_loss: 0.6442 - val_accuracy: 0.6722\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5973 - val_loss: 0.6410 - val_accuracy: 0.6633\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6667 - accuracy: 0.5958 - val_loss: 0.6423 - val_accuracy: 0.6722\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6060 - val_loss: 0.6405 - val_accuracy: 0.6658\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6039 - val_loss: 0.6432 - val_accuracy: 0.6709\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6080 - val_loss: 0.6359 - val_accuracy: 0.6786\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6046 - val_loss: 0.6389 - val_accuracy: 0.6709\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6031 - val_loss: 0.6396 - val_accuracy: 0.6747\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6071 - val_loss: 0.6346 - val_accuracy: 0.6735\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6049 - val_loss: 0.6345 - val_accuracy: 0.6722\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6593 - accuracy: 0.6098 - val_loss: 0.6328 - val_accuracy: 0.6709\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6631 - accuracy: 0.6035 - val_loss: 0.6399 - val_accuracy: 0.6786\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6138 - val_loss: 0.6343 - val_accuracy: 0.6798\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6570 - accuracy: 0.6105 - val_loss: 0.6351 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6130 - val_loss: 0.6346 - val_accuracy: 0.6837\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6187 - val_loss: 0.6303 - val_accuracy: 0.6811\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6601 - accuracy: 0.6103 - val_loss: 0.6309 - val_accuracy: 0.6862\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6154 - val_loss: 0.6346 - val_accuracy: 0.6773\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6218 - val_loss: 0.6284 - val_accuracy: 0.6849\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6203 - val_loss: 0.6288 - val_accuracy: 0.6837\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6119 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6217 - val_loss: 0.6326 - val_accuracy: 0.6773\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6266 - val_loss: 0.6332 - val_accuracy: 0.6671\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6172 - val_loss: 0.6313 - val_accuracy: 0.6811\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6238 - val_loss: 0.6320 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6280 - val_loss: 0.6289 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6265 - val_loss: 0.6290 - val_accuracy: 0.6901\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6266 - val_loss: 0.6311 - val_accuracy: 0.6875\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6212 - val_loss: 0.6282 - val_accuracy: 0.6862\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6216 - val_loss: 0.6238 - val_accuracy: 0.6875\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6265 - val_loss: 0.6317 - val_accuracy: 0.6696\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6274 - val_loss: 0.6266 - val_accuracy: 0.6760\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6326 - val_loss: 0.6273 - val_accuracy: 0.6824\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6300 - val_loss: 0.6228 - val_accuracy: 0.6811\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6361 - val_loss: 0.6196 - val_accuracy: 0.6888\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6374 - val_loss: 0.6237 - val_accuracy: 0.6862\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6361 - val_loss: 0.6195 - val_accuracy: 0.6862\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6296 - val_loss: 0.6300 - val_accuracy: 0.6798\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6343 - val_loss: 0.6247 - val_accuracy: 0.6837\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6368 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6302 - val_loss: 0.6237 - val_accuracy: 0.6837\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6436 - val_loss: 0.6291 - val_accuracy: 0.6760\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6427 - val_loss: 0.6218 - val_accuracy: 0.6849\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6369 - val_loss: 0.6198 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6409 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6353 - val_loss: 0.6197 - val_accuracy: 0.6888\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6350 - val_loss: 0.6241 - val_accuracy: 0.6786\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6364 - val_loss: 0.6209 - val_accuracy: 0.6875\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6441 - val_loss: 0.6191 - val_accuracy: 0.6888\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6410 - val_loss: 0.6223 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6384 - accuracy: 0.6441 - val_loss: 0.6179 - val_accuracy: 0.6862\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6365 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6372 - val_loss: 0.6159 - val_accuracy: 0.6875\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6443 - val_loss: 0.6185 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6456 - val_loss: 0.6228 - val_accuracy: 0.6837\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6420 - val_loss: 0.6209 - val_accuracy: 0.6760\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6458 - val_loss: 0.6211 - val_accuracy: 0.6837\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6434 - val_loss: 0.6185 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6373 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6413 - val_loss: 0.6181 - val_accuracy: 0.6875\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6477 - val_loss: 0.6188 - val_accuracy: 0.6901\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6457 - val_loss: 0.6207 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6415 - val_loss: 0.6201 - val_accuracy: 0.6875\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6466 - val_loss: 0.6224 - val_accuracy: 0.6862\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6527 - val_loss: 0.6194 - val_accuracy: 0.6901\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6522 - val_loss: 0.6180 - val_accuracy: 0.6913\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6451 - val_loss: 0.6205 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6480 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6546 - val_loss: 0.6207 - val_accuracy: 0.6926\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6511 - val_loss: 0.6201 - val_accuracy: 0.6939\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6476 - val_loss: 0.6199 - val_accuracy: 0.6964\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6477 - val_loss: 0.6209 - val_accuracy: 0.6939\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6521 - val_loss: 0.6195 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6599 - val_loss: 0.6215 - val_accuracy: 0.6875\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6532 - val_loss: 0.6215 - val_accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6554 - val_loss: 0.6158 - val_accuracy: 0.6901\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6314 - accuracy: 0.6520 - val_loss: 0.6154 - val_accuracy: 0.6913\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6536 - val_loss: 0.6168 - val_accuracy: 0.6952\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6497 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6603 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6600 - val_loss: 0.6195 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6581 - val_loss: 0.6138 - val_accuracy: 0.6888\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6559 - val_loss: 0.6166 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6606 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6588 - val_loss: 0.6163 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6541 - val_loss: 0.6193 - val_accuracy: 0.6939\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6629 - val_loss: 0.6147 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6598 - val_loss: 0.6175 - val_accuracy: 0.6837\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6580 - val_loss: 0.6147 - val_accuracy: 0.6913\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6557 - val_loss: 0.6138 - val_accuracy: 0.6913\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6628 - val_loss: 0.6181 - val_accuracy: 0.6888\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6212 - accuracy: 0.6626 - val_loss: 0.6150 - val_accuracy: 0.6888\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6616 - val_loss: 0.6175 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6576 - val_loss: 0.6199 - val_accuracy: 0.6888\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6223 - accuracy: 0.6590 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6217 - accuracy: 0.6623 - val_loss: 0.6197 - val_accuracy: 0.6824\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6583 - val_loss: 0.6155 - val_accuracy: 0.6901\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6146 - accuracy: 0.6683 - val_loss: 0.6197 - val_accuracy: 0.6888\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6571 - val_loss: 0.6152 - val_accuracy: 0.6913\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6175 - accuracy: 0.6731 - val_loss: 0.6132 - val_accuracy: 0.6888\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6650 - val_loss: 0.6133 - val_accuracy: 0.6901\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6635 - val_loss: 0.6146 - val_accuracy: 0.6926\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6623 - val_loss: 0.6160 - val_accuracy: 0.6888\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6717 - val_loss: 0.6186 - val_accuracy: 0.6888\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6153 - accuracy: 0.6669 - val_loss: 0.6129 - val_accuracy: 0.6901\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6136 - accuracy: 0.6664 - val_loss: 0.6130 - val_accuracy: 0.6888\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6128 - accuracy: 0.6729 - val_loss: 0.6155 - val_accuracy: 0.6901\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6603 - val_loss: 0.6175 - val_accuracy: 0.6926\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6703 - val_loss: 0.6190 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6648 - val_loss: 0.6201 - val_accuracy: 0.6862\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6117 - accuracy: 0.6697 - val_loss: 0.6176 - val_accuracy: 0.6926\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6123 - accuracy: 0.6703 - val_loss: 0.6190 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6630 - val_loss: 0.6212 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6683 - val_loss: 0.6214 - val_accuracy: 0.6862\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6631 - val_loss: 0.6151 - val_accuracy: 0.6862\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6689 - val_loss: 0.6160 - val_accuracy: 0.6901\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6116 - accuracy: 0.6721 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6699 - val_loss: 0.6205 - val_accuracy: 0.6901\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6650 - val_loss: 0.6166 - val_accuracy: 0.6888\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6122 - accuracy: 0.6706 - val_loss: 0.6185 - val_accuracy: 0.6798\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6131 - accuracy: 0.6723 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6139 - accuracy: 0.6644 - val_loss: 0.6195 - val_accuracy: 0.6837\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6768 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6100 - accuracy: 0.6756 - val_loss: 0.6219 - val_accuracy: 0.6837\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6063 - accuracy: 0.6727 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6755 - val_loss: 0.6170 - val_accuracy: 0.6849\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6737 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6047 - accuracy: 0.6763 - val_loss: 0.6169 - val_accuracy: 0.6888\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6098 - accuracy: 0.6751 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6736 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6024 - accuracy: 0.6822 - val_loss: 0.6192 - val_accuracy: 0.6862\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6087 - accuracy: 0.6708 - val_loss: 0.6202 - val_accuracy: 0.6837\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6083 - accuracy: 0.6791 - val_loss: 0.6160 - val_accuracy: 0.6913\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6804 - val_loss: 0.6225 - val_accuracy: 0.6849\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6048 - accuracy: 0.6791 - val_loss: 0.6212 - val_accuracy: 0.6875\n",
      "Calculating for: 700 50 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_284 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8244 - accuracy: 0.5211 - val_loss: 0.6552 - val_accuracy: 0.6250\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7430 - accuracy: 0.5225 - val_loss: 0.6606 - val_accuracy: 0.6390\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7099 - accuracy: 0.5252 - val_loss: 0.6643 - val_accuracy: 0.6339\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6995 - accuracy: 0.5275 - val_loss: 0.6620 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6938 - accuracy: 0.5315 - val_loss: 0.6621 - val_accuracy: 0.6403\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5453 - val_loss: 0.6576 - val_accuracy: 0.6429\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6841 - accuracy: 0.5472 - val_loss: 0.6606 - val_accuracy: 0.6467\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6838 - accuracy: 0.5549 - val_loss: 0.6545 - val_accuracy: 0.6378\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5649 - val_loss: 0.6592 - val_accuracy: 0.6454\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6797 - accuracy: 0.5697 - val_loss: 0.6569 - val_accuracy: 0.6531\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5710 - val_loss: 0.6582 - val_accuracy: 0.6518\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6797 - accuracy: 0.5752 - val_loss: 0.6540 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6803 - accuracy: 0.5722 - val_loss: 0.6545 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6798 - accuracy: 0.5739 - val_loss: 0.6569 - val_accuracy: 0.6556\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6762 - accuracy: 0.5764 - val_loss: 0.6484 - val_accuracy: 0.6505\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6745 - accuracy: 0.5849 - val_loss: 0.6521 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6746 - accuracy: 0.5819 - val_loss: 0.6496 - val_accuracy: 0.6543\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6735 - accuracy: 0.5804 - val_loss: 0.6483 - val_accuracy: 0.6531\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6757 - accuracy: 0.5784 - val_loss: 0.6475 - val_accuracy: 0.6543\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6769 - accuracy: 0.5715 - val_loss: 0.6553 - val_accuracy: 0.6633\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5844 - val_loss: 0.6474 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5850 - val_loss: 0.6513 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5854 - val_loss: 0.6431 - val_accuracy: 0.6684\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5919 - val_loss: 0.6444 - val_accuracy: 0.6709\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6691 - accuracy: 0.5956 - val_loss: 0.6454 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5867 - val_loss: 0.6423 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6684 - accuracy: 0.5943 - val_loss: 0.6429 - val_accuracy: 0.6709\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5913 - val_loss: 0.6436 - val_accuracy: 0.6671\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6715 - accuracy: 0.5941 - val_loss: 0.6517 - val_accuracy: 0.6709\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.6034 - val_loss: 0.6393 - val_accuracy: 0.6633\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6021 - val_loss: 0.6391 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5936 - val_loss: 0.6428 - val_accuracy: 0.6684\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5996 - val_loss: 0.6434 - val_accuracy: 0.6684\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5977 - val_loss: 0.6410 - val_accuracy: 0.6747\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.6001 - val_loss: 0.6364 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6670 - accuracy: 0.5976 - val_loss: 0.6384 - val_accuracy: 0.6696\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6006 - val_loss: 0.6373 - val_accuracy: 0.6773\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6619 - accuracy: 0.6024 - val_loss: 0.6342 - val_accuracy: 0.6658\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.5992 - val_loss: 0.6400 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6665 - accuracy: 0.5996 - val_loss: 0.6407 - val_accuracy: 0.6735\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6039 - val_loss: 0.6383 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6036 - val_loss: 0.6402 - val_accuracy: 0.6735\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6056 - val_loss: 0.6370 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6641 - accuracy: 0.6049 - val_loss: 0.6362 - val_accuracy: 0.6722\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6045 - val_loss: 0.6411 - val_accuracy: 0.6709\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6645 - accuracy: 0.6010 - val_loss: 0.6385 - val_accuracy: 0.6709\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6107 - val_loss: 0.6365 - val_accuracy: 0.6747\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6099 - val_loss: 0.6343 - val_accuracy: 0.6773\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6039 - val_loss: 0.6395 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6118 - val_loss: 0.6333 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6640 - accuracy: 0.6012 - val_loss: 0.6372 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6133 - val_loss: 0.6344 - val_accuracy: 0.6773\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6113 - val_loss: 0.6354 - val_accuracy: 0.6747\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6134 - val_loss: 0.6404 - val_accuracy: 0.6645\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6138 - val_loss: 0.6361 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6149 - val_loss: 0.6303 - val_accuracy: 0.6786\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6113 - val_loss: 0.6305 - val_accuracy: 0.6798\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6563 - accuracy: 0.6230 - val_loss: 0.6280 - val_accuracy: 0.6786\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6271 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6099 - val_loss: 0.6302 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6150 - val_loss: 0.6335 - val_accuracy: 0.6735\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6144 - val_loss: 0.6296 - val_accuracy: 0.6824\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6236 - val_loss: 0.6295 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6114 - val_loss: 0.6340 - val_accuracy: 0.6837\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6203 - val_loss: 0.6301 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6218 - val_loss: 0.6314 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6222 - val_loss: 0.6264 - val_accuracy: 0.6824\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6245 - val_loss: 0.6281 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6184 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6183 - val_loss: 0.6305 - val_accuracy: 0.6786\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6242 - val_loss: 0.6293 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6245 - val_loss: 0.6284 - val_accuracy: 0.6862\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6174 - val_loss: 0.6260 - val_accuracy: 0.6811\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6222 - val_loss: 0.6301 - val_accuracy: 0.6798\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6245 - val_loss: 0.6282 - val_accuracy: 0.6837\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6162 - val_loss: 0.6278 - val_accuracy: 0.6849\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6193 - val_loss: 0.6280 - val_accuracy: 0.6875\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6261 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6261 - val_loss: 0.6356 - val_accuracy: 0.6760\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6251 - val_loss: 0.6269 - val_accuracy: 0.6862\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6211 - val_loss: 0.6290 - val_accuracy: 0.6888\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6250 - val_loss: 0.6267 - val_accuracy: 0.6901\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6227 - val_loss: 0.6247 - val_accuracy: 0.6888\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6228 - val_loss: 0.6259 - val_accuracy: 0.6888\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6481 - accuracy: 0.6302 - val_loss: 0.6256 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6275 - val_loss: 0.6269 - val_accuracy: 0.6862\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6301 - val_loss: 0.6236 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6277 - val_loss: 0.6244 - val_accuracy: 0.6913\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6266 - val_loss: 0.6278 - val_accuracy: 0.6824\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6260 - val_loss: 0.6313 - val_accuracy: 0.6901\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6297 - val_loss: 0.6238 - val_accuracy: 0.6862\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6272 - val_loss: 0.6237 - val_accuracy: 0.6926\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6294 - val_loss: 0.6226 - val_accuracy: 0.6875\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6258 - val_loss: 0.6190 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6355 - val_loss: 0.6233 - val_accuracy: 0.6901\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6504 - accuracy: 0.6319 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6415 - val_loss: 0.6179 - val_accuracy: 0.6952\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6321 - val_loss: 0.6226 - val_accuracy: 0.6837\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6280 - val_loss: 0.6202 - val_accuracy: 0.6824\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6380 - val_loss: 0.6174 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6296 - val_loss: 0.6215 - val_accuracy: 0.6824\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6450 - accuracy: 0.6353 - val_loss: 0.6220 - val_accuracy: 0.6862\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6378 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6272 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6377 - val_loss: 0.6214 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6319 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.6284 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6349 - val_loss: 0.6195 - val_accuracy: 0.6888\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6398 - accuracy: 0.6422 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6310 - val_loss: 0.6268 - val_accuracy: 0.6837\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6304 - val_loss: 0.6283 - val_accuracy: 0.6888\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6350 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6390 - val_loss: 0.6226 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6321 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6374 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6344 - val_loss: 0.6217 - val_accuracy: 0.6849\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6417 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6373 - val_loss: 0.6204 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6428 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6395 - val_loss: 0.6205 - val_accuracy: 0.6862\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6422 - accuracy: 0.6400 - val_loss: 0.6208 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.6310 - val_loss: 0.6238 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6392 - val_loss: 0.6196 - val_accuracy: 0.6875\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6321 - val_loss: 0.6178 - val_accuracy: 0.6875\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6412 - val_loss: 0.6191 - val_accuracy: 0.6862\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6379 - val_loss: 0.6181 - val_accuracy: 0.6875\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6420 - val_loss: 0.6171 - val_accuracy: 0.6888\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6358 - val_loss: 0.6163 - val_accuracy: 0.6913\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6413 - val_loss: 0.6220 - val_accuracy: 0.6913\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6424 - val_loss: 0.6190 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6423 - val_loss: 0.6152 - val_accuracy: 0.6926\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6433 - val_loss: 0.6178 - val_accuracy: 0.6913\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6417 - accuracy: 0.6387 - val_loss: 0.6229 - val_accuracy: 0.6888\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6417 - val_loss: 0.6152 - val_accuracy: 0.6939\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6374 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6387 - val_loss: 0.6195 - val_accuracy: 0.6901\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.6516 - val_loss: 0.6142 - val_accuracy: 0.6888\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6433 - val_loss: 0.6155 - val_accuracy: 0.6952\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6461 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6418 - val_loss: 0.6182 - val_accuracy: 0.6913\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6431 - val_loss: 0.6185 - val_accuracy: 0.6901\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6429 - val_loss: 0.6151 - val_accuracy: 0.6913\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6473 - val_loss: 0.6166 - val_accuracy: 0.6888\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6471 - val_loss: 0.6135 - val_accuracy: 0.6888\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6487 - val_loss: 0.6168 - val_accuracy: 0.6926\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6508 - val_loss: 0.6163 - val_accuracy: 0.6926\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6475 - val_loss: 0.6164 - val_accuracy: 0.6926\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6501 - val_loss: 0.6166 - val_accuracy: 0.6952\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6408 - val_loss: 0.6158 - val_accuracy: 0.6913\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6468 - val_loss: 0.6146 - val_accuracy: 0.7003\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6530 - val_loss: 0.6168 - val_accuracy: 0.6964\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6427 - val_loss: 0.6183 - val_accuracy: 0.6862\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6535 - val_loss: 0.6118 - val_accuracy: 0.7015\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6516 - val_loss: 0.6132 - val_accuracy: 0.6977\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6512 - val_loss: 0.6144 - val_accuracy: 0.6977\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6518 - val_loss: 0.6137 - val_accuracy: 0.6990\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6518 - val_loss: 0.6136 - val_accuracy: 0.6952\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6507 - val_loss: 0.6104 - val_accuracy: 0.6964\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6544 - val_loss: 0.6125 - val_accuracy: 0.7015\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6472 - val_loss: 0.6148 - val_accuracy: 0.7015\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6534 - val_loss: 0.6130 - val_accuracy: 0.7015\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6517 - val_loss: 0.6147 - val_accuracy: 0.6952\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6593 - val_loss: 0.6219 - val_accuracy: 0.6952\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6527 - val_loss: 0.6169 - val_accuracy: 0.7015\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6410 - val_loss: 0.6150 - val_accuracy: 0.6964\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6525 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6462 - val_loss: 0.6116 - val_accuracy: 0.6926\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6595 - val_loss: 0.6108 - val_accuracy: 0.6964\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6264 - accuracy: 0.6575 - val_loss: 0.6147 - val_accuracy: 0.7003\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6541 - val_loss: 0.6113 - val_accuracy: 0.7003\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6497 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6600 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6515 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6603 - val_loss: 0.6123 - val_accuracy: 0.6939\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6600 - val_loss: 0.6187 - val_accuracy: 0.6849\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6604 - val_loss: 0.6147 - val_accuracy: 0.6862\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6606 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6532 - val_loss: 0.6116 - val_accuracy: 0.6849\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6561 - val_loss: 0.6126 - val_accuracy: 0.6913\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6580 - val_loss: 0.6096 - val_accuracy: 0.6913\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6551 - val_loss: 0.6085 - val_accuracy: 0.6913\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6556 - val_loss: 0.6153 - val_accuracy: 0.6901\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6565 - val_loss: 0.6161 - val_accuracy: 0.6888\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6626 - val_loss: 0.6155 - val_accuracy: 0.6875\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6235 - accuracy: 0.6594 - val_loss: 0.6094 - val_accuracy: 0.6888\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6559 - val_loss: 0.6163 - val_accuracy: 0.6926\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6552 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6579 - val_loss: 0.6104 - val_accuracy: 0.6901\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6637 - val_loss: 0.6133 - val_accuracy: 0.6875\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6605 - val_loss: 0.6132 - val_accuracy: 0.6875\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6586 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6229 - accuracy: 0.6614 - val_loss: 0.6145 - val_accuracy: 0.6913\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6596 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6611 - val_loss: 0.6184 - val_accuracy: 0.6875\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6567 - val_loss: 0.6129 - val_accuracy: 0.6888\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6516 - val_loss: 0.6159 - val_accuracy: 0.6926\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6606 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6613 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6583 - val_loss: 0.6179 - val_accuracy: 0.6913\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6604 - val_loss: 0.6165 - val_accuracy: 0.6875\n",
      "Calculating for: 700 100 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_288 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8017 - accuracy: 0.5308 - val_loss: 0.6415 - val_accuracy: 0.6480\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7085 - accuracy: 0.5605 - val_loss: 0.6405 - val_accuracy: 0.6696\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6848 - accuracy: 0.5718 - val_loss: 0.6413 - val_accuracy: 0.6658\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5823 - val_loss: 0.6432 - val_accuracy: 0.6671\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.5824 - val_loss: 0.6390 - val_accuracy: 0.6709\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.5987 - val_loss: 0.6398 - val_accuracy: 0.6786\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6678 - accuracy: 0.5901 - val_loss: 0.6462 - val_accuracy: 0.6747\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6623 - accuracy: 0.6025 - val_loss: 0.6360 - val_accuracy: 0.6773\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6017 - val_loss: 0.6360 - val_accuracy: 0.6747\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6616 - accuracy: 0.6074 - val_loss: 0.6364 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6070 - val_loss: 0.6388 - val_accuracy: 0.6747\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6098 - val_loss: 0.6361 - val_accuracy: 0.6786\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6184 - val_loss: 0.6289 - val_accuracy: 0.6773\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6179 - val_loss: 0.6326 - val_accuracy: 0.6747\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6147 - val_loss: 0.6330 - val_accuracy: 0.6722\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6184 - val_loss: 0.6305 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6193 - val_loss: 0.6327 - val_accuracy: 0.6760\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6152 - val_loss: 0.6303 - val_accuracy: 0.6747\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6253 - val_loss: 0.6208 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6228 - val_loss: 0.6290 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6233 - val_loss: 0.6291 - val_accuracy: 0.6811\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6251 - val_loss: 0.6242 - val_accuracy: 0.6824\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6287 - val_loss: 0.6259 - val_accuracy: 0.6811\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6261 - val_loss: 0.6273 - val_accuracy: 0.6837\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6304 - val_loss: 0.6253 - val_accuracy: 0.6862\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6353 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6246 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6384 - val_loss: 0.6185 - val_accuracy: 0.6811\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6305 - val_loss: 0.6158 - val_accuracy: 0.6798\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6383 - val_loss: 0.6239 - val_accuracy: 0.6824\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6419 - val_loss: 0.6273 - val_accuracy: 0.6658\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6365 - accuracy: 0.6400 - val_loss: 0.6223 - val_accuracy: 0.6684\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6309 - val_loss: 0.6248 - val_accuracy: 0.6735\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6356 - val_loss: 0.6175 - val_accuracy: 0.6798\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6379 - val_loss: 0.6223 - val_accuracy: 0.6811\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6387 - val_loss: 0.6214 - val_accuracy: 0.6824\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6418 - val_loss: 0.6206 - val_accuracy: 0.6811\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6500 - val_loss: 0.6192 - val_accuracy: 0.6811\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6497 - val_loss: 0.6168 - val_accuracy: 0.6811\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6451 - val_loss: 0.6178 - val_accuracy: 0.6760\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6461 - val_loss: 0.6194 - val_accuracy: 0.6773\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6518 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6506 - val_loss: 0.6168 - val_accuracy: 0.6811\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6555 - val_loss: 0.6183 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6525 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6541 - val_loss: 0.6182 - val_accuracy: 0.6786\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6570 - val_loss: 0.6129 - val_accuracy: 0.6901\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6485 - val_loss: 0.6143 - val_accuracy: 0.6849\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6291 - accuracy: 0.6462 - val_loss: 0.6135 - val_accuracy: 0.6811\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6569 - val_loss: 0.6170 - val_accuracy: 0.6824\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6571 - val_loss: 0.6183 - val_accuracy: 0.6760\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6614 - val_loss: 0.6150 - val_accuracy: 0.6837\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6580 - val_loss: 0.6165 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6521 - val_loss: 0.6125 - val_accuracy: 0.6824\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6645 - val_loss: 0.6162 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6621 - val_loss: 0.6132 - val_accuracy: 0.6862\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6654 - val_loss: 0.6139 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6610 - val_loss: 0.6142 - val_accuracy: 0.6888\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6630 - val_loss: 0.6161 - val_accuracy: 0.6798\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6600 - val_loss: 0.6164 - val_accuracy: 0.6824\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6621 - val_loss: 0.6156 - val_accuracy: 0.6773\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6170 - accuracy: 0.6679 - val_loss: 0.6147 - val_accuracy: 0.6811\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6672 - val_loss: 0.6179 - val_accuracy: 0.6837\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6166 - accuracy: 0.6640 - val_loss: 0.6184 - val_accuracy: 0.6849\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6154 - accuracy: 0.6668 - val_loss: 0.6175 - val_accuracy: 0.6862\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6621 - val_loss: 0.6150 - val_accuracy: 0.6875\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6643 - val_loss: 0.6193 - val_accuracy: 0.6798\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6137 - accuracy: 0.6689 - val_loss: 0.6204 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6729 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6097 - accuracy: 0.6736 - val_loss: 0.6201 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6067 - accuracy: 0.6760 - val_loss: 0.6207 - val_accuracy: 0.6786\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6704 - val_loss: 0.6195 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6103 - accuracy: 0.6675 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6079 - accuracy: 0.6757 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6111 - accuracy: 0.6697 - val_loss: 0.6163 - val_accuracy: 0.6824\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6112 - accuracy: 0.6741 - val_loss: 0.6124 - val_accuracy: 0.6875\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6144 - accuracy: 0.6782 - val_loss: 0.6104 - val_accuracy: 0.6901\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6760 - val_loss: 0.6139 - val_accuracy: 0.6811\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6113 - accuracy: 0.6719 - val_loss: 0.6157 - val_accuracy: 0.6862\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6040 - accuracy: 0.6845 - val_loss: 0.6163 - val_accuracy: 0.6862\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5971 - accuracy: 0.6858 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6030 - accuracy: 0.6861 - val_loss: 0.6182 - val_accuracy: 0.6862\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5971 - accuracy: 0.6806 - val_loss: 0.6144 - val_accuracy: 0.6901\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6052 - accuracy: 0.6775 - val_loss: 0.6156 - val_accuracy: 0.6837\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6009 - accuracy: 0.6830 - val_loss: 0.6199 - val_accuracy: 0.6862\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6035 - accuracy: 0.6732 - val_loss: 0.6170 - val_accuracy: 0.6837\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5969 - accuracy: 0.6858 - val_loss: 0.6175 - val_accuracy: 0.6824\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6827 - val_loss: 0.6170 - val_accuracy: 0.6811\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5980 - accuracy: 0.6767 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5992 - accuracy: 0.6786 - val_loss: 0.6206 - val_accuracy: 0.6837\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5967 - accuracy: 0.6874 - val_loss: 0.6180 - val_accuracy: 0.6875\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6009 - accuracy: 0.6868 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.5954 - accuracy: 0.6868 - val_loss: 0.6231 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5974 - accuracy: 0.6820 - val_loss: 0.6201 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5924 - accuracy: 0.6893 - val_loss: 0.6240 - val_accuracy: 0.6786\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5919 - accuracy: 0.6869 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5952 - accuracy: 0.6904 - val_loss: 0.6215 - val_accuracy: 0.6824\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5966 - accuracy: 0.6863 - val_loss: 0.6151 - val_accuracy: 0.6888\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5950 - accuracy: 0.6952 - val_loss: 0.6182 - val_accuracy: 0.6849\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5942 - accuracy: 0.6908 - val_loss: 0.6189 - val_accuracy: 0.6862\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5883 - accuracy: 0.6958 - val_loss: 0.6183 - val_accuracy: 0.6849\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5890 - accuracy: 0.6930 - val_loss: 0.6193 - val_accuracy: 0.6824\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5926 - accuracy: 0.6898 - val_loss: 0.6233 - val_accuracy: 0.6811\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5862 - accuracy: 0.6958 - val_loss: 0.6265 - val_accuracy: 0.6786\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5961 - accuracy: 0.6874 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5819 - accuracy: 0.6971 - val_loss: 0.6248 - val_accuracy: 0.6773\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5856 - accuracy: 0.6962 - val_loss: 0.6228 - val_accuracy: 0.6875\n",
      "Calculating for: 700 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_292 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8087 - accuracy: 0.5252 - val_loss: 0.6748 - val_accuracy: 0.6148\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7191 - accuracy: 0.5455 - val_loss: 0.6628 - val_accuracy: 0.6352\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6921 - accuracy: 0.5491 - val_loss: 0.6558 - val_accuracy: 0.6467\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6824 - accuracy: 0.5649 - val_loss: 0.6525 - val_accuracy: 0.6658\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6767 - accuracy: 0.5696 - val_loss: 0.6507 - val_accuracy: 0.6709\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6755 - accuracy: 0.5820 - val_loss: 0.6483 - val_accuracy: 0.6696\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5840 - val_loss: 0.6486 - val_accuracy: 0.6607\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5860 - val_loss: 0.6448 - val_accuracy: 0.6645\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6648 - accuracy: 0.5985 - val_loss: 0.6452 - val_accuracy: 0.6467\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5899 - val_loss: 0.6447 - val_accuracy: 0.6531\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5973 - val_loss: 0.6473 - val_accuracy: 0.6416\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6703 - accuracy: 0.5906 - val_loss: 0.6470 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.5916 - val_loss: 0.6378 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6042 - val_loss: 0.6399 - val_accuracy: 0.6492\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5971 - val_loss: 0.6425 - val_accuracy: 0.6518\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.6004 - val_loss: 0.6427 - val_accuracy: 0.6518\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.5987 - val_loss: 0.6363 - val_accuracy: 0.6607\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6040 - val_loss: 0.6420 - val_accuracy: 0.6429\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6598 - accuracy: 0.6114 - val_loss: 0.6401 - val_accuracy: 0.6518\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6105 - val_loss: 0.6373 - val_accuracy: 0.6543\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6124 - val_loss: 0.6376 - val_accuracy: 0.6556\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6605 - accuracy: 0.6135 - val_loss: 0.6418 - val_accuracy: 0.6480\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6112 - val_loss: 0.6395 - val_accuracy: 0.6645\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6099 - val_loss: 0.6395 - val_accuracy: 0.6556\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6184 - val_loss: 0.6389 - val_accuracy: 0.6633\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6149 - val_loss: 0.6346 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6194 - val_loss: 0.6347 - val_accuracy: 0.6645\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6198 - val_loss: 0.6332 - val_accuracy: 0.6582\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6148 - val_loss: 0.6308 - val_accuracy: 0.6633\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6204 - val_loss: 0.6368 - val_accuracy: 0.6518\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6289 - val_loss: 0.6429 - val_accuracy: 0.6365\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6204 - val_loss: 0.6382 - val_accuracy: 0.6531\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6202 - val_loss: 0.6326 - val_accuracy: 0.6658\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6222 - val_loss: 0.6343 - val_accuracy: 0.6620\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6281 - val_loss: 0.6325 - val_accuracy: 0.6607\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6207 - val_loss: 0.6303 - val_accuracy: 0.6773\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6267 - val_loss: 0.6284 - val_accuracy: 0.6722\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6507 - accuracy: 0.6216 - val_loss: 0.6253 - val_accuracy: 0.6811\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6476 - accuracy: 0.6282 - val_loss: 0.6322 - val_accuracy: 0.6607\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6285 - val_loss: 0.6352 - val_accuracy: 0.6569\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6246 - val_loss: 0.6298 - val_accuracy: 0.6645\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6474 - accuracy: 0.6256 - val_loss: 0.6274 - val_accuracy: 0.6594\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6515 - accuracy: 0.6235 - val_loss: 0.6355 - val_accuracy: 0.6684\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6428 - accuracy: 0.6330 - val_loss: 0.6307 - val_accuracy: 0.6696\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6481 - accuracy: 0.6335 - val_loss: 0.6248 - val_accuracy: 0.6773\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6282 - val_loss: 0.6240 - val_accuracy: 0.6798\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6333 - val_loss: 0.6243 - val_accuracy: 0.6811\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6356 - val_loss: 0.6291 - val_accuracy: 0.6658\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6448 - accuracy: 0.6377 - val_loss: 0.6280 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6394 - accuracy: 0.6334 - val_loss: 0.6304 - val_accuracy: 0.6607\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6356 - val_loss: 0.6274 - val_accuracy: 0.6645\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6379 - val_loss: 0.6327 - val_accuracy: 0.6671\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.6353 - val_loss: 0.6271 - val_accuracy: 0.6684\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6412 - accuracy: 0.6321 - val_loss: 0.6289 - val_accuracy: 0.6696\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6420 - accuracy: 0.6368 - val_loss: 0.6225 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6414 - accuracy: 0.6392 - val_loss: 0.6262 - val_accuracy: 0.6684\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6400 - accuracy: 0.6432 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6417 - accuracy: 0.6395 - val_loss: 0.6240 - val_accuracy: 0.6735\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6400 - accuracy: 0.6387 - val_loss: 0.6249 - val_accuracy: 0.6735\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6388 - accuracy: 0.6402 - val_loss: 0.6273 - val_accuracy: 0.6722\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6390 - accuracy: 0.6413 - val_loss: 0.6214 - val_accuracy: 0.6760\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6369 - accuracy: 0.6429 - val_loss: 0.6267 - val_accuracy: 0.6633\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6418 - accuracy: 0.6368 - val_loss: 0.6278 - val_accuracy: 0.6735\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6401 - accuracy: 0.6377 - val_loss: 0.6315 - val_accuracy: 0.6671\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6316 - accuracy: 0.6464 - val_loss: 0.6272 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6361 - accuracy: 0.6434 - val_loss: 0.6246 - val_accuracy: 0.6786\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6343 - accuracy: 0.6461 - val_loss: 0.6259 - val_accuracy: 0.6773\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6377 - accuracy: 0.6438 - val_loss: 0.6264 - val_accuracy: 0.6760\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6358 - accuracy: 0.6454 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6372 - accuracy: 0.6442 - val_loss: 0.6211 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6338 - accuracy: 0.6487 - val_loss: 0.6235 - val_accuracy: 0.6722\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6447 - val_loss: 0.6264 - val_accuracy: 0.6786\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6393 - val_loss: 0.6216 - val_accuracy: 0.6824\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6512 - val_loss: 0.6152 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6332 - accuracy: 0.6490 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6313 - accuracy: 0.6486 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6335 - accuracy: 0.6490 - val_loss: 0.6244 - val_accuracy: 0.6760\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6306 - accuracy: 0.6473 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 2s 7ms/step - loss: 0.6327 - accuracy: 0.6466 - val_loss: 0.6246 - val_accuracy: 0.6773\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6310 - accuracy: 0.6569 - val_loss: 0.6261 - val_accuracy: 0.6798\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6273 - accuracy: 0.6566 - val_loss: 0.6224 - val_accuracy: 0.6811\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6274 - accuracy: 0.6531 - val_loss: 0.6256 - val_accuracy: 0.6773\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6596 - val_loss: 0.6247 - val_accuracy: 0.6786\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6559 - val_loss: 0.6275 - val_accuracy: 0.6709\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6270 - accuracy: 0.6527 - val_loss: 0.6241 - val_accuracy: 0.6824\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6505 - val_loss: 0.6219 - val_accuracy: 0.6862\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6547 - val_loss: 0.6212 - val_accuracy: 0.6901\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6238 - accuracy: 0.6572 - val_loss: 0.6224 - val_accuracy: 0.6901\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6577 - val_loss: 0.6282 - val_accuracy: 0.6811\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6275 - accuracy: 0.6567 - val_loss: 0.6235 - val_accuracy: 0.6875\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6552 - val_loss: 0.6251 - val_accuracy: 0.6786\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6201 - accuracy: 0.6610 - val_loss: 0.6197 - val_accuracy: 0.6901\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6193 - accuracy: 0.6664 - val_loss: 0.6251 - val_accuracy: 0.6811\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6187 - accuracy: 0.6606 - val_loss: 0.6228 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6267 - accuracy: 0.6547 - val_loss: 0.6194 - val_accuracy: 0.6875\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6583 - val_loss: 0.6159 - val_accuracy: 0.6952\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6185 - accuracy: 0.6643 - val_loss: 0.6213 - val_accuracy: 0.6888\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6610 - val_loss: 0.6209 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6634 - val_loss: 0.6216 - val_accuracy: 0.6837\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6227 - accuracy: 0.6580 - val_loss: 0.6255 - val_accuracy: 0.6798\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6631 - val_loss: 0.6273 - val_accuracy: 0.6747\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6576 - val_loss: 0.6203 - val_accuracy: 0.6913\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6645 - val_loss: 0.6240 - val_accuracy: 0.6837\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6165 - accuracy: 0.6613 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Calculating for: 700 100 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_296 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.8482 - accuracy: 0.5014 - val_loss: 0.6598 - val_accuracy: 0.6390\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 2s 8ms/step - loss: 0.7272 - accuracy: 0.5387 - val_loss: 0.6581 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7087 - accuracy: 0.5203 - val_loss: 0.6567 - val_accuracy: 0.6416\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6953 - accuracy: 0.5296 - val_loss: 0.6654 - val_accuracy: 0.6518\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6914 - accuracy: 0.5396 - val_loss: 0.6673 - val_accuracy: 0.6492\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6894 - accuracy: 0.5389 - val_loss: 0.6618 - val_accuracy: 0.6403\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6841 - accuracy: 0.5558 - val_loss: 0.6534 - val_accuracy: 0.6390\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6849 - accuracy: 0.5551 - val_loss: 0.6569 - val_accuracy: 0.6403\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6849 - accuracy: 0.5494 - val_loss: 0.6592 - val_accuracy: 0.6441\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.5664 - val_loss: 0.6529 - val_accuracy: 0.6390\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6813 - accuracy: 0.5649 - val_loss: 0.6532 - val_accuracy: 0.6441\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.5575 - val_loss: 0.6547 - val_accuracy: 0.6531\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6800 - accuracy: 0.5707 - val_loss: 0.6538 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5622 - val_loss: 0.6568 - val_accuracy: 0.6645\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6784 - accuracy: 0.5732 - val_loss: 0.6486 - val_accuracy: 0.6492\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6799 - accuracy: 0.5678 - val_loss: 0.6547 - val_accuracy: 0.6645\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6750 - accuracy: 0.5726 - val_loss: 0.6500 - val_accuracy: 0.6594\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6790 - accuracy: 0.5723 - val_loss: 0.6498 - val_accuracy: 0.6620\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5779 - val_loss: 0.6486 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6752 - accuracy: 0.5794 - val_loss: 0.6458 - val_accuracy: 0.6671\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6734 - accuracy: 0.5865 - val_loss: 0.6456 - val_accuracy: 0.6671\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6701 - accuracy: 0.5907 - val_loss: 0.6430 - val_accuracy: 0.6671\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6729 - accuracy: 0.5815 - val_loss: 0.6404 - val_accuracy: 0.6645\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5921 - val_loss: 0.6428 - val_accuracy: 0.6696\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5917 - val_loss: 0.6420 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.5907 - val_loss: 0.6450 - val_accuracy: 0.6569\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6677 - accuracy: 0.5927 - val_loss: 0.6391 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5971 - val_loss: 0.6365 - val_accuracy: 0.6684\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6684 - accuracy: 0.5961 - val_loss: 0.6375 - val_accuracy: 0.6671\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6657 - accuracy: 0.5980 - val_loss: 0.6378 - val_accuracy: 0.6671\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.6099 - val_loss: 0.6387 - val_accuracy: 0.6671\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6639 - accuracy: 0.6011 - val_loss: 0.6344 - val_accuracy: 0.6709\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.6017 - val_loss: 0.6380 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6661 - accuracy: 0.6035 - val_loss: 0.6431 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5955 - val_loss: 0.6404 - val_accuracy: 0.6735\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.6024 - val_loss: 0.6434 - val_accuracy: 0.6684\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.5992 - val_loss: 0.6400 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6017 - val_loss: 0.6353 - val_accuracy: 0.6760\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6649 - accuracy: 0.5967 - val_loss: 0.6382 - val_accuracy: 0.6722\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6081 - val_loss: 0.6457 - val_accuracy: 0.6658\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6617 - accuracy: 0.6107 - val_loss: 0.6386 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6604 - accuracy: 0.6118 - val_loss: 0.6370 - val_accuracy: 0.6811\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6608 - accuracy: 0.6042 - val_loss: 0.6362 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6110 - val_loss: 0.6326 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6091 - val_loss: 0.6343 - val_accuracy: 0.6837\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6569 - accuracy: 0.6162 - val_loss: 0.6321 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6115 - val_loss: 0.6399 - val_accuracy: 0.6696\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6627 - accuracy: 0.6041 - val_loss: 0.6394 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6091 - val_loss: 0.6355 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6594 - accuracy: 0.6168 - val_loss: 0.6331 - val_accuracy: 0.6862\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6587 - accuracy: 0.6113 - val_loss: 0.6305 - val_accuracy: 0.6798\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6123 - val_loss: 0.6323 - val_accuracy: 0.6862\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6212 - val_loss: 0.6303 - val_accuracy: 0.6773\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6153 - val_loss: 0.6278 - val_accuracy: 0.6824\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6152 - val_loss: 0.6387 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6556 - accuracy: 0.6125 - val_loss: 0.6289 - val_accuracy: 0.6837\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6564 - accuracy: 0.6158 - val_loss: 0.6349 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6162 - val_loss: 0.6358 - val_accuracy: 0.6798\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6541 - accuracy: 0.6194 - val_loss: 0.6334 - val_accuracy: 0.6798\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6163 - val_loss: 0.6362 - val_accuracy: 0.6773\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6173 - val_loss: 0.6281 - val_accuracy: 0.6875\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6232 - val_loss: 0.6306 - val_accuracy: 0.6773\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6194 - val_loss: 0.6297 - val_accuracy: 0.6786\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6222 - val_loss: 0.6275 - val_accuracy: 0.6773\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6524 - accuracy: 0.6204 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6238 - val_loss: 0.6335 - val_accuracy: 0.6849\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6184 - val_loss: 0.6370 - val_accuracy: 0.6747\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.6228 - val_loss: 0.6348 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6267 - val_loss: 0.6319 - val_accuracy: 0.6811\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6252 - val_loss: 0.6289 - val_accuracy: 0.6901\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6232 - val_loss: 0.6315 - val_accuracy: 0.6837\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6256 - val_loss: 0.6271 - val_accuracy: 0.6862\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6543 - accuracy: 0.6251 - val_loss: 0.6277 - val_accuracy: 0.6888\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.6216 - val_loss: 0.6280 - val_accuracy: 0.6824\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6495 - accuracy: 0.6266 - val_loss: 0.6257 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6509 - accuracy: 0.6324 - val_loss: 0.6263 - val_accuracy: 0.6939\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6285 - val_loss: 0.6301 - val_accuracy: 0.6939\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6247 - val_loss: 0.6300 - val_accuracy: 0.6901\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6260 - val_loss: 0.6246 - val_accuracy: 0.6875\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6503 - accuracy: 0.6248 - val_loss: 0.6310 - val_accuracy: 0.6658\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6515 - accuracy: 0.6280 - val_loss: 0.6251 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6490 - accuracy: 0.6285 - val_loss: 0.6304 - val_accuracy: 0.6747\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6295 - val_loss: 0.6268 - val_accuracy: 0.6862\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6292 - val_loss: 0.6261 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6296 - val_loss: 0.6238 - val_accuracy: 0.6964\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6294 - val_loss: 0.6220 - val_accuracy: 0.6901\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6368 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6356 - val_loss: 0.6236 - val_accuracy: 0.6926\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6444 - accuracy: 0.6369 - val_loss: 0.6226 - val_accuracy: 0.6901\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6344 - val_loss: 0.6236 - val_accuracy: 0.6773\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6331 - val_loss: 0.6251 - val_accuracy: 0.6773\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6349 - val_loss: 0.6203 - val_accuracy: 0.6901\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6349 - val_loss: 0.6252 - val_accuracy: 0.6888\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6340 - val_loss: 0.6226 - val_accuracy: 0.6888\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6346 - val_loss: 0.6261 - val_accuracy: 0.6862\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6331 - val_loss: 0.6275 - val_accuracy: 0.6875\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6323 - val_loss: 0.6250 - val_accuracy: 0.6849\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6441 - accuracy: 0.6353 - val_loss: 0.6251 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6432 - accuracy: 0.6358 - val_loss: 0.6237 - val_accuracy: 0.6862\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6437 - accuracy: 0.6372 - val_loss: 0.6213 - val_accuracy: 0.6901\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6369 - val_loss: 0.6210 - val_accuracy: 0.6952\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6452 - accuracy: 0.6290 - val_loss: 0.6267 - val_accuracy: 0.6875\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6424 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6419 - val_loss: 0.6212 - val_accuracy: 0.6862\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6414 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6451 - accuracy: 0.6363 - val_loss: 0.6216 - val_accuracy: 0.6837\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6418 - val_loss: 0.6168 - val_accuracy: 0.6901\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6328 - val_loss: 0.6198 - val_accuracy: 0.6862\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6405 - val_loss: 0.6231 - val_accuracy: 0.6824\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6378 - val_loss: 0.6235 - val_accuracy: 0.6811\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6404 - accuracy: 0.6428 - val_loss: 0.6189 - val_accuracy: 0.6901\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6382 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6413 - val_loss: 0.6247 - val_accuracy: 0.6849\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6392 - accuracy: 0.6427 - val_loss: 0.6151 - val_accuracy: 0.6913\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6452 - val_loss: 0.6198 - val_accuracy: 0.6824\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6355 - val_loss: 0.6195 - val_accuracy: 0.6875\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6427 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6446 - val_loss: 0.6250 - val_accuracy: 0.6811\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6448 - val_loss: 0.6266 - val_accuracy: 0.6760\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6393 - accuracy: 0.6429 - val_loss: 0.6176 - val_accuracy: 0.6926\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6461 - val_loss: 0.6173 - val_accuracy: 0.6913\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6428 - val_loss: 0.6186 - val_accuracy: 0.6901\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6463 - val_loss: 0.6174 - val_accuracy: 0.6849\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6417 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6446 - val_loss: 0.6179 - val_accuracy: 0.6888\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6399 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6437 - val_loss: 0.6163 - val_accuracy: 0.6901\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6431 - val_loss: 0.6197 - val_accuracy: 0.6926\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6339 - accuracy: 0.6442 - val_loss: 0.6203 - val_accuracy: 0.6862\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6469 - val_loss: 0.6192 - val_accuracy: 0.6901\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6496 - val_loss: 0.6160 - val_accuracy: 0.6913\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6331 - accuracy: 0.6529 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6442 - val_loss: 0.6192 - val_accuracy: 0.6888\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6410 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6373 - accuracy: 0.6467 - val_loss: 0.6172 - val_accuracy: 0.6913\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6351 - accuracy: 0.6497 - val_loss: 0.6182 - val_accuracy: 0.6875\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6327 - accuracy: 0.6493 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6427 - val_loss: 0.6184 - val_accuracy: 0.6913\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6463 - val_loss: 0.6133 - val_accuracy: 0.6939\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6467 - val_loss: 0.6145 - val_accuracy: 0.6952\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6506 - val_loss: 0.6141 - val_accuracy: 0.6939\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6497 - val_loss: 0.6137 - val_accuracy: 0.6913\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6541 - val_loss: 0.6155 - val_accuracy: 0.6913\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6547 - val_loss: 0.6172 - val_accuracy: 0.6939\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6317 - accuracy: 0.6498 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6503 - val_loss: 0.6137 - val_accuracy: 0.6952\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6502 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6518 - val_loss: 0.6149 - val_accuracy: 0.6939\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6559 - val_loss: 0.6167 - val_accuracy: 0.6964\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6541 - val_loss: 0.6191 - val_accuracy: 0.6875\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6488 - val_loss: 0.6184 - val_accuracy: 0.6875\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6505 - val_loss: 0.6155 - val_accuracy: 0.7003\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6513 - val_loss: 0.6135 - val_accuracy: 0.6977\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6561 - val_loss: 0.6119 - val_accuracy: 0.6926\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6550 - val_loss: 0.6169 - val_accuracy: 0.6888\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6307 - accuracy: 0.6522 - val_loss: 0.6152 - val_accuracy: 0.6913\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6311 - accuracy: 0.6556 - val_loss: 0.6165 - val_accuracy: 0.6901\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6536 - val_loss: 0.6166 - val_accuracy: 0.6926\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6279 - accuracy: 0.6567 - val_loss: 0.6137 - val_accuracy: 0.6939\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6232 - accuracy: 0.6624 - val_loss: 0.6104 - val_accuracy: 0.6913\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6277 - accuracy: 0.6554 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6262 - accuracy: 0.6546 - val_loss: 0.6122 - val_accuracy: 0.6888\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6541 - val_loss: 0.6202 - val_accuracy: 0.6849\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6257 - accuracy: 0.6593 - val_loss: 0.6105 - val_accuracy: 0.6926\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6320 - accuracy: 0.6572 - val_loss: 0.6182 - val_accuracy: 0.6901\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6235 - accuracy: 0.6640 - val_loss: 0.6126 - val_accuracy: 0.6964\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6536 - val_loss: 0.6143 - val_accuracy: 0.6913\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6248 - accuracy: 0.6606 - val_loss: 0.6121 - val_accuracy: 0.6901\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6262 - accuracy: 0.6598 - val_loss: 0.6137 - val_accuracy: 0.6926\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6261 - accuracy: 0.6562 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6267 - accuracy: 0.6532 - val_loss: 0.6174 - val_accuracy: 0.6913\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6258 - accuracy: 0.6626 - val_loss: 0.6162 - val_accuracy: 0.6926\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6545 - val_loss: 0.6141 - val_accuracy: 0.6990\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6608 - val_loss: 0.6130 - val_accuracy: 0.6939\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6657 - val_loss: 0.6098 - val_accuracy: 0.7015\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6252 - accuracy: 0.6619 - val_loss: 0.6150 - val_accuracy: 0.6901\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6269 - accuracy: 0.6594 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6577 - val_loss: 0.6213 - val_accuracy: 0.6888\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6618 - val_loss: 0.6211 - val_accuracy: 0.6888\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6257 - accuracy: 0.6577 - val_loss: 0.6170 - val_accuracy: 0.6875\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6182 - accuracy: 0.6686 - val_loss: 0.6185 - val_accuracy: 0.6875\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6195 - accuracy: 0.6644 - val_loss: 0.6177 - val_accuracy: 0.6862\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6216 - accuracy: 0.6609 - val_loss: 0.6193 - val_accuracy: 0.6875\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6174 - accuracy: 0.6689 - val_loss: 0.6121 - val_accuracy: 0.6862\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6208 - accuracy: 0.6619 - val_loss: 0.6168 - val_accuracy: 0.6875\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6240 - accuracy: 0.6586 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6175 - accuracy: 0.6649 - val_loss: 0.6128 - val_accuracy: 0.6862\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6236 - accuracy: 0.6613 - val_loss: 0.6173 - val_accuracy: 0.6837\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6231 - accuracy: 0.6583 - val_loss: 0.6182 - val_accuracy: 0.6837\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6570 - val_loss: 0.6176 - val_accuracy: 0.6913\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6223 - accuracy: 0.6628 - val_loss: 0.6192 - val_accuracy: 0.6901\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6191 - accuracy: 0.6668 - val_loss: 0.6165 - val_accuracy: 0.6913\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6240 - accuracy: 0.6630 - val_loss: 0.6180 - val_accuracy: 0.6837\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6180 - accuracy: 0.6699 - val_loss: 0.6189 - val_accuracy: 0.6862\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6686 - val_loss: 0.6162 - val_accuracy: 0.6849\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6201 - accuracy: 0.6648 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6624 - val_loss: 0.6139 - val_accuracy: 0.6875\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6148 - accuracy: 0.6609 - val_loss: 0.6148 - val_accuracy: 0.6862\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6205 - accuracy: 0.6586 - val_loss: 0.6148 - val_accuracy: 0.6875\n",
      "Epoch 200/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6200 - accuracy: 0.6652 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Calculating for: 700 150 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_300 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.7952 - accuracy: 0.5344 - val_loss: 0.6661 - val_accuracy: 0.6301\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7031 - accuracy: 0.5698 - val_loss: 0.6623 - val_accuracy: 0.6237\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6816 - accuracy: 0.5740 - val_loss: 0.6517 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6753 - accuracy: 0.5841 - val_loss: 0.6435 - val_accuracy: 0.6645\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6679 - accuracy: 0.5968 - val_loss: 0.6424 - val_accuracy: 0.6658\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6642 - accuracy: 0.6034 - val_loss: 0.6431 - val_accuracy: 0.6454\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6654 - accuracy: 0.6002 - val_loss: 0.6393 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6621 - accuracy: 0.6040 - val_loss: 0.6412 - val_accuracy: 0.6480\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6616 - accuracy: 0.6045 - val_loss: 0.6395 - val_accuracy: 0.6543\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6597 - accuracy: 0.6088 - val_loss: 0.6409 - val_accuracy: 0.6454\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6568 - accuracy: 0.6194 - val_loss: 0.6365 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6589 - accuracy: 0.6162 - val_loss: 0.6318 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6577 - accuracy: 0.6143 - val_loss: 0.6314 - val_accuracy: 0.6658\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6534 - accuracy: 0.6186 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6228 - val_loss: 0.6287 - val_accuracy: 0.6747\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6535 - accuracy: 0.6221 - val_loss: 0.6333 - val_accuracy: 0.6518\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6199 - val_loss: 0.6333 - val_accuracy: 0.6505\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6255 - val_loss: 0.6308 - val_accuracy: 0.6594\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6490 - accuracy: 0.6226 - val_loss: 0.6304 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6470 - accuracy: 0.6260 - val_loss: 0.6219 - val_accuracy: 0.6824\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6500 - accuracy: 0.6291 - val_loss: 0.6294 - val_accuracy: 0.6594\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6442 - accuracy: 0.6297 - val_loss: 0.6262 - val_accuracy: 0.6620\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6455 - accuracy: 0.6281 - val_loss: 0.6257 - val_accuracy: 0.6607\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6361 - val_loss: 0.6266 - val_accuracy: 0.6607\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6402 - accuracy: 0.6408 - val_loss: 0.6268 - val_accuracy: 0.6633\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6457 - accuracy: 0.6305 - val_loss: 0.6285 - val_accuracy: 0.6620\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6405 - accuracy: 0.6377 - val_loss: 0.6276 - val_accuracy: 0.6607\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6394 - val_loss: 0.6270 - val_accuracy: 0.6607\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6373 - accuracy: 0.6442 - val_loss: 0.6250 - val_accuracy: 0.6658\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6388 - val_loss: 0.6254 - val_accuracy: 0.6735\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6390 - accuracy: 0.6374 - val_loss: 0.6262 - val_accuracy: 0.6594\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6380 - accuracy: 0.6426 - val_loss: 0.6278 - val_accuracy: 0.6594\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6414 - accuracy: 0.6385 - val_loss: 0.6254 - val_accuracy: 0.6620\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6369 - val_loss: 0.6256 - val_accuracy: 0.6696\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6359 - accuracy: 0.6466 - val_loss: 0.6252 - val_accuracy: 0.6658\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6280 - accuracy: 0.6523 - val_loss: 0.6239 - val_accuracy: 0.6735\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6377 - accuracy: 0.6368 - val_loss: 0.6259 - val_accuracy: 0.6684\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6394 - val_loss: 0.6203 - val_accuracy: 0.6760\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6290 - accuracy: 0.6481 - val_loss: 0.6176 - val_accuracy: 0.6747\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6323 - accuracy: 0.6448 - val_loss: 0.6209 - val_accuracy: 0.6671\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6448 - val_loss: 0.6259 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6307 - accuracy: 0.6500 - val_loss: 0.6227 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6269 - accuracy: 0.6486 - val_loss: 0.6258 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6278 - accuracy: 0.6535 - val_loss: 0.6293 - val_accuracy: 0.6569\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6280 - accuracy: 0.6477 - val_loss: 0.6214 - val_accuracy: 0.6760\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6545 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6537 - val_loss: 0.6247 - val_accuracy: 0.6747\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6501 - val_loss: 0.6254 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6236 - accuracy: 0.6537 - val_loss: 0.6245 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6215 - accuracy: 0.6580 - val_loss: 0.6206 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6216 - accuracy: 0.6560 - val_loss: 0.6226 - val_accuracy: 0.6709\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6230 - accuracy: 0.6555 - val_loss: 0.6222 - val_accuracy: 0.6735\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6221 - accuracy: 0.6566 - val_loss: 0.6205 - val_accuracy: 0.6760\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6226 - accuracy: 0.6537 - val_loss: 0.6242 - val_accuracy: 0.6684\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6203 - accuracy: 0.6628 - val_loss: 0.6208 - val_accuracy: 0.6696\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6186 - accuracy: 0.6579 - val_loss: 0.6219 - val_accuracy: 0.6747\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6644 - val_loss: 0.6255 - val_accuracy: 0.6722\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6626 - val_loss: 0.6263 - val_accuracy: 0.6747\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6163 - accuracy: 0.6644 - val_loss: 0.6227 - val_accuracy: 0.6709\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6158 - accuracy: 0.6673 - val_loss: 0.6225 - val_accuracy: 0.6798\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6668 - val_loss: 0.6230 - val_accuracy: 0.6786\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6664 - val_loss: 0.6254 - val_accuracy: 0.6773\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6114 - accuracy: 0.6721 - val_loss: 0.6264 - val_accuracy: 0.6722\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6144 - accuracy: 0.6678 - val_loss: 0.6238 - val_accuracy: 0.6798\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6142 - accuracy: 0.6703 - val_loss: 0.6239 - val_accuracy: 0.6786\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6142 - accuracy: 0.6682 - val_loss: 0.6247 - val_accuracy: 0.6760\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6654 - val_loss: 0.6216 - val_accuracy: 0.6901\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6131 - accuracy: 0.6625 - val_loss: 0.6222 - val_accuracy: 0.6811\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6109 - accuracy: 0.6652 - val_loss: 0.6218 - val_accuracy: 0.6888\n",
      "Calculating for: 700 150 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_304 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8170 - accuracy: 0.5280 - val_loss: 0.6689 - val_accuracy: 0.6199\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7183 - accuracy: 0.5659 - val_loss: 0.6428 - val_accuracy: 0.6633\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 2s 6ms/step - loss: 0.6892 - accuracy: 0.5732 - val_loss: 0.6450 - val_accuracy: 0.6645\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6794 - accuracy: 0.5723 - val_loss: 0.6477 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6762 - accuracy: 0.5737 - val_loss: 0.6410 - val_accuracy: 0.6633\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6722 - accuracy: 0.5855 - val_loss: 0.6425 - val_accuracy: 0.6556\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6703 - accuracy: 0.5962 - val_loss: 0.6426 - val_accuracy: 0.6645\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5883 - val_loss: 0.6398 - val_accuracy: 0.6645\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6686 - accuracy: 0.5967 - val_loss: 0.6373 - val_accuracy: 0.6696\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6031 - val_loss: 0.6334 - val_accuracy: 0.6684\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6052 - val_loss: 0.6377 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6635 - accuracy: 0.6039 - val_loss: 0.6389 - val_accuracy: 0.6582\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.6095 - val_loss: 0.6311 - val_accuracy: 0.6760\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6638 - accuracy: 0.6058 - val_loss: 0.6359 - val_accuracy: 0.6696\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6614 - accuracy: 0.6140 - val_loss: 0.6331 - val_accuracy: 0.6696\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6625 - accuracy: 0.5958 - val_loss: 0.6331 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6611 - accuracy: 0.6020 - val_loss: 0.6362 - val_accuracy: 0.6747\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6096 - val_loss: 0.6305 - val_accuracy: 0.6811\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6601 - accuracy: 0.6107 - val_loss: 0.6281 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6582 - accuracy: 0.6078 - val_loss: 0.6269 - val_accuracy: 0.6760\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6561 - accuracy: 0.6179 - val_loss: 0.6283 - val_accuracy: 0.6773\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6570 - accuracy: 0.6076 - val_loss: 0.6297 - val_accuracy: 0.6798\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6093 - val_loss: 0.6289 - val_accuracy: 0.6773\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6552 - accuracy: 0.6202 - val_loss: 0.6290 - val_accuracy: 0.6760\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6174 - val_loss: 0.6260 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6546 - accuracy: 0.6178 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6218 - val_loss: 0.6243 - val_accuracy: 0.6798\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6187 - val_loss: 0.6260 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6258 - val_loss: 0.6267 - val_accuracy: 0.6824\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6498 - accuracy: 0.6242 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6473 - accuracy: 0.6321 - val_loss: 0.6198 - val_accuracy: 0.6798\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6526 - accuracy: 0.6289 - val_loss: 0.6301 - val_accuracy: 0.6837\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6236 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6513 - accuracy: 0.6235 - val_loss: 0.6226 - val_accuracy: 0.6875\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6227 - val_loss: 0.6247 - val_accuracy: 0.6837\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.6299 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6471 - accuracy: 0.6299 - val_loss: 0.6212 - val_accuracy: 0.6888\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6453 - accuracy: 0.6312 - val_loss: 0.6240 - val_accuracy: 0.6888\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6486 - accuracy: 0.6301 - val_loss: 0.6217 - val_accuracy: 0.6913\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6439 - accuracy: 0.6316 - val_loss: 0.6196 - val_accuracy: 0.6952\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6277 - val_loss: 0.6249 - val_accuracy: 0.6760\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6385 - val_loss: 0.6210 - val_accuracy: 0.6875\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6290 - val_loss: 0.6199 - val_accuracy: 0.6888\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6320 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6450 - accuracy: 0.6319 - val_loss: 0.6216 - val_accuracy: 0.6849\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6459 - accuracy: 0.6335 - val_loss: 0.6207 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6341 - val_loss: 0.6169 - val_accuracy: 0.6862\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6354 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6422 - accuracy: 0.6387 - val_loss: 0.6168 - val_accuracy: 0.6849\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6394 - val_loss: 0.6195 - val_accuracy: 0.6837\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6382 - val_loss: 0.6189 - val_accuracy: 0.6875\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6405 - accuracy: 0.6404 - val_loss: 0.6183 - val_accuracy: 0.6888\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6389 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6390 - accuracy: 0.6395 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6388 - val_loss: 0.6207 - val_accuracy: 0.6862\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6409 - val_loss: 0.6172 - val_accuracy: 0.6901\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6373 - val_loss: 0.6214 - val_accuracy: 0.6875\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6347 - accuracy: 0.6468 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6399 - val_loss: 0.6157 - val_accuracy: 0.6862\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6386 - accuracy: 0.6418 - val_loss: 0.6134 - val_accuracy: 0.6888\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6369 - accuracy: 0.6486 - val_loss: 0.6180 - val_accuracy: 0.6849\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6364 - accuracy: 0.6383 - val_loss: 0.6223 - val_accuracy: 0.6837\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6463 - val_loss: 0.6162 - val_accuracy: 0.6875\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6468 - val_loss: 0.6196 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6495 - val_loss: 0.6201 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6476 - val_loss: 0.6174 - val_accuracy: 0.6811\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6510 - val_loss: 0.6125 - val_accuracy: 0.6926\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6382 - accuracy: 0.6443 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6463 - val_loss: 0.6155 - val_accuracy: 0.6926\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6502 - val_loss: 0.6116 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6321 - accuracy: 0.6443 - val_loss: 0.6133 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6492 - val_loss: 0.6187 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6544 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6463 - val_loss: 0.6157 - val_accuracy: 0.6901\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6286 - accuracy: 0.6526 - val_loss: 0.6134 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6309 - accuracy: 0.6521 - val_loss: 0.6179 - val_accuracy: 0.6837\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6539 - val_loss: 0.6137 - val_accuracy: 0.6888\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6283 - accuracy: 0.6505 - val_loss: 0.6144 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6570 - val_loss: 0.6172 - val_accuracy: 0.6862\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6510 - val_loss: 0.6126 - val_accuracy: 0.6901\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6557 - val_loss: 0.6097 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6285 - accuracy: 0.6539 - val_loss: 0.6155 - val_accuracy: 0.6888\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6572 - val_loss: 0.6122 - val_accuracy: 0.6964\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6546 - val_loss: 0.6143 - val_accuracy: 0.6913\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6536 - val_loss: 0.6176 - val_accuracy: 0.6824\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6628 - val_loss: 0.6137 - val_accuracy: 0.6811\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6503 - val_loss: 0.6125 - val_accuracy: 0.6913\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6598 - val_loss: 0.6153 - val_accuracy: 0.6837\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6642 - val_loss: 0.6135 - val_accuracy: 0.6926\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6556 - val_loss: 0.6132 - val_accuracy: 0.6888\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6550 - val_loss: 0.6124 - val_accuracy: 0.6888\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6539 - val_loss: 0.6181 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6583 - val_loss: 0.6167 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6624 - val_loss: 0.6146 - val_accuracy: 0.6811\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6224 - accuracy: 0.6620 - val_loss: 0.6131 - val_accuracy: 0.6849\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6611 - val_loss: 0.6126 - val_accuracy: 0.6798\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6605 - val_loss: 0.6163 - val_accuracy: 0.6786\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6585 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6637 - val_loss: 0.6143 - val_accuracy: 0.6811\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6590 - val_loss: 0.6143 - val_accuracy: 0.6760\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6168 - accuracy: 0.6613 - val_loss: 0.6134 - val_accuracy: 0.6798\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6176 - accuracy: 0.6650 - val_loss: 0.6138 - val_accuracy: 0.6798\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6172 - accuracy: 0.6644 - val_loss: 0.6149 - val_accuracy: 0.6773\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6192 - accuracy: 0.6556 - val_loss: 0.6131 - val_accuracy: 0.6824\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6629 - val_loss: 0.6161 - val_accuracy: 0.6811\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6140 - accuracy: 0.6663 - val_loss: 0.6163 - val_accuracy: 0.6760\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6669 - val_loss: 0.6166 - val_accuracy: 0.6747\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6625 - val_loss: 0.6167 - val_accuracy: 0.6747\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6669 - val_loss: 0.6162 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6134 - accuracy: 0.6729 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6672 - val_loss: 0.6184 - val_accuracy: 0.6798\n",
      "Calculating for: 700 150 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_308 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8167 - accuracy: 0.5200 - val_loss: 0.6532 - val_accuracy: 0.6378\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7285 - accuracy: 0.5359 - val_loss: 0.6523 - val_accuracy: 0.6390\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7020 - accuracy: 0.5418 - val_loss: 0.6525 - val_accuracy: 0.6429\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6908 - accuracy: 0.5446 - val_loss: 0.6553 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6789 - accuracy: 0.5667 - val_loss: 0.6480 - val_accuracy: 0.6480\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6840 - accuracy: 0.5609 - val_loss: 0.6531 - val_accuracy: 0.6480\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6827 - accuracy: 0.5676 - val_loss: 0.6524 - val_accuracy: 0.6492\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6787 - accuracy: 0.5656 - val_loss: 0.6466 - val_accuracy: 0.6556\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6794 - accuracy: 0.5666 - val_loss: 0.6479 - val_accuracy: 0.6582\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6805 - accuracy: 0.5623 - val_loss: 0.6471 - val_accuracy: 0.6582\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6774 - accuracy: 0.5725 - val_loss: 0.6436 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6758 - accuracy: 0.5782 - val_loss: 0.6435 - val_accuracy: 0.6594\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6756 - accuracy: 0.5759 - val_loss: 0.6423 - val_accuracy: 0.6556\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6735 - accuracy: 0.5775 - val_loss: 0.6399 - val_accuracy: 0.6582\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6706 - accuracy: 0.5946 - val_loss: 0.6369 - val_accuracy: 0.6607\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6740 - accuracy: 0.5830 - val_loss: 0.6422 - val_accuracy: 0.6696\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6717 - accuracy: 0.5823 - val_loss: 0.6441 - val_accuracy: 0.6696\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5879 - val_loss: 0.6461 - val_accuracy: 0.6658\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5889 - val_loss: 0.6390 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5962 - val_loss: 0.6390 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6688 - accuracy: 0.5909 - val_loss: 0.6349 - val_accuracy: 0.6696\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6693 - accuracy: 0.5941 - val_loss: 0.6369 - val_accuracy: 0.6709\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5981 - val_loss: 0.6372 - val_accuracy: 0.6747\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6659 - accuracy: 0.5948 - val_loss: 0.6336 - val_accuracy: 0.6735\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6029 - val_loss: 0.6340 - val_accuracy: 0.6773\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5990 - val_loss: 0.6326 - val_accuracy: 0.6747\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6642 - accuracy: 0.6061 - val_loss: 0.6335 - val_accuracy: 0.6786\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6655 - accuracy: 0.6022 - val_loss: 0.6336 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6676 - accuracy: 0.5937 - val_loss: 0.6325 - val_accuracy: 0.6747\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6644 - accuracy: 0.6026 - val_loss: 0.6356 - val_accuracy: 0.6735\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6009 - val_loss: 0.6312 - val_accuracy: 0.6735\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6636 - accuracy: 0.6065 - val_loss: 0.6332 - val_accuracy: 0.6658\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.6039 - val_loss: 0.6307 - val_accuracy: 0.6773\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6597 - accuracy: 0.6085 - val_loss: 0.6278 - val_accuracy: 0.6747\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.6010 - val_loss: 0.6308 - val_accuracy: 0.6760\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6060 - val_loss: 0.6301 - val_accuracy: 0.6735\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6078 - val_loss: 0.6284 - val_accuracy: 0.6747\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6115 - val_loss: 0.6320 - val_accuracy: 0.6811\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6622 - accuracy: 0.6058 - val_loss: 0.6312 - val_accuracy: 0.6811\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6598 - accuracy: 0.6085 - val_loss: 0.6284 - val_accuracy: 0.6811\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6602 - accuracy: 0.6114 - val_loss: 0.6276 - val_accuracy: 0.6811\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6122 - val_loss: 0.6289 - val_accuracy: 0.6709\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6554 - accuracy: 0.6133 - val_loss: 0.6258 - val_accuracy: 0.6824\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6100 - val_loss: 0.6262 - val_accuracy: 0.6837\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6117 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6117 - val_loss: 0.6260 - val_accuracy: 0.6849\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6161 - val_loss: 0.6236 - val_accuracy: 0.6798\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6143 - val_loss: 0.6285 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6166 - val_loss: 0.6279 - val_accuracy: 0.6798\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6588 - accuracy: 0.6058 - val_loss: 0.6238 - val_accuracy: 0.6811\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6150 - val_loss: 0.6250 - val_accuracy: 0.6849\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6562 - accuracy: 0.6167 - val_loss: 0.6229 - val_accuracy: 0.6824\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6222 - val_loss: 0.6264 - val_accuracy: 0.6811\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6551 - accuracy: 0.6172 - val_loss: 0.6308 - val_accuracy: 0.6824\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6574 - accuracy: 0.6117 - val_loss: 0.6239 - val_accuracy: 0.6849\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6216 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6191 - val_loss: 0.6232 - val_accuracy: 0.6837\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6184 - val_loss: 0.6223 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6545 - accuracy: 0.6153 - val_loss: 0.6255 - val_accuracy: 0.6875\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6177 - val_loss: 0.6273 - val_accuracy: 0.6786\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6544 - accuracy: 0.6233 - val_loss: 0.6254 - val_accuracy: 0.6837\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6531 - accuracy: 0.6196 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6548 - accuracy: 0.6194 - val_loss: 0.6210 - val_accuracy: 0.6837\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6521 - accuracy: 0.6235 - val_loss: 0.6217 - val_accuracy: 0.6811\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6512 - accuracy: 0.6266 - val_loss: 0.6231 - val_accuracy: 0.6849\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6305 - val_loss: 0.6270 - val_accuracy: 0.6735\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6518 - accuracy: 0.6275 - val_loss: 0.6225 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6269 - val_loss: 0.6225 - val_accuracy: 0.6875\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6533 - accuracy: 0.6226 - val_loss: 0.6265 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6294 - val_loss: 0.6224 - val_accuracy: 0.6824\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6482 - accuracy: 0.6272 - val_loss: 0.6228 - val_accuracy: 0.6862\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6271 - val_loss: 0.6227 - val_accuracy: 0.6875\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6491 - accuracy: 0.6307 - val_loss: 0.6239 - val_accuracy: 0.6849\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6503 - accuracy: 0.6266 - val_loss: 0.6241 - val_accuracy: 0.6862\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6257 - val_loss: 0.6188 - val_accuracy: 0.6824\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6483 - accuracy: 0.6257 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6323 - val_loss: 0.6232 - val_accuracy: 0.6849\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6252 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 6ms/step - loss: 0.6499 - accuracy: 0.6270 - val_loss: 0.6209 - val_accuracy: 0.6824\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6320 - val_loss: 0.6204 - val_accuracy: 0.6875\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6287 - val_loss: 0.6218 - val_accuracy: 0.6901\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6279 - val_loss: 0.6215 - val_accuracy: 0.6849\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6459 - accuracy: 0.6336 - val_loss: 0.6145 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6436 - accuracy: 0.6349 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6477 - accuracy: 0.6285 - val_loss: 0.6250 - val_accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6445 - accuracy: 0.6295 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6370 - val_loss: 0.6223 - val_accuracy: 0.6849\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6295 - val_loss: 0.6207 - val_accuracy: 0.6837\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6432 - accuracy: 0.6393 - val_loss: 0.6176 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6452 - accuracy: 0.6339 - val_loss: 0.6191 - val_accuracy: 0.6798\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6484 - accuracy: 0.6300 - val_loss: 0.6170 - val_accuracy: 0.6862\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6325 - val_loss: 0.6137 - val_accuracy: 0.6888\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6442 - accuracy: 0.6324 - val_loss: 0.6155 - val_accuracy: 0.6837\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6379 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6388 - val_loss: 0.6155 - val_accuracy: 0.6837\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6407 - accuracy: 0.6400 - val_loss: 0.6184 - val_accuracy: 0.6888\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6432 - accuracy: 0.6346 - val_loss: 0.6144 - val_accuracy: 0.6837\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6452 - accuracy: 0.6385 - val_loss: 0.6166 - val_accuracy: 0.6862\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6415 - accuracy: 0.6407 - val_loss: 0.6170 - val_accuracy: 0.6862\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6418 - accuracy: 0.6390 - val_loss: 0.6153 - val_accuracy: 0.6837\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6423 - accuracy: 0.6361 - val_loss: 0.6157 - val_accuracy: 0.6875\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6449 - val_loss: 0.6192 - val_accuracy: 0.6888\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6426 - accuracy: 0.6370 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6389 - val_loss: 0.6173 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6427 - val_loss: 0.6162 - val_accuracy: 0.6888\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6437 - val_loss: 0.6172 - val_accuracy: 0.6811\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6441 - accuracy: 0.6387 - val_loss: 0.6203 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6381 - accuracy: 0.6404 - val_loss: 0.6138 - val_accuracy: 0.6875\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6439 - val_loss: 0.6167 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6400 - accuracy: 0.6372 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6387 - accuracy: 0.6432 - val_loss: 0.6176 - val_accuracy: 0.6875\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6448 - val_loss: 0.6148 - val_accuracy: 0.6862\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6428 - accuracy: 0.6384 - val_loss: 0.6181 - val_accuracy: 0.6862\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6393 - accuracy: 0.6353 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6385 - accuracy: 0.6375 - val_loss: 0.6157 - val_accuracy: 0.6875\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6399 - val_loss: 0.6181 - val_accuracy: 0.6875\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6377 - val_loss: 0.6167 - val_accuracy: 0.6901\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6437 - val_loss: 0.6226 - val_accuracy: 0.6786\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6457 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6382 - val_loss: 0.6178 - val_accuracy: 0.6824\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6429 - val_loss: 0.6132 - val_accuracy: 0.6875\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6432 - val_loss: 0.6152 - val_accuracy: 0.6888\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6358 - accuracy: 0.6462 - val_loss: 0.6184 - val_accuracy: 0.6849\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6467 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6355 - accuracy: 0.6461 - val_loss: 0.6118 - val_accuracy: 0.6926\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6544 - val_loss: 0.6137 - val_accuracy: 0.6888\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6467 - val_loss: 0.6136 - val_accuracy: 0.6901\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6408 - val_loss: 0.6152 - val_accuracy: 0.6939\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6516 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6367 - accuracy: 0.6476 - val_loss: 0.6151 - val_accuracy: 0.6926\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6463 - val_loss: 0.6160 - val_accuracy: 0.6849\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6468 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6368 - accuracy: 0.6477 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6324 - accuracy: 0.6500 - val_loss: 0.6137 - val_accuracy: 0.6875\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6351 - accuracy: 0.6560 - val_loss: 0.6146 - val_accuracy: 0.6901\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6510 - val_loss: 0.6145 - val_accuracy: 0.6939\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6439 - val_loss: 0.6209 - val_accuracy: 0.6824\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6350 - accuracy: 0.6448 - val_loss: 0.6142 - val_accuracy: 0.6939\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6546 - val_loss: 0.6092 - val_accuracy: 0.6990\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6525 - val_loss: 0.6118 - val_accuracy: 0.6939\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6511 - val_loss: 0.6140 - val_accuracy: 0.6875\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6277 - accuracy: 0.6541 - val_loss: 0.6074 - val_accuracy: 0.6990\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6442 - val_loss: 0.6149 - val_accuracy: 0.6875\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6491 - val_loss: 0.6119 - val_accuracy: 0.7054\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6463 - val_loss: 0.6149 - val_accuracy: 0.6964\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6502 - val_loss: 0.6119 - val_accuracy: 0.6952\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6500 - val_loss: 0.6116 - val_accuracy: 0.6952\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6530 - val_loss: 0.6152 - val_accuracy: 0.6901\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6335 - accuracy: 0.6529 - val_loss: 0.6145 - val_accuracy: 0.6862\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6601 - val_loss: 0.6131 - val_accuracy: 0.6901\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6480 - val_loss: 0.6102 - val_accuracy: 0.6990\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6595 - val_loss: 0.6148 - val_accuracy: 0.6849\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6523 - val_loss: 0.6135 - val_accuracy: 0.6888\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6300 - accuracy: 0.6510 - val_loss: 0.6115 - val_accuracy: 0.7028\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6550 - val_loss: 0.6101 - val_accuracy: 0.6990\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6626 - val_loss: 0.6117 - val_accuracy: 0.6977\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6266 - accuracy: 0.6581 - val_loss: 0.6134 - val_accuracy: 0.6875\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6541 - val_loss: 0.6158 - val_accuracy: 0.6875\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6629 - val_loss: 0.6117 - val_accuracy: 0.6926\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6601 - val_loss: 0.6145 - val_accuracy: 0.6888\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6537 - val_loss: 0.6127 - val_accuracy: 0.6990\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6254 - accuracy: 0.6588 - val_loss: 0.6113 - val_accuracy: 0.6901\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6566 - val_loss: 0.6112 - val_accuracy: 0.6862\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6620 - val_loss: 0.6096 - val_accuracy: 0.7003\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6226 - accuracy: 0.6614 - val_loss: 0.6100 - val_accuracy: 0.6926\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6588 - val_loss: 0.6108 - val_accuracy: 0.6926\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6531 - val_loss: 0.6148 - val_accuracy: 0.6939\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6593 - val_loss: 0.6120 - val_accuracy: 0.6901\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6275 - accuracy: 0.6584 - val_loss: 0.6121 - val_accuracy: 0.6939\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6284 - accuracy: 0.6523 - val_loss: 0.6100 - val_accuracy: 0.6913\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6225 - accuracy: 0.6615 - val_loss: 0.6085 - val_accuracy: 0.6990\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6589 - val_loss: 0.6144 - val_accuracy: 0.6888\n",
      "Calculating for: 750 50 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_312 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7893 - accuracy: 0.5373 - val_loss: 0.6534 - val_accuracy: 0.6429\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7096 - accuracy: 0.5528 - val_loss: 0.6440 - val_accuracy: 0.6531\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5705 - val_loss: 0.6411 - val_accuracy: 0.6696\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.5811 - val_loss: 0.6435 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6741 - accuracy: 0.5909 - val_loss: 0.6415 - val_accuracy: 0.6760\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6654 - accuracy: 0.5958 - val_loss: 0.6428 - val_accuracy: 0.6735\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6653 - accuracy: 0.5911 - val_loss: 0.6365 - val_accuracy: 0.6722\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6015 - val_loss: 0.6378 - val_accuracy: 0.6760\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6088 - val_loss: 0.6364 - val_accuracy: 0.6722\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6632 - accuracy: 0.6090 - val_loss: 0.6333 - val_accuracy: 0.6747\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6133 - val_loss: 0.6361 - val_accuracy: 0.6696\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6618 - accuracy: 0.6047 - val_loss: 0.6360 - val_accuracy: 0.6735\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6596 - accuracy: 0.6075 - val_loss: 0.6325 - val_accuracy: 0.6837\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6537 - accuracy: 0.6152 - val_loss: 0.6290 - val_accuracy: 0.6811\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6159 - val_loss: 0.6269 - val_accuracy: 0.6798\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6177 - val_loss: 0.6306 - val_accuracy: 0.6760\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6519 - accuracy: 0.6261 - val_loss: 0.6299 - val_accuracy: 0.6837\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6236 - val_loss: 0.6257 - val_accuracy: 0.6735\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6204 - val_loss: 0.6314 - val_accuracy: 0.6735\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6260 - val_loss: 0.6270 - val_accuracy: 0.6811\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6256 - val_loss: 0.6275 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6243 - val_loss: 0.6272 - val_accuracy: 0.6875\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6276 - val_loss: 0.6239 - val_accuracy: 0.6913\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6311 - val_loss: 0.6274 - val_accuracy: 0.6888\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6282 - val_loss: 0.6236 - val_accuracy: 0.6862\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6330 - val_loss: 0.6258 - val_accuracy: 0.6875\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6325 - val_loss: 0.6202 - val_accuracy: 0.6875\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6343 - val_loss: 0.6232 - val_accuracy: 0.6837\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6472 - accuracy: 0.6325 - val_loss: 0.6263 - val_accuracy: 0.6875\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6359 - val_loss: 0.6229 - val_accuracy: 0.6849\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6334 - val_loss: 0.6267 - val_accuracy: 0.6837\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6398 - accuracy: 0.6438 - val_loss: 0.6232 - val_accuracy: 0.6875\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 2ms/step - loss: 0.6405 - accuracy: 0.6385 - val_loss: 0.6223 - val_accuracy: 0.6888\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6383 - accuracy: 0.6415 - val_loss: 0.6213 - val_accuracy: 0.6862\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6427 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6457 - val_loss: 0.6241 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6461 - val_loss: 0.6213 - val_accuracy: 0.6837\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6532 - val_loss: 0.6183 - val_accuracy: 0.6837\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6535 - val_loss: 0.6118 - val_accuracy: 0.6901\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6545 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6561 - val_loss: 0.6172 - val_accuracy: 0.6849\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6488 - val_loss: 0.6172 - val_accuracy: 0.6849\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6496 - val_loss: 0.6200 - val_accuracy: 0.6862\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6334 - accuracy: 0.6507 - val_loss: 0.6217 - val_accuracy: 0.6888\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6339 - accuracy: 0.6469 - val_loss: 0.6194 - val_accuracy: 0.6735\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6466 - val_loss: 0.6173 - val_accuracy: 0.6786\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6512 - val_loss: 0.6236 - val_accuracy: 0.6709\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6567 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6561 - val_loss: 0.6170 - val_accuracy: 0.6901\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6501 - val_loss: 0.6197 - val_accuracy: 0.6824\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6557 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6520 - val_loss: 0.6197 - val_accuracy: 0.6901\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6529 - val_loss: 0.6193 - val_accuracy: 0.6901\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6185 - accuracy: 0.6662 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6550 - val_loss: 0.6177 - val_accuracy: 0.6913\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6247 - accuracy: 0.6549 - val_loss: 0.6189 - val_accuracy: 0.6849\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6213 - accuracy: 0.6640 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6235 - accuracy: 0.6601 - val_loss: 0.6193 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6668 - val_loss: 0.6144 - val_accuracy: 0.6913\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6584 - val_loss: 0.6153 - val_accuracy: 0.6913\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6199 - accuracy: 0.6619 - val_loss: 0.6219 - val_accuracy: 0.6798\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.6619 - val_loss: 0.6151 - val_accuracy: 0.6913\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6643 - val_loss: 0.6171 - val_accuracy: 0.6824\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6166 - accuracy: 0.6683 - val_loss: 0.6227 - val_accuracy: 0.6760\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6179 - accuracy: 0.6616 - val_loss: 0.6168 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.6672 - val_loss: 0.6181 - val_accuracy: 0.6849\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6645 - val_loss: 0.6200 - val_accuracy: 0.6786\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6203 - accuracy: 0.6630 - val_loss: 0.6173 - val_accuracy: 0.6875\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6074 - accuracy: 0.6776 - val_loss: 0.6189 - val_accuracy: 0.6824\n",
      "Calculating for: 750 50 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_316 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8143 - accuracy: 0.5136 - val_loss: 0.6771 - val_accuracy: 0.6365\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7340 - accuracy: 0.5286 - val_loss: 0.6625 - val_accuracy: 0.6569\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7002 - accuracy: 0.5504 - val_loss: 0.6564 - val_accuracy: 0.6543\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6861 - accuracy: 0.5671 - val_loss: 0.6543 - val_accuracy: 0.6658\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6815 - accuracy: 0.5703 - val_loss: 0.6532 - val_accuracy: 0.6607\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6781 - accuracy: 0.5777 - val_loss: 0.6512 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6723 - accuracy: 0.5829 - val_loss: 0.6521 - val_accuracy: 0.6569\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5814 - val_loss: 0.6501 - val_accuracy: 0.6684\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5923 - val_loss: 0.6457 - val_accuracy: 0.6607\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5845 - val_loss: 0.6475 - val_accuracy: 0.6556\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6669 - accuracy: 0.6005 - val_loss: 0.6432 - val_accuracy: 0.6620\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6708 - accuracy: 0.5916 - val_loss: 0.6459 - val_accuracy: 0.6594\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.5988 - val_loss: 0.6425 - val_accuracy: 0.6607\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5917 - val_loss: 0.6374 - val_accuracy: 0.6798\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6634 - accuracy: 0.6022 - val_loss: 0.6344 - val_accuracy: 0.6862\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6663 - accuracy: 0.5985 - val_loss: 0.6396 - val_accuracy: 0.6684\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6045 - val_loss: 0.6358 - val_accuracy: 0.6837\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6668 - accuracy: 0.6029 - val_loss: 0.6379 - val_accuracy: 0.6773\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6628 - accuracy: 0.6000 - val_loss: 0.6319 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6640 - accuracy: 0.6027 - val_loss: 0.6428 - val_accuracy: 0.6620\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6581 - accuracy: 0.6154 - val_loss: 0.6423 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6137 - val_loss: 0.6353 - val_accuracy: 0.6684\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6107 - val_loss: 0.6363 - val_accuracy: 0.6684\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6591 - accuracy: 0.6154 - val_loss: 0.6342 - val_accuracy: 0.6811\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6047 - val_loss: 0.6362 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6158 - val_loss: 0.6428 - val_accuracy: 0.6620\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6582 - accuracy: 0.6150 - val_loss: 0.6385 - val_accuracy: 0.6658\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6182 - val_loss: 0.6351 - val_accuracy: 0.6747\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6571 - accuracy: 0.6183 - val_loss: 0.6325 - val_accuracy: 0.6722\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6223 - val_loss: 0.6326 - val_accuracy: 0.6773\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6207 - val_loss: 0.6303 - val_accuracy: 0.6811\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6512 - accuracy: 0.6269 - val_loss: 0.6303 - val_accuracy: 0.6811\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6555 - accuracy: 0.6196 - val_loss: 0.6278 - val_accuracy: 0.6837\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6171 - val_loss: 0.6338 - val_accuracy: 0.6658\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6201 - val_loss: 0.6317 - val_accuracy: 0.6760\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6546 - accuracy: 0.6211 - val_loss: 0.6287 - val_accuracy: 0.6862\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6323 - val_accuracy: 0.6773\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6262 - val_loss: 0.6289 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6281 - val_loss: 0.6266 - val_accuracy: 0.6849\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6486 - accuracy: 0.6262 - val_loss: 0.6279 - val_accuracy: 0.6875\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6233 - val_loss: 0.6265 - val_accuracy: 0.6901\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6482 - accuracy: 0.6285 - val_loss: 0.6287 - val_accuracy: 0.6862\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6457 - accuracy: 0.6329 - val_loss: 0.6276 - val_accuracy: 0.6773\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6500 - accuracy: 0.6297 - val_loss: 0.6271 - val_accuracy: 0.6901\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6470 - accuracy: 0.6306 - val_loss: 0.6338 - val_accuracy: 0.6798\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6304 - val_loss: 0.6280 - val_accuracy: 0.6849\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6402 - val_loss: 0.6270 - val_accuracy: 0.6862\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6320 - val_loss: 0.6254 - val_accuracy: 0.6875\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6326 - val_loss: 0.6271 - val_accuracy: 0.6990\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6463 - accuracy: 0.6358 - val_loss: 0.6243 - val_accuracy: 0.6913\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6271 - val_loss: 0.6240 - val_accuracy: 0.6811\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6423 - accuracy: 0.6404 - val_loss: 0.6216 - val_accuracy: 0.6875\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6326 - val_loss: 0.6208 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6366 - val_loss: 0.6209 - val_accuracy: 0.6811\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6366 - val_loss: 0.6210 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6319 - val_loss: 0.6229 - val_accuracy: 0.6837\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6351 - val_loss: 0.6194 - val_accuracy: 0.6926\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6393 - val_loss: 0.6243 - val_accuracy: 0.6862\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6408 - accuracy: 0.6323 - val_loss: 0.6242 - val_accuracy: 0.6837\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6420 - val_loss: 0.6205 - val_accuracy: 0.6862\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6377 - val_loss: 0.6215 - val_accuracy: 0.6811\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6403 - val_loss: 0.6235 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6482 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6487 - val_loss: 0.6165 - val_accuracy: 0.6888\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6393 - accuracy: 0.6402 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6448 - val_loss: 0.6288 - val_accuracy: 0.6645\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6390 - val_loss: 0.6194 - val_accuracy: 0.6875\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6319 - accuracy: 0.6517 - val_loss: 0.6157 - val_accuracy: 0.6926\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6458 - val_loss: 0.6179 - val_accuracy: 0.6901\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6469 - val_loss: 0.6209 - val_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6480 - val_loss: 0.6153 - val_accuracy: 0.6926\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6447 - val_loss: 0.6189 - val_accuracy: 0.6901\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6518 - val_loss: 0.6204 - val_accuracy: 0.6862\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6487 - val_loss: 0.6195 - val_accuracy: 0.6888\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6550 - val_loss: 0.6179 - val_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6546 - val_loss: 0.6150 - val_accuracy: 0.6952\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6522 - val_loss: 0.6180 - val_accuracy: 0.6990\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6294 - accuracy: 0.6500 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6501 - val_loss: 0.6175 - val_accuracy: 0.6926\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6535 - val_loss: 0.6196 - val_accuracy: 0.6837\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6545 - val_loss: 0.6185 - val_accuracy: 0.6926\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6523 - val_loss: 0.6130 - val_accuracy: 0.6977\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6541 - val_loss: 0.6188 - val_accuracy: 0.6875\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6560 - val_loss: 0.6159 - val_accuracy: 0.6875\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6285 - accuracy: 0.6508 - val_loss: 0.6180 - val_accuracy: 0.6747\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6259 - accuracy: 0.6518 - val_loss: 0.6171 - val_accuracy: 0.6875\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6557 - val_loss: 0.6150 - val_accuracy: 0.6939\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6277 - accuracy: 0.6552 - val_loss: 0.6159 - val_accuracy: 0.6901\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6520 - val_loss: 0.6132 - val_accuracy: 0.6964\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6644 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6575 - val_loss: 0.6153 - val_accuracy: 0.6939\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6604 - val_loss: 0.6149 - val_accuracy: 0.6939\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6237 - accuracy: 0.6611 - val_loss: 0.6161 - val_accuracy: 0.6926\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6628 - val_loss: 0.6178 - val_accuracy: 0.6837\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6222 - accuracy: 0.6613 - val_loss: 0.6177 - val_accuracy: 0.6837\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6219 - accuracy: 0.6629 - val_loss: 0.6145 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6221 - accuracy: 0.6631 - val_loss: 0.6155 - val_accuracy: 0.6888\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6260 - accuracy: 0.6554 - val_loss: 0.6154 - val_accuracy: 0.6849\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6225 - accuracy: 0.6647 - val_loss: 0.6162 - val_accuracy: 0.6824\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6231 - accuracy: 0.6640 - val_loss: 0.6159 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6577 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6179 - accuracy: 0.6683 - val_loss: 0.6148 - val_accuracy: 0.6901\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6228 - accuracy: 0.6633 - val_loss: 0.6156 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6201 - accuracy: 0.6658 - val_loss: 0.6180 - val_accuracy: 0.6875\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6667 - val_loss: 0.6152 - val_accuracy: 0.6811\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6184 - accuracy: 0.6717 - val_loss: 0.6155 - val_accuracy: 0.6849\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6693 - val_loss: 0.6168 - val_accuracy: 0.6849\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6183 - accuracy: 0.6624 - val_loss: 0.6174 - val_accuracy: 0.6837\n",
      "Calculating for: 750 50 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_320 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.8224 - accuracy: 0.5231 - val_loss: 0.6602 - val_accuracy: 0.6390\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7400 - accuracy: 0.5262 - val_loss: 0.6585 - val_accuracy: 0.6441\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.7060 - accuracy: 0.5377 - val_loss: 0.6631 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6930 - accuracy: 0.5456 - val_loss: 0.6551 - val_accuracy: 0.6454\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6890 - accuracy: 0.5522 - val_loss: 0.6593 - val_accuracy: 0.6352\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6855 - accuracy: 0.5496 - val_loss: 0.6585 - val_accuracy: 0.6467\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.5666 - val_loss: 0.6572 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5676 - val_loss: 0.6502 - val_accuracy: 0.6416\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6795 - accuracy: 0.5667 - val_loss: 0.6545 - val_accuracy: 0.6531\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6792 - accuracy: 0.5688 - val_loss: 0.6520 - val_accuracy: 0.6505\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6793 - accuracy: 0.5656 - val_loss: 0.6557 - val_accuracy: 0.6594\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5776 - val_loss: 0.6454 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6817 - accuracy: 0.5695 - val_loss: 0.6523 - val_accuracy: 0.6543\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6746 - accuracy: 0.5845 - val_loss: 0.6466 - val_accuracy: 0.6556\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6772 - accuracy: 0.5756 - val_loss: 0.6457 - val_accuracy: 0.6531\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6722 - accuracy: 0.5813 - val_loss: 0.6442 - val_accuracy: 0.6709\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5737 - val_loss: 0.6449 - val_accuracy: 0.6696\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5838 - val_loss: 0.6485 - val_accuracy: 0.6607\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6712 - accuracy: 0.5904 - val_loss: 0.6421 - val_accuracy: 0.6671\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6717 - accuracy: 0.5928 - val_loss: 0.6426 - val_accuracy: 0.6709\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6731 - accuracy: 0.5894 - val_loss: 0.6485 - val_accuracy: 0.6505\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6705 - accuracy: 0.5884 - val_loss: 0.6455 - val_accuracy: 0.6633\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5869 - val_loss: 0.6434 - val_accuracy: 0.6671\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6714 - accuracy: 0.5944 - val_loss: 0.6404 - val_accuracy: 0.6696\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6691 - accuracy: 0.5943 - val_loss: 0.6391 - val_accuracy: 0.6684\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6705 - accuracy: 0.5922 - val_loss: 0.6436 - val_accuracy: 0.6658\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5929 - val_loss: 0.6422 - val_accuracy: 0.6671\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6689 - accuracy: 0.5990 - val_loss: 0.6447 - val_accuracy: 0.6594\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6675 - accuracy: 0.5948 - val_loss: 0.6388 - val_accuracy: 0.6722\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6693 - accuracy: 0.5929 - val_loss: 0.6406 - val_accuracy: 0.6722\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6669 - accuracy: 0.6012 - val_loss: 0.6399 - val_accuracy: 0.6633\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.6009 - val_loss: 0.6391 - val_accuracy: 0.6607\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6683 - accuracy: 0.5968 - val_loss: 0.6372 - val_accuracy: 0.6658\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6658 - accuracy: 0.5941 - val_loss: 0.6379 - val_accuracy: 0.6671\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6015 - val_loss: 0.6353 - val_accuracy: 0.6658\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6647 - accuracy: 0.6078 - val_loss: 0.6398 - val_accuracy: 0.6658\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6093 - val_loss: 0.6358 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6034 - val_loss: 0.6384 - val_accuracy: 0.6658\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6630 - accuracy: 0.6109 - val_loss: 0.6343 - val_accuracy: 0.6722\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6638 - accuracy: 0.5997 - val_loss: 0.6386 - val_accuracy: 0.6620\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6631 - accuracy: 0.6068 - val_loss: 0.6337 - val_accuracy: 0.6671\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6606 - accuracy: 0.6084 - val_loss: 0.6341 - val_accuracy: 0.6645\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6620 - accuracy: 0.6004 - val_loss: 0.6342 - val_accuracy: 0.6658\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6599 - accuracy: 0.6120 - val_loss: 0.6358 - val_accuracy: 0.6671\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6576 - accuracy: 0.6181 - val_loss: 0.6354 - val_accuracy: 0.6696\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6576 - accuracy: 0.6138 - val_loss: 0.6336 - val_accuracy: 0.6645\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6629 - accuracy: 0.6076 - val_loss: 0.6346 - val_accuracy: 0.6722\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6108 - val_loss: 0.6358 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6583 - accuracy: 0.6177 - val_loss: 0.6334 - val_accuracy: 0.6722\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6606 - accuracy: 0.6129 - val_loss: 0.6313 - val_accuracy: 0.6747\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6117 - val_loss: 0.6339 - val_accuracy: 0.6722\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6600 - accuracy: 0.6098 - val_loss: 0.6316 - val_accuracy: 0.6633\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6579 - accuracy: 0.6159 - val_loss: 0.6289 - val_accuracy: 0.6722\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6571 - accuracy: 0.6127 - val_loss: 0.6303 - val_accuracy: 0.6709\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6534 - accuracy: 0.6267 - val_loss: 0.6303 - val_accuracy: 0.6760\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6071 - val_loss: 0.6359 - val_accuracy: 0.6696\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6186 - val_loss: 0.6268 - val_accuracy: 0.6709\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6586 - accuracy: 0.6112 - val_loss: 0.6344 - val_accuracy: 0.6760\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6526 - accuracy: 0.6187 - val_loss: 0.6255 - val_accuracy: 0.6735\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6162 - val_loss: 0.6296 - val_accuracy: 0.6773\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6558 - accuracy: 0.6242 - val_loss: 0.6280 - val_accuracy: 0.6798\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6197 - val_loss: 0.6301 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6573 - accuracy: 0.6230 - val_loss: 0.6283 - val_accuracy: 0.6735\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6588 - accuracy: 0.6149 - val_loss: 0.6338 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6133 - val_loss: 0.6322 - val_accuracy: 0.6811\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6536 - accuracy: 0.6208 - val_loss: 0.6290 - val_accuracy: 0.6824\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6257 - val_loss: 0.6255 - val_accuracy: 0.6709\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6262 - val_loss: 0.6297 - val_accuracy: 0.6824\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6196 - val_loss: 0.6292 - val_accuracy: 0.6837\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6238 - val_loss: 0.6273 - val_accuracy: 0.6760\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6556 - accuracy: 0.6183 - val_loss: 0.6338 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6560 - accuracy: 0.6194 - val_loss: 0.6309 - val_accuracy: 0.6722\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6523 - accuracy: 0.6230 - val_loss: 0.6283 - val_accuracy: 0.6811\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6262 - val_loss: 0.6233 - val_accuracy: 0.6773\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6201 - val_loss: 0.6256 - val_accuracy: 0.6875\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6514 - accuracy: 0.6247 - val_loss: 0.6306 - val_accuracy: 0.6773\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6267 - val_loss: 0.6277 - val_accuracy: 0.6824\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6508 - accuracy: 0.6264 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6287 - val_loss: 0.6239 - val_accuracy: 0.6824\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6257 - val_loss: 0.6251 - val_accuracy: 0.6824\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6492 - accuracy: 0.6305 - val_loss: 0.6251 - val_accuracy: 0.6837\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6335 - val_loss: 0.6222 - val_accuracy: 0.6837\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6506 - accuracy: 0.6305 - val_loss: 0.6253 - val_accuracy: 0.6837\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6465 - accuracy: 0.6336 - val_loss: 0.6221 - val_accuracy: 0.6849\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6296 - val_loss: 0.6221 - val_accuracy: 0.6849\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6286 - val_loss: 0.6267 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6296 - val_loss: 0.6217 - val_accuracy: 0.6786\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6275 - val_loss: 0.6237 - val_accuracy: 0.6849\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6334 - val_loss: 0.6214 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6265 - val_loss: 0.6235 - val_accuracy: 0.6837\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6488 - accuracy: 0.6325 - val_loss: 0.6256 - val_accuracy: 0.6798\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6306 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6493 - accuracy: 0.6338 - val_loss: 0.6248 - val_accuracy: 0.6786\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6407 - val_loss: 0.6238 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6364 - val_loss: 0.6209 - val_accuracy: 0.6913\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6494 - accuracy: 0.6330 - val_loss: 0.6229 - val_accuracy: 0.6862\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6384 - val_loss: 0.6203 - val_accuracy: 0.6901\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6482 - accuracy: 0.6324 - val_loss: 0.6212 - val_accuracy: 0.6901\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6339 - val_loss: 0.6192 - val_accuracy: 0.6875\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6445 - accuracy: 0.6321 - val_loss: 0.6250 - val_accuracy: 0.6824\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6355 - val_loss: 0.6180 - val_accuracy: 0.6888\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6355 - val_loss: 0.6199 - val_accuracy: 0.6760\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6410 - accuracy: 0.6379 - val_loss: 0.6160 - val_accuracy: 0.6888\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6479 - accuracy: 0.6333 - val_loss: 0.6218 - val_accuracy: 0.6849\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6418 - val_loss: 0.6236 - val_accuracy: 0.6684\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6405 - val_loss: 0.6222 - val_accuracy: 0.6862\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6319 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6390 - val_loss: 0.6178 - val_accuracy: 0.6798\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6453 - val_loss: 0.6166 - val_accuracy: 0.6811\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6370 - val_loss: 0.6208 - val_accuracy: 0.6811\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6427 - accuracy: 0.6366 - val_loss: 0.6197 - val_accuracy: 0.6811\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6403 - val_loss: 0.6201 - val_accuracy: 0.6798\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6338 - val_loss: 0.6188 - val_accuracy: 0.6837\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6383 - accuracy: 0.6395 - val_loss: 0.6204 - val_accuracy: 0.6837\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6452 - val_loss: 0.6172 - val_accuracy: 0.6773\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6437 - accuracy: 0.6404 - val_loss: 0.6196 - val_accuracy: 0.6849\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6463 - val_loss: 0.6212 - val_accuracy: 0.6862\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6428 - val_loss: 0.6200 - val_accuracy: 0.6837\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6477 - val_loss: 0.6177 - val_accuracy: 0.6824\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6443 - val_loss: 0.6200 - val_accuracy: 0.6824\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6420 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6452 - val_loss: 0.6172 - val_accuracy: 0.6875\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6366 - val_loss: 0.6192 - val_accuracy: 0.6849\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6511 - val_loss: 0.6156 - val_accuracy: 0.6862\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6441 - val_loss: 0.6147 - val_accuracy: 0.6849\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6400 - accuracy: 0.6452 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6439 - val_loss: 0.6147 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6349 - accuracy: 0.6481 - val_loss: 0.6128 - val_accuracy: 0.6888\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6531 - val_loss: 0.6143 - val_accuracy: 0.6837\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6478 - val_loss: 0.6166 - val_accuracy: 0.6875\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6453 - val_loss: 0.6170 - val_accuracy: 0.6913\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6416 - accuracy: 0.6375 - val_loss: 0.6150 - val_accuracy: 0.6875\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6490 - val_loss: 0.6133 - val_accuracy: 0.6837\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6413 - val_loss: 0.6148 - val_accuracy: 0.6913\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6367 - accuracy: 0.6536 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6459 - val_loss: 0.6163 - val_accuracy: 0.6913\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6351 - accuracy: 0.6483 - val_loss: 0.6141 - val_accuracy: 0.6901\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6456 - val_loss: 0.6181 - val_accuracy: 0.6964\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6501 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6354 - accuracy: 0.6488 - val_loss: 0.6135 - val_accuracy: 0.6901\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6348 - accuracy: 0.6500 - val_loss: 0.6141 - val_accuracy: 0.6952\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6468 - val_loss: 0.6134 - val_accuracy: 0.6990\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6513 - val_loss: 0.6167 - val_accuracy: 0.6913\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6486 - val_loss: 0.6201 - val_accuracy: 0.6901\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6472 - val_loss: 0.6157 - val_accuracy: 0.6990\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6370 - accuracy: 0.6463 - val_loss: 0.6118 - val_accuracy: 0.6913\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6321 - accuracy: 0.6492 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6313 - accuracy: 0.6503 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6323 - accuracy: 0.6510 - val_loss: 0.6114 - val_accuracy: 0.6939\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6506 - val_loss: 0.6130 - val_accuracy: 0.6913\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6340 - accuracy: 0.6560 - val_loss: 0.6103 - val_accuracy: 0.6977\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6531 - val_loss: 0.6166 - val_accuracy: 0.6926\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6292 - accuracy: 0.6555 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6535 - val_loss: 0.6135 - val_accuracy: 0.6939\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6590 - val_loss: 0.6126 - val_accuracy: 0.6939\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6567 - val_loss: 0.6104 - val_accuracy: 0.6913\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6301 - accuracy: 0.6595 - val_loss: 0.6117 - val_accuracy: 0.6901\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6544 - val_loss: 0.6088 - val_accuracy: 0.6888\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6527 - val_loss: 0.6154 - val_accuracy: 0.6837\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6554 - val_loss: 0.6106 - val_accuracy: 0.6888\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6544 - val_loss: 0.6143 - val_accuracy: 0.6888\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6284 - accuracy: 0.6546 - val_loss: 0.6107 - val_accuracy: 0.6913\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.6527 - val_loss: 0.6133 - val_accuracy: 0.6888\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6287 - accuracy: 0.6605 - val_loss: 0.6098 - val_accuracy: 0.6875\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6588 - val_loss: 0.6108 - val_accuracy: 0.6926\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6281 - accuracy: 0.6585 - val_loss: 0.6117 - val_accuracy: 0.6901\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6589 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6297 - accuracy: 0.6536 - val_loss: 0.6151 - val_accuracy: 0.6939\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6645 - val_loss: 0.6080 - val_accuracy: 0.6888\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6569 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6257 - accuracy: 0.6625 - val_loss: 0.6114 - val_accuracy: 0.6939\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6215 - accuracy: 0.6633 - val_loss: 0.6107 - val_accuracy: 0.6913\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6511 - val_loss: 0.6087 - val_accuracy: 0.6952\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6272 - accuracy: 0.6603 - val_loss: 0.6101 - val_accuracy: 0.6875\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6539 - val_loss: 0.6127 - val_accuracy: 0.6888\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6279 - accuracy: 0.6575 - val_loss: 0.6100 - val_accuracy: 0.6913\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6541 - val_loss: 0.6174 - val_accuracy: 0.6862\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6614 - val_loss: 0.6111 - val_accuracy: 0.6926\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6311 - accuracy: 0.6491 - val_loss: 0.6143 - val_accuracy: 0.6862\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6574 - val_loss: 0.6135 - val_accuracy: 0.6849\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6667 - val_loss: 0.6111 - val_accuracy: 0.6913\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6571 - val_loss: 0.6130 - val_accuracy: 0.6926\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6605 - val_loss: 0.6144 - val_accuracy: 0.6849\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6562 - val_loss: 0.6151 - val_accuracy: 0.6901\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6546 - val_loss: 0.6125 - val_accuracy: 0.6952\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6287 - accuracy: 0.6576 - val_loss: 0.6116 - val_accuracy: 0.6901\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6234 - accuracy: 0.6584 - val_loss: 0.6082 - val_accuracy: 0.6901\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6667 - val_loss: 0.6105 - val_accuracy: 0.6913\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6191 - accuracy: 0.6658 - val_loss: 0.6115 - val_accuracy: 0.6901\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6246 - accuracy: 0.6585 - val_loss: 0.6131 - val_accuracy: 0.6926\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6198 - accuracy: 0.6677 - val_loss: 0.6135 - val_accuracy: 0.6862\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6635 - val_loss: 0.6124 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6204 - accuracy: 0.6640 - val_loss: 0.6141 - val_accuracy: 0.6926\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6702 - val_loss: 0.6146 - val_accuracy: 0.6862\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6652 - val_loss: 0.6087 - val_accuracy: 0.6875\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6236 - accuracy: 0.6581 - val_loss: 0.6115 - val_accuracy: 0.6913\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6594 - val_loss: 0.6118 - val_accuracy: 0.6939\n",
      "Epoch 198/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6239 - accuracy: 0.6601 - val_loss: 0.6096 - val_accuracy: 0.6926\n",
      "Epoch 199/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6584 - val_loss: 0.6118 - val_accuracy: 0.7028\n",
      "Calculating for: 750 100 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_324 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8121 - accuracy: 0.5428 - val_loss: 0.6473 - val_accuracy: 0.6518\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7294 - accuracy: 0.5534 - val_loss: 0.6378 - val_accuracy: 0.6607\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6949 - accuracy: 0.5672 - val_loss: 0.6415 - val_accuracy: 0.6671\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6761 - accuracy: 0.5786 - val_loss: 0.6410 - val_accuracy: 0.6684\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5836 - val_loss: 0.6384 - val_accuracy: 0.6633\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5955 - val_loss: 0.6428 - val_accuracy: 0.6684\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6656 - accuracy: 0.5978 - val_loss: 0.6379 - val_accuracy: 0.6658\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6635 - accuracy: 0.6086 - val_loss: 0.6423 - val_accuracy: 0.6658\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6040 - val_loss: 0.6360 - val_accuracy: 0.6671\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6091 - val_loss: 0.6357 - val_accuracy: 0.6709\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6598 - accuracy: 0.6086 - val_loss: 0.6374 - val_accuracy: 0.6582\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6578 - accuracy: 0.6133 - val_loss: 0.6359 - val_accuracy: 0.6709\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6150 - val_loss: 0.6370 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6207 - val_loss: 0.6310 - val_accuracy: 0.6735\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6144 - val_loss: 0.6267 - val_accuracy: 0.6824\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6157 - val_loss: 0.6302 - val_accuracy: 0.6824\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6479 - accuracy: 0.6236 - val_loss: 0.6278 - val_accuracy: 0.6849\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6168 - val_loss: 0.6304 - val_accuracy: 0.6798\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6261 - val_loss: 0.6314 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6485 - accuracy: 0.6289 - val_loss: 0.6295 - val_accuracy: 0.6747\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6476 - accuracy: 0.6248 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6274 - val_loss: 0.6247 - val_accuracy: 0.6773\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6286 - val_loss: 0.6256 - val_accuracy: 0.6837\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6320 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6290 - val_loss: 0.6279 - val_accuracy: 0.6735\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6305 - val_loss: 0.6271 - val_accuracy: 0.6760\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6378 - val_loss: 0.6251 - val_accuracy: 0.6811\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6419 - accuracy: 0.6316 - val_loss: 0.6290 - val_accuracy: 0.6645\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6392 - accuracy: 0.6378 - val_loss: 0.6224 - val_accuracy: 0.6824\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6402 - accuracy: 0.6390 - val_loss: 0.6228 - val_accuracy: 0.6837\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6410 - accuracy: 0.6304 - val_loss: 0.6225 - val_accuracy: 0.6837\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6387 - accuracy: 0.6377 - val_loss: 0.6207 - val_accuracy: 0.6798\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6394 - val_loss: 0.6225 - val_accuracy: 0.6760\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6317 - accuracy: 0.6510 - val_loss: 0.6194 - val_accuracy: 0.6837\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6466 - val_loss: 0.6221 - val_accuracy: 0.6735\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6343 - accuracy: 0.6395 - val_loss: 0.6227 - val_accuracy: 0.6786\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6314 - accuracy: 0.6424 - val_loss: 0.6216 - val_accuracy: 0.6798\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6477 - val_loss: 0.6189 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6433 - val_loss: 0.6179 - val_accuracy: 0.6773\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6482 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6545 - val_loss: 0.6218 - val_accuracy: 0.6798\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6296 - accuracy: 0.6487 - val_loss: 0.6245 - val_accuracy: 0.6747\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6269 - accuracy: 0.6547 - val_loss: 0.6223 - val_accuracy: 0.6735\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6535 - val_loss: 0.6188 - val_accuracy: 0.6786\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6547 - val_loss: 0.6182 - val_accuracy: 0.6824\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6520 - val_loss: 0.6193 - val_accuracy: 0.6824\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6495 - val_loss: 0.6210 - val_accuracy: 0.6773\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6520 - val_loss: 0.6185 - val_accuracy: 0.6811\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6590 - val_loss: 0.6190 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6227 - accuracy: 0.6647 - val_loss: 0.6217 - val_accuracy: 0.6735\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6220 - accuracy: 0.6581 - val_loss: 0.6168 - val_accuracy: 0.6786\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6218 - accuracy: 0.6660 - val_loss: 0.6220 - val_accuracy: 0.6773\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6194 - accuracy: 0.6609 - val_loss: 0.6239 - val_accuracy: 0.6747\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6625 - val_loss: 0.6197 - val_accuracy: 0.6773\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6177 - accuracy: 0.6576 - val_loss: 0.6248 - val_accuracy: 0.6811\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6137 - accuracy: 0.6590 - val_loss: 0.6203 - val_accuracy: 0.6824\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6615 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6183 - accuracy: 0.6621 - val_loss: 0.6203 - val_accuracy: 0.6837\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6637 - val_loss: 0.6175 - val_accuracy: 0.6811\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6724 - val_loss: 0.6253 - val_accuracy: 0.6735\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6107 - accuracy: 0.6693 - val_loss: 0.6181 - val_accuracy: 0.6735\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6106 - accuracy: 0.6691 - val_loss: 0.6189 - val_accuracy: 0.6760\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6120 - accuracy: 0.6652 - val_loss: 0.6207 - val_accuracy: 0.6747\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6090 - accuracy: 0.6747 - val_loss: 0.6234 - val_accuracy: 0.6786\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6083 - accuracy: 0.6768 - val_loss: 0.6245 - val_accuracy: 0.6773\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6083 - accuracy: 0.6740 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6092 - accuracy: 0.6740 - val_loss: 0.6221 - val_accuracy: 0.6735\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6099 - accuracy: 0.6716 - val_loss: 0.6205 - val_accuracy: 0.6773\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6034 - accuracy: 0.6746 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6093 - accuracy: 0.6707 - val_loss: 0.6265 - val_accuracy: 0.6594\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6025 - accuracy: 0.6762 - val_loss: 0.6226 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6038 - accuracy: 0.6768 - val_loss: 0.6252 - val_accuracy: 0.6696\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6066 - accuracy: 0.6707 - val_loss: 0.6218 - val_accuracy: 0.6722\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6016 - accuracy: 0.6796 - val_loss: 0.6233 - val_accuracy: 0.6773\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5997 - accuracy: 0.6822 - val_loss: 0.6261 - val_accuracy: 0.6709\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5982 - accuracy: 0.6807 - val_loss: 0.6288 - val_accuracy: 0.6633\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5986 - accuracy: 0.6806 - val_loss: 0.6276 - val_accuracy: 0.6658\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6032 - accuracy: 0.6804 - val_loss: 0.6265 - val_accuracy: 0.6722\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6008 - accuracy: 0.6817 - val_loss: 0.6243 - val_accuracy: 0.6760\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6001 - accuracy: 0.6855 - val_loss: 0.6291 - val_accuracy: 0.6760\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6022 - accuracy: 0.6796 - val_loss: 0.6253 - val_accuracy: 0.6824\n",
      "Calculating for: 750 100 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_328 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.8021 - accuracy: 0.5197 - val_loss: 0.6681 - val_accuracy: 0.6416\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7135 - accuracy: 0.5467 - val_loss: 0.6592 - val_accuracy: 0.6518\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6920 - accuracy: 0.5574 - val_loss: 0.6618 - val_accuracy: 0.6390\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6837 - accuracy: 0.5673 - val_loss: 0.6586 - val_accuracy: 0.6416\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6819 - accuracy: 0.5671 - val_loss: 0.6620 - val_accuracy: 0.6390\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6765 - accuracy: 0.5850 - val_loss: 0.6543 - val_accuracy: 0.6543\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6724 - accuracy: 0.5841 - val_loss: 0.6507 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5814 - val_loss: 0.6519 - val_accuracy: 0.6454\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6704 - accuracy: 0.5893 - val_loss: 0.6546 - val_accuracy: 0.6543\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6711 - accuracy: 0.5899 - val_loss: 0.6538 - val_accuracy: 0.6594\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5948 - val_loss: 0.6539 - val_accuracy: 0.6492\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.5927 - val_loss: 0.6502 - val_accuracy: 0.6543\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6687 - accuracy: 0.5953 - val_loss: 0.6492 - val_accuracy: 0.6492\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.5986 - val_loss: 0.6444 - val_accuracy: 0.6671\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6616 - accuracy: 0.6042 - val_loss: 0.6403 - val_accuracy: 0.6735\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6617 - accuracy: 0.6143 - val_loss: 0.6469 - val_accuracy: 0.6505\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6620 - accuracy: 0.6099 - val_loss: 0.6493 - val_accuracy: 0.6518\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6063 - val_loss: 0.6463 - val_accuracy: 0.6582\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6614 - accuracy: 0.6052 - val_loss: 0.6456 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6607 - accuracy: 0.6039 - val_loss: 0.6400 - val_accuracy: 0.6684\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6591 - accuracy: 0.6109 - val_loss: 0.6472 - val_accuracy: 0.6492\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6178 - val_loss: 0.6457 - val_accuracy: 0.6531\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6132 - val_loss: 0.6427 - val_accuracy: 0.6582\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6129 - val_loss: 0.6435 - val_accuracy: 0.6607\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6572 - accuracy: 0.6177 - val_loss: 0.6416 - val_accuracy: 0.6645\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6139 - val_loss: 0.6346 - val_accuracy: 0.6773\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6144 - val_loss: 0.6424 - val_accuracy: 0.6518\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6138 - val_loss: 0.6363 - val_accuracy: 0.6684\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6173 - val_loss: 0.6379 - val_accuracy: 0.6645\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6235 - val_loss: 0.6352 - val_accuracy: 0.6811\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6539 - accuracy: 0.6198 - val_loss: 0.6332 - val_accuracy: 0.6786\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6243 - val_loss: 0.6414 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6525 - accuracy: 0.6150 - val_loss: 0.6386 - val_accuracy: 0.6633\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6511 - accuracy: 0.6250 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6498 - accuracy: 0.6207 - val_loss: 0.6310 - val_accuracy: 0.6773\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6222 - val_loss: 0.6316 - val_accuracy: 0.6747\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6497 - accuracy: 0.6284 - val_loss: 0.6345 - val_accuracy: 0.6671\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6500 - accuracy: 0.6262 - val_loss: 0.6293 - val_accuracy: 0.6798\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6310 - val_loss: 0.6337 - val_accuracy: 0.6645\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6368 - val_loss: 0.6354 - val_accuracy: 0.6696\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6240 - val_loss: 0.6306 - val_accuracy: 0.6798\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6302 - val_loss: 0.6357 - val_accuracy: 0.6658\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6291 - val_loss: 0.6343 - val_accuracy: 0.6722\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6489 - accuracy: 0.6272 - val_loss: 0.6350 - val_accuracy: 0.6747\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6354 - val_loss: 0.6371 - val_accuracy: 0.6582\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6475 - accuracy: 0.6351 - val_loss: 0.6317 - val_accuracy: 0.6760\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6310 - val_loss: 0.6329 - val_accuracy: 0.6760\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6426 - accuracy: 0.6359 - val_loss: 0.6326 - val_accuracy: 0.6709\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6336 - val_loss: 0.6320 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6306 - val_loss: 0.6264 - val_accuracy: 0.6798\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6461 - accuracy: 0.6359 - val_loss: 0.6293 - val_accuracy: 0.6747\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6412 - val_loss: 0.6304 - val_accuracy: 0.6760\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6292 - val_loss: 0.6249 - val_accuracy: 0.6837\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6404 - accuracy: 0.6398 - val_loss: 0.6305 - val_accuracy: 0.6747\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6462 - accuracy: 0.6301 - val_loss: 0.6345 - val_accuracy: 0.6709\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6372 - val_loss: 0.6322 - val_accuracy: 0.6684\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6403 - accuracy: 0.6384 - val_loss: 0.6270 - val_accuracy: 0.6696\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6388 - accuracy: 0.6389 - val_loss: 0.6233 - val_accuracy: 0.6786\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6355 - val_loss: 0.6294 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6423 - val_loss: 0.6217 - val_accuracy: 0.6760\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6380 - accuracy: 0.6365 - val_loss: 0.6238 - val_accuracy: 0.6747\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6429 - val_loss: 0.6278 - val_accuracy: 0.6709\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6354 - val_loss: 0.6244 - val_accuracy: 0.6773\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6361 - accuracy: 0.6433 - val_loss: 0.6314 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6433 - val_loss: 0.6276 - val_accuracy: 0.6709\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6442 - val_loss: 0.6313 - val_accuracy: 0.6709\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6379 - accuracy: 0.6463 - val_loss: 0.6277 - val_accuracy: 0.6722\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6517 - val_loss: 0.6233 - val_accuracy: 0.6722\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6481 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6345 - accuracy: 0.6477 - val_loss: 0.6282 - val_accuracy: 0.6735\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6448 - val_loss: 0.6285 - val_accuracy: 0.6735\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6341 - accuracy: 0.6518 - val_loss: 0.6227 - val_accuracy: 0.6747\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6326 - accuracy: 0.6473 - val_loss: 0.6239 - val_accuracy: 0.6735\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6295 - accuracy: 0.6520 - val_loss: 0.6219 - val_accuracy: 0.6709\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6308 - accuracy: 0.6550 - val_loss: 0.6230 - val_accuracy: 0.6760\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6325 - accuracy: 0.6510 - val_loss: 0.6214 - val_accuracy: 0.6773\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6476 - val_loss: 0.6229 - val_accuracy: 0.6773\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6506 - val_loss: 0.6269 - val_accuracy: 0.6773\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6575 - val_loss: 0.6179 - val_accuracy: 0.6824\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6539 - val_loss: 0.6228 - val_accuracy: 0.6786\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6304 - accuracy: 0.6508 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6275 - accuracy: 0.6531 - val_loss: 0.6210 - val_accuracy: 0.6786\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6293 - accuracy: 0.6611 - val_loss: 0.6252 - val_accuracy: 0.6773\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6559 - val_loss: 0.6238 - val_accuracy: 0.6760\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6306 - accuracy: 0.6549 - val_loss: 0.6236 - val_accuracy: 0.6722\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6551 - val_loss: 0.6296 - val_accuracy: 0.6735\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6243 - accuracy: 0.6566 - val_loss: 0.6290 - val_accuracy: 0.6735\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6286 - accuracy: 0.6522 - val_loss: 0.6256 - val_accuracy: 0.6747\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6566 - val_loss: 0.6251 - val_accuracy: 0.6722\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6541 - val_loss: 0.6207 - val_accuracy: 0.6722\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6276 - accuracy: 0.6520 - val_loss: 0.6279 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6680 - val_loss: 0.6238 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6188 - accuracy: 0.6555 - val_loss: 0.6246 - val_accuracy: 0.6798\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6626 - val_loss: 0.6194 - val_accuracy: 0.6747\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6529 - val_loss: 0.6283 - val_accuracy: 0.6798\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6583 - val_loss: 0.6209 - val_accuracy: 0.6786\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6590 - val_loss: 0.6221 - val_accuracy: 0.6786\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6588 - val_loss: 0.6232 - val_accuracy: 0.6811\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6177 - accuracy: 0.6643 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6629 - val_loss: 0.6235 - val_accuracy: 0.6798\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6648 - val_loss: 0.6221 - val_accuracy: 0.6773\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6176 - accuracy: 0.6598 - val_loss: 0.6232 - val_accuracy: 0.6786\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6190 - accuracy: 0.6615 - val_loss: 0.6215 - val_accuracy: 0.6760\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6219 - accuracy: 0.6613 - val_loss: 0.6286 - val_accuracy: 0.6786\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6214 - accuracy: 0.6631 - val_loss: 0.6235 - val_accuracy: 0.6773\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6609 - val_loss: 0.6240 - val_accuracy: 0.6824\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6618 - val_loss: 0.6174 - val_accuracy: 0.6837\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6623 - val_loss: 0.6266 - val_accuracy: 0.6773\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6253 - accuracy: 0.6598 - val_loss: 0.6220 - val_accuracy: 0.6798\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6614 - val_loss: 0.6232 - val_accuracy: 0.6709\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6159 - accuracy: 0.6618 - val_loss: 0.6261 - val_accuracy: 0.6747\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6181 - accuracy: 0.6663 - val_loss: 0.6242 - val_accuracy: 0.6773\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6117 - accuracy: 0.6728 - val_loss: 0.6323 - val_accuracy: 0.6760\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6134 - accuracy: 0.6677 - val_loss: 0.6290 - val_accuracy: 0.6709\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6122 - accuracy: 0.6729 - val_loss: 0.6258 - val_accuracy: 0.6735\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6731 - val_loss: 0.6322 - val_accuracy: 0.6735\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6121 - accuracy: 0.6719 - val_loss: 0.6296 - val_accuracy: 0.6696\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6152 - accuracy: 0.6688 - val_loss: 0.6272 - val_accuracy: 0.6773\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6084 - accuracy: 0.6740 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6148 - accuracy: 0.6670 - val_loss: 0.6263 - val_accuracy: 0.6786\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6105 - accuracy: 0.6757 - val_loss: 0.6280 - val_accuracy: 0.6773\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6649 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6775 - val_loss: 0.6282 - val_accuracy: 0.6722\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6108 - accuracy: 0.6729 - val_loss: 0.6295 - val_accuracy: 0.6786\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6084 - accuracy: 0.6734 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6086 - accuracy: 0.6763 - val_loss: 0.6242 - val_accuracy: 0.6798\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6082 - accuracy: 0.6740 - val_loss: 0.6276 - val_accuracy: 0.6798\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6072 - accuracy: 0.6762 - val_loss: 0.6287 - val_accuracy: 0.6837\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6083 - accuracy: 0.6738 - val_loss: 0.6303 - val_accuracy: 0.6684\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6057 - accuracy: 0.6772 - val_loss: 0.6261 - val_accuracy: 0.6824\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6018 - accuracy: 0.6826 - val_loss: 0.6247 - val_accuracy: 0.6837\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6094 - accuracy: 0.6726 - val_loss: 0.6235 - val_accuracy: 0.6849\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6067 - accuracy: 0.6746 - val_loss: 0.6232 - val_accuracy: 0.6862\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6041 - accuracy: 0.6809 - val_loss: 0.6283 - val_accuracy: 0.6824\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6056 - accuracy: 0.6787 - val_loss: 0.6248 - val_accuracy: 0.6824\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6080 - accuracy: 0.6766 - val_loss: 0.6248 - val_accuracy: 0.6824\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6037 - accuracy: 0.6766 - val_loss: 0.6218 - val_accuracy: 0.6862\n",
      "Calculating for: 750 100 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_332 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8436 - accuracy: 0.5099 - val_loss: 0.6710 - val_accuracy: 0.6441\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7321 - accuracy: 0.5328 - val_loss: 0.6658 - val_accuracy: 0.6454\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6997 - accuracy: 0.5349 - val_loss: 0.6622 - val_accuracy: 0.6531\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6882 - accuracy: 0.5525 - val_loss: 0.6644 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6822 - accuracy: 0.5593 - val_loss: 0.6621 - val_accuracy: 0.6645\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6831 - accuracy: 0.5568 - val_loss: 0.6627 - val_accuracy: 0.6620\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6828 - accuracy: 0.5657 - val_loss: 0.6628 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6844 - accuracy: 0.5587 - val_loss: 0.6604 - val_accuracy: 0.6569\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6811 - accuracy: 0.5629 - val_loss: 0.6609 - val_accuracy: 0.6594\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6808 - accuracy: 0.5687 - val_loss: 0.6564 - val_accuracy: 0.6594\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5732 - val_loss: 0.6533 - val_accuracy: 0.6671\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6786 - accuracy: 0.5723 - val_loss: 0.6529 - val_accuracy: 0.6620\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.5767 - val_loss: 0.6493 - val_accuracy: 0.6633\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6754 - accuracy: 0.5869 - val_loss: 0.6519 - val_accuracy: 0.6658\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6766 - accuracy: 0.5782 - val_loss: 0.6512 - val_accuracy: 0.6671\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6749 - accuracy: 0.5796 - val_loss: 0.6513 - val_accuracy: 0.6569\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6720 - accuracy: 0.5809 - val_loss: 0.6464 - val_accuracy: 0.6658\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6730 - accuracy: 0.5831 - val_loss: 0.6491 - val_accuracy: 0.6556\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6710 - accuracy: 0.5970 - val_loss: 0.6526 - val_accuracy: 0.6531\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6707 - accuracy: 0.5904 - val_loss: 0.6424 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.5864 - val_loss: 0.6468 - val_accuracy: 0.6543\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.6010 - val_loss: 0.6445 - val_accuracy: 0.6543\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6683 - accuracy: 0.5948 - val_loss: 0.6484 - val_accuracy: 0.6518\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6667 - accuracy: 0.5972 - val_loss: 0.6433 - val_accuracy: 0.6607\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6674 - accuracy: 0.5950 - val_loss: 0.6466 - val_accuracy: 0.6569\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6652 - accuracy: 0.6017 - val_loss: 0.6462 - val_accuracy: 0.6633\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6679 - accuracy: 0.5955 - val_loss: 0.6407 - val_accuracy: 0.6633\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6625 - accuracy: 0.6032 - val_loss: 0.6477 - val_accuracy: 0.6607\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6636 - accuracy: 0.6069 - val_loss: 0.6419 - val_accuracy: 0.6594\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5966 - val_loss: 0.6411 - val_accuracy: 0.6633\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6027 - val_loss: 0.6430 - val_accuracy: 0.6620\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6630 - accuracy: 0.6010 - val_loss: 0.6445 - val_accuracy: 0.6633\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6086 - val_loss: 0.6364 - val_accuracy: 0.6658\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6615 - accuracy: 0.6073 - val_loss: 0.6401 - val_accuracy: 0.6684\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6105 - val_loss: 0.6350 - val_accuracy: 0.6735\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6054 - val_loss: 0.6408 - val_accuracy: 0.6645\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6647 - accuracy: 0.6027 - val_loss: 0.6412 - val_accuracy: 0.6760\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6579 - accuracy: 0.6089 - val_loss: 0.6412 - val_accuracy: 0.6645\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.6105 - val_loss: 0.6393 - val_accuracy: 0.6671\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6138 - val_loss: 0.6362 - val_accuracy: 0.6709\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6104 - val_loss: 0.6350 - val_accuracy: 0.6722\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6164 - val_loss: 0.6351 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6595 - accuracy: 0.6172 - val_loss: 0.6382 - val_accuracy: 0.6696\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6591 - accuracy: 0.6095 - val_loss: 0.6361 - val_accuracy: 0.6786\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6595 - accuracy: 0.6154 - val_loss: 0.6403 - val_accuracy: 0.6722\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6129 - val_loss: 0.6383 - val_accuracy: 0.6722\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6533 - accuracy: 0.6166 - val_loss: 0.6358 - val_accuracy: 0.6747\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6118 - val_loss: 0.6479 - val_accuracy: 0.6276\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6598 - accuracy: 0.6135 - val_loss: 0.6366 - val_accuracy: 0.6773\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6133 - val_loss: 0.6340 - val_accuracy: 0.6837\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6198 - val_loss: 0.6433 - val_accuracy: 0.6416\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6543 - accuracy: 0.6139 - val_loss: 0.6323 - val_accuracy: 0.6798\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6235 - val_loss: 0.6375 - val_accuracy: 0.6658\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6566 - accuracy: 0.6162 - val_loss: 0.6313 - val_accuracy: 0.6773\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6538 - accuracy: 0.6178 - val_loss: 0.6323 - val_accuracy: 0.6798\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6553 - accuracy: 0.6109 - val_loss: 0.6342 - val_accuracy: 0.6811\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6212 - val_loss: 0.6292 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6568 - accuracy: 0.6179 - val_loss: 0.6363 - val_accuracy: 0.6620\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6528 - accuracy: 0.6257 - val_loss: 0.6307 - val_accuracy: 0.6811\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6555 - accuracy: 0.6245 - val_loss: 0.6315 - val_accuracy: 0.6837\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6206 - val_loss: 0.6431 - val_accuracy: 0.6416\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6552 - accuracy: 0.6199 - val_loss: 0.6369 - val_accuracy: 0.6620\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6540 - accuracy: 0.6246 - val_loss: 0.6315 - val_accuracy: 0.6773\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6227 - val_loss: 0.6358 - val_accuracy: 0.6531\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6230 - val_loss: 0.6313 - val_accuracy: 0.6786\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6258 - val_loss: 0.6250 - val_accuracy: 0.6888\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6267 - val_loss: 0.6288 - val_accuracy: 0.6862\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6502 - accuracy: 0.6290 - val_loss: 0.6264 - val_accuracy: 0.6837\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6522 - accuracy: 0.6250 - val_loss: 0.6284 - val_accuracy: 0.6824\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6520 - accuracy: 0.6285 - val_loss: 0.6284 - val_accuracy: 0.6773\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6192 - val_loss: 0.6296 - val_accuracy: 0.6760\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6527 - accuracy: 0.6198 - val_loss: 0.6339 - val_accuracy: 0.6684\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6509 - accuracy: 0.6284 - val_loss: 0.6332 - val_accuracy: 0.6684\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6499 - accuracy: 0.6290 - val_loss: 0.6284 - val_accuracy: 0.6811\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6491 - accuracy: 0.6285 - val_loss: 0.6325 - val_accuracy: 0.6722\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6457 - accuracy: 0.6329 - val_loss: 0.6322 - val_accuracy: 0.6760\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6469 - accuracy: 0.6328 - val_loss: 0.6284 - val_accuracy: 0.6798\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6292 - val_loss: 0.6323 - val_accuracy: 0.6684\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6485 - accuracy: 0.6339 - val_loss: 0.6249 - val_accuracy: 0.6811\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6467 - accuracy: 0.6307 - val_loss: 0.6268 - val_accuracy: 0.6735\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6282 - val_loss: 0.6286 - val_accuracy: 0.6773\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6272 - val_loss: 0.6284 - val_accuracy: 0.6760\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6306 - val_loss: 0.6273 - val_accuracy: 0.6760\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6483 - accuracy: 0.6363 - val_loss: 0.6307 - val_accuracy: 0.6722\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6257 - val_loss: 0.6234 - val_accuracy: 0.6760\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6330 - val_loss: 0.6252 - val_accuracy: 0.6798\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6312 - val_loss: 0.6319 - val_accuracy: 0.6620\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6351 - val_loss: 0.6300 - val_accuracy: 0.6645\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6462 - accuracy: 0.6326 - val_loss: 0.6277 - val_accuracy: 0.6798\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6330 - val_loss: 0.6217 - val_accuracy: 0.6824\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6449 - accuracy: 0.6348 - val_loss: 0.6297 - val_accuracy: 0.6747\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6434 - accuracy: 0.6360 - val_loss: 0.6270 - val_accuracy: 0.6824\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6442 - accuracy: 0.6311 - val_loss: 0.6310 - val_accuracy: 0.6671\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6460 - accuracy: 0.6274 - val_loss: 0.6267 - val_accuracy: 0.6786\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6456 - accuracy: 0.6331 - val_loss: 0.6290 - val_accuracy: 0.6709\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6471 - accuracy: 0.6315 - val_loss: 0.6254 - val_accuracy: 0.6709\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6392 - val_loss: 0.6305 - val_accuracy: 0.6645\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6431 - accuracy: 0.6353 - val_loss: 0.6285 - val_accuracy: 0.6684\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6393 - accuracy: 0.6410 - val_loss: 0.6208 - val_accuracy: 0.6735\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6411 - accuracy: 0.6359 - val_loss: 0.6276 - val_accuracy: 0.6658\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6446 - accuracy: 0.6296 - val_loss: 0.6295 - val_accuracy: 0.6684\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6493 - val_loss: 0.6307 - val_accuracy: 0.6607\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6428 - accuracy: 0.6378 - val_loss: 0.6256 - val_accuracy: 0.6760\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6374 - val_loss: 0.6234 - val_accuracy: 0.6811\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.6372 - val_loss: 0.6237 - val_accuracy: 0.6747\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6421 - accuracy: 0.6387 - val_loss: 0.6270 - val_accuracy: 0.6671\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6397 - accuracy: 0.6432 - val_loss: 0.6227 - val_accuracy: 0.6824\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6353 - accuracy: 0.6456 - val_loss: 0.6253 - val_accuracy: 0.6786\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6401 - accuracy: 0.6444 - val_loss: 0.6250 - val_accuracy: 0.6747\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6384 - accuracy: 0.6451 - val_loss: 0.6214 - val_accuracy: 0.6798\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6386 - accuracy: 0.6491 - val_loss: 0.6242 - val_accuracy: 0.6773\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6411 - accuracy: 0.6410 - val_loss: 0.6209 - val_accuracy: 0.6773\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6380 - accuracy: 0.6437 - val_loss: 0.6195 - val_accuracy: 0.6773\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6371 - accuracy: 0.6412 - val_loss: 0.6234 - val_accuracy: 0.6773\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6360 - accuracy: 0.6449 - val_loss: 0.6226 - val_accuracy: 0.6773\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6378 - accuracy: 0.6451 - val_loss: 0.6193 - val_accuracy: 0.6811\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6418 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6434 - val_loss: 0.6186 - val_accuracy: 0.6824\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6366 - accuracy: 0.6404 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6374 - accuracy: 0.6405 - val_loss: 0.6236 - val_accuracy: 0.6811\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6510 - val_loss: 0.6234 - val_accuracy: 0.6798\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6471 - val_loss: 0.6202 - val_accuracy: 0.6798\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6353 - accuracy: 0.6402 - val_loss: 0.6183 - val_accuracy: 0.6824\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6374 - accuracy: 0.6409 - val_loss: 0.6206 - val_accuracy: 0.6811\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6478 - val_loss: 0.6168 - val_accuracy: 0.6875\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6346 - accuracy: 0.6517 - val_loss: 0.6178 - val_accuracy: 0.6875\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6449 - val_loss: 0.6180 - val_accuracy: 0.6786\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6466 - val_loss: 0.6210 - val_accuracy: 0.6811\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6373 - accuracy: 0.6457 - val_loss: 0.6233 - val_accuracy: 0.6811\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6316 - accuracy: 0.6457 - val_loss: 0.6171 - val_accuracy: 0.6837\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6329 - accuracy: 0.6525 - val_loss: 0.6180 - val_accuracy: 0.6811\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6350 - accuracy: 0.6466 - val_loss: 0.6196 - val_accuracy: 0.6811\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6368 - accuracy: 0.6505 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6478 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6319 - accuracy: 0.6513 - val_loss: 0.6188 - val_accuracy: 0.6811\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6555 - val_loss: 0.6203 - val_accuracy: 0.6798\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6303 - accuracy: 0.6507 - val_loss: 0.6215 - val_accuracy: 0.6798\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6493 - val_loss: 0.6197 - val_accuracy: 0.6811\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6322 - accuracy: 0.6493 - val_loss: 0.6129 - val_accuracy: 0.6913\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6327 - accuracy: 0.6467 - val_loss: 0.6212 - val_accuracy: 0.6773\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6505 - val_loss: 0.6180 - val_accuracy: 0.6824\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6298 - accuracy: 0.6467 - val_loss: 0.6156 - val_accuracy: 0.6798\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6305 - accuracy: 0.6503 - val_loss: 0.6173 - val_accuracy: 0.6798\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6512 - val_loss: 0.6189 - val_accuracy: 0.6811\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6515 - val_loss: 0.6169 - val_accuracy: 0.6786\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6574 - val_loss: 0.6221 - val_accuracy: 0.6837\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6556 - val_loss: 0.6181 - val_accuracy: 0.6811\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6566 - val_loss: 0.6194 - val_accuracy: 0.6849\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6537 - val_loss: 0.6207 - val_accuracy: 0.6849\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6307 - accuracy: 0.6537 - val_loss: 0.6183 - val_accuracy: 0.6875\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6540 - val_loss: 0.6156 - val_accuracy: 0.6888\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6549 - val_loss: 0.6200 - val_accuracy: 0.6798\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6565 - val_loss: 0.6205 - val_accuracy: 0.6837\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6299 - accuracy: 0.6498 - val_loss: 0.6233 - val_accuracy: 0.6786\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6252 - accuracy: 0.6611 - val_loss: 0.6168 - val_accuracy: 0.6862\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6591 - val_loss: 0.6233 - val_accuracy: 0.6773\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6576 - val_loss: 0.6187 - val_accuracy: 0.6862\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6255 - accuracy: 0.6575 - val_loss: 0.6213 - val_accuracy: 0.6849\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6601 - val_loss: 0.6168 - val_accuracy: 0.6888\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6296 - accuracy: 0.6536 - val_loss: 0.6174 - val_accuracy: 0.6901\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6245 - accuracy: 0.6559 - val_loss: 0.6149 - val_accuracy: 0.6913\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6559 - val_loss: 0.6199 - val_accuracy: 0.6875\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6280 - accuracy: 0.6580 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6595 - val_loss: 0.6205 - val_accuracy: 0.6849\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6620 - val_loss: 0.6173 - val_accuracy: 0.6913\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6291 - accuracy: 0.6487 - val_loss: 0.6166 - val_accuracy: 0.6862\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6261 - accuracy: 0.6561 - val_loss: 0.6162 - val_accuracy: 0.6837\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6616 - val_loss: 0.6171 - val_accuracy: 0.6862\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6210 - accuracy: 0.6642 - val_loss: 0.6156 - val_accuracy: 0.6913\n",
      "Calculating for: 750 150 0.6 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_336 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8099 - accuracy: 0.5481 - val_loss: 0.6704 - val_accuracy: 0.6224\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7152 - accuracy: 0.5565 - val_loss: 0.6458 - val_accuracy: 0.6505\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6853 - accuracy: 0.5642 - val_loss: 0.6404 - val_accuracy: 0.6607\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6692 - accuracy: 0.5977 - val_loss: 0.6449 - val_accuracy: 0.6620\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6692 - accuracy: 0.5883 - val_loss: 0.6400 - val_accuracy: 0.6658\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6677 - accuracy: 0.6002 - val_loss: 0.6372 - val_accuracy: 0.6671\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6624 - accuracy: 0.6058 - val_loss: 0.6390 - val_accuracy: 0.6594\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6589 - accuracy: 0.6068 - val_loss: 0.6379 - val_accuracy: 0.6633\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6580 - accuracy: 0.6135 - val_loss: 0.6357 - val_accuracy: 0.6633\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6171 - val_loss: 0.6336 - val_accuracy: 0.6607\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6548 - accuracy: 0.6124 - val_loss: 0.6354 - val_accuracy: 0.6658\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6559 - accuracy: 0.6066 - val_loss: 0.6359 - val_accuracy: 0.6722\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6542 - accuracy: 0.6140 - val_loss: 0.6365 - val_accuracy: 0.6684\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6530 - accuracy: 0.6142 - val_loss: 0.6318 - val_accuracy: 0.6633\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6238 - val_loss: 0.6324 - val_accuracy: 0.6696\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6478 - accuracy: 0.6226 - val_loss: 0.6345 - val_accuracy: 0.6620\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6505 - accuracy: 0.6270 - val_loss: 0.6297 - val_accuracy: 0.6633\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6359 - val_loss: 0.6343 - val_accuracy: 0.6633\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6474 - accuracy: 0.6338 - val_loss: 0.6270 - val_accuracy: 0.6696\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6413 - accuracy: 0.6296 - val_loss: 0.6249 - val_accuracy: 0.6735\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6334 - val_loss: 0.6301 - val_accuracy: 0.6633\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6436 - accuracy: 0.6364 - val_loss: 0.6306 - val_accuracy: 0.6607\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6444 - accuracy: 0.6356 - val_loss: 0.6314 - val_accuracy: 0.6582\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6448 - accuracy: 0.6265 - val_loss: 0.6294 - val_accuracy: 0.6658\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6396 - accuracy: 0.6392 - val_loss: 0.6269 - val_accuracy: 0.6684\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6375 - val_loss: 0.6253 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6391 - accuracy: 0.6395 - val_loss: 0.6254 - val_accuracy: 0.6696\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6409 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6368 - accuracy: 0.6420 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6334 - accuracy: 0.6461 - val_loss: 0.6229 - val_accuracy: 0.6696\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6372 - accuracy: 0.6378 - val_loss: 0.6273 - val_accuracy: 0.6709\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6334 - accuracy: 0.6462 - val_loss: 0.6261 - val_accuracy: 0.6735\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6490 - val_loss: 0.6266 - val_accuracy: 0.6722\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6330 - accuracy: 0.6448 - val_loss: 0.6291 - val_accuracy: 0.6607\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6316 - accuracy: 0.6471 - val_loss: 0.6247 - val_accuracy: 0.6747\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6315 - accuracy: 0.6441 - val_loss: 0.6266 - val_accuracy: 0.6684\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6303 - accuracy: 0.6515 - val_loss: 0.6236 - val_accuracy: 0.6684\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6303 - accuracy: 0.6521 - val_loss: 0.6244 - val_accuracy: 0.6696\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6319 - accuracy: 0.6518 - val_loss: 0.6251 - val_accuracy: 0.6696\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6289 - accuracy: 0.6530 - val_loss: 0.6307 - val_accuracy: 0.6543\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6253 - accuracy: 0.6556 - val_loss: 0.6271 - val_accuracy: 0.6696\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6264 - accuracy: 0.6591 - val_loss: 0.6245 - val_accuracy: 0.6722\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6228 - accuracy: 0.6570 - val_loss: 0.6242 - val_accuracy: 0.6747\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6265 - accuracy: 0.6599 - val_loss: 0.6244 - val_accuracy: 0.6798\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6564 - val_loss: 0.6221 - val_accuracy: 0.6811\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6591 - val_loss: 0.6281 - val_accuracy: 0.6684\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6206 - accuracy: 0.6589 - val_loss: 0.6252 - val_accuracy: 0.6735\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6207 - accuracy: 0.6630 - val_loss: 0.6223 - val_accuracy: 0.6824\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6186 - accuracy: 0.6591 - val_loss: 0.6199 - val_accuracy: 0.6760\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6202 - accuracy: 0.6614 - val_loss: 0.6239 - val_accuracy: 0.6747\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6541 - val_loss: 0.6247 - val_accuracy: 0.6709\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6659 - val_loss: 0.6223 - val_accuracy: 0.6786\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6167 - accuracy: 0.6570 - val_loss: 0.6236 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6189 - accuracy: 0.6590 - val_loss: 0.6248 - val_accuracy: 0.6747\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6178 - accuracy: 0.6665 - val_loss: 0.6236 - val_accuracy: 0.6798\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6668 - val_loss: 0.6252 - val_accuracy: 0.6722\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6674 - val_loss: 0.6209 - val_accuracy: 0.6837\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6072 - accuracy: 0.6727 - val_loss: 0.6251 - val_accuracy: 0.6709\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6667 - val_loss: 0.6310 - val_accuracy: 0.6658\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6106 - accuracy: 0.6767 - val_loss: 0.6271 - val_accuracy: 0.6684\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6108 - accuracy: 0.6688 - val_loss: 0.6238 - val_accuracy: 0.6747\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6140 - accuracy: 0.6688 - val_loss: 0.6255 - val_accuracy: 0.6671\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6151 - accuracy: 0.6659 - val_loss: 0.6229 - val_accuracy: 0.6747\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6119 - accuracy: 0.6679 - val_loss: 0.6235 - val_accuracy: 0.6735\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6156 - accuracy: 0.6665 - val_loss: 0.6277 - val_accuracy: 0.6735\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6118 - accuracy: 0.6680 - val_loss: 0.6252 - val_accuracy: 0.6696\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6096 - accuracy: 0.6721 - val_loss: 0.6264 - val_accuracy: 0.6722\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6018 - accuracy: 0.6758 - val_loss: 0.6291 - val_accuracy: 0.6684\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6041 - accuracy: 0.6783 - val_loss: 0.6283 - val_accuracy: 0.6696\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6063 - accuracy: 0.6805 - val_loss: 0.6278 - val_accuracy: 0.6696\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6042 - accuracy: 0.6726 - val_loss: 0.6247 - val_accuracy: 0.6709\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6068 - accuracy: 0.6714 - val_loss: 0.6301 - val_accuracy: 0.6620\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6085 - accuracy: 0.6742 - val_loss: 0.6293 - val_accuracy: 0.6709\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6003 - accuracy: 0.6788 - val_loss: 0.6335 - val_accuracy: 0.6671\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6029 - accuracy: 0.6863 - val_loss: 0.6278 - val_accuracy: 0.6735\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6031 - accuracy: 0.6786 - val_loss: 0.6274 - val_accuracy: 0.6849\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6017 - accuracy: 0.6786 - val_loss: 0.6316 - val_accuracy: 0.6633\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6025 - accuracy: 0.6802 - val_loss: 0.6336 - val_accuracy: 0.6696\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.5973 - accuracy: 0.6809 - val_loss: 0.6359 - val_accuracy: 0.6645\n",
      "Calculating for: 750 150 0.7 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_340 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8125 - accuracy: 0.5309 - val_loss: 0.6549 - val_accuracy: 0.6339\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7156 - accuracy: 0.5501 - val_loss: 0.6424 - val_accuracy: 0.6416\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5522 - val_loss: 0.6460 - val_accuracy: 0.6492\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6800 - accuracy: 0.5736 - val_loss: 0.6452 - val_accuracy: 0.6492\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5772 - val_loss: 0.6451 - val_accuracy: 0.6556\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6730 - accuracy: 0.5786 - val_loss: 0.6419 - val_accuracy: 0.6505\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6725 - accuracy: 0.5791 - val_loss: 0.6431 - val_accuracy: 0.6505\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6718 - accuracy: 0.5903 - val_loss: 0.6393 - val_accuracy: 0.6531\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6686 - accuracy: 0.5880 - val_loss: 0.6384 - val_accuracy: 0.6556\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6693 - accuracy: 0.5904 - val_loss: 0.6357 - val_accuracy: 0.6518\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6632 - accuracy: 0.5976 - val_loss: 0.6334 - val_accuracy: 0.6607\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6650 - accuracy: 0.5990 - val_loss: 0.6374 - val_accuracy: 0.6798\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.5996 - val_loss: 0.6337 - val_accuracy: 0.6760\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6646 - accuracy: 0.5982 - val_loss: 0.6309 - val_accuracy: 0.6760\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6634 - accuracy: 0.5997 - val_loss: 0.6332 - val_accuracy: 0.6760\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6592 - accuracy: 0.6071 - val_loss: 0.6286 - val_accuracy: 0.6773\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6039 - val_loss: 0.6303 - val_accuracy: 0.6798\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6603 - accuracy: 0.6101 - val_loss: 0.6299 - val_accuracy: 0.6760\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6567 - accuracy: 0.6181 - val_loss: 0.6255 - val_accuracy: 0.6786\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6569 - accuracy: 0.6155 - val_loss: 0.6275 - val_accuracy: 0.6773\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6112 - val_loss: 0.6264 - val_accuracy: 0.6760\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6557 - accuracy: 0.6174 - val_loss: 0.6260 - val_accuracy: 0.6837\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6098 - val_loss: 0.6280 - val_accuracy: 0.6811\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6532 - accuracy: 0.6211 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6198 - val_loss: 0.6262 - val_accuracy: 0.6747\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6197 - val_loss: 0.6264 - val_accuracy: 0.6760\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6527 - accuracy: 0.6242 - val_loss: 0.6214 - val_accuracy: 0.6786\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6541 - accuracy: 0.6237 - val_loss: 0.6267 - val_accuracy: 0.6798\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6516 - accuracy: 0.6233 - val_loss: 0.6260 - val_accuracy: 0.6786\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6551 - accuracy: 0.6179 - val_loss: 0.6253 - val_accuracy: 0.6798\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6506 - accuracy: 0.6204 - val_loss: 0.6221 - val_accuracy: 0.6747\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6497 - accuracy: 0.6226 - val_loss: 0.6212 - val_accuracy: 0.6747\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6494 - accuracy: 0.6255 - val_loss: 0.6201 - val_accuracy: 0.6811\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6464 - accuracy: 0.6348 - val_loss: 0.6212 - val_accuracy: 0.6849\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6475 - accuracy: 0.6255 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6500 - accuracy: 0.6230 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6481 - accuracy: 0.6252 - val_loss: 0.6221 - val_accuracy: 0.6824\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6438 - accuracy: 0.6301 - val_loss: 0.6228 - val_accuracy: 0.6837\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6479 - accuracy: 0.6294 - val_loss: 0.6214 - val_accuracy: 0.6837\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6289 - val_loss: 0.6204 - val_accuracy: 0.6849\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6471 - accuracy: 0.6255 - val_loss: 0.6223 - val_accuracy: 0.6849\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6445 - accuracy: 0.6320 - val_loss: 0.6216 - val_accuracy: 0.6913\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6364 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6419 - accuracy: 0.6344 - val_loss: 0.6188 - val_accuracy: 0.6862\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6374 - val_loss: 0.6158 - val_accuracy: 0.6888\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6360 - val_loss: 0.6154 - val_accuracy: 0.6888\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6433 - accuracy: 0.6349 - val_loss: 0.6143 - val_accuracy: 0.6888\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6404 - accuracy: 0.6413 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.6436 - val_loss: 0.6193 - val_accuracy: 0.6824\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6395 - accuracy: 0.6399 - val_loss: 0.6153 - val_accuracy: 0.6875\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6394 - accuracy: 0.6363 - val_loss: 0.6125 - val_accuracy: 0.6849\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6414 - accuracy: 0.6382 - val_loss: 0.6120 - val_accuracy: 0.6913\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6350 - val_loss: 0.6145 - val_accuracy: 0.6824\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6335 - val_loss: 0.6167 - val_accuracy: 0.6849\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6381 - accuracy: 0.6389 - val_loss: 0.6123 - val_accuracy: 0.6901\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6408 - accuracy: 0.6380 - val_loss: 0.6133 - val_accuracy: 0.6888\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6332 - accuracy: 0.6444 - val_loss: 0.6173 - val_accuracy: 0.6849\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6376 - accuracy: 0.6436 - val_loss: 0.6150 - val_accuracy: 0.6913\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6412 - val_loss: 0.6155 - val_accuracy: 0.6926\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6344 - accuracy: 0.6457 - val_loss: 0.6157 - val_accuracy: 0.6901\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6352 - accuracy: 0.6472 - val_loss: 0.6111 - val_accuracy: 0.6939\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6364 - accuracy: 0.6423 - val_loss: 0.6161 - val_accuracy: 0.6901\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6341 - accuracy: 0.6448 - val_loss: 0.6115 - val_accuracy: 0.6926\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6372 - accuracy: 0.6451 - val_loss: 0.6166 - val_accuracy: 0.6875\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6363 - accuracy: 0.6448 - val_loss: 0.6127 - val_accuracy: 0.6888\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6310 - accuracy: 0.6426 - val_loss: 0.6121 - val_accuracy: 0.6952\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6537 - val_loss: 0.6084 - val_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6343 - accuracy: 0.6452 - val_loss: 0.6109 - val_accuracy: 0.6926\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6339 - accuracy: 0.6393 - val_loss: 0.6136 - val_accuracy: 0.6990\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6451 - val_loss: 0.6083 - val_accuracy: 0.6939\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6254 - accuracy: 0.6555 - val_loss: 0.6080 - val_accuracy: 0.6977\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6294 - accuracy: 0.6525 - val_loss: 0.6110 - val_accuracy: 0.7015\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6300 - accuracy: 0.6485 - val_loss: 0.6112 - val_accuracy: 0.6977\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6527 - val_loss: 0.6115 - val_accuracy: 0.6990\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6279 - accuracy: 0.6517 - val_loss: 0.6138 - val_accuracy: 0.6977\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6537 - val_loss: 0.6121 - val_accuracy: 0.6964\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6539 - val_loss: 0.6112 - val_accuracy: 0.6964\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6604 - val_loss: 0.6138 - val_accuracy: 0.6990\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6274 - accuracy: 0.6551 - val_loss: 0.6122 - val_accuracy: 0.6990\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6258 - accuracy: 0.6564 - val_loss: 0.6123 - val_accuracy: 0.6926\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6224 - accuracy: 0.6511 - val_loss: 0.6114 - val_accuracy: 0.6926\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6263 - accuracy: 0.6495 - val_loss: 0.6111 - val_accuracy: 0.6926\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6255 - accuracy: 0.6561 - val_loss: 0.6145 - val_accuracy: 0.6952\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6596 - val_loss: 0.6152 - val_accuracy: 0.6964\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6230 - accuracy: 0.6584 - val_loss: 0.6126 - val_accuracy: 0.6952\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6242 - accuracy: 0.6570 - val_loss: 0.6108 - val_accuracy: 0.6888\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6567 - val_loss: 0.6105 - val_accuracy: 0.6964\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6171 - accuracy: 0.6665 - val_loss: 0.6086 - val_accuracy: 0.6939\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6259 - accuracy: 0.6570 - val_loss: 0.6115 - val_accuracy: 0.6939\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6243 - accuracy: 0.6619 - val_loss: 0.6122 - val_accuracy: 0.6913\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6220 - accuracy: 0.6598 - val_loss: 0.6112 - val_accuracy: 0.6964\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6624 - val_loss: 0.6137 - val_accuracy: 0.6913\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6190 - accuracy: 0.6624 - val_loss: 0.6146 - val_accuracy: 0.6952\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6233 - accuracy: 0.6596 - val_loss: 0.6135 - val_accuracy: 0.6926\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6173 - accuracy: 0.6649 - val_loss: 0.6159 - val_accuracy: 0.6913\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6174 - accuracy: 0.6640 - val_loss: 0.6125 - val_accuracy: 0.6964\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6197 - accuracy: 0.6684 - val_loss: 0.6158 - val_accuracy: 0.6913\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6200 - accuracy: 0.6635 - val_loss: 0.6127 - val_accuracy: 0.6926\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6195 - accuracy: 0.6693 - val_loss: 0.6142 - val_accuracy: 0.6952\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6161 - accuracy: 0.6668 - val_loss: 0.6154 - val_accuracy: 0.6926\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6169 - accuracy: 0.6689 - val_loss: 0.6173 - val_accuracy: 0.6849\n",
      "Calculating for: 750 150 0.8 sgd\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_344 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.8212 - accuracy: 0.5134 - val_loss: 0.6767 - val_accuracy: 0.6288\n",
      "Epoch 2/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7347 - accuracy: 0.5226 - val_loss: 0.6676 - val_accuracy: 0.6480\n",
      "Epoch 3/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.7010 - accuracy: 0.5398 - val_loss: 0.6620 - val_accuracy: 0.6441\n",
      "Epoch 4/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6907 - accuracy: 0.5408 - val_loss: 0.6577 - val_accuracy: 0.6441\n",
      "Epoch 5/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6867 - accuracy: 0.5534 - val_loss: 0.6565 - val_accuracy: 0.6441\n",
      "Epoch 6/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6865 - accuracy: 0.5525 - val_loss: 0.6553 - val_accuracy: 0.6429\n",
      "Epoch 7/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6844 - accuracy: 0.5561 - val_loss: 0.6573 - val_accuracy: 0.6467\n",
      "Epoch 8/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6817 - accuracy: 0.5641 - val_loss: 0.6551 - val_accuracy: 0.6454\n",
      "Epoch 9/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6834 - accuracy: 0.5580 - val_loss: 0.6574 - val_accuracy: 0.6441\n",
      "Epoch 10/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6788 - accuracy: 0.5674 - val_loss: 0.6524 - val_accuracy: 0.6480\n",
      "Epoch 11/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6809 - accuracy: 0.5623 - val_loss: 0.6532 - val_accuracy: 0.6467\n",
      "Epoch 12/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6804 - accuracy: 0.5564 - val_loss: 0.6516 - val_accuracy: 0.6492\n",
      "Epoch 13/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6780 - accuracy: 0.5766 - val_loss: 0.6500 - val_accuracy: 0.6531\n",
      "Epoch 14/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6783 - accuracy: 0.5776 - val_loss: 0.6495 - val_accuracy: 0.6492\n",
      "Epoch 15/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.5785 - val_loss: 0.6436 - val_accuracy: 0.6531\n",
      "Epoch 16/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.5833 - val_loss: 0.6409 - val_accuracy: 0.6531\n",
      "Epoch 17/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6733 - accuracy: 0.5870 - val_loss: 0.6474 - val_accuracy: 0.6569\n",
      "Epoch 18/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6726 - accuracy: 0.5838 - val_loss: 0.6423 - val_accuracy: 0.6518\n",
      "Epoch 19/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6732 - accuracy: 0.5800 - val_loss: 0.6450 - val_accuracy: 0.6620\n",
      "Epoch 20/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6713 - accuracy: 0.5898 - val_loss: 0.6465 - val_accuracy: 0.6645\n",
      "Epoch 21/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6714 - accuracy: 0.5894 - val_loss: 0.6443 - val_accuracy: 0.6645\n",
      "Epoch 22/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6698 - accuracy: 0.5904 - val_loss: 0.6443 - val_accuracy: 0.6645\n",
      "Epoch 23/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6737 - accuracy: 0.5852 - val_loss: 0.6450 - val_accuracy: 0.6684\n",
      "Epoch 24/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6693 - accuracy: 0.5873 - val_loss: 0.6404 - val_accuracy: 0.6645\n",
      "Epoch 25/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6651 - accuracy: 0.5977 - val_loss: 0.6348 - val_accuracy: 0.6658\n",
      "Epoch 26/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6683 - accuracy: 0.5944 - val_loss: 0.6352 - val_accuracy: 0.6696\n",
      "Epoch 27/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6662 - accuracy: 0.5963 - val_loss: 0.6356 - val_accuracy: 0.6671\n",
      "Epoch 28/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6666 - accuracy: 0.5967 - val_loss: 0.6393 - val_accuracy: 0.6633\n",
      "Epoch 29/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6664 - accuracy: 0.5967 - val_loss: 0.6352 - val_accuracy: 0.6735\n",
      "Epoch 30/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6637 - accuracy: 0.6069 - val_loss: 0.6374 - val_accuracy: 0.6747\n",
      "Epoch 31/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.5976 - val_loss: 0.6386 - val_accuracy: 0.6760\n",
      "Epoch 32/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6627 - accuracy: 0.6016 - val_loss: 0.6369 - val_accuracy: 0.6747\n",
      "Epoch 33/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6637 - accuracy: 0.6025 - val_loss: 0.6345 - val_accuracy: 0.6722\n",
      "Epoch 34/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6643 - accuracy: 0.5966 - val_loss: 0.6358 - val_accuracy: 0.6709\n",
      "Epoch 35/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6626 - accuracy: 0.6063 - val_loss: 0.6333 - val_accuracy: 0.6684\n",
      "Epoch 36/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6661 - accuracy: 0.5939 - val_loss: 0.6335 - val_accuracy: 0.6709\n",
      "Epoch 37/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6633 - accuracy: 0.6052 - val_loss: 0.6342 - val_accuracy: 0.6735\n",
      "Epoch 38/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6602 - accuracy: 0.6110 - val_loss: 0.6359 - val_accuracy: 0.6735\n",
      "Epoch 39/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6607 - accuracy: 0.6098 - val_loss: 0.6308 - val_accuracy: 0.6811\n",
      "Epoch 40/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6600 - accuracy: 0.6061 - val_loss: 0.6329 - val_accuracy: 0.6773\n",
      "Epoch 41/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6621 - accuracy: 0.6050 - val_loss: 0.6391 - val_accuracy: 0.6735\n",
      "Epoch 42/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6565 - accuracy: 0.6155 - val_loss: 0.6364 - val_accuracy: 0.6824\n",
      "Epoch 43/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6093 - val_loss: 0.6343 - val_accuracy: 0.6786\n",
      "Epoch 44/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6036 - val_loss: 0.6345 - val_accuracy: 0.6798\n",
      "Epoch 45/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6162 - val_loss: 0.6295 - val_accuracy: 0.6773\n",
      "Epoch 46/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6583 - accuracy: 0.6174 - val_loss: 0.6337 - val_accuracy: 0.6773\n",
      "Epoch 47/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6604 - accuracy: 0.6079 - val_loss: 0.6385 - val_accuracy: 0.6760\n",
      "Epoch 48/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6088 - val_loss: 0.6417 - val_accuracy: 0.6645\n",
      "Epoch 49/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6163 - val_loss: 0.6357 - val_accuracy: 0.6786\n",
      "Epoch 50/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6134 - val_loss: 0.6329 - val_accuracy: 0.6786\n",
      "Epoch 51/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6611 - accuracy: 0.6101 - val_loss: 0.6297 - val_accuracy: 0.6837\n",
      "Epoch 52/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6145 - val_loss: 0.6286 - val_accuracy: 0.6811\n",
      "Epoch 53/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6585 - accuracy: 0.6130 - val_loss: 0.6342 - val_accuracy: 0.6798\n",
      "Epoch 54/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6551 - accuracy: 0.6183 - val_loss: 0.6279 - val_accuracy: 0.6875\n",
      "Epoch 55/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6564 - accuracy: 0.6193 - val_loss: 0.6302 - val_accuracy: 0.6773\n",
      "Epoch 56/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6228 - val_loss: 0.6312 - val_accuracy: 0.6811\n",
      "Epoch 57/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6547 - accuracy: 0.6238 - val_loss: 0.6305 - val_accuracy: 0.6824\n",
      "Epoch 58/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6550 - accuracy: 0.6227 - val_loss: 0.6325 - val_accuracy: 0.6811\n",
      "Epoch 59/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6517 - accuracy: 0.6226 - val_loss: 0.6312 - val_accuracy: 0.6786\n",
      "Epoch 60/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6545 - accuracy: 0.6189 - val_loss: 0.6337 - val_accuracy: 0.6811\n",
      "Epoch 61/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6538 - accuracy: 0.6237 - val_loss: 0.6326 - val_accuracy: 0.6786\n",
      "Epoch 62/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6535 - accuracy: 0.6218 - val_loss: 0.6286 - val_accuracy: 0.6837\n",
      "Epoch 63/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6543 - accuracy: 0.6150 - val_loss: 0.6293 - val_accuracy: 0.6798\n",
      "Epoch 64/200\n",
      "249/249 [==============================] - 1s 5ms/step - loss: 0.6520 - accuracy: 0.6212 - val_loss: 0.6313 - val_accuracy: 0.6798\n",
      "Epoch 65/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6536 - accuracy: 0.6212 - val_loss: 0.6299 - val_accuracy: 0.6824\n",
      "Epoch 66/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6529 - accuracy: 0.6178 - val_loss: 0.6242 - val_accuracy: 0.6849\n",
      "Epoch 67/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6519 - accuracy: 0.6286 - val_loss: 0.6276 - val_accuracy: 0.6824\n",
      "Epoch 68/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6238 - val_loss: 0.6245 - val_accuracy: 0.6849\n",
      "Epoch 69/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6520 - accuracy: 0.6264 - val_loss: 0.6260 - val_accuracy: 0.6862\n",
      "Epoch 70/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6528 - accuracy: 0.6237 - val_loss: 0.6262 - val_accuracy: 0.6862\n",
      "Epoch 71/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6488 - accuracy: 0.6247 - val_loss: 0.6279 - val_accuracy: 0.6875\n",
      "Epoch 72/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6537 - accuracy: 0.6168 - val_loss: 0.6237 - val_accuracy: 0.6824\n",
      "Epoch 73/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6507 - accuracy: 0.6161 - val_loss: 0.6264 - val_accuracy: 0.6811\n",
      "Epoch 74/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.6247 - val_loss: 0.6274 - val_accuracy: 0.6875\n",
      "Epoch 75/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6517 - accuracy: 0.6242 - val_loss: 0.6241 - val_accuracy: 0.6888\n",
      "Epoch 76/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6510 - accuracy: 0.6186 - val_loss: 0.6258 - val_accuracy: 0.6862\n",
      "Epoch 77/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6267 - val_loss: 0.6215 - val_accuracy: 0.6875\n",
      "Epoch 78/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6501 - accuracy: 0.6325 - val_loss: 0.6213 - val_accuracy: 0.6888\n",
      "Epoch 79/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6291 - val_loss: 0.6228 - val_accuracy: 0.6849\n",
      "Epoch 80/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6484 - accuracy: 0.6304 - val_loss: 0.6251 - val_accuracy: 0.6837\n",
      "Epoch 81/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6466 - accuracy: 0.6318 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 82/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6451 - accuracy: 0.6366 - val_loss: 0.6276 - val_accuracy: 0.6811\n",
      "Epoch 83/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6458 - accuracy: 0.6304 - val_loss: 0.6226 - val_accuracy: 0.6862\n",
      "Epoch 84/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6477 - accuracy: 0.6338 - val_loss: 0.6217 - val_accuracy: 0.6837\n",
      "Epoch 85/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6453 - accuracy: 0.6350 - val_loss: 0.6209 - val_accuracy: 0.6811\n",
      "Epoch 86/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6425 - accuracy: 0.6390 - val_loss: 0.6192 - val_accuracy: 0.6824\n",
      "Epoch 87/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6440 - accuracy: 0.6326 - val_loss: 0.6181 - val_accuracy: 0.6939\n",
      "Epoch 88/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6468 - accuracy: 0.6316 - val_loss: 0.6276 - val_accuracy: 0.6798\n",
      "Epoch 89/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6488 - accuracy: 0.6311 - val_loss: 0.6234 - val_accuracy: 0.6849\n",
      "Epoch 90/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6447 - accuracy: 0.6311 - val_loss: 0.6159 - val_accuracy: 0.6837\n",
      "Epoch 91/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6447 - accuracy: 0.6359 - val_loss: 0.6199 - val_accuracy: 0.6849\n",
      "Epoch 92/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6438 - accuracy: 0.6366 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 93/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6487 - accuracy: 0.6266 - val_loss: 0.6248 - val_accuracy: 0.6760\n",
      "Epoch 94/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6473 - accuracy: 0.6356 - val_loss: 0.6252 - val_accuracy: 0.6824\n",
      "Epoch 95/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6280 - val_loss: 0.6215 - val_accuracy: 0.6773\n",
      "Epoch 96/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6460 - accuracy: 0.6331 - val_loss: 0.6216 - val_accuracy: 0.6849\n",
      "Epoch 97/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6446 - accuracy: 0.6341 - val_loss: 0.6212 - val_accuracy: 0.6798\n",
      "Epoch 98/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6424 - accuracy: 0.6394 - val_loss: 0.6218 - val_accuracy: 0.6837\n",
      "Epoch 99/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6412 - accuracy: 0.6439 - val_loss: 0.6221 - val_accuracy: 0.6798\n",
      "Epoch 100/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6338 - val_loss: 0.6220 - val_accuracy: 0.6811\n",
      "Epoch 101/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6427 - accuracy: 0.6323 - val_loss: 0.6230 - val_accuracy: 0.6760\n",
      "Epoch 102/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6439 - accuracy: 0.6358 - val_loss: 0.6227 - val_accuracy: 0.6862\n",
      "Epoch 103/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6401 - accuracy: 0.6338 - val_loss: 0.6216 - val_accuracy: 0.6849\n",
      "Epoch 104/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6407 - accuracy: 0.6461 - val_loss: 0.6208 - val_accuracy: 0.6837\n",
      "Epoch 105/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6441 - accuracy: 0.6364 - val_loss: 0.6160 - val_accuracy: 0.6837\n",
      "Epoch 106/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6404 - accuracy: 0.6382 - val_loss: 0.6187 - val_accuracy: 0.6837\n",
      "Epoch 107/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6391 - accuracy: 0.6420 - val_loss: 0.6222 - val_accuracy: 0.6849\n",
      "Epoch 108/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6429 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 109/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6435 - accuracy: 0.6346 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 110/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6359 - accuracy: 0.6426 - val_loss: 0.6193 - val_accuracy: 0.6849\n",
      "Epoch 111/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6429 - accuracy: 0.6372 - val_loss: 0.6175 - val_accuracy: 0.6888\n",
      "Epoch 112/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6399 - accuracy: 0.6402 - val_loss: 0.6206 - val_accuracy: 0.6824\n",
      "Epoch 113/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6382 - accuracy: 0.6431 - val_loss: 0.6164 - val_accuracy: 0.6875\n",
      "Epoch 114/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6423 - val_loss: 0.6193 - val_accuracy: 0.6786\n",
      "Epoch 115/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6413 - accuracy: 0.6344 - val_loss: 0.6175 - val_accuracy: 0.6798\n",
      "Epoch 116/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6459 - val_loss: 0.6139 - val_accuracy: 0.6888\n",
      "Epoch 117/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6385 - accuracy: 0.6382 - val_loss: 0.6171 - val_accuracy: 0.6901\n",
      "Epoch 118/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6417 - val_loss: 0.6176 - val_accuracy: 0.6901\n",
      "Epoch 119/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6397 - accuracy: 0.6350 - val_loss: 0.6148 - val_accuracy: 0.6888\n",
      "Epoch 120/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6392 - accuracy: 0.6447 - val_loss: 0.6179 - val_accuracy: 0.6811\n",
      "Epoch 121/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6355 - accuracy: 0.6457 - val_loss: 0.6153 - val_accuracy: 0.6862\n",
      "Epoch 122/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6357 - accuracy: 0.6447 - val_loss: 0.6199 - val_accuracy: 0.6786\n",
      "Epoch 123/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6357 - accuracy: 0.6477 - val_loss: 0.6151 - val_accuracy: 0.6875\n",
      "Epoch 124/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6333 - accuracy: 0.6492 - val_loss: 0.6110 - val_accuracy: 0.6926\n",
      "Epoch 125/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6362 - accuracy: 0.6458 - val_loss: 0.6191 - val_accuracy: 0.6811\n",
      "Epoch 126/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6348 - accuracy: 0.6530 - val_loss: 0.6171 - val_accuracy: 0.6849\n",
      "Epoch 127/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6350 - accuracy: 0.6462 - val_loss: 0.6167 - val_accuracy: 0.6875\n",
      "Epoch 128/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6337 - accuracy: 0.6564 - val_loss: 0.6181 - val_accuracy: 0.6862\n",
      "Epoch 129/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6342 - accuracy: 0.6500 - val_loss: 0.6165 - val_accuracy: 0.6837\n",
      "Epoch 130/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6356 - accuracy: 0.6444 - val_loss: 0.6181 - val_accuracy: 0.6811\n",
      "Epoch 131/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6322 - accuracy: 0.6542 - val_loss: 0.6122 - val_accuracy: 0.6901\n",
      "Epoch 132/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6381 - accuracy: 0.6443 - val_loss: 0.6146 - val_accuracy: 0.6913\n",
      "Epoch 133/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6482 - val_loss: 0.6141 - val_accuracy: 0.6913\n",
      "Epoch 134/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6325 - accuracy: 0.6478 - val_loss: 0.6141 - val_accuracy: 0.6862\n",
      "Epoch 135/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6372 - accuracy: 0.6510 - val_loss: 0.6199 - val_accuracy: 0.6837\n",
      "Epoch 136/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6302 - accuracy: 0.6483 - val_loss: 0.6178 - val_accuracy: 0.6913\n",
      "Epoch 137/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.6491 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 138/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6340 - accuracy: 0.6467 - val_loss: 0.6105 - val_accuracy: 0.6888\n",
      "Epoch 139/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6333 - accuracy: 0.6458 - val_loss: 0.6136 - val_accuracy: 0.6888\n",
      "Epoch 140/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6315 - accuracy: 0.6477 - val_loss: 0.6124 - val_accuracy: 0.6926\n",
      "Epoch 141/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6485 - val_loss: 0.6145 - val_accuracy: 0.6888\n",
      "Epoch 142/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6318 - accuracy: 0.6464 - val_loss: 0.6124 - val_accuracy: 0.6939\n",
      "Epoch 143/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6324 - accuracy: 0.6527 - val_loss: 0.6187 - val_accuracy: 0.6888\n",
      "Epoch 144/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6545 - val_loss: 0.6164 - val_accuracy: 0.6849\n",
      "Epoch 145/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6336 - accuracy: 0.6503 - val_loss: 0.6156 - val_accuracy: 0.6901\n",
      "Epoch 146/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6273 - accuracy: 0.6497 - val_loss: 0.6167 - val_accuracy: 0.6875\n",
      "Epoch 147/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6298 - accuracy: 0.6594 - val_loss: 0.6106 - val_accuracy: 0.6939\n",
      "Epoch 148/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6330 - accuracy: 0.6496 - val_loss: 0.6199 - val_accuracy: 0.6875\n",
      "Epoch 149/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6590 - val_loss: 0.6159 - val_accuracy: 0.6913\n",
      "Epoch 150/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6309 - accuracy: 0.6508 - val_loss: 0.6121 - val_accuracy: 0.6926\n",
      "Epoch 151/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6354 - accuracy: 0.6502 - val_loss: 0.6139 - val_accuracy: 0.6952\n",
      "Epoch 152/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6328 - accuracy: 0.6545 - val_loss: 0.6131 - val_accuracy: 0.6939\n",
      "Epoch 153/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6289 - accuracy: 0.6574 - val_loss: 0.6208 - val_accuracy: 0.6824\n",
      "Epoch 154/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6244 - accuracy: 0.6608 - val_loss: 0.6113 - val_accuracy: 0.6849\n",
      "Epoch 155/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6541 - val_loss: 0.6101 - val_accuracy: 0.6875\n",
      "Epoch 156/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6272 - accuracy: 0.6542 - val_loss: 0.6097 - val_accuracy: 0.6875\n",
      "Epoch 157/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6265 - accuracy: 0.6540 - val_loss: 0.6118 - val_accuracy: 0.6888\n",
      "Epoch 158/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6256 - accuracy: 0.6634 - val_loss: 0.6114 - val_accuracy: 0.6875\n",
      "Epoch 159/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6282 - accuracy: 0.6536 - val_loss: 0.6136 - val_accuracy: 0.6849\n",
      "Epoch 160/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6290 - accuracy: 0.6472 - val_loss: 0.6152 - val_accuracy: 0.6875\n",
      "Epoch 161/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6288 - accuracy: 0.6532 - val_loss: 0.6138 - val_accuracy: 0.6901\n",
      "Epoch 162/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6267 - accuracy: 0.6546 - val_loss: 0.6110 - val_accuracy: 0.6875\n",
      "Epoch 163/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6271 - accuracy: 0.6542 - val_loss: 0.6136 - val_accuracy: 0.6811\n",
      "Epoch 164/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6268 - accuracy: 0.6541 - val_loss: 0.6109 - val_accuracy: 0.6901\n",
      "Epoch 165/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6281 - accuracy: 0.6517 - val_loss: 0.6114 - val_accuracy: 0.6952\n",
      "Epoch 166/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6222 - accuracy: 0.6609 - val_loss: 0.6140 - val_accuracy: 0.6888\n",
      "Epoch 167/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6230 - accuracy: 0.6593 - val_loss: 0.6068 - val_accuracy: 0.6939\n",
      "Epoch 168/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6270 - accuracy: 0.6561 - val_loss: 0.6124 - val_accuracy: 0.6862\n",
      "Epoch 169/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6278 - accuracy: 0.6569 - val_loss: 0.6134 - val_accuracy: 0.6849\n",
      "Epoch 170/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6250 - accuracy: 0.6579 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Epoch 171/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6262 - accuracy: 0.6565 - val_loss: 0.6159 - val_accuracy: 0.6862\n",
      "Epoch 172/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6256 - accuracy: 0.6701 - val_loss: 0.6126 - val_accuracy: 0.6849\n",
      "Epoch 173/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6271 - accuracy: 0.6570 - val_loss: 0.6138 - val_accuracy: 0.6862\n",
      "Epoch 174/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6589 - val_loss: 0.6122 - val_accuracy: 0.6875\n",
      "Epoch 175/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6249 - accuracy: 0.6586 - val_loss: 0.6151 - val_accuracy: 0.6862\n",
      "Epoch 176/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6232 - accuracy: 0.6670 - val_loss: 0.6144 - val_accuracy: 0.6837\n",
      "Epoch 177/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6251 - accuracy: 0.6569 - val_loss: 0.6128 - val_accuracy: 0.6901\n",
      "Epoch 178/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6624 - val_loss: 0.6181 - val_accuracy: 0.6837\n",
      "Epoch 179/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6645 - val_loss: 0.6133 - val_accuracy: 0.6849\n",
      "Epoch 180/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6196 - accuracy: 0.6638 - val_loss: 0.6105 - val_accuracy: 0.6849\n",
      "Epoch 181/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6218 - accuracy: 0.6590 - val_loss: 0.6098 - val_accuracy: 0.6862\n",
      "Epoch 182/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6211 - accuracy: 0.6653 - val_loss: 0.6121 - val_accuracy: 0.6888\n",
      "Epoch 183/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6205 - accuracy: 0.6628 - val_loss: 0.6113 - val_accuracy: 0.6875\n",
      "Epoch 184/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6216 - accuracy: 0.6644 - val_loss: 0.6142 - val_accuracy: 0.6798\n",
      "Epoch 185/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6234 - accuracy: 0.6618 - val_loss: 0.6138 - val_accuracy: 0.6798\n",
      "Epoch 186/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6187 - accuracy: 0.6639 - val_loss: 0.6110 - val_accuracy: 0.6875\n",
      "Epoch 187/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6214 - accuracy: 0.6591 - val_loss: 0.6157 - val_accuracy: 0.6786\n",
      "Epoch 188/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6238 - accuracy: 0.6619 - val_loss: 0.6130 - val_accuracy: 0.6786\n",
      "Epoch 189/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6187 - accuracy: 0.6697 - val_loss: 0.6135 - val_accuracy: 0.6798\n",
      "Epoch 190/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6197 - accuracy: 0.6644 - val_loss: 0.6134 - val_accuracy: 0.6849\n",
      "Epoch 191/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6193 - accuracy: 0.6624 - val_loss: 0.6136 - val_accuracy: 0.6824\n",
      "Epoch 192/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6199 - accuracy: 0.6703 - val_loss: 0.6101 - val_accuracy: 0.6862\n",
      "Epoch 193/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6203 - accuracy: 0.6599 - val_loss: 0.6089 - val_accuracy: 0.6837\n",
      "Epoch 194/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6200 - accuracy: 0.6694 - val_loss: 0.6127 - val_accuracy: 0.6837\n",
      "Epoch 195/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6689 - val_loss: 0.6092 - val_accuracy: 0.6824\n",
      "Epoch 196/200\n",
      "249/249 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6701 - val_loss: 0.6090 - val_accuracy: 0.6862\n",
      "Epoch 197/200\n",
      "249/249 [==============================] - 1s 4ms/step - loss: 0.6188 - accuracy: 0.6649 - val_loss: 0.6129 - val_accuracy: 0.6824\n"
     ]
    }
   ],
   "source": [
    "dense1 = [650, 700, 750]\n",
    "dense2 = [50, 100, 150]\n",
    "dropout = [0.6, 0.7, 0.8]\n",
    "\n",
    "\n",
    "for d1 in dense1:\n",
    "    for d2 in dense2:\n",
    "        for dr in dropout:\n",
    "            print('Calculating for:', d1, d2, dr, opt)\n",
    "            keras_model = init_keras_model(dense1=d1, dense2=d2, dropout=dr, optimizer=opt)\n",
    "            keras_model.fit(X_train_tensor, \n",
    "                            y_train_tensor, \n",
    "                            epochs=n_epochs, \n",
    "                            validation_data=(X_val_tensor, y_val_tensor),\n",
    "                            callbacks=[es] \n",
    "                            )\n",
    "            score = keras_model.evaluate(X_test_tensor, y_test_tensor, verbose=0)[1]\n",
    "            scores[(d1, d2, dr, opt)] = score\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'dense1':d1, 'dense2':d2, 'dropout':dr, 'optimizer':opt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params: {'dense1': 750, 'dense2': 50, 'dropout': 0.8, 'optimizer': 'sgd'}, best_score: 0.6883780360221863\n"
     ]
    }
   ],
   "source": [
    "print(f'best_params: {best_params}, best_score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.8071 - accuracy: 0.5182 - val_loss: 0.6550 - val_accuracy: 0.6684\n",
      "Epoch 2/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.7157 - accuracy: 0.5388 - val_loss: 0.6495 - val_accuracy: 0.6684\n",
      "Epoch 3/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6957 - accuracy: 0.5455 - val_loss: 0.6400 - val_accuracy: 0.6645\n",
      "Epoch 4/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6896 - accuracy: 0.5511 - val_loss: 0.6469 - val_accuracy: 0.6569\n",
      "Epoch 5/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6845 - accuracy: 0.5589 - val_loss: 0.6518 - val_accuracy: 0.6620\n",
      "Epoch 6/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6844 - accuracy: 0.5573 - val_loss: 0.6514 - val_accuracy: 0.6773\n",
      "Epoch 7/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6823 - accuracy: 0.5668 - val_loss: 0.6473 - val_accuracy: 0.6747\n",
      "Epoch 8/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6802 - accuracy: 0.5673 - val_loss: 0.6396 - val_accuracy: 0.6684\n",
      "Epoch 9/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6820 - accuracy: 0.5682 - val_loss: 0.6428 - val_accuracy: 0.6696\n",
      "Epoch 10/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6800 - accuracy: 0.5665 - val_loss: 0.6429 - val_accuracy: 0.6849\n",
      "Epoch 11/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6796 - accuracy: 0.5692 - val_loss: 0.6411 - val_accuracy: 0.6824\n",
      "Epoch 12/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6736 - accuracy: 0.5856 - val_loss: 0.6352 - val_accuracy: 0.6773\n",
      "Epoch 13/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6769 - accuracy: 0.5783 - val_loss: 0.6319 - val_accuracy: 0.6837\n",
      "Epoch 14/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6727 - accuracy: 0.5850 - val_loss: 0.6286 - val_accuracy: 0.6862\n",
      "Epoch 15/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6748 - accuracy: 0.5855 - val_loss: 0.6308 - val_accuracy: 0.6709\n",
      "Epoch 16/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6721 - accuracy: 0.5841 - val_loss: 0.6308 - val_accuracy: 0.6849\n",
      "Epoch 17/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6722 - accuracy: 0.5888 - val_loss: 0.6296 - val_accuracy: 0.6837\n",
      "Epoch 18/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6705 - accuracy: 0.5925 - val_loss: 0.6382 - val_accuracy: 0.6811\n",
      "Epoch 19/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6705 - accuracy: 0.6025 - val_loss: 0.6286 - val_accuracy: 0.6875\n",
      "Epoch 20/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6697 - accuracy: 0.5903 - val_loss: 0.6216 - val_accuracy: 0.6862\n",
      "Epoch 21/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6664 - accuracy: 0.6049 - val_loss: 0.6229 - val_accuracy: 0.6837\n",
      "Epoch 22/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6676 - accuracy: 0.5976 - val_loss: 0.6263 - val_accuracy: 0.6837\n",
      "Epoch 23/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6648 - accuracy: 0.6073 - val_loss: 0.6290 - val_accuracy: 0.6824\n",
      "Epoch 24/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6648 - accuracy: 0.6032 - val_loss: 0.6261 - val_accuracy: 0.6824\n",
      "Epoch 25/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6660 - accuracy: 0.6012 - val_loss: 0.6203 - val_accuracy: 0.6862\n",
      "Epoch 26/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6650 - accuracy: 0.6085 - val_loss: 0.6260 - val_accuracy: 0.6901\n",
      "Epoch 27/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6676 - accuracy: 0.5960 - val_loss: 0.6264 - val_accuracy: 0.6901\n",
      "Epoch 28/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6677 - accuracy: 0.5963 - val_loss: 0.6248 - val_accuracy: 0.6875\n",
      "Epoch 29/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6624 - accuracy: 0.6119 - val_loss: 0.6235 - val_accuracy: 0.6875\n",
      "Epoch 30/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6636 - accuracy: 0.6044 - val_loss: 0.6208 - val_accuracy: 0.6901\n",
      "Epoch 31/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6644 - accuracy: 0.6019 - val_loss: 0.6238 - val_accuracy: 0.6888\n",
      "Epoch 32/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6623 - accuracy: 0.6071 - val_loss: 0.6250 - val_accuracy: 0.6837\n",
      "Epoch 33/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6602 - accuracy: 0.6124 - val_loss: 0.6404 - val_accuracy: 0.6811\n",
      "Epoch 34/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6608 - accuracy: 0.6085 - val_loss: 0.6207 - val_accuracy: 0.6875\n",
      "Epoch 35/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6601 - accuracy: 0.6136 - val_loss: 0.6222 - val_accuracy: 0.6901\n",
      "Epoch 36/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6631 - accuracy: 0.6060 - val_loss: 0.6248 - val_accuracy: 0.6862\n",
      "Epoch 37/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6616 - accuracy: 0.6122 - val_loss: 0.6292 - val_accuracy: 0.6760\n",
      "Epoch 38/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6590 - accuracy: 0.6148 - val_loss: 0.6160 - val_accuracy: 0.6913\n",
      "Epoch 39/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6609 - accuracy: 0.6108 - val_loss: 0.6175 - val_accuracy: 0.6875\n",
      "Epoch 40/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6579 - accuracy: 0.6162 - val_loss: 0.6159 - val_accuracy: 0.6888\n",
      "Epoch 41/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6596 - accuracy: 0.6150 - val_loss: 0.6252 - val_accuracy: 0.6786\n",
      "Epoch 42/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6612 - accuracy: 0.6112 - val_loss: 0.6186 - val_accuracy: 0.6875\n",
      "Epoch 43/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6581 - accuracy: 0.6169 - val_loss: 0.6200 - val_accuracy: 0.6977\n",
      "Epoch 44/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6583 - accuracy: 0.6113 - val_loss: 0.6179 - val_accuracy: 0.6926\n",
      "Epoch 45/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6549 - accuracy: 0.6213 - val_loss: 0.6259 - val_accuracy: 0.6811\n",
      "Epoch 46/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6565 - accuracy: 0.6170 - val_loss: 0.6231 - val_accuracy: 0.6849\n",
      "Epoch 47/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6586 - accuracy: 0.6219 - val_loss: 0.6263 - val_accuracy: 0.6760\n",
      "Epoch 48/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6601 - accuracy: 0.6174 - val_loss: 0.6237 - val_accuracy: 0.6837\n",
      "Epoch 49/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6542 - accuracy: 0.6229 - val_loss: 0.6130 - val_accuracy: 0.6964\n",
      "Epoch 50/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6585 - accuracy: 0.6137 - val_loss: 0.6166 - val_accuracy: 0.6913\n",
      "Epoch 51/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6578 - accuracy: 0.6189 - val_loss: 0.6117 - val_accuracy: 0.7066\n",
      "Epoch 52/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6573 - accuracy: 0.6179 - val_loss: 0.6169 - val_accuracy: 0.6990\n",
      "Epoch 53/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6574 - accuracy: 0.6179 - val_loss: 0.6179 - val_accuracy: 0.6977\n",
      "Epoch 54/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6577 - accuracy: 0.6217 - val_loss: 0.6188 - val_accuracy: 0.7028\n",
      "Epoch 55/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6575 - accuracy: 0.6212 - val_loss: 0.6125 - val_accuracy: 0.6977\n",
      "Epoch 56/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6561 - accuracy: 0.6145 - val_loss: 0.6176 - val_accuracy: 0.6926\n",
      "Epoch 57/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6562 - accuracy: 0.6175 - val_loss: 0.6114 - val_accuracy: 0.6990\n",
      "Epoch 58/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6529 - accuracy: 0.6213 - val_loss: 0.6168 - val_accuracy: 0.6939\n",
      "Epoch 59/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6553 - accuracy: 0.6185 - val_loss: 0.6182 - val_accuracy: 0.7003\n",
      "Epoch 60/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6575 - accuracy: 0.6185 - val_loss: 0.6269 - val_accuracy: 0.6824\n",
      "Epoch 61/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6555 - accuracy: 0.6275 - val_loss: 0.6166 - val_accuracy: 0.6990\n",
      "Epoch 62/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6205 - val_loss: 0.6283 - val_accuracy: 0.6824\n",
      "Epoch 63/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6526 - accuracy: 0.6222 - val_loss: 0.6178 - val_accuracy: 0.6913\n",
      "Epoch 64/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6510 - accuracy: 0.6241 - val_loss: 0.6121 - val_accuracy: 0.6913\n",
      "Epoch 65/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6531 - accuracy: 0.6274 - val_loss: 0.6126 - val_accuracy: 0.6901\n",
      "Epoch 66/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6517 - accuracy: 0.6203 - val_loss: 0.6178 - val_accuracy: 0.6837\n",
      "Epoch 67/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6537 - accuracy: 0.6208 - val_loss: 0.6089 - val_accuracy: 0.6964\n",
      "Epoch 68/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6266 - val_loss: 0.6177 - val_accuracy: 0.6939\n",
      "Epoch 69/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6564 - accuracy: 0.6191 - val_loss: 0.6144 - val_accuracy: 0.6964\n",
      "Epoch 70/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6527 - accuracy: 0.6205 - val_loss: 0.6087 - val_accuracy: 0.7015\n",
      "Epoch 71/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6539 - accuracy: 0.6238 - val_loss: 0.6218 - val_accuracy: 0.6964\n",
      "Epoch 72/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6512 - accuracy: 0.6295 - val_loss: 0.6074 - val_accuracy: 0.7041\n",
      "Epoch 73/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6500 - accuracy: 0.6276 - val_loss: 0.6097 - val_accuracy: 0.7003\n",
      "Epoch 74/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6288 - val_loss: 0.6122 - val_accuracy: 0.6977\n",
      "Epoch 75/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6504 - accuracy: 0.6253 - val_loss: 0.6134 - val_accuracy: 0.6964\n",
      "Epoch 76/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6523 - accuracy: 0.6289 - val_loss: 0.6147 - val_accuracy: 0.7028\n",
      "Epoch 77/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.6253 - val_loss: 0.6160 - val_accuracy: 0.6926\n",
      "Epoch 78/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6495 - accuracy: 0.6231 - val_loss: 0.6085 - val_accuracy: 0.7054\n",
      "Epoch 79/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6520 - accuracy: 0.6281 - val_loss: 0.6160 - val_accuracy: 0.6964\n",
      "Epoch 80/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6505 - accuracy: 0.6267 - val_loss: 0.6095 - val_accuracy: 0.7028\n",
      "Epoch 81/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6486 - accuracy: 0.6262 - val_loss: 0.6068 - val_accuracy: 0.7041\n",
      "Epoch 82/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6475 - accuracy: 0.6356 - val_loss: 0.6080 - val_accuracy: 0.7028\n",
      "Epoch 83/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6501 - accuracy: 0.6296 - val_loss: 0.6084 - val_accuracy: 0.7015\n",
      "Epoch 84/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6479 - accuracy: 0.6293 - val_loss: 0.6160 - val_accuracy: 0.7003\n",
      "Epoch 85/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6516 - accuracy: 0.6278 - val_loss: 0.6223 - val_accuracy: 0.6875\n",
      "Epoch 86/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6489 - accuracy: 0.6355 - val_loss: 0.6109 - val_accuracy: 0.7041\n",
      "Epoch 87/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6489 - accuracy: 0.6278 - val_loss: 0.6008 - val_accuracy: 0.7092\n",
      "Epoch 88/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6466 - accuracy: 0.6360 - val_loss: 0.6119 - val_accuracy: 0.7041\n",
      "Epoch 89/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6495 - accuracy: 0.6270 - val_loss: 0.6157 - val_accuracy: 0.6990\n",
      "Epoch 90/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6480 - accuracy: 0.6291 - val_loss: 0.6032 - val_accuracy: 0.7054\n",
      "Epoch 91/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6493 - accuracy: 0.6303 - val_loss: 0.6044 - val_accuracy: 0.7028\n",
      "Epoch 92/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6459 - accuracy: 0.6347 - val_loss: 0.6125 - val_accuracy: 0.7041\n",
      "Epoch 93/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6508 - accuracy: 0.6270 - val_loss: 0.6111 - val_accuracy: 0.7003\n",
      "Epoch 94/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6482 - accuracy: 0.6285 - val_loss: 0.6157 - val_accuracy: 0.7054\n",
      "Epoch 95/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6487 - accuracy: 0.6294 - val_loss: 0.6166 - val_accuracy: 0.7015\n",
      "Epoch 96/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6461 - accuracy: 0.6283 - val_loss: 0.6099 - val_accuracy: 0.7054\n",
      "Epoch 97/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6450 - accuracy: 0.6309 - val_loss: 0.6119 - val_accuracy: 0.7054\n",
      "Epoch 98/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6442 - accuracy: 0.6398 - val_loss: 0.6098 - val_accuracy: 0.7003\n",
      "Epoch 99/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6451 - accuracy: 0.6305 - val_loss: 0.6084 - val_accuracy: 0.7079\n",
      "Epoch 100/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6446 - accuracy: 0.6347 - val_loss: 0.6064 - val_accuracy: 0.7066\n",
      "Epoch 101/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6478 - accuracy: 0.6317 - val_loss: 0.6143 - val_accuracy: 0.7015\n",
      "Epoch 102/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6456 - accuracy: 0.6347 - val_loss: 0.6183 - val_accuracy: 0.7028\n",
      "Epoch 103/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6434 - accuracy: 0.6351 - val_loss: 0.6088 - val_accuracy: 0.7066\n",
      "Epoch 104/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6448 - accuracy: 0.6321 - val_loss: 0.6113 - val_accuracy: 0.7003\n",
      "Epoch 105/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6438 - accuracy: 0.6369 - val_loss: 0.6054 - val_accuracy: 0.7054\n",
      "Epoch 106/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6471 - accuracy: 0.6272 - val_loss: 0.6102 - val_accuracy: 0.7028\n",
      "Epoch 107/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6462 - accuracy: 0.6307 - val_loss: 0.6051 - val_accuracy: 0.7066\n",
      "Epoch 108/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6444 - accuracy: 0.6328 - val_loss: 0.6073 - val_accuracy: 0.6977\n",
      "Epoch 109/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6437 - accuracy: 0.6289 - val_loss: 0.6056 - val_accuracy: 0.7015\n",
      "Epoch 110/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6466 - accuracy: 0.6364 - val_loss: 0.6088 - val_accuracy: 0.6977\n",
      "Epoch 111/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6443 - accuracy: 0.6364 - val_loss: 0.6070 - val_accuracy: 0.7003\n",
      "Epoch 112/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6444 - accuracy: 0.6343 - val_loss: 0.6090 - val_accuracy: 0.6990\n",
      "Epoch 113/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6436 - accuracy: 0.6357 - val_loss: 0.6034 - val_accuracy: 0.7041\n",
      "Epoch 114/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6415 - accuracy: 0.6405 - val_loss: 0.6111 - val_accuracy: 0.6875\n",
      "Epoch 115/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.6372 - val_loss: 0.6092 - val_accuracy: 0.6977\n",
      "Epoch 116/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6419 - accuracy: 0.6404 - val_loss: 0.6052 - val_accuracy: 0.7041\n",
      "Epoch 117/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6454 - accuracy: 0.6342 - val_loss: 0.6056 - val_accuracy: 0.7003\n",
      "Epoch 118/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6417 - accuracy: 0.6353 - val_loss: 0.6044 - val_accuracy: 0.7003\n",
      "Epoch 119/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6434 - accuracy: 0.6347 - val_loss: 0.6000 - val_accuracy: 0.7066\n",
      "Epoch 120/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6446 - accuracy: 0.6364 - val_loss: 0.6021 - val_accuracy: 0.7054\n",
      "Epoch 121/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6436 - accuracy: 0.6396 - val_loss: 0.6076 - val_accuracy: 0.7066\n",
      "Epoch 122/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6455 - accuracy: 0.6345 - val_loss: 0.6080 - val_accuracy: 0.7003\n",
      "Epoch 123/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6423 - accuracy: 0.6366 - val_loss: 0.6037 - val_accuracy: 0.7015\n",
      "Epoch 124/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6420 - accuracy: 0.6408 - val_loss: 0.6068 - val_accuracy: 0.7041\n",
      "Epoch 125/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6438 - accuracy: 0.6336 - val_loss: 0.6049 - val_accuracy: 0.7028\n",
      "Epoch 126/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6385 - accuracy: 0.6415 - val_loss: 0.6111 - val_accuracy: 0.7003\n",
      "Epoch 127/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6456 - accuracy: 0.6342 - val_loss: 0.6063 - val_accuracy: 0.6952\n",
      "Epoch 128/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6432 - accuracy: 0.6377 - val_loss: 0.6071 - val_accuracy: 0.7054\n",
      "Epoch 129/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6435 - accuracy: 0.6275 - val_loss: 0.6048 - val_accuracy: 0.7054\n",
      "Epoch 130/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.6438 - val_loss: 0.6004 - val_accuracy: 0.7079\n",
      "Epoch 131/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6421 - accuracy: 0.6386 - val_loss: 0.6075 - val_accuracy: 0.6977\n",
      "Epoch 132/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6406 - accuracy: 0.6434 - val_loss: 0.6113 - val_accuracy: 0.6952\n",
      "Epoch 133/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6389 - accuracy: 0.6437 - val_loss: 0.6067 - val_accuracy: 0.6964\n",
      "Epoch 134/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6388 - accuracy: 0.6418 - val_loss: 0.6088 - val_accuracy: 0.6990\n",
      "Epoch 135/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6421 - accuracy: 0.6396 - val_loss: 0.6046 - val_accuracy: 0.7003\n",
      "Epoch 136/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.6452 - val_loss: 0.6089 - val_accuracy: 0.6952\n",
      "Epoch 137/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6388 - accuracy: 0.6427 - val_loss: 0.6040 - val_accuracy: 0.7054\n",
      "Epoch 138/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6421 - accuracy: 0.6367 - val_loss: 0.5977 - val_accuracy: 0.7041\n",
      "Epoch 139/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6378 - accuracy: 0.6436 - val_loss: 0.6041 - val_accuracy: 0.7003\n",
      "Epoch 140/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6409 - accuracy: 0.6410 - val_loss: 0.5972 - val_accuracy: 0.7117\n",
      "Epoch 141/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6380 - accuracy: 0.6485 - val_loss: 0.6104 - val_accuracy: 0.7015\n",
      "Epoch 142/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6431 - accuracy: 0.6319 - val_loss: 0.6127 - val_accuracy: 0.6926\n",
      "Epoch 143/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6399 - accuracy: 0.6405 - val_loss: 0.6061 - val_accuracy: 0.6990\n",
      "Epoch 144/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.6415 - val_loss: 0.5930 - val_accuracy: 0.7054\n",
      "Epoch 145/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6396 - accuracy: 0.6432 - val_loss: 0.5995 - val_accuracy: 0.7066\n",
      "Epoch 146/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6409 - accuracy: 0.6377 - val_loss: 0.6002 - val_accuracy: 0.7054\n",
      "Epoch 147/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6364 - accuracy: 0.6436 - val_loss: 0.5995 - val_accuracy: 0.6964\n",
      "Epoch 148/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6405 - accuracy: 0.6448 - val_loss: 0.6034 - val_accuracy: 0.6952\n",
      "Epoch 149/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6399 - accuracy: 0.6390 - val_loss: 0.6030 - val_accuracy: 0.6990\n",
      "Epoch 150/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6390 - accuracy: 0.6415 - val_loss: 0.6014 - val_accuracy: 0.7003\n",
      "Epoch 151/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6379 - accuracy: 0.6436 - val_loss: 0.6096 - val_accuracy: 0.6926\n",
      "Epoch 152/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6381 - accuracy: 0.6410 - val_loss: 0.6048 - val_accuracy: 0.6939\n",
      "Epoch 153/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6373 - accuracy: 0.6476 - val_loss: 0.6020 - val_accuracy: 0.7028\n",
      "Epoch 154/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6370 - accuracy: 0.6423 - val_loss: 0.6016 - val_accuracy: 0.7028\n",
      "Epoch 155/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6376 - accuracy: 0.6485 - val_loss: 0.6042 - val_accuracy: 0.6926\n",
      "Epoch 156/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6381 - accuracy: 0.6410 - val_loss: 0.6021 - val_accuracy: 0.7054\n",
      "Epoch 157/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6372 - accuracy: 0.6453 - val_loss: 0.6020 - val_accuracy: 0.7117\n",
      "Epoch 158/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6378 - accuracy: 0.6458 - val_loss: 0.6035 - val_accuracy: 0.6964\n",
      "Epoch 159/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6363 - accuracy: 0.6432 - val_loss: 0.6058 - val_accuracy: 0.7092\n",
      "Epoch 160/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6335 - accuracy: 0.6484 - val_loss: 0.5969 - val_accuracy: 0.7041\n",
      "Epoch 161/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6361 - accuracy: 0.6474 - val_loss: 0.5967 - val_accuracy: 0.7092\n",
      "Epoch 162/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6351 - accuracy: 0.6462 - val_loss: 0.6005 - val_accuracy: 0.7041\n",
      "Epoch 163/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6329 - accuracy: 0.6552 - val_loss: 0.5963 - val_accuracy: 0.7092\n",
      "Epoch 164/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6337 - accuracy: 0.6506 - val_loss: 0.5997 - val_accuracy: 0.7066\n",
      "Epoch 165/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6348 - accuracy: 0.6517 - val_loss: 0.5990 - val_accuracy: 0.7079\n",
      "Epoch 166/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6303 - accuracy: 0.6537 - val_loss: 0.5943 - val_accuracy: 0.7054\n",
      "Epoch 167/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6340 - accuracy: 0.6432 - val_loss: 0.6080 - val_accuracy: 0.6913\n",
      "Epoch 168/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6352 - accuracy: 0.6484 - val_loss: 0.5983 - val_accuracy: 0.7028\n",
      "Epoch 169/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6359 - accuracy: 0.6443 - val_loss: 0.5991 - val_accuracy: 0.7041\n",
      "Epoch 170/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6354 - accuracy: 0.6463 - val_loss: 0.6123 - val_accuracy: 0.6901\n",
      "Epoch 171/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6340 - accuracy: 0.6528 - val_loss: 0.6026 - val_accuracy: 0.6901\n",
      "Epoch 172/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6397 - accuracy: 0.6408 - val_loss: 0.6030 - val_accuracy: 0.7054\n",
      "Epoch 173/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6380 - accuracy: 0.6441 - val_loss: 0.5931 - val_accuracy: 0.6977\n",
      "Epoch 174/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6377 - accuracy: 0.6458 - val_loss: 0.6009 - val_accuracy: 0.6926\n",
      "Epoch 175/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6357 - accuracy: 0.6475 - val_loss: 0.5955 - val_accuracy: 0.7015\n",
      "Epoch 176/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6327 - accuracy: 0.6496 - val_loss: 0.6005 - val_accuracy: 0.7003\n",
      "Epoch 177/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6328 - accuracy: 0.6456 - val_loss: 0.6036 - val_accuracy: 0.7003\n",
      "Epoch 178/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6357 - accuracy: 0.6417 - val_loss: 0.6018 - val_accuracy: 0.6939\n",
      "Epoch 179/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6322 - accuracy: 0.6490 - val_loss: 0.5977 - val_accuracy: 0.6926\n",
      "Epoch 180/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6358 - accuracy: 0.6442 - val_loss: 0.5985 - val_accuracy: 0.6939\n",
      "Epoch 181/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6329 - accuracy: 0.6518 - val_loss: 0.6483 - val_accuracy: 0.6224\n",
      "Epoch 182/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6358 - accuracy: 0.6447 - val_loss: 0.5990 - val_accuracy: 0.6952\n",
      "Epoch 183/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6321 - accuracy: 0.6509 - val_loss: 0.5985 - val_accuracy: 0.6939\n",
      "Epoch 184/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6358 - accuracy: 0.6458 - val_loss: 0.5996 - val_accuracy: 0.7028\n",
      "Epoch 185/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6363 - accuracy: 0.6474 - val_loss: 0.6018 - val_accuracy: 0.7028\n",
      "Epoch 186/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6323 - accuracy: 0.6499 - val_loss: 0.5954 - val_accuracy: 0.7105\n",
      "Epoch 187/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6355 - accuracy: 0.6472 - val_loss: 0.6015 - val_accuracy: 0.7092\n",
      "Epoch 188/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6290 - accuracy: 0.6523 - val_loss: 0.6008 - val_accuracy: 0.6964\n",
      "Epoch 189/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6358 - accuracy: 0.6453 - val_loss: 0.6023 - val_accuracy: 0.6875\n",
      "Epoch 190/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6341 - accuracy: 0.6496 - val_loss: 0.6059 - val_accuracy: 0.6849\n",
      "Epoch 191/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6339 - accuracy: 0.6420 - val_loss: 0.5974 - val_accuracy: 0.7041\n",
      "Epoch 192/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6325 - accuracy: 0.6461 - val_loss: 0.5974 - val_accuracy: 0.7066\n",
      "Epoch 193/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6316 - accuracy: 0.6532 - val_loss: 0.6006 - val_accuracy: 0.7015\n",
      "Epoch 194/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6323 - accuracy: 0.6505 - val_loss: 0.5950 - val_accuracy: 0.7105\n",
      "Epoch 195/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6354 - accuracy: 0.6429 - val_loss: 0.5932 - val_accuracy: 0.7168\n",
      "Epoch 196/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6327 - accuracy: 0.6485 - val_loss: 0.6025 - val_accuracy: 0.6964\n",
      "Epoch 197/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6311 - accuracy: 0.6562 - val_loss: 0.6036 - val_accuracy: 0.7015\n",
      "Epoch 198/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6307 - accuracy: 0.6510 - val_loss: 0.5966 - val_accuracy: 0.7079\n",
      "Epoch 199/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6300 - accuracy: 0.6529 - val_loss: 0.5915 - val_accuracy: 0.7079\n",
      "Epoch 200/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6328 - accuracy: 0.6505 - val_loss: 0.5943 - val_accuracy: 0.7117\n",
      "Epoch 201/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6308 - accuracy: 0.6548 - val_loss: 0.5956 - val_accuracy: 0.7168\n",
      "Epoch 202/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6292 - accuracy: 0.6558 - val_loss: 0.5918 - val_accuracy: 0.7054\n",
      "Epoch 203/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6335 - accuracy: 0.6494 - val_loss: 0.5903 - val_accuracy: 0.7130\n",
      "Epoch 204/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6301 - accuracy: 0.6528 - val_loss: 0.5955 - val_accuracy: 0.6939\n",
      "Epoch 205/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6341 - accuracy: 0.6524 - val_loss: 0.5973 - val_accuracy: 0.6977\n",
      "Epoch 206/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6311 - accuracy: 0.6548 - val_loss: 0.5898 - val_accuracy: 0.7041\n",
      "Epoch 207/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6293 - accuracy: 0.6532 - val_loss: 0.6063 - val_accuracy: 0.7054\n",
      "Epoch 208/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6307 - accuracy: 0.6499 - val_loss: 0.5948 - val_accuracy: 0.7054\n",
      "Epoch 209/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6300 - accuracy: 0.6558 - val_loss: 0.5950 - val_accuracy: 0.7079\n",
      "Epoch 210/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6295 - accuracy: 0.6523 - val_loss: 0.5970 - val_accuracy: 0.7028\n",
      "Epoch 211/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6282 - accuracy: 0.6534 - val_loss: 0.5935 - val_accuracy: 0.7054\n",
      "Epoch 212/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6290 - accuracy: 0.6505 - val_loss: 0.5971 - val_accuracy: 0.7054\n",
      "Epoch 213/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6264 - accuracy: 0.6581 - val_loss: 0.6054 - val_accuracy: 0.6926\n",
      "Epoch 214/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6298 - accuracy: 0.6522 - val_loss: 0.5972 - val_accuracy: 0.7003\n",
      "Epoch 215/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6301 - accuracy: 0.6557 - val_loss: 0.6086 - val_accuracy: 0.6888\n",
      "Epoch 216/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6323 - accuracy: 0.6532 - val_loss: 0.5941 - val_accuracy: 0.7041\n",
      "Epoch 217/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6282 - accuracy: 0.6541 - val_loss: 0.5911 - val_accuracy: 0.7066\n",
      "Epoch 218/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6282 - accuracy: 0.6571 - val_loss: 0.5955 - val_accuracy: 0.7003\n",
      "Epoch 219/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6311 - accuracy: 0.6479 - val_loss: 0.6024 - val_accuracy: 0.6964\n",
      "Epoch 220/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6297 - accuracy: 0.6560 - val_loss: 0.5974 - val_accuracy: 0.7015\n",
      "Epoch 221/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6258 - accuracy: 0.6586 - val_loss: 0.5935 - val_accuracy: 0.6939\n",
      "Epoch 222/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6327 - accuracy: 0.6551 - val_loss: 0.5980 - val_accuracy: 0.6939\n",
      "Epoch 223/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6289 - accuracy: 0.6563 - val_loss: 0.5965 - val_accuracy: 0.7143\n",
      "Epoch 224/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.6551 - val_loss: 0.6012 - val_accuracy: 0.6977\n",
      "Epoch 225/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 0.6563 - val_loss: 0.5962 - val_accuracy: 0.7092\n",
      "Epoch 226/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6260 - accuracy: 0.6581 - val_loss: 0.5983 - val_accuracy: 0.7092\n",
      "Epoch 227/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6264 - accuracy: 0.6556 - val_loss: 0.5967 - val_accuracy: 0.7041\n",
      "Epoch 228/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6231 - accuracy: 0.6599 - val_loss: 0.5982 - val_accuracy: 0.6964\n",
      "Epoch 229/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6241 - accuracy: 0.6599 - val_loss: 0.5988 - val_accuracy: 0.7028\n",
      "Epoch 230/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6295 - accuracy: 0.6587 - val_loss: 0.5908 - val_accuracy: 0.6990\n",
      "Epoch 231/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6277 - accuracy: 0.6573 - val_loss: 0.5963 - val_accuracy: 0.6939\n",
      "Epoch 232/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6288 - accuracy: 0.6517 - val_loss: 0.5956 - val_accuracy: 0.6977\n",
      "Epoch 233/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6286 - accuracy: 0.6538 - val_loss: 0.5978 - val_accuracy: 0.6939\n",
      "Epoch 234/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6277 - accuracy: 0.6577 - val_loss: 0.5934 - val_accuracy: 0.6952\n",
      "Epoch 235/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6232 - accuracy: 0.6567 - val_loss: 0.5956 - val_accuracy: 0.6926\n",
      "Epoch 236/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6294 - accuracy: 0.6506 - val_loss: 0.6010 - val_accuracy: 0.6990\n",
      "Epoch 237/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6325 - accuracy: 0.6524 - val_loss: 0.6030 - val_accuracy: 0.6990\n",
      "Epoch 238/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6246 - accuracy: 0.6577 - val_loss: 0.5900 - val_accuracy: 0.7079\n",
      "Epoch 239/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6244 - accuracy: 0.6589 - val_loss: 0.5940 - val_accuracy: 0.7066\n",
      "Epoch 240/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 0.6615 - val_loss: 0.5947 - val_accuracy: 0.6952\n",
      "Epoch 241/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6241 - accuracy: 0.6616 - val_loss: 0.5951 - val_accuracy: 0.6977\n",
      "Epoch 242/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6228 - accuracy: 0.6598 - val_loss: 0.5940 - val_accuracy: 0.7028\n",
      "Epoch 243/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6248 - accuracy: 0.6616 - val_loss: 0.5984 - val_accuracy: 0.6990\n",
      "Epoch 244/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6233 - accuracy: 0.6577 - val_loss: 0.5979 - val_accuracy: 0.7003\n",
      "Epoch 245/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6172 - accuracy: 0.6649 - val_loss: 0.5929 - val_accuracy: 0.6977\n",
      "Epoch 246/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6240 - accuracy: 0.6601 - val_loss: 0.5964 - val_accuracy: 0.6990\n",
      "Epoch 247/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6228 - accuracy: 0.6614 - val_loss: 0.5937 - val_accuracy: 0.7015\n",
      "Epoch 248/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6208 - accuracy: 0.6662 - val_loss: 0.5977 - val_accuracy: 0.6990\n",
      "Epoch 249/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6225 - accuracy: 0.6611 - val_loss: 0.5956 - val_accuracy: 0.7003\n",
      "Epoch 250/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6195 - accuracy: 0.6638 - val_loss: 0.5979 - val_accuracy: 0.6913\n",
      "Epoch 251/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6246 - accuracy: 0.6553 - val_loss: 0.5987 - val_accuracy: 0.6964\n",
      "Epoch 252/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6246 - accuracy: 0.6610 - val_loss: 0.5983 - val_accuracy: 0.6939\n",
      "Epoch 253/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6204 - accuracy: 0.6627 - val_loss: 0.5961 - val_accuracy: 0.6964\n",
      "Epoch 254/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6210 - accuracy: 0.6614 - val_loss: 0.5907 - val_accuracy: 0.7041\n",
      "Epoch 255/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6216 - accuracy: 0.6624 - val_loss: 0.5919 - val_accuracy: 0.7015\n",
      "Epoch 256/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6213 - accuracy: 0.6587 - val_loss: 0.5960 - val_accuracy: 0.7041\n",
      "Epoch 257/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6209 - accuracy: 0.6643 - val_loss: 0.5978 - val_accuracy: 0.6990\n",
      "Epoch 258/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6223 - accuracy: 0.6594 - val_loss: 0.5923 - val_accuracy: 0.6977\n",
      "Epoch 259/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6205 - accuracy: 0.6599 - val_loss: 0.5875 - val_accuracy: 0.7028\n",
      "Epoch 260/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6241 - accuracy: 0.6581 - val_loss: 0.5902 - val_accuracy: 0.7041\n",
      "Epoch 261/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6210 - accuracy: 0.6639 - val_loss: 0.5925 - val_accuracy: 0.7054\n",
      "Epoch 262/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.6603 - val_loss: 0.5953 - val_accuracy: 0.7066\n",
      "Epoch 263/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6202 - accuracy: 0.6633 - val_loss: 0.5955 - val_accuracy: 0.7079\n",
      "Epoch 264/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.6627 - val_loss: 0.5960 - val_accuracy: 0.7079\n",
      "Epoch 265/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6172 - accuracy: 0.6582 - val_loss: 0.5962 - val_accuracy: 0.7028\n",
      "Epoch 266/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6192 - accuracy: 0.6657 - val_loss: 0.5883 - val_accuracy: 0.7041\n",
      "Epoch 267/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6198 - accuracy: 0.6665 - val_loss: 0.5978 - val_accuracy: 0.7015\n",
      "Epoch 268/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6197 - accuracy: 0.6589 - val_loss: 0.5921 - val_accuracy: 0.7117\n",
      "Epoch 269/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6250 - accuracy: 0.6591 - val_loss: 0.5935 - val_accuracy: 0.7092\n",
      "Epoch 270/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6215 - accuracy: 0.6618 - val_loss: 0.5944 - val_accuracy: 0.7092\n",
      "Epoch 271/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6236 - accuracy: 0.6644 - val_loss: 0.5854 - val_accuracy: 0.7079\n",
      "Epoch 272/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6210 - accuracy: 0.6642 - val_loss: 0.5993 - val_accuracy: 0.7003\n",
      "Epoch 273/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6181 - accuracy: 0.6642 - val_loss: 0.5931 - val_accuracy: 0.7079\n",
      "Epoch 274/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6218 - accuracy: 0.6616 - val_loss: 0.5929 - val_accuracy: 0.7117\n",
      "Epoch 275/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6216 - accuracy: 0.6629 - val_loss: 0.5963 - val_accuracy: 0.7079\n",
      "Epoch 276/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6183 - accuracy: 0.6673 - val_loss: 0.5940 - val_accuracy: 0.7066\n",
      "Epoch 277/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6202 - accuracy: 0.6633 - val_loss: 0.5927 - val_accuracy: 0.7028\n",
      "Epoch 278/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6214 - accuracy: 0.6595 - val_loss: 0.5979 - val_accuracy: 0.7028\n",
      "Epoch 279/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6182 - accuracy: 0.6653 - val_loss: 0.5962 - val_accuracy: 0.7041\n",
      "Epoch 280/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6186 - accuracy: 0.6620 - val_loss: 0.5877 - val_accuracy: 0.7079\n",
      "Epoch 281/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6246 - accuracy: 0.6585 - val_loss: 0.5911 - val_accuracy: 0.7028\n",
      "Epoch 282/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6194 - accuracy: 0.6616 - val_loss: 0.5819 - val_accuracy: 0.7156\n",
      "Epoch 283/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6226 - accuracy: 0.6637 - val_loss: 0.5976 - val_accuracy: 0.6952\n",
      "Epoch 284/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6188 - accuracy: 0.6620 - val_loss: 0.5949 - val_accuracy: 0.6977\n",
      "Epoch 285/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6206 - accuracy: 0.6684 - val_loss: 0.5930 - val_accuracy: 0.7003\n",
      "Epoch 286/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6155 - accuracy: 0.6672 - val_loss: 0.5938 - val_accuracy: 0.7028\n",
      "Epoch 287/500\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 0.6201 - accuracy: 0.6646 - val_loss: 0.6001 - val_accuracy: 0.7003\n",
      "Epoch 288/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6180 - accuracy: 0.6624 - val_loss: 0.5878 - val_accuracy: 0.7041\n",
      "Epoch 289/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6160 - accuracy: 0.6630 - val_loss: 0.5889 - val_accuracy: 0.7003\n",
      "Epoch 290/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6221 - accuracy: 0.6654 - val_loss: 0.5927 - val_accuracy: 0.6990\n",
      "Epoch 291/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6266 - accuracy: 0.6571 - val_loss: 0.6015 - val_accuracy: 0.7015\n",
      "Epoch 292/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6160 - accuracy: 0.6725 - val_loss: 0.5917 - val_accuracy: 0.7092\n",
      "Epoch 293/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6201 - accuracy: 0.6634 - val_loss: 0.5905 - val_accuracy: 0.7054\n",
      "Epoch 294/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6228 - accuracy: 0.6653 - val_loss: 0.5947 - val_accuracy: 0.7015\n",
      "Epoch 295/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6158 - accuracy: 0.6694 - val_loss: 0.5901 - val_accuracy: 0.7066\n",
      "Epoch 296/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6163 - accuracy: 0.6654 - val_loss: 0.5917 - val_accuracy: 0.7003\n",
      "Epoch 297/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6150 - accuracy: 0.6746 - val_loss: 0.5873 - val_accuracy: 0.6990\n",
      "Epoch 298/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6150 - accuracy: 0.6678 - val_loss: 0.5917 - val_accuracy: 0.7041\n",
      "Epoch 299/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6132 - accuracy: 0.6692 - val_loss: 0.5907 - val_accuracy: 0.7003\n",
      "Epoch 300/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6153 - accuracy: 0.6603 - val_loss: 0.5877 - val_accuracy: 0.7015\n",
      "Epoch 301/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6161 - accuracy: 0.6681 - val_loss: 0.5907 - val_accuracy: 0.7130\n",
      "Epoch 302/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6180 - accuracy: 0.6651 - val_loss: 0.5903 - val_accuracy: 0.7066\n",
      "Epoch 303/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6182 - accuracy: 0.6663 - val_loss: 0.5956 - val_accuracy: 0.7117\n",
      "Epoch 304/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6179 - accuracy: 0.6661 - val_loss: 0.5989 - val_accuracy: 0.6977\n",
      "Epoch 305/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6202 - accuracy: 0.6603 - val_loss: 0.5986 - val_accuracy: 0.7041\n",
      "Epoch 306/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6116 - accuracy: 0.6687 - val_loss: 0.5893 - val_accuracy: 0.6926\n",
      "Epoch 307/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6212 - accuracy: 0.6582 - val_loss: 0.6032 - val_accuracy: 0.6977\n",
      "Epoch 308/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6187 - accuracy: 0.6641 - val_loss: 0.5967 - val_accuracy: 0.7079\n",
      "Epoch 309/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6131 - accuracy: 0.6729 - val_loss: 0.6307 - val_accuracy: 0.6760\n",
      "Epoch 310/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6159 - accuracy: 0.6705 - val_loss: 0.5965 - val_accuracy: 0.6990\n",
      "Epoch 311/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.6663 - val_loss: 0.5944 - val_accuracy: 0.7028\n",
      "Epoch 312/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6148 - accuracy: 0.6687 - val_loss: 0.5917 - val_accuracy: 0.7156\n",
      "Epoch 313/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6692 - val_loss: 0.5978 - val_accuracy: 0.7028\n",
      "Epoch 314/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6164 - accuracy: 0.6675 - val_loss: 0.5933 - val_accuracy: 0.7003\n",
      "Epoch 315/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6124 - accuracy: 0.6692 - val_loss: 0.6018 - val_accuracy: 0.6977\n",
      "Epoch 316/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6131 - accuracy: 0.6696 - val_loss: 0.6005 - val_accuracy: 0.7041\n",
      "Epoch 317/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6095 - accuracy: 0.6705 - val_loss: 0.5880 - val_accuracy: 0.7130\n",
      "Epoch 318/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6118 - accuracy: 0.6714 - val_loss: 0.5903 - val_accuracy: 0.7041\n",
      "Epoch 319/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6164 - accuracy: 0.6678 - val_loss: 0.5979 - val_accuracy: 0.7079\n",
      "Epoch 320/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.6666 - val_loss: 0.5923 - val_accuracy: 0.7092\n",
      "Epoch 321/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6166 - accuracy: 0.6672 - val_loss: 0.5881 - val_accuracy: 0.7143\n",
      "Epoch 322/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6144 - accuracy: 0.6704 - val_loss: 0.5929 - val_accuracy: 0.7079\n",
      "Epoch 323/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6142 - accuracy: 0.6680 - val_loss: 0.5930 - val_accuracy: 0.7066\n",
      "Epoch 324/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6183 - accuracy: 0.6695 - val_loss: 0.5930 - val_accuracy: 0.7079\n",
      "Epoch 325/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6133 - accuracy: 0.6737 - val_loss: 0.5924 - val_accuracy: 0.7003\n",
      "Epoch 326/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6149 - accuracy: 0.6668 - val_loss: 0.5999 - val_accuracy: 0.6990\n",
      "Epoch 327/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6121 - accuracy: 0.6764 - val_loss: 0.5998 - val_accuracy: 0.6952\n",
      "Epoch 328/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6191 - accuracy: 0.6687 - val_loss: 0.5923 - val_accuracy: 0.7028\n",
      "Epoch 329/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6149 - accuracy: 0.6696 - val_loss: 0.5904 - val_accuracy: 0.7054\n",
      "Epoch 330/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6090 - accuracy: 0.6815 - val_loss: 0.5945 - val_accuracy: 0.7003\n",
      "Epoch 331/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6145 - accuracy: 0.6706 - val_loss: 0.5890 - val_accuracy: 0.7054\n",
      "Epoch 332/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.6733 - val_loss: 0.5929 - val_accuracy: 0.7028\n",
      "Epoch 333/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6174 - accuracy: 0.6718 - val_loss: 0.5965 - val_accuracy: 0.6977\n",
      "Epoch 334/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6128 - accuracy: 0.6732 - val_loss: 0.5864 - val_accuracy: 0.7117\n",
      "Epoch 335/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6080 - accuracy: 0.6759 - val_loss: 0.5858 - val_accuracy: 0.7143\n",
      "Epoch 336/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6158 - accuracy: 0.6716 - val_loss: 0.5921 - val_accuracy: 0.7054\n",
      "Epoch 337/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6121 - accuracy: 0.6654 - val_loss: 0.5978 - val_accuracy: 0.6952\n",
      "Epoch 338/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6144 - accuracy: 0.6758 - val_loss: 0.5985 - val_accuracy: 0.7079\n",
      "Epoch 339/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6142 - accuracy: 0.6730 - val_loss: 0.5897 - val_accuracy: 0.7079\n",
      "Epoch 340/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6705 - val_loss: 0.5922 - val_accuracy: 0.7117\n",
      "Epoch 341/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6144 - accuracy: 0.6700 - val_loss: 0.5952 - val_accuracy: 0.7105\n",
      "Epoch 342/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6134 - accuracy: 0.6734 - val_loss: 0.5879 - val_accuracy: 0.7066\n",
      "Epoch 343/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6137 - accuracy: 0.6670 - val_loss: 0.5967 - val_accuracy: 0.6977\n",
      "Epoch 344/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6141 - accuracy: 0.6795 - val_loss: 0.5877 - val_accuracy: 0.7054\n",
      "Epoch 345/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6132 - accuracy: 0.6716 - val_loss: 0.5961 - val_accuracy: 0.7003\n",
      "Epoch 346/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6102 - accuracy: 0.6753 - val_loss: 0.5875 - val_accuracy: 0.7092\n",
      "Epoch 347/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6150 - accuracy: 0.6676 - val_loss: 0.5936 - val_accuracy: 0.7105\n",
      "Epoch 348/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6114 - accuracy: 0.6724 - val_loss: 0.5908 - val_accuracy: 0.7054\n",
      "Epoch 349/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6108 - accuracy: 0.6732 - val_loss: 0.5939 - val_accuracy: 0.7015\n",
      "Epoch 350/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6116 - accuracy: 0.6730 - val_loss: 0.5975 - val_accuracy: 0.6964\n",
      "Epoch 351/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6076 - accuracy: 0.6771 - val_loss: 0.5908 - val_accuracy: 0.6990\n",
      "Epoch 352/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6091 - accuracy: 0.6767 - val_loss: 0.5899 - val_accuracy: 0.7079\n",
      "Epoch 353/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6101 - accuracy: 0.6734 - val_loss: 0.5958 - val_accuracy: 0.7003\n",
      "Epoch 354/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6136 - accuracy: 0.6686 - val_loss: 0.5944 - val_accuracy: 0.7015\n",
      "Epoch 355/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6095 - accuracy: 0.6684 - val_loss: 0.5925 - val_accuracy: 0.7015\n",
      "Epoch 356/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6069 - accuracy: 0.6789 - val_loss: 0.5954 - val_accuracy: 0.7041\n",
      "Epoch 357/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6067 - accuracy: 0.6852 - val_loss: 0.5933 - val_accuracy: 0.7015\n",
      "Epoch 358/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6118 - accuracy: 0.6725 - val_loss: 0.5966 - val_accuracy: 0.7105\n",
      "Epoch 359/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6127 - accuracy: 0.6734 - val_loss: 0.5954 - val_accuracy: 0.7028\n",
      "Epoch 360/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6116 - accuracy: 0.6730 - val_loss: 0.5973 - val_accuracy: 0.7028\n",
      "Epoch 361/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6120 - accuracy: 0.6757 - val_loss: 0.5967 - val_accuracy: 0.6977\n",
      "Epoch 362/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6093 - accuracy: 0.6724 - val_loss: 0.5938 - val_accuracy: 0.7015\n",
      "Epoch 363/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6062 - accuracy: 0.6810 - val_loss: 0.6027 - val_accuracy: 0.7003\n",
      "Epoch 364/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6091 - accuracy: 0.6795 - val_loss: 0.6003 - val_accuracy: 0.7015\n",
      "Epoch 365/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6020 - accuracy: 0.6856 - val_loss: 0.5932 - val_accuracy: 0.7028\n",
      "Epoch 366/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6047 - accuracy: 0.6806 - val_loss: 0.6014 - val_accuracy: 0.6952\n",
      "Epoch 367/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6052 - accuracy: 0.6835 - val_loss: 0.5903 - val_accuracy: 0.7028\n",
      "Epoch 368/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6090 - accuracy: 0.6753 - val_loss: 0.5894 - val_accuracy: 0.7041\n",
      "Epoch 369/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6029 - accuracy: 0.6799 - val_loss: 0.5900 - val_accuracy: 0.7015\n",
      "Epoch 370/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6081 - accuracy: 0.6799 - val_loss: 0.5900 - val_accuracy: 0.6990\n",
      "Epoch 371/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6082 - accuracy: 0.6777 - val_loss: 0.5892 - val_accuracy: 0.7028\n",
      "Epoch 372/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6095 - accuracy: 0.6806 - val_loss: 0.5900 - val_accuracy: 0.7079\n",
      "Epoch 373/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6079 - accuracy: 0.6735 - val_loss: 0.5947 - val_accuracy: 0.7003\n",
      "Epoch 374/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6065 - accuracy: 0.6850 - val_loss: 0.5928 - val_accuracy: 0.7041\n",
      "Epoch 375/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6071 - accuracy: 0.6758 - val_loss: 0.5963 - val_accuracy: 0.6964\n",
      "Epoch 376/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6115 - accuracy: 0.6744 - val_loss: 0.5969 - val_accuracy: 0.7041\n",
      "Epoch 377/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6085 - accuracy: 0.6758 - val_loss: 0.5980 - val_accuracy: 0.7015\n",
      "Epoch 378/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6015 - accuracy: 0.6861 - val_loss: 0.5948 - val_accuracy: 0.6977\n",
      "Epoch 379/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6072 - accuracy: 0.6824 - val_loss: 0.5925 - val_accuracy: 0.7054\n",
      "Epoch 380/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6039 - accuracy: 0.6792 - val_loss: 0.5940 - val_accuracy: 0.6990\n",
      "Epoch 381/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6103 - accuracy: 0.6742 - val_loss: 0.5983 - val_accuracy: 0.6977\n",
      "Epoch 382/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6115 - accuracy: 0.6701 - val_loss: 0.5897 - val_accuracy: 0.7003\n",
      "Epoch 383/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6073 - accuracy: 0.6792 - val_loss: 0.5911 - val_accuracy: 0.7092\n",
      "Epoch 384/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6036 - accuracy: 0.6834 - val_loss: 0.5921 - val_accuracy: 0.7092\n",
      "Epoch 385/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6127 - accuracy: 0.6730 - val_loss: 0.5997 - val_accuracy: 0.7015\n",
      "Epoch 386/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6021 - accuracy: 0.6849 - val_loss: 0.5901 - val_accuracy: 0.7117\n",
      "Epoch 387/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6062 - accuracy: 0.6801 - val_loss: 0.5974 - val_accuracy: 0.7079\n",
      "Epoch 388/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6067 - accuracy: 0.6789 - val_loss: 0.5879 - val_accuracy: 0.7143\n",
      "Epoch 389/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6036 - accuracy: 0.6852 - val_loss: 0.5933 - val_accuracy: 0.7079\n",
      "Epoch 390/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6066 - accuracy: 0.6762 - val_loss: 0.5966 - val_accuracy: 0.7066\n",
      "Epoch 391/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6078 - accuracy: 0.6752 - val_loss: 0.5957 - val_accuracy: 0.7041\n",
      "Epoch 392/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6037 - accuracy: 0.6802 - val_loss: 0.5968 - val_accuracy: 0.7092\n",
      "Epoch 393/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6058 - accuracy: 0.6800 - val_loss: 0.5923 - val_accuracy: 0.7066\n",
      "Epoch 394/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6045 - accuracy: 0.6809 - val_loss: 0.5953 - val_accuracy: 0.6952\n",
      "Epoch 395/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6046 - accuracy: 0.6800 - val_loss: 0.5948 - val_accuracy: 0.7079\n",
      "Epoch 396/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6055 - accuracy: 0.6781 - val_loss: 0.6103 - val_accuracy: 0.6798\n",
      "Epoch 397/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6062 - accuracy: 0.6810 - val_loss: 0.6035 - val_accuracy: 0.6990\n",
      "Epoch 398/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6028 - accuracy: 0.6858 - val_loss: 0.5929 - val_accuracy: 0.7117\n",
      "Epoch 399/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6037 - accuracy: 0.6778 - val_loss: 0.5939 - val_accuracy: 0.7117\n",
      "Epoch 400/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6062 - accuracy: 0.6778 - val_loss: 0.5917 - val_accuracy: 0.7092\n",
      "Epoch 401/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6111 - accuracy: 0.6754 - val_loss: 0.5928 - val_accuracy: 0.7105\n",
      "Epoch 402/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6001 - accuracy: 0.6844 - val_loss: 0.5908 - val_accuracy: 0.7130\n",
      "Epoch 403/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6025 - accuracy: 0.6794 - val_loss: 0.5949 - val_accuracy: 0.7041\n",
      "Epoch 404/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6028 - accuracy: 0.6833 - val_loss: 0.5926 - val_accuracy: 0.7015\n",
      "Epoch 405/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6047 - accuracy: 0.6829 - val_loss: 0.6010 - val_accuracy: 0.7105\n",
      "Epoch 406/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6026 - accuracy: 0.6787 - val_loss: 0.5905 - val_accuracy: 0.7143\n",
      "Epoch 407/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6016 - accuracy: 0.6823 - val_loss: 0.5956 - val_accuracy: 0.7092\n",
      "Epoch 408/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6077 - accuracy: 0.6746 - val_loss: 0.6064 - val_accuracy: 0.6939\n",
      "Epoch 409/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6038 - accuracy: 0.6790 - val_loss: 0.6129 - val_accuracy: 0.6837\n",
      "Epoch 410/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6011 - accuracy: 0.6877 - val_loss: 0.5942 - val_accuracy: 0.7079\n",
      "Epoch 411/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6072 - accuracy: 0.6757 - val_loss: 0.5991 - val_accuracy: 0.7054\n",
      "Epoch 412/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6117 - accuracy: 0.6682 - val_loss: 0.5923 - val_accuracy: 0.7079\n",
      "Epoch 413/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6072 - accuracy: 0.6796 - val_loss: 0.5937 - val_accuracy: 0.7092\n",
      "Epoch 414/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6027 - accuracy: 0.6863 - val_loss: 0.5985 - val_accuracy: 0.7015\n",
      "Epoch 415/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6016 - accuracy: 0.6842 - val_loss: 0.5948 - val_accuracy: 0.7015\n",
      "Epoch 416/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6047 - accuracy: 0.6772 - val_loss: 0.5924 - val_accuracy: 0.7117\n",
      "Epoch 417/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6075 - accuracy: 0.6733 - val_loss: 0.5986 - val_accuracy: 0.7003\n",
      "Epoch 418/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6077 - accuracy: 0.6744 - val_loss: 0.6010 - val_accuracy: 0.7003\n",
      "Epoch 419/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.5967 - accuracy: 0.6849 - val_loss: 0.5930 - val_accuracy: 0.6964\n",
      "Epoch 420/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6025 - accuracy: 0.6826 - val_loss: 0.5991 - val_accuracy: 0.6913\n",
      "Epoch 421/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6048 - accuracy: 0.6773 - val_loss: 0.5965 - val_accuracy: 0.7041\n",
      "Epoch 422/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6059 - accuracy: 0.6732 - val_loss: 0.6042 - val_accuracy: 0.7003\n",
      "Epoch 423/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6029 - accuracy: 0.6824 - val_loss: 0.5944 - val_accuracy: 0.6990\n",
      "Epoch 424/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6000 - accuracy: 0.6877 - val_loss: 0.5907 - val_accuracy: 0.7143\n",
      "Epoch 425/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6038 - accuracy: 0.6828 - val_loss: 0.5947 - val_accuracy: 0.7054\n",
      "Epoch 426/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6024 - accuracy: 0.6790 - val_loss: 0.5938 - val_accuracy: 0.7054\n",
      "Epoch 427/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5981 - accuracy: 0.6849 - val_loss: 0.5878 - val_accuracy: 0.7117\n",
      "Epoch 428/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6051 - accuracy: 0.6758 - val_loss: 0.5918 - val_accuracy: 0.7079\n",
      "Epoch 429/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5986 - accuracy: 0.6867 - val_loss: 0.5955 - val_accuracy: 0.7028\n",
      "Epoch 430/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6042 - accuracy: 0.6838 - val_loss: 0.5921 - val_accuracy: 0.7092\n",
      "Epoch 431/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6013 - accuracy: 0.6881 - val_loss: 0.5898 - val_accuracy: 0.7079\n",
      "Epoch 432/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6030 - accuracy: 0.6783 - val_loss: 0.5921 - val_accuracy: 0.7028\n",
      "Epoch 433/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6044 - accuracy: 0.6794 - val_loss: 0.5912 - val_accuracy: 0.7003\n",
      "Epoch 434/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6029 - accuracy: 0.6849 - val_loss: 0.5995 - val_accuracy: 0.6939\n",
      "Epoch 435/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6019 - accuracy: 0.6861 - val_loss: 0.5913 - val_accuracy: 0.6977\n",
      "Epoch 436/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5996 - accuracy: 0.6852 - val_loss: 0.5968 - val_accuracy: 0.6977\n",
      "Epoch 437/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6011 - accuracy: 0.6804 - val_loss: 0.5913 - val_accuracy: 0.7105\n",
      "Epoch 438/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5950 - accuracy: 0.6923 - val_loss: 0.5929 - val_accuracy: 0.7079\n",
      "Epoch 439/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6008 - accuracy: 0.6778 - val_loss: 0.5895 - val_accuracy: 0.7130\n",
      "Epoch 440/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6028 - accuracy: 0.6797 - val_loss: 0.6021 - val_accuracy: 0.7105\n",
      "Epoch 441/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6018 - accuracy: 0.6845 - val_loss: 0.5906 - val_accuracy: 0.7066\n",
      "Epoch 442/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5959 - accuracy: 0.6843 - val_loss: 0.5895 - val_accuracy: 0.7066\n",
      "Epoch 443/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6008 - accuracy: 0.6816 - val_loss: 0.5855 - val_accuracy: 0.7117\n",
      "Epoch 444/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6011 - accuracy: 0.6840 - val_loss: 0.5915 - val_accuracy: 0.7054\n",
      "Epoch 445/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5961 - accuracy: 0.6899 - val_loss: 0.5900 - val_accuracy: 0.6990\n",
      "Epoch 446/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6002 - accuracy: 0.6833 - val_loss: 0.5903 - val_accuracy: 0.7079\n",
      "Epoch 447/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.5980 - accuracy: 0.6840 - val_loss: 0.5940 - val_accuracy: 0.7105\n",
      "Epoch 448/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5999 - accuracy: 0.6830 - val_loss: 0.5904 - val_accuracy: 0.7015\n",
      "Epoch 449/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6042 - accuracy: 0.6805 - val_loss: 0.5925 - val_accuracy: 0.7041\n",
      "Epoch 450/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5965 - accuracy: 0.6858 - val_loss: 0.5875 - val_accuracy: 0.7092\n",
      "Epoch 451/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6002 - accuracy: 0.6876 - val_loss: 0.5889 - val_accuracy: 0.7105\n",
      "Epoch 452/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5992 - accuracy: 0.6842 - val_loss: 0.5906 - val_accuracy: 0.7130\n",
      "Epoch 453/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5985 - accuracy: 0.6866 - val_loss: 0.5877 - val_accuracy: 0.7079\n",
      "Epoch 454/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5985 - accuracy: 0.6861 - val_loss: 0.5931 - val_accuracy: 0.7168\n",
      "Epoch 455/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5984 - accuracy: 0.6832 - val_loss: 0.5916 - val_accuracy: 0.7066\n",
      "Epoch 456/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5944 - accuracy: 0.6910 - val_loss: 0.5910 - val_accuracy: 0.7105\n",
      "Epoch 457/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5967 - accuracy: 0.6862 - val_loss: 0.5852 - val_accuracy: 0.7092\n",
      "Epoch 458/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5944 - accuracy: 0.6919 - val_loss: 0.5901 - val_accuracy: 0.7079\n",
      "Epoch 459/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5970 - accuracy: 0.6835 - val_loss: 0.5878 - val_accuracy: 0.7105\n",
      "Epoch 460/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5987 - accuracy: 0.6890 - val_loss: 0.5885 - val_accuracy: 0.7003\n",
      "Epoch 461/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5963 - accuracy: 0.6897 - val_loss: 0.5930 - val_accuracy: 0.7015\n",
      "Epoch 462/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5956 - accuracy: 0.6912 - val_loss: 0.5940 - val_accuracy: 0.7079\n",
      "Epoch 463/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.6007 - accuracy: 0.6764 - val_loss: 0.5894 - val_accuracy: 0.7015\n",
      "Epoch 464/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5932 - accuracy: 0.6924 - val_loss: 0.5923 - val_accuracy: 0.7028\n",
      "Epoch 465/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5971 - accuracy: 0.6825 - val_loss: 0.5955 - val_accuracy: 0.7028\n",
      "Epoch 466/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5941 - accuracy: 0.6902 - val_loss: 0.5942 - val_accuracy: 0.7015\n",
      "Epoch 467/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5972 - accuracy: 0.6899 - val_loss: 0.5895 - val_accuracy: 0.7105\n",
      "Epoch 468/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5906 - accuracy: 0.6906 - val_loss: 0.5871 - val_accuracy: 0.7092\n",
      "Epoch 469/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5927 - accuracy: 0.6849 - val_loss: 0.5887 - val_accuracy: 0.7041\n",
      "Epoch 470/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5971 - accuracy: 0.6807 - val_loss: 0.5988 - val_accuracy: 0.7054\n",
      "Epoch 471/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5987 - accuracy: 0.6915 - val_loss: 0.5925 - val_accuracy: 0.7041\n",
      "Epoch 472/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5970 - accuracy: 0.6897 - val_loss: 0.5914 - val_accuracy: 0.7015\n",
      "Epoch 473/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5946 - accuracy: 0.6891 - val_loss: 0.5918 - val_accuracy: 0.7028\n",
      "Epoch 474/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5975 - accuracy: 0.6901 - val_loss: 0.5890 - val_accuracy: 0.7130\n",
      "Epoch 475/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5954 - accuracy: 0.6849 - val_loss: 0.5898 - val_accuracy: 0.7092\n",
      "Epoch 476/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5915 - accuracy: 0.6887 - val_loss: 0.5932 - val_accuracy: 0.7117\n",
      "Epoch 477/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5917 - accuracy: 0.6892 - val_loss: 0.6160 - val_accuracy: 0.6735\n",
      "Epoch 478/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5933 - accuracy: 0.6924 - val_loss: 0.5914 - val_accuracy: 0.7156\n",
      "Epoch 479/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5912 - accuracy: 0.6977 - val_loss: 0.5904 - val_accuracy: 0.7130\n",
      "Epoch 480/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5975 - accuracy: 0.6875 - val_loss: 0.5914 - val_accuracy: 0.7117\n",
      "Epoch 481/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5919 - accuracy: 0.6939 - val_loss: 0.5874 - val_accuracy: 0.7143\n",
      "Epoch 482/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5913 - accuracy: 0.6839 - val_loss: 0.5867 - val_accuracy: 0.7181\n",
      "Epoch 483/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5925 - accuracy: 0.6909 - val_loss: 0.5847 - val_accuracy: 0.7117\n",
      "Epoch 484/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5929 - accuracy: 0.6864 - val_loss: 0.5841 - val_accuracy: 0.7168\n",
      "Epoch 485/500\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.5980 - accuracy: 0.6845 - val_loss: 0.5990 - val_accuracy: 0.7092\n",
      "Epoch 486/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5891 - accuracy: 0.6933 - val_loss: 0.5959 - val_accuracy: 0.7156\n",
      "Epoch 487/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5925 - accuracy: 0.6949 - val_loss: 0.5857 - val_accuracy: 0.7092\n",
      "Epoch 488/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5923 - accuracy: 0.6882 - val_loss: 0.5965 - val_accuracy: 0.7041\n",
      "Epoch 489/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5903 - accuracy: 0.6943 - val_loss: 0.5958 - val_accuracy: 0.7054\n",
      "Epoch 490/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5927 - accuracy: 0.6916 - val_loss: 0.5919 - val_accuracy: 0.7117\n",
      "Epoch 491/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5847 - accuracy: 0.6971 - val_loss: 0.5928 - val_accuracy: 0.7054\n",
      "Epoch 492/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5856 - accuracy: 0.6987 - val_loss: 0.5955 - val_accuracy: 0.7079\n",
      "Epoch 493/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5909 - accuracy: 0.6923 - val_loss: 0.5923 - val_accuracy: 0.7092\n",
      "Epoch 494/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5907 - accuracy: 0.6918 - val_loss: 0.5939 - val_accuracy: 0.7117\n",
      "Epoch 495/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5881 - accuracy: 0.6915 - val_loss: 0.5921 - val_accuracy: 0.7092\n",
      "Epoch 496/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5921 - accuracy: 0.6904 - val_loss: 0.5888 - val_accuracy: 0.7130\n",
      "Epoch 497/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5897 - accuracy: 0.6953 - val_loss: 0.5938 - val_accuracy: 0.7079\n",
      "Epoch 498/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5933 - accuracy: 0.6911 - val_loss: 0.5853 - val_accuracy: 0.7117\n",
      "Epoch 499/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5980 - accuracy: 0.6820 - val_loss: 0.5887 - val_accuracy: 0.7079\n",
      "Epoch 500/500\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.5918 - accuracy: 0.6923 - val_loss: 0.5900 - val_accuracy: 0.7117\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "\n",
    "model = init_keras_model(dense1=750, dense2=150, dropout=0.8, optimizer='sgd')\n",
    "\n",
    "history = model.fit(X_train_tensor,\n",
    "          y_train_tensor,\n",
    "          epochs=n_epochs,\n",
    "          validation_data=(X_val_tensor, y_val_tensor),\n",
    "          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(history):\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    ax1.plot(history.history['loss'], label='Training Loss', c='red')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy', c='blue')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax1.set_title('Training Loss/Accuracy')\n",
    "\n",
    "\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='lower right')\n",
    "\n",
    "\n",
    "\n",
    "    ax3 = plt.subplot(1, 2, 2)\n",
    "    ax3.plot(history.history['val_accuracy'], label='Validation Accuracy', c='purple')\n",
    "    ax3.set_ylabel('Accuracy')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "\n",
    "    ax3.plot(history.history['accuracy'], label='Training Accuracy', c='orange')\n",
    "    ax3.set_title('Training Accuracy/Validation Accuracy')\n",
    "    ax3.legend(loc='lower right')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6743295019157088 \n",
      "F2 score: 0.6743295019157088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "y_pred = model.predict(X_test_tensor)\n",
    "\n",
    "\n",
    "f1score = f1_score(y_test_tensor, y_pred.round(), average='micro')\n",
    "f2score = fbeta_score(y_test_tensor, y_pred.round(), beta=2, average='micro')\n",
    "\n",
    "\n",
    "print(f'F1 score: {f1score} \\nF2 score: {f2score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x212ec329860>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test_tensor, y_pred.round())\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x212ed1c0940>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score\n",
    "\n",
    "roc_score = roc_auc_score(y_test_tensor, y_pred)\n",
    "RocCurveDisplay.from_predictions(y_test_tensor, y_pred)\n",
    "plt.plot([0, 1], [0, 1], 'k--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x212ee208eb8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test_tensor, y_pred)\n",
    "\n",
    "threshold = 0.4\n",
    "\n",
    "idx = (thresholds >= threshold).argmax()\n",
    "\n",
    "plt.plot(thresholds, precision[:-1], label='precision')\n",
    "plt.plot(thresholds, recall[:-1], label='recall')\n",
    "plt.vlines(threshold, 0, 1.0, colors='r', linestyles='--', label='threshold')\n",
    "\n",
    "plt.plot(thresholds[idx], precision[idx], 'bo')\n",
    "plt.plot(thresholds[idx], recall[idx], 'o', c='orange')\n",
    "\n",
    "plt.axis([0.2, 0.9, 0, 1])\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 score: 0.573936529372046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x212ee456e10>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Since we want to minimise false negatives, we should favour recall over precision\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "\n",
    "\n",
    "threshold = 0.5\n",
    "\n",
    "y_pred = model.predict(X_test_tensor)\n",
    "\n",
    "y_pred = np.where(y_pred >= threshold, 1, 0)\n",
    "\n",
    "score = fbeta_score(y_test_tensor, y_pred, beta=2)\n",
    "print(f'F2 score: {score}')\n",
    "\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test_tensor, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.23 \n",
      "Best F2 score: 0.7509157509157509\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Determine the ideal threshold for f2\n",
    "\n",
    "threshold_dict = {}\n",
    "best_score = float('-inf')\n",
    "best_threshold = 0\n",
    "\n",
    "for i in range(0,100, 1):\n",
    "    \n",
    "    threshold = i/100\n",
    "\n",
    "    y_pred = model.predict(X_test_tensor)\n",
    "\n",
    "    y_pred = np.where(y_pred >= threshold, 1, 0)\n",
    "\n",
    "    score = fbeta_score(y_test_tensor, y_pred, beta=2)\n",
    "\n",
    "    threshold_dict[threshold] = score\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_threshold = threshold\n",
    "\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_dict.items(), columns=['threshold', 'f2score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: 0.23 \n",
      "Best F2 score: 0.7509157509157509\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(threshold_df['threshold'], threshold_df['f2score'], label='f2score')\n",
    "\n",
    "plt.vlines(best_threshold, 0, 0.8, colors='r', linestyles='--', label='threshold')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F2 score')\n",
    "plt.legend()\n",
    "print(f'Best threshold: {best_threshold} \\nBest F2 score: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.5578231292517006 \n",
      "F2 score: 0.7509157509157509 \n",
      " Accuracy: 0.41890166028097064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x212eccc6f60>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This looks like a really high f2 but I can already presume that the confusion matrix will look horrible (super high false positives)\n",
    "#Lets take a look\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = np.where(model.predict(X_test_tensor) >= best_threshold, 1, 0)\n",
    "\n",
    "f1 = f1_score(y_test_tensor, y_pred)\n",
    "f2 = fbeta_score(y_test_tensor, y_pred, beta=2)\n",
    "accuracy = accuracy_score(y_test_tensor, y_pred)\n",
    "\n",
    "print(f'F1 score: {f1} \\nF2 score: {f2} \\n Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test_tensor, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.545961</td>\n",
       "      <td>0.750383</td>\n",
       "      <td>0.375479</td>\n",
       "      <td>0.648172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.02</th>\n",
       "      <td>0.545961</td>\n",
       "      <td>0.750383</td>\n",
       "      <td>0.375479</td>\n",
       "      <td>0.648172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.03</th>\n",
       "      <td>0.545961</td>\n",
       "      <td>0.750383</td>\n",
       "      <td>0.375479</td>\n",
       "      <td>0.648172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.04</th>\n",
       "      <td>0.545961</td>\n",
       "      <td>0.750383</td>\n",
       "      <td>0.375479</td>\n",
       "      <td>0.648172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.05</th>\n",
       "      <td>0.545961</td>\n",
       "      <td>0.750383</td>\n",
       "      <td>0.375479</td>\n",
       "      <td>0.648172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.008489</td>\n",
       "      <td>0.627075</td>\n",
       "      <td>0.011001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.96</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624521</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.97</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624521</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.98</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624521</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624521</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1        f2  accuracy  combined\n",
       "0.01  0.545961  0.750383  0.375479  0.648172\n",
       "0.02  0.545961  0.750383  0.375479  0.648172\n",
       "0.03  0.545961  0.750383  0.375479  0.648172\n",
       "0.04  0.545961  0.750383  0.375479  0.648172\n",
       "0.05  0.545961  0.750383  0.375479  0.648172\n",
       "...        ...       ...       ...       ...\n",
       "0.95  0.013514  0.008489  0.627075  0.011001\n",
       "0.96  0.000000  0.000000  0.624521  0.000000\n",
       "0.97  0.000000  0.000000  0.624521  0.000000\n",
       "0.98  0.000000  0.000000  0.624521  0.000000\n",
       "0.99  0.000000  0.000000  0.624521  0.000000\n",
       "\n",
       "[99 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets try the same, taking into account both f1 and f2\n",
    "\n",
    "best_threshold = {}\n",
    "best_f1_score = 0\n",
    "best_f2_score = 0\n",
    "best_combined_score = 0\n",
    "best_accuracy_score = 0\n",
    "\n",
    "threshold_scores = {}\n",
    "\n",
    "\n",
    "for i in range(1,100, 1):\n",
    "    \n",
    "    threshold = i/100\n",
    "\n",
    "    y_pred = np.where(model.predict(X_test_tensor) >= threshold, 1, 0)\n",
    "\n",
    "    f1 = f1_score(y_test_tensor, y_pred)\n",
    "    f2 = fbeta_score(y_test_tensor, y_pred, beta=2)\n",
    "    accuracy = accuracy_score(y_test_tensor, y_pred)\n",
    "\n",
    "    combined_score = (f1 + f2) / 2\n",
    "\n",
    "    threshold_scores[threshold] = (f1, f2, accuracy, combined_score)\n",
    "\n",
    "    if combined_score > best_combined_score:\n",
    "        best_combined_score = combined_score\n",
    "        best_threshold['combined'] = threshold\n",
    "\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_threshold['f1'] = threshold\n",
    "\n",
    "    if f2 > best_f2_score:\n",
    "        best_f2_score = f2\n",
    "        best_threshold['f2'] = threshold\n",
    "\n",
    "    if accuracy > best_accuracy_score:\n",
    "        best_accuracy_score = accuracy\n",
    "        best_threshold['accuracy'] = threshold\n",
    "\n",
    "threshold_scores_df = pd.DataFrame.from_dict(threshold_scores, orient='index', columns=['f1', 'f2', 'accuracy', 'combined'])\n",
    "\n",
    "threshold_scores_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Score')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshold_scores_df.plot()\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 score: 0.6085753803596128 at threshold: 0.35\n",
      "      Best F2 score: 0.7509157509157509 at threshold: 0.23\n",
      "      Best Accuracy score: 0.685823754789272 at threshold: 0.6\n",
      "      Best Combined score: 0.6571120379692219 at threshold: 0.31\n"
     ]
    }
   ],
   "source": [
    "print(f'''Best F1 score: {best_f1_score} at threshold: {best_threshold[\"f1\"]}\n",
    "      Best F2 score: {best_f2_score} at threshold: {best_threshold[\"f2\"]}\n",
    "      Best Accuracy score: {best_accuracy_score} at threshold: {best_threshold[\"accuracy\"]}\n",
    "      Best Combined score: {best_combined_score} at threshold: {best_threshold[\"combined\"]}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31, 0.35, 0.23, 0.6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.37249999999999994"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold_list = list(best_threshold.values())\n",
    "print(threshold_list)\n",
    "\n",
    "combined_threshold = np.average(threshold_list)\n",
    "combined_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x212f0a540b8>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = np.where(model.predict(X_test_tensor) >= combined_threshold, 1, 0)\n",
    "\n",
    "cm = confusion_matrix(y_test_tensor, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_for_chemists_tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
